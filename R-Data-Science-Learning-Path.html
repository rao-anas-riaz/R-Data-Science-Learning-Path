<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=1200" />
    <title>Data Science Learning Path with R</title>

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"
      rel="stylesheet"
    />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/r.min.js"></script>

    <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <script>
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
        },
        options: {
          skipHtmlTags: [
            "script",
            "noscript",
            "style",
            "textarea",
            "pre",
            "code",
          ],
          ignoreHtmlClass: "tex2jax_ignore",
        },
      };
    </script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>

    <style>
      :root {
        --level-basic-color: #10b981;
        --level-intermediate-color: #f59e0b;
        --level-advanced-color: #ef4444;
        --primary: #3b82f6;
        --secondary: #8b5cf6;
        --accent: #f59e0b;
        --success: #10b981;
        --warning: #f59e0b;
        --error: #ef4444;
        --dark-base: #1f2937;
        --light-base: #f8fafc;
        --gray-base: #6b7280;
        --sidebar-width-expanded: 340px;
        --sidebar-width-collapsed: 0px;
        --header-height: 70px;
        --border-radius: 12px;
        --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        --shadow-lg: 0 20px 25px -5px rgba(0, 0, 0, 0.1),
          0 10px 10px -5px rgba(0, 0, 0, 0.04);
        --code-bg-dark: #282c34;
      }
      body:not(.dark-theme) {
        --bg-color: #f0f2f5; 
        --sidebar-bg: #fafafa; 
        --header-bg: linear-gradient(
          135deg,
          var(--primary) 0%,
          var(--secondary) 100%
        ); 
        --header-text: white;
        --card-bg: #ffffff; 
        --text-color: var(--dark-base); 
        --text-muted: #52525b; 
        --border-color: #e4e4e7; 
        --shadow-color: rgba(0, 0, 0, 0.08); 
        --category-bg: linear-gradient(135deg, #f3f4f6 0%, #e5e7eb 100%);
        --category-text: #1f2937;
        --category-border: #d1d5db;
        --nav-item-parent-bg: #ffffff; 
        --nav-item-parent-text: var(--gray-base); 
        --nav-item-parent-border: #e4e4e7; 
        --nav-item-parent-expanded-bg: #f4f4f5; 
        --nav-item-hover-bg: linear-gradient(
          135deg,
          #f0f4ff 0%,
          #e6efff 100%
        ); 
        --nav-item-active-bg: linear-gradient(
          135deg,
          var(--primary) 0%,
          var(--secondary) 100%
        ); 
        --nav-item-active-text: white; 
        --tab-bg: #fafafa;
        --subsection-bg: #f9fafb; 
        --plot-bg: #f9fafb; 
        --code-bg: var(--code-bg-dark); 
        --flow-bg: #ffffff; 
        --flow-border: #e4e4e7; 
        --decision-bg: #fefce8; 
        --decision-border: #eab308;
        --tag-bg: #dcfce7; 
        --tag-text: #166534; 
        --d3-link-stroke: #d1d5db;
        --d3-leaf-fill: #ffffff; 
        --d3-leaf-stroke: var(--primary); 
        --d3-leaf-text: var(--dark-base); 
        --d3-internal-fill: var(--primary); 
        --d3-internal-stroke: var(--secondary); 
        --d3-internal-text: white; 
        --d3-internal-open-fill: #ffffff; 
        --d3-internal-open-stroke: var(--primary); 
        --d3-internal-open-text: var(--dark-base); 
      }
      body.dark-theme {
        --bg-color: #1a1d21;
        --sidebar-bg: #23272c;
        --header-bg: #1f2227;
        --header-text: #e9ecef;
        --card-bg: #282c34;
        --text-color: #e9ecef;
        --text-muted: #adb5bd;
        --border-color: #373a40;
        --shadow-color: rgba(0, 0, 0, 0.2);
        --category-bg: #282c34;
        --category-text: #e9ecef;
        --category-border: #373a40;
        --nav-item-parent-bg: #282c34;
        --nav-item-parent-text: #adb5bd;
        --nav-item-parent-border: #373a40;
        --nav-item-parent-expanded-bg: #282c34;
        --nav-item-parent-expanded-text: #e9ecef;
        --nav-item-hover-bg: #1f2227;
        --nav-item-active-bg: linear-gradient(
          135deg,
          var(--primary) 0%,
          var(--secondary) 100%
        );
        --nav-item-active-text: white;
        --tab-bg: #1f2227;
        --subsection-bg: #1f2227;
        --plot-bg: #282c34;
        --plot-text: #e9ecef;
        --code-bg: var(--code-bg-dark);
        --flow-bg: #1f2937;
        --flow-border: #374151;
        --decision-bg: #4a401c;
        --decision-border: #facc15;
        --tag-bg: #3730a3;
        --tag-text: #e0e7ff;
        --d3-link-stroke: #373a40;
        --d3-leaf-fill: #282c34;
        --d3-leaf-stroke: #4dabf7;
        --d3-leaf-text: #e9ecef;
        --d3-internal-fill: #339af0;
        --d3-internal-stroke: #4dabf7;
        --d3-internal-text: white;
        --d3-internal-open-fill: #1f2227;
        --d3-internal-open-stroke: #339af0;
        --d3-internal-open-text: #e9ecef;
      }
      *,
      *::before,
      *::after {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }
      body {
        font-family: "Poppins", sans-serif;
        background: var(--bg-color);
        color: var(--text-color);
        display: flex;
      }
      ::-webkit-scrollbar {
        width: 8px;
      }
      ::-webkit-scrollbar-track {
        background: var(--sidebar-bg);
      }
      ::-webkit-scrollbar-thumb {
        background: linear-gradient(135deg, var(--primary), var(--secondary));
        border-radius: 4px;
      }
      .sidebar {
        width: var(--sidebar-width-expanded);
        height: 100vh;
        background: var(--sidebar-bg);
        border-right: 1px solid var(--border-color);
        display: flex;
        flex-direction: column;
        position: fixed;
        top: 0;
        left: 0;
        z-index: 1000;
        transition: transform 0.4s cubic-bezier(0.25, 0.8, 0.25, 1);
        transform: translateX(0);
        box-shadow: var(--shadow-lg);
      }
      .main-content {
        flex-grow: 1;
        height: 100vh;
        display: flex;
        flex-direction: column;
        margin-left: var(--sidebar-width-expanded);
        transition: margin-left 0.4s cubic-bezier(0.25, 0.8, 0.25, 1);
      }
      body.sidebar-collapsed .sidebar {
        transform: translateX(calc(-1 * var(--sidebar-width-expanded)));
      }
      body.sidebar-collapsed .main-content {
        margin-left: var(--sidebar-width-collapsed);
      }
      .main-header {
        height: var(--header-height);
        background: var(--header-bg);
        color: var(--header-text);
        border-bottom: 1px solid var(--border-color);
        display: flex;
        align-items: center;
        padding: 0 20px;
        flex-shrink: 0;
        position: sticky;
        top: 0;
        z-index: 999;
        box-shadow: var(--shadow-lg);
      }
      .sidebar-toggle {
        background: rgba(255, 255, 255, 0.1);
        border: none;
        color: var(--header-text);
        cursor: pointer;
        padding: 8px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.3s ease;
      }
      .sidebar-toggle:hover {
        background: rgba(255, 255, 255, 0.2);
        transform: scale(1.1) rotate(5deg);
      }
      .header-title {
        margin-left: 16px;
        font-size: 1.5rem;
        font-weight: 700;
        flex-grow: 1;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
      }
      .theme-toggle {
        margin-left: auto;
        background: rgba(255, 255, 255, 0.1);
        border: none;
        color: var(--header-text);
        cursor: pointer;
        padding: 8px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.3s ease;
      }
      .theme-toggle:hover {
        background: rgba(255, 255, 255, 0.2);
        transform: scale(1.1) rotate(-5deg);
      }
      .content-wrapper {
        flex-grow: 1;
        overflow-y: auto;
        padding: 2rem;
      }
      .content-section {
        display: none;
        margin-bottom: 2rem;
        border-radius: var(--border-radius);
        box-shadow: var(--shadow);
        overflow: hidden;
        opacity: 0;
        transform: translateY(20px);
        transition: all 0.6s cubic-bezier(0.4, 0, 0.2, 1);
        background: var(--card-bg);
      }
      .content-section.active {
        display: block;
        opacity: 1;
        transform: translateY(0);
        animation: fadeInUp 0.6s ease-out;
      }
      @keyframes fadeInUp {
        from {
          opacity: 0;
          transform: translateY(30px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }
      .card-header {
        background: var(--header-bg);
        color: var(--header-text);
        padding: 2rem;
      }
      .card-header h1 {
        font-size: 2rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        display: flex;
        align-items: center;
        gap: 1rem;
      }
      .card-tags {
        display: flex;
        flex-wrap: wrap;
        gap: 0.5rem;
        margin-top: 1rem;
      }
      .tag {
        background-color: var(--tag-bg);
        color: var(--tag-text);
        padding: 0.25rem 0.75rem;
        border-radius: 9999px;
        font-size: 0.8rem;
        font-weight: 600;
      }
      .sidebar-header {
        padding: 1.5rem;
        border-bottom: 1px solid var(--border-color);
        background: var(--bg-color);
      }
      .search-bar {
        width: 100%;
        padding: 0.75rem 1rem;
        border: 2px solid var(--border-color);
        border-radius: var(--border-radius);
        font-size: 0.9rem;
        transition: all 0.3s ease;
        background: var(--sidebar-bg);
        color: var(--text-color);
      }
      .search-bar:focus {
        outline: none;
        border-color: var(--primary);
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
      }
      .nav-tree-container {
        flex-grow: 1;
        overflow-y: auto;
        padding: 1rem 0;
      }
      .nav-category-title {
        padding: 0.8rem 1.2rem;
        font-weight: 700;
        color: var(--header-text);
        font-size: 0.9rem;
        text-transform: uppercase;
        letter-spacing: 1px;
        background: var(--header-bg);
        margin: 0 0.5rem 0.5rem 0.5rem;
        border-radius: var(--border-radius);
        display: flex;
        align-items: center;
        gap: 0.75rem;
        box-shadow: var(--shadow);
      }
      .nav-category {
        margin-bottom: 0.5rem;
      }
      .nav-items {
        list-style: none;
        padding: 0;
        margin: 0;
      }
      .nav-items li {
        margin: 0 0.5rem 0.25rem 0.5rem;
        border-left: 4px solid transparent;
        border-radius: var(--border-radius);
        transition: border-color 0.3s ease, background-color 0.3s ease;
      }
      .nav-items li.level-basic {
        border-left-color: var(--level-basic-color);
      }
      .nav-items li.level-intermediate {
        border-left-color: var(--level-intermediate-color);
      }
      .nav-items li.level-advanced {
        border-left-color: var(--level-advanced-color);
      }
      .nav-items li:hover:not(:has(> .nav-item.active)) {
        border-left-color: var(--secondary);
      }
      .nav-item {
        padding: 0.6rem 1.2rem;
        cursor: pointer;
        transition: all 0.3s ease;
        border-radius: var(--border-radius);
        font-weight: 500;
        display: flex;
        align-items: center;
        gap: 0.75rem;
        text-decoration: none;
        color: var(--text-muted);
        font-size: 0.9rem;
      }
      .nav-item.has-children:not(.expanded):not(.active) {
        background-color: var(--nav-item-parent-bg);
        color: var(--nav-item-parent-text);
        border: 1px solid var(--nav-item-parent-border);
      }
      .nav-items
        li.level-basic
        .nav-item.has-children:not(.expanded):not(.active),
      .nav-items
        li.level-intermediate
        .nav-item.has-children:not(.expanded):not(.active),
      .nav-items
        li.level-advanced
        .nav-item.has-children:not(.expanded):not(.active) {
        border: none;
      }
      .nav-item.has-children:not(.expanded):not(.active) .toggle-icon {
        stroke: var(--nav-item-parent-text);
      }
      .nav-item.has-children.expanded:not(.active) {
        background-color: var(--nav-item-parent-expanded-bg);
        color: var(--nav-item-parent-expanded-text);
      }
      .nav-item.has-children.expanded:not(.active) .toggle-icon {
        stroke: var(--nav-item-parent-expanded-text);
      }
      .nav-item:not(.active):hover {
        background: var(--nav-item-hover-bg);
        color: var(--text-color);
        transform: translateX(5px);
        box-shadow: var(--shadow);
      }
      .nav-item:not(.active):hover .toggle-icon {
        stroke: var(--text-color);
      }
      .nav-item.active {
        background: var(--nav-item-active-bg);
        color: var(--nav-item-active-text);
        box-shadow: var(--shadow);
      }
      .nav-item.active .toggle-icon {
        stroke: var(--nav-item-active-text);
      }
      .content-section.level-basic .card-header {
        border-top: 4px solid var(--level-basic-color);
      }
      .content-section.level-intermediate .card-header {
        border-top: 4px solid var(--level-intermediate-color);
      }
      .content-section.level-advanced .card-header {
        border-top: 4px solid var(--level-advanced-color);
      }
      .nav-item > span:first-of-type {
        flex-grow: 1;
      }
      .toggle-icon {
        transform: rotate(-90deg);
        transition: transform 0.3s ease;
      }
      .toggle-icon {
        margin-left: auto;
        stroke: var(--nav-item-parent-text);
        transition: transform 0.3s ease, stroke 0.3s ease;
      }
      .nav-item.has-children > .toggle-icon {
        margin-left: auto;
      }
      .nav-item.expanded > .toggle-icon {
        transform: rotate(0deg);
      }
      .nav-item.has-children.expanded > .toggle-icon {
        stroke: var(--nav-item-parent-expanded-text);
      }
      .nav-item.active > .toggle-icon {
        stroke: var(--nav-item-active-text);
      }
      .nav-item:not(.active):hover > .toggle-icon {
        stroke: var(--text-color);
      }
      .nav-tree ul.nested {
        list-style: none;
        padding-left: 1.2rem;
        margin: 0.25rem 0;
        max-height: 0;
        overflow: hidden;
        opacity: 0;
        pointer-events: none;
        transition: max-height 0.3s cubic-bezier(0.4, 0, 0.2, 1),
          opacity 0.2s cubic-bezier(0.4, 0, 0.2, 1);
      }

      .nav-tree ul.nested.expanded {
        opacity: 1;
        pointer-events: auto;
      }

      .tab-content {
        padding: 2rem;
      }
      .subsection {
        margin-bottom: 2rem;
        padding: 1.5rem;
        border-radius: var(--border-radius);
        background: var(--subsection-bg);
        border: 1px solid var(--border-color);
      }
      .subsection-title {
        font-size: 1.25rem;
        color: var(--text-color);
        margin-bottom: 1rem;
        padding-bottom: 0.5rem;
        border-bottom: 2px solid var(--primary);
        font-weight: 600;
      }
      .scenario-content {
        background: var(--sidebar-bg);
        padding: 1.5rem;
        border-radius: var(--border-radius);
        border-left: 4px solid var(--primary);
      }
      .scenario-content p {
        font-size: 1rem;
        line-height: 1.7;
        color: var(--text-muted);
      }
      .flow-diagram {
        display: flex;
        flex-direction: column;
        gap: 1rem;
        align-items: center;
      }
      .flow-node {
        background: var(--flow-bg);
        border: 1px solid var(--flow-border);
        border-radius: var(--border-radius);
        padding: 1.5rem;
        width: 100%;
        max-width: 900px;
        box-shadow: var(--shadow);
      }
      .flow-node-header {
        text-align: center;
        cursor: pointer;
      }
      .flow-node-header h3 {
        font-size: 1.5rem;
        font-weight: 700;
        color: var(--primary);
        margin-bottom: 0.75rem;
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 0.75rem;
      }
      .flow-node-header p {
        color: var(--text-muted);
        line-height: 1.6;
      }
      .flow-node-content {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.5s ease-in-out;
      }
      .phase-topics {
        list-style: none;
        padding: 0;
        margin-top: 1rem;
        text-align: left;
      }
      .phase-topics > li {
        margin-bottom: 1rem;
      }
      .topic-link {
        color: var(--text-color);
        font-weight: 600;
        text-decoration: none;
        display: block;
        padding: 0.75rem;
        border-radius: 8px;
        transition: background-color 0.2s ease;
      }
      .topic-link:hover {
        background-color: var(--subsection-bg);
      }
      .topic-link i {
        margin-right: 0.5rem;
        color: var(--secondary);
      }
      .topic-description {
        font-size: 0.9rem;
        color: var(--text-muted);
        padding-left: 2rem;
      }
      .flow-arrow {
        font-size: 2rem;
        color: var(--text-muted);
      }
      .overview-card {
        background: var(--sidebar-bg);
        border: 1px solid var(--border-color);
        border-radius: var(--border-radius);
        box-shadow: var(--shadow);
        overflow: hidden;
      }
      .code-container {
        position: relative;
        margin: 1rem 0;
      }
      .code-container .copy-btn {
        position: absolute;
        top: 10px;
        right: 10px;
        background: var(--flow-bg);
        border: 1px solid var(--border-color);
        color: var(--text-muted);
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        cursor: pointer;
        transition: all 0.2s ease;
        opacity: 0.6;
      }
      .code-container:hover .copy-btn {
        opacity: 1;
      }
      .code-container .copy-btn:hover {
        background: var(--primary);
        color: white;
      }
      .code-container pre code {
        border-radius: var(--border-radius);
        padding: 1.5rem;
        background: var(--code-bg);
      }
      .node {
        cursor: pointer;
        transition: transform 0.3s ease;
      }
      .node rect {
        stroke-width: 1.5px;
        rx: 8;
        transition: fill 0.3s ease, stroke 0.3s ease;
      }
      .node--leaf rect {
        fill: var(--d3-leaf-fill);
        stroke: var(--d3-leaf-stroke);
      }
      .node--internal rect {
        fill: var(--d3-internal-fill);
        stroke: var(--d3-internal-stroke);
      }
      .node--internal.node--open rect {
        fill: var(--d3-internal-open-fill);
        stroke: var(--d3-internal-open-stroke);
      }
      .link {
        fill: none;
        stroke: var(--d3-link-stroke);
        stroke-width: 1.5px;
        transition: d 0.3s ease;
      }
      .node foreignObject div {
        color: var(--d3-leaf-text);
      }
      .node--internal foreignObject div {
        color: var(--d3-internal-text);
      }
      .node--internal.node--open foreignObject div {
        color: var(--d3-internal-open-text);
      }
      .node g {
        cursor: pointer;
        transition: transform 0.3s ease;
      }
      .node g circle {
        transition: opacity 0.2s ease, fill 0.2s ease;
        fill: var(--primary);
        opacity: 0.8;
      }
      .node g:hover circle {
        opacity: 1;
        fill: var(--secondary);
      }
      .node g:hover circle {
        opacity: 1;
        fill: var(--secondary);
      }
      .tree-nav-dot {
        width: 12px;
        height: 12px;
        border-radius: 50%;
        background-color: var(
          --border-color
        ); 
        cursor: pointer;
        transition: background-color 0.3s ease, transform 0.3s ease;
      }
      .prose-list {
        list-style: none;
        padding-left: 0;
      }
      .prose-list li {
        padding: 0.5rem 0;
        padding-left: 1.5rem;
        position: relative;
        color: var(--text-color);
      }
      .prose-list li::before {
        content: "•";
        color: var(--primary);
        font-weight: bold;
        position: absolute;
        left: 0;
      }
      .decision-point {
        background: var(--decision-bg);
        border: 2px dashed var(--decision-border);
        padding: 1.5rem;
        border-radius: var(--border-radius);
        margin-top: 1rem;
      }
      .decision-point h4 {
        font-size: 1.2rem;
        font-weight: 700;
        color: var(--warning);
        margin-bottom: 1rem;
        text-align: center;
      }
      .decision-branches {
        display: flex;
        flex-wrap: wrap;
        gap: 1rem;
        justify-content: center;
      }
      .decision-branch {
        background: var(--flow-bg);
        border-radius: var(--border-radius);
        padding: 1rem;
        flex-grow: 1;
        border: 1px solid var(--flow-border);
        text-align: left;
      }
      .decision-branch strong {
        color: var(--text-color);
        display: block;
        margin-bottom: 0.5rem;
      }
      .tabs {
        display: flex;
        border-bottom: 2px solid var(--border-color);
        margin-bottom: 1.5rem;
      }
      .tab-button {
        padding: 1rem 1.5rem;
        cursor: pointer;
        border: none;
        background: none;
        font-size: 1rem;
        font-weight: 600;
        color: var(--text-muted);
        position: relative;
        transition: color 0.3s ease;
      }
      .tab-button::after {
        content: "";
        position: absolute;
        bottom: -2px;
        left: 0;
        width: 100%;
        height: 2px;
        background: var(--primary);
        transform: scaleX(0);
        transition: transform 0.3s ease;
      }
      .tab-button.active {
        color: var(--primary);
      }
      .tab-button.active::after {
        transform: scaleX(1);
      }
      .tab-pane {
        display: none;
        animation: fadeIn 0.5s ease-in-out;
      }
      .tab-pane.active {
        display: block;
      }
      @keyframes fadeIn {
        from {
          opacity: 0;
          transform: translateY(10px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }
      .mindmap-container {
        text-align: center;
        padding: 2rem 0;
      }
      .mindmap-root {
        display: inline-block;
        padding: 1rem 1.5rem;
        border-radius: var(--border-radius);
        background: var(--subsection-bg);
        border: 2px solid var(--primary);
        box-shadow: var(--shadow);
        max-width: 600px;
      }
      .mindmap-root .title {
        font-weight: 700;
        font-size: 1.2rem;
        color: var(--primary);
        margin-bottom: 0.5rem;
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
      }
      .mindmap-root .scenario {
        color: var(--text-muted);
      }
      .mindmap-branches-container {
        position: relative;
        margin-top: 2rem;
        padding-top: 2rem;
      }
      .mindmap-branches-container::before {
        content: "";
        position: absolute;
        top: 0;
        left: 50%;
        width: 2px;
        height: 2rem;
        background-color: var(--border-color);
        transform: translateX(-50%);
      }
      .mindmap-branches-container::after {
        content: "";
        position: absolute;
        top: 2rem;
        left: 10%;
        width: 80%;
        height: 2px;
        background-color: var(--border-color);
      }
      .mindmap-branches {
        display: flex;
        justify-content: center;
        flex-wrap: wrap;
        gap: 2rem;
        padding-top: 2rem;
      }
      .mindmap-branch {
        position: relative;
        flex: 1 1 250px;
        max-width: 300px;
      }
      .mindmap-branch::before {
        content: "";
        position: absolute;
        top: -2rem;
        left: 50%;
        width: 2px;
        height: 2rem;
        background-color: var(--border-color);
        transform: translateX(-50%);
      }
      .mindmap-branch .content {
        padding: 1rem;
        border-radius: var(--border-radius);
        background: var(--flow-bg);
        border: 1px solid var(--border-color);
        box-shadow: var(--shadow);
        height: 100%;
      }
      .mindmap-branch .title {
        font-weight: 600;
        color: var(--secondary);
        margin-bottom: 0.5rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }
      .mindmap-branch .scenario {
        font-size: 0.9rem;
        color: var(--text-muted);
        line-height: 1.6;
      }
      #overview.content-section {
        background: transparent;
        box-shadow: none;
        padding: 0;
      }

      #overview .tab-content {
        padding: 0;
      }
      .overview-card-header {
        border-bottom: 1px solid var(--border-color);
        padding: 1.5rem;
      }

      body:not(.dark-theme) .overview-card-header {
        background: var(
          --header-bg
        ); 
      }

      body.dark-theme .overview-card-header {
        background: #1a1d21; 
      }

      .overview-card-header h1 {
        font-size: 1.75rem;
        color: var(
          --text-color
        ); 
      }

      .overview-card-header p {
        margin-top: 0.5rem;
        color: var(
          --text-muted
        ); 
      }
      body:not(.dark-theme) .overview-card-header h1,
      body:not(.dark-theme) .overview-card-header p {
        color: var(--header-text); 
      }
      .site-logo {
        width: 40px;
        height: 40px;
        background: linear-gradient(
          135deg,
          rgba(255, 255, 255, 0.2),
          rgba(255, 255, 255, 0.1)
        );
        color: white;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        font-family: "JetBrains Mono", monospace;
        font-weight: 700;
        font-size: 1.5rem;
        flex-shrink: 0;
        margin-left: 10px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        border: 1px solid rgba(255, 255, 255, 0.2);
      }
      body.dark-theme .site-logo {
        background: linear-gradient(135deg, var(--primary), var(--secondary));
      }
      .intro-header .site-logo {
        width: 80px;
        height: 80px;
        font-size: 3rem;
        background: var(--header-bg);
        border: none;
      }
      body.dark-theme .intro-header .site-logo {
        background: linear-gradient(135deg, var(--primary), var(--secondary));
        color: var(--text-color);
      }

      
      #animated-bg {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100vh;
        z-index: -1;
        overflow: hidden;
        background: var(--bg-color);
      }

      
      #animated-bg::before {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        width: 200%;
        height: 200%;
        background-image: linear-gradient(
            var(--border-color) 1px,
            transparent 1px
          ),
          linear-gradient(to right, var(--border-color) 1px, transparent 1px);
        background-size: 50px 50px;
        opacity: 0.1;
        transform-origin: top left;
        transform: translateY(var(--scroll-y, 0));
      }

      
      @keyframes float {
        0% {
          transform: translateY(0) translateX(0);
          opacity: 0;
        }
        10% {
          opacity: 1;
        }
        90% {
          opacity: 1;
        }
        100% {
          transform: translateY(-100vh) translateX(var(--x-end));
          opacity: 0;
        }
      }

      .dot {
        position: absolute;
        bottom: -10px;
        border-radius: 50%;
        background-color: var(--primary);
        animation: float linear infinite;
      }

      
      .intro-header {
        display: flex;
        align-items: center;
        gap: 1.5rem;
        padding: 2rem;
        border-bottom: 1px solid var(--border-color);
      }
      .intro-header-text h1 {
        font-size: 1.75rem;
        font-weight: 700;
        color: var(--text-color);
        margin: 0;
        line-height: 1.2;
      }
      .intro-header-text .subtitle {
        font-size: 1.1rem;
        color: var(--text-muted);
        margin: 0.25rem 0 0.5rem 0;
      }
      .intro-header-text .author {
        font-size: 1rem;
        color: var(--text-muted);
        margin-top: 0.25rem;
      }

      .intro-content {
        padding: 2rem;
      }
      .intro-content p {
        font-size: 1rem;
        line-height: 1.8;
        color: var(--text-muted);
        margin-bottom: 1.5rem;
      }
      .intro-content p:last-of-type {
        margin-bottom: 0;
      }

      
      .cta-buttons {
        margin-top: 2rem;
        display: flex;
        flex-wrap: wrap;
        gap: 1rem;
      }
      .cta-btn {
        padding: 0.75rem 1.5rem;
        border-radius: 8px;
        text-decoration: none;
        font-weight: 600;
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        transition: all 0.3s ease;
        border: 2px solid transparent;
      }
      .cta-btn.primary {
        background: var(--primary);
        color: white;
        box-shadow: 0 4px 6px -1px rgba(59, 130, 246, 0.3);
      }
      .cta-btn.primary:hover {
        transform: translateY(-2px);
        box-shadow: 0 10px 15px -3px rgba(59, 130, 246, 0.3);
      }
      .cta-btn.secondary {
        background: transparent;
        color: var(--primary);
        border-color: var(--primary);
      }
      .cta-btn.secondary:hover {
        background: var(--primary);
        color: white;
        transform: translateY(-2px);
      }

      
      .objectives-content {
        padding: 2rem;
      }
      .objectives-list {
        list-style: none;
        padding: 0;
        margin: 0;
      }
      .objectives-list li {
        padding: 0.75rem 0;
        display: flex;
        align-items: flex-start;
        gap: 1rem;
        font-size: 1rem;
        line-height: 1.6;
        color: var(--text-muted);
      }
      .objectives-list li .fas {
        color: var(--success);
        font-size: 1.25rem;
        margin-top: 4px;
      }
      .objectives-list li strong {
        color: var(--text-color);
      }

      
      .animate-on-scroll {
        opacity: 0;
        transform: translateY(30px);
        transition: opacity 0.6s ease-out, transform 0.6s ease-out;
      }
      .animate-on-scroll.is-visible {
        opacity: 1;
        transform: translateY(0);
      }

      
      .main-footer-content {
        flex-shrink: 0;
        padding: 1rem 2rem; 
        background: var(--card-bg);
        border-top: 1px solid var(--border-color);
        color: var(--text-muted);
        display: flex;
        flex-wrap: wrap;
        justify-content: space-between;
        align-items: center;
        gap: 1rem;
      }
      .footer-links {
        display: flex;
        gap: 1.5rem;
        margin: 0;
        order: 2;
      }
      .footer-social {
        display: flex;
        gap: 1.5rem;
        margin: 0;
        order: 3;
      }
      .footer-copyright {
        order: 1;
        flex-grow: 1;
        text-align: left;
      }
      @media (max-width: 768px) {
        .main-footer-content {
          flex-direction: column;
          gap: 0.75rem;
        }
        .footer-copyright {
          order: 3;
          text-align: center;
        }
        .footer-links {
          order: 2;
        }
        .footer-social {
          order: 1;
        }
      }
      .footer-links a,
      .footer-link {
        color: var(--text-muted);
        text-decoration: none;
        transition: color 0.3s ease;
        font-size: 0.9rem;
        cursor: pointer;
      }
      .footer-links a:hover,
      .footer-link:hover {
        color: var(--primary);
      }
      .footer-social a {
        color: var(--text-muted);
        font-size: 1.25rem; 
        transition: transform 0.3s ease, color 0.3s ease;
      }
      .footer-social a:hover {
        color: var(--primary);
        transform: scale(1.2);
      }
      .footer-copyright p {
        margin: 0;
        font-size: 0.9rem;
      }

      #overview .overview-card.animate-on-scroll {
        margin-bottom: 2rem;
      }
      

      
      .sidebar-footer {
        flex-shrink: 0;
        padding: 1rem 0;
        margin: 0 0.5rem;
        border-top: 1px solid var(--border-color);
      }
      .sidebar-footer .nav-item {
        padding: 0.5rem 1.2rem;
      }
      .sidebar-footer .nav-item i {
        width: 1.25em; 
      }

      .legal-content {
        padding: 2rem;
      }
      .legal-content h2 {
        font-size: 1.5rem;
        font-weight: 700;
        margin-bottom: 1rem;
        padding-bottom: 0.5rem;
        border-bottom: 2px solid var(--primary);
        color: var(--text-color);
      }
      .legal-content h3 {
        font-size: 1.2rem;
        font-weight: 600;
        margin-top: 2rem;
        margin-bottom: 0.75rem;
        color: var(--text-color);
      }
      .legal-content p,
      .legal-content li {
        line-height: 1.7;
        color: var(--text-muted);
        font-size: 0.95rem;
      }
      .legal-content ul {
        list-style-position: outside;
        padding-left: 1.5rem;
        margin-bottom: 1rem;
      }
      .legal-content a {
        color: var(--primary);
        text-decoration: none;
        font-weight: 500;
      }
      .legal-content a:hover {
        text-decoration: underline;
      }
      
      .d3-controls button:hover {
        background-color: var(--nav-item-hover-bg);
        color: var(--text-color);
        transform: scale(1.05);
      }
      
      .flow-node {
        background: var(--flow-bg);
        border: 1px solid var(--flow-border);
        border-radius: var(--border-radius);
        padding: 1.5rem;
        width: 100%;
        max-width: 900px;
        box-shadow: var(--shadow);
        transition: transform 0.3s ease-out, box-shadow 0.3s ease-out; 
      }

      .code-container pre {
        white-space: pre-wrap; 
        word-break: break-word; 
        overflow-x: auto; 
      }

      #overlay {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background-color: rgba(0, 0, 0, 0.5);
        z-index: 998; 
        display: none;
        opacity: 0;
        transition: opacity 0.3s ease;
      }

      body.sidebar-expanded-mobile #overlay {
        display: block;
        opacity: 1;
      }

      
      .nav-item.expanded + ul.nested {
        opacity: 1;
        pointer-events: auto;
      }

      .flow-node:hover {
        transform: translateY(-6px) scale(1.01);
        box-shadow: var(--shadow-lg);
        cursor: pointer;
      }
      html {
        
        
        font-size: 80%;
      }
    </style>
  </head>
  <body>
    <div id="overlay" style="display: none"></div>
    <div id="animated-bg"></div>
    <nav class="sidebar">
      <div class="sidebar-header">
        <input
          type="search"
          class="search-bar"
          id="nav-search"
          placeholder="🔍 Search topics or tags..."
        />
      </div>
      <div class="nav-tree-container nav-tree" id="nav-tree-container"></div>
      <div class="sidebar-footer">
        <ul class="nav-items">
          <li>
            <a href="#" class="nav-item" data-id="terms-of-service">
              <i class="fas fa-file-contract fa-fw"></i>
              <span>Terms of Service</span>
            </a>
          </li>
          <li>
            <a href="#" class="nav-item" data-id="privacy-policy">
              <i class="fas fa-shield-alt fa-fw"></i>
              <span>Privacy Policy</span>
            </a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="main-content">
      <header class="main-header">
        <button
          class="sidebar-toggle"
          id="sidebar-toggle"
          aria-label="Toggle sidebar"
        >
          <svg
            class="open-icon"
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
            stroke-linecap="round"
            stroke-linejoin="round"
          >
            <line x1="19" y1="12" x2="5" y2="12"></line>
            <polyline points="12 19 5 12 12 5"></polyline>
          </svg>
          <svg
            class="close-icon"
            style="display: none"
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
            stroke-linecap="round"
            stroke-linejoin="round"
          >
            <line x1="5" y1="12" x2="19" y2="12"></line>
            <polyline points="12 5 19 12 12 19"></polyline>
          </svg>
        </button>
        <div class="site-logo">R</div>
        <h1 class="header-title" id="header-title">Learning Path Overview</h1>
        <button
          id="theme-toggle"
          class="theme-toggle"
          aria-label="Toggle theme"
        >
          <svg
            class="sun-icon"
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
          <svg
            class="moon-icon"
            style="display: none"
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
        </button>
      </header>

      <main class="content-wrapper" id="content-wrapper">
        <div class="content-section" id="terms-of-service">
          <div class="card-header">
            <h1><i class="fas fa-file-contract"></i>Terms of Service</h1>
          </div>
          <div class="legal-content">
            <h2>Terms of Service</h2>
            <p><strong>Last Updated: September 22, 2025</strong></p>
            <p>
              Welcome to the R Data Science Learning Path ("the Service"), a
              resource curated and provided by Anas Riaz. By accessing or using
              our Service, you agree to be bound by these Terms of Service
              ("Terms"). If you disagree with any part of the terms, then you
              may not access the Service.
            </p>

            <h3>1. Use of Content</h3>
            <p>
              All content provided on this Service is for informational and
              educational purposes only. The owner of this Service makes no
              representations as to the accuracy or completeness of any
              information on this site or found by following any link on this
              site.
            </p>
            <ul>
              <li>
                The content is provided free of charge and is intended for
                personal, non-commercial use.
              </li>
              <li>
                You may use, copy, and distribute the code snippets and examples
                for your own educational purposes. However, you may not
                republish the content wholesale without permission.
              </li>
            </ul>

            <h3>2. Disclaimer of Warranties</h3>
            <p>
              The Service is provided on an "AS IS" and "AS AVAILABLE" basis.
              The information and materials on this website are compiled by a
              single individual using a personal computer. Much of the
              information is sourced from generative AI tools (such as Google's
              Gemini) and personal knowledge.
            </p>
            <ul>
              <li>
                No guarantee is made regarding the accuracy, timeliness, or
                completeness of the content. There may be errors, omissions, or
                outdated information.
              </li>
              <li>
                The owner will not be liable for any errors or omissions in this
                information nor for the availability of this information. The
                owner will not be liable for any losses, injuries, or damages
                from the display or use of this information.
              </li>
            </ul>
            <h3>3. User Responsibilities</h3>
            <p>
              You are responsible for verifying the accuracy of the information
              before relying on it for any professional, academic, or commercial
              projects. The content should be treated as a supplementary guide,
              not as an authoritative source of truth.
            </p>

            <h3>4. Intellectual Property</h3>
            <p>
              The Service and its original content (excluding user-provided
              content), features, and functionality are and will remain the
              exclusive property of Anas Riaz.
            </p>

            <h3>5. Links To Other Web Sites</h3>
            <p>
              Our Service may contain links to third-party web sites or services
              that are not owned or controlled by Anas Riaz. We have no control
              over, and assume no responsibility for, the content, privacy
              policies, or practices of any third-party web sites or services.
            </p>

            <h3>6. Changes to Terms</h3>
            <p>
              We reserve the right, at our sole discretion, to modify or replace
              these Terms at any time. We will provide notice of any changes by
              posting the new Terms on this page.
            </p>

            <h3>7. Contact Us</h3>
            <p>
              If you have any questions about these Terms, please contact us at
              <a href="mailto:raoanasriaz@gmail.com">raoanasriaz@gmail.com</a>.
            </p>
          </div>
        </div>

        <div class="content-section" id="privacy-policy">
          <div class="card-header">
            <h1><i class="fas fa-shield-alt"></i>Privacy Policy</h1>
          </div>
          <div class="legal-content">
            <h2>Privacy Policy</h2>
            <p><strong>Last Updated: September 22, 2025</strong></p>
            <p>
              This Privacy Policy describes how the R Data Science Learning Path
              ("the Service," "we," "us," or "our") handles your information.
              This Service is a static educational resource and is designed to
              respect your privacy.
            </p>

            <h3>1. Information Collection and Use</h3>
            <p>
              We do not collect any personally identifiable information (PII)
              from our users.
            </p>
            <ul>
              <li>
                <strong>No User Accounts:</strong> You can access the Service
                without creating an account or logging in.
              </li>
              <li>
                <strong>No Cookies:</strong> We do not use tracking cookies to
                monitor your activity across websites. The only cookie used is
                for remembering your theme preference (light/dark mode), which
                is stored locally in your browser and is not transmitted to us.
              </li>
              <li>
                <strong>No Server-Side Analytics:</strong> We do not run any
                server-side analytics that track your IP address or other
                personal data. The site is a static collection of files.
              </li>
            </ul>

            <h3>2. Information You Provide</h3>
            <p>
              The only way we might receive personal information is if you
              voluntarily contact us via email. If you contact us, we will only
              use your email address and any information you provide to respond
              to your inquiry. We will not sell, rent, or share this information
              with third parties.
            </p>

            <h3>3. Third-Party Services</h3>
            <p>
              This website uses several third-party services to function. These
              services have their own privacy policies, and we encourage you to
              review them:
            </p>
            <ul>
              <li>
                <strong>Google Fonts:</strong> To display the fonts used on this
                site.
              </li>
              <li><strong>Font Awesome:</strong> For icons.</li>
              <li>
                <strong>Highlight.js, Plotly, MathJax, D3.js:</strong> For code
                highlighting, plotting, and visualizations.
              </li>
            </ul>
            <p>
              These services may collect data as described in their respective
              privacy policies.
            </p>

            <h3>4. Security</h3>
            <p>
              We take reasonable precautions to protect your information. Since
              we do not collect sensitive personal data, the security risk is
              minimal.
            </p>

            <h3>5. Changes to This Privacy Policy</h3>
            <p>
              We may update our Privacy Policy from time to time. We will notify
              you of any changes by posting the new Privacy Policy on this page.
              You are advised to review this Privacy Policy periodically for any
              changes.
            </p>

            <h3>6. Contact Us</h3>
            <p>
              If you have any questions about this Privacy Policy, please
              contact us at
              <a href="mailto:raoanasriaz@gmail.com">raoanasriaz@gmail.com</a>.
            </p>
          </div>
        </div>
      </main>

      <footer class="main-footer-content">
        <div class="footer-copyright">
          <p>&copy; 2025 Anas Riaz. All Rights Reserved.</p>
        </div>
        <div class="footer-links">
          <a href="#" class="footer-link" data-id="privacy-policy"
            >Privacy Policy</a
          >
          <a href="#" class="footer-link" data-id="terms-of-service"
            >Terms of Service</a
          >
          <a href="mailto:raoanasriaz@gmail.com">Contact</a>
        </div>
        <div class="footer-social">
          <a
            href="https://github.com/rao-anas-riaz"
            target="_blank"
            aria-label="GitHub"
            ><i class="fab fa-github"></i
          ></a>
          <a
            href="https://linkedin.com/in/raoanasriaz"
            target="_blank"
            aria-label="LinkedIn"
            ><i class="fab fa-linkedin"></i
          ></a>
        </div>
      </footer>
    </div>

    <script>
      document.addEventListener("DOMContentLoaded", function () {
        const contentWrapper = document.getElementById("content-wrapper");
        const navContainer = document.getElementById("nav-tree-container");

        const learningPathData = {
          phases: [
            {
              id: "phase1",
              title: "Phase 1: Foundations",
              icon: "fa-drafting-compass",
              description:
                "Before you can analyze any data, you need to set up your tools, learn the language's grammar, and understand the fundamental mathematical principles that power data science. This phase builds your essential toolkit from the ground up.",
              scenario:
                "You are an apprentice builder. Before you can construct a house, you must first gather your tools, learn how to use them, and understand the blueprints. This phase is about building your foundational toolkit.",
              sections: [
                {
                  id: "p1s1",
                  title: "Introduction to R & Workflow",
                  icon: "fa-play-circle",
                  description:
                    "Getting started with R, RStudio, and establishing good project workflows.",
                  why: "To begin your data science journey, you first need to install the necessary software, understand its interface, and learn the best practices for organizing your work to ensure it's reliable and understandable by others.",
                  tags: ["R", "RStudio", "Workflow", "Setup"],
                  difficulty: "basic",
                  scenario:
                    "Your goal is to start a data science project. The first step is to set up a robust and reproducible environment.",
                  subsections: [
                    {
                      id: "p1s1ss1",
                      title: "The R Ecosystem",
                      description:
                        "History, use cases, and community (CRAN, Bioconductor).",
                      why: "To understand why R is a powerful choice for data science, you need to know its history, where it excels, and the vast community resources (like CRAN) available to help you.",
                      tags: ["CRAN", "Bioconductor", "Community"],
                      scenario:
                        "Decision: First, you must understand why R is a good choice. You need to know its history, strengths, and the community resources available.",
                    },
                    {
                      id: "p1s1ss2",
                      title: "R & RStudio Installation",
                      description: "Setup and configuration.",
                      why: "To start writing code, you must first set up and configure the R language and the RStudio Integrated Development Environment (IDE) on your computer.",
                      tags: ["Installation", "Configuration"],
                      scenario:
                        "Task: To start coding, you need to install the core software: the R language (the engine) and RStudio (the interface).",
                    },
                    {
                      id: "p1s1ss3",
                      title: "The RStudio IDE",
                      description: "Panes, projects, and settings.",
                      why: "To work efficiently, you need to learn how to navigate the different panes of the RStudio interface and use projects to keep your analyses organized.",
                      tags: ["IDE", "Interface", "Projects"],
                      scenario:
                        "Goal: To work efficiently, you must become familiar with your primary tool. This means learning the layout and features of the RStudio IDE.",
                    },
                    {
                      id: "p1s1ss4",
                      title: "R Packages",
                      description: "The role and management of packages.",
                      why: "When R's built-in functions aren't enough, you need to understand how to find, install, and use external packages that provide specialized functionality.",
                      tags: ["Packages", "Management"],
                      scenario:
                        "Condition: When base R doesn't have the specific function you need (e.g., to read an Excel file), you must extend its capabilities by using external packages.",
                    },
                    {
                      id: "p1s1ss5",
                      title: "Project Organization",
                      description:
                        "Best practices for project structure and file management.",
                      why: "To avoid chaos in your projects, you need to learn best practices for structuring your files and folders so your work is easy to find and understand later.",
                      tags: ["Organization", "Structure", "Best Practices"],
                      scenario:
                        "Task: To prevent future chaos, you must establish a clean and logical folder structure for your project's files (data, scripts, reports).",
                    },
                    {
                      id: "p1s1ss6",
                      title: "Reproducibility",
                      description:
                        "Core principles and best practices for beginners.",
                      why: "To ensure you or others can get the same results from your code in the future, you must learn the core principles of writing reproducible analyses.",
                      tags: ["Reproducibility", "Principles"],
                      scenario:
                        "Goal: To ensure your work is credible and can be verified by others (and your future self), you must adopt core principles for making your analysis reproducible.",
                    },
                  ],
                },
                {
                  id: "p1s2",
                  title: "Core R Programming",
                  icon: "fa-code",
                  description:
                    "Master the fundamental programming concepts in R, from basic syntax to advanced paradigms.",
                  why: "To instruct the computer to perform data analysis, you must learn the fundamental syntax, data structures, and control mechanisms of the R programming language.",
                  tags: ["Programming", "Syntax", "Functions", "Control Flow"],
                  difficulty: "basic",
                  scenario:
                    "Your environment is set up. Now you must learn the actual R language to instruct the computer.",
                  subsections: [
                    {
                      id: "p1s2ss1",
                      title: "Syntax & Data Types",
                      description:
                        "Fundamentals of R syntax, variables, and data structures.",
                      why: "To store and manipulate information, you need to understand R's basic grammar and the different types of data structures it uses.",
                      tags: ["Syntax", "Data Types", "Vectors", "Data Frames"],
                      scenario:
                        "Your goal is to store information in R. You need to choose the correct structure for the job.",
                      topics: [
                        {
                          id: "p1s2ss1t1",
                          title: "Fundamentals",
                          description: "Variables, assignment, and comments.",
                          why: "To perform the most basic tasks, you need to know how to create variables, assign values to them, and leave comments for yourself and others.",
                          tags: ["Variables", "Assignment", "Comments"],
                          scenario:
                            "Task: To store a single piece of information, you use a variable.",
                        },
                        {
                          id: "p1s2ss1t2",
                          title: "Atomic Data Types",
                          description: "Vectors, lists, matrices, and arrays.",
                          why: "To work with collections of the same data type, you need to understand vectors (a sequence), matrices (a 2D grid), and arrays (multidimensional grids).",
                          tags: ["Vector", "List", "Matrix", "Array"],
                          scenario:
                            "Condition: If you need to store items that are all the same type, you use an atomic vector.",
                        },
                        {
                          id: "p1s2ss1t3",
                          title: "Composite Data Types",
                          description: "Data frames, tibbles, and factors.",
                          why: "To work with tabular data that resembles a spreadsheet, you need to understand data frames and tibbles, and how to represent categorical information using factors.",
                          tags: ["Data Frame", "Tibble", "Factor"],
                          scenario:
                            "Condition: If you need a table where columns can be different types, you use a data frame.",
                        },
                        {
                          id: "p1s2ss1t4",
                          title: "Type Handling",
                          description: "Coercion and conversion functions.",
                          why: "When R automatically changes a data type unexpectedly (coercion), you need to know why it happens and how to manually convert between types.",
                          tags: ["Coercion", "Conversion", "as.*"],
                          scenario:
                            "Problem: R misinterpreted a data type on import. You must learn how to fix it.",
                        },
                        {
                          id: "p1s2ss1t5",
                          title: "Subsetting & Indexing",
                          description: "Accessing elements of data structures.",
                          why: "To access specific pieces of your data, you need to learn how to select elements, rows, or columns from your data structures.",
                          tags: ["Subsetting", "Indexing", "[]", "$"],
                          scenario:
                            "Goal: You only need a specific piece of your data. You must learn how to access it.",
                        },
                        {
                          id: "p1s2ss1t6",
                          title: "Special Values",
                          description: "Handling NA, NaN, and Inf.",
                          why: "When your data is incomplete or contains mathematical errors, you need to know how to handle special values like NA (Not Available), NaN (Not a Number), and Inf (Infinity).",
                          tags: ["NA", "NaN", "Inf", "Missing Data"],
                          scenario:
                            "Problem: Your data has missing values (NA) and calculations are failing. You need to learn how to handle them.",
                        },
                      ],
                    },
                    {
                      id: "p1s2ss2",
                      title: "Control Flow & Functions",
                      description:
                        "Conditional logic, loops, and function creation.",
                      why: "To write efficient and reusable code, you need to control the flow of your program's execution and package repetitive tasks into functions.",
                      tags: [
                        "Control Flow",
                        "Functions",
                        "Loops",
                        "Conditionals",
                      ],
                      scenario:
                        "You know R's syntax. Now you want to write smarter, more efficient, and reusable code.",
                      topics: [
                        {
                          id: "p1s2ss2t1",
                          title: "Conditional Logic & Loops",
                          description: "if/else, for/while/repeat, break/next.",
                          why: "When your code needs to make a decision or repeat an action, you'll use if/else for conditions and for/while for loops.",
                          tags: ["if/else", "for", "while", "Loops"],
                          scenario:
                            "Condition: Your code needs to perform an action only if a condition is met, or it needs to repeat an action.",
                        },
                        {
                          id: "p1s2ss2t2",
                          title: "Vectorized Operations",
                          description:
                            "Applying operations to entire vectors at once.",
                          why: "To perform operations on many data points at once efficiently, you'll leverage R's vectorized nature instead of slow loops.",
                          tags: ["Vectorization", "Performance", "ifelse"],
                          scenario:
                            'Goal: Your loop is too slow. You need a faster, "R-like" way to perform the operation on a whole vector at once.',
                        },
                        {
                          id: "p1s2ss2t3",
                          title: "Function Definition",
                          description:
                            "Arguments, return values, and scoping rules.",
                          why: "When you find yourself writing the same block of code repeatedly, you should package it into a reusable function.",
                          tags: ["Functions", "Arguments", "Return", "Scope"],
                          scenario:
                            "Problem: You are writing the same code repeatedly. You need to package it into a reusable function.",
                        },
                        {
                          id: "p1s2ss2t4",
                          title: "Advanced Functions",
                          description: "Anonymous functions and closures.",
                          why: "To create more flexible and powerful functions, you can use anonymous (unnamed) functions for quick, one-off tasks.",
                          tags: [
                            "Anonymous Functions",
                            "Closures",
                            "Factories",
                          ],
                          scenario:
                            'Goal: You need to create a one-off function to pass to another function, or build a "function factory".',
                        },
                        {
                          id: "p1s2ss2t5",
                          title: "Functional Programming",
                          description: "Using map, reduce, filter, and walk.",
                          why: "To apply a function to each element of a list or vector without a loop, you use tools from functional programming like map().",
                          tags: ["purrr", "map", "reduce", "filter"],
                          scenario:
                            "Goal: You want to replace a complex loop with a clean, readable pipeline of operations.",
                        },
                      ],
                    },
                    {
                      id: "p1s2ss3",
                      title: "Packages & Environments",
                      description:
                        "Package management and understanding R environments.",
                      why: "To extend R's capabilities and manage complexity, you need to understand how to use external packages and how R organizes its variables and functions in different environments.",
                      tags: ["Packages", "Tidyverse", "Environments"],
                      scenario:
                        "Your goal is to manage the complexity of your R projects and extend R's built-in capabilities.",
                      topics: [
                        {
                          id: "p1s2ss3t1",
                          title: "Package Management",
                          description:
                            "Installing, loading, updating, and removing packages.",
                          why: "To use code written by others, you must know how to install, load, update, and remove packages.",
                          tags: ["install.packages", "library", "CRAN"],
                          scenario:
                            "Task: You need a tool not built into R. You must learn how to find, install, and manage this external code.",
                        },
                        {
                          id: "p1s2ss3t2",
                          title: "The Tidyverse Ecosystem",
                          description:
                            "Overview of dplyr, ggplot2, tidyr, readr, purrr, tibble, stringr, and forcats.",
                          why: "To adopt a modern, intuitive, and powerful workflow for data manipulation and visualization, you'll learn the core packages of the Tidyverse.",
                          tags: ["Tidyverse", "dplyr", "ggplot2"],
                          scenario:
                            "Goal: You want to adopt a modern, intuitive, and consistent workflow for data science.",
                        },
                        {
                          id: "p1s2ss3t3",
                          title: "Programming Paradigms",
                          description:
                            "Comparing Base R vs. Tidyverse approaches.",
                          why: "To make an informed choice about your coding style, you should understand the differences and trade-offs between Base R and the Tidyverse.",
                          tags: ["Base R", "Tidyverse", "Pipe"],
                          scenario:
                            "Decision: You need to understand the different styles of R code (Base R vs. Tidyverse) to choose the right approach.",
                        },
                        {
                          id: "p1s2ss3t4",
                          title: "Environments & Scoping",
                          description: "Understanding R's environment rules.",
                          why: "To understand where R looks for a variable or function and avoid naming conflicts, you need to learn about R's environment rules.",
                          tags: ["Environment", "Scope", "Functions"],
                          scenario:
                            "Problem: Your variables seem to have unexpected values. You must understand how R finds them.",
                        },
                        {
                          id: "p1s2ss3t5",
                          title: "Metaprogramming",
                          description:
                            "Introduction to Non-standard evaluation (NSE).",
                          why: "To write functions that can manipulate code itself (common in the Tidyverse), you need an introduction to non-standard evaluation.",
                          tags: ["NSE", "Metaprogramming", "{{}}", "rlang"],
                          scenario:
                            "Goal: You want to build your own functions that work with the same clean syntax as Tidyverse functions.",
                        },
                      ],
                    },
                  ],
                },
                {
                  id: "p1s3",
                  title: "Data Structures & Manipulation",
                  icon: "fa-table",
                  description:
                    "Learn to work with R's data structures and perform essential data manipulation tasks.",
                  why: "To prepare your data for analysis, you need to master the tools for importing, cleaning, reshaping, and manipulating it into the desired format.",
                  tags: [
                    "Data Frames",
                    "dplyr",
                    "Data Wrangling",
                    "Pipe Operator",
                  ],
                  difficulty: "basic",
                  scenario:
                    "You have raw data. Now you need to get it into the right shape and format for analysis.",
                  subsections: [
                    {
                      id: "p1s3ss1",
                      title: "Data Frames & Tibbles",
                      description: "Creation, subsetting, and manipulation.",
                      why: "To work with your primary dataset, you need to be proficient in creating, subsetting, and changing data frames and tibbles.",
                      tags: ["Data Frames", "Tibbles"],
                      scenario:
                        "Task: Your primary goal is to work with tabular (spreadsheet-like) data.",
                    },
                    {
                      id: "p1s3ss2",
                      title: "Lists & Nested Data",
                      description: "Working with complex, hierarchical data.",
                      why: "When your data is not flat and contains complex, hierarchical structures (like JSON from an API), you need to work with lists.",
                      tags: ["Lists", "Nested Data"],
                      scenario:
                        "Condition: Your data isn't flat, it's hierarchical (like JSON). You must use lists.",
                    },
                    {
                      id: "p1s3ss3",
                      title: "Factors & Categorical Data",
                      description:
                        "Managing categorical variables effectively.",
                      why: "To correctly handle variables with a fixed set of categories (e.g., 'Low,' 'Medium,' 'High'), you must manage them as factors.",
                      tags: ["Factors", "Categorical"],
                      scenario:
                        "Problem: Your plot's axes are in the wrong (alphabetical) order. You must manage factor levels.",
                    },
                    {
                      id: "p1s3ss4",
                      title: "Data Import & Export",
                      description:
                        "Handling formats like CSV, Excel, JSON, RDS, and feather.",
                      why: "To get data into and out of R, you need to know how to read and write common file formats like CSV, Excel, and JSON.",
                      tags: ["Import", "Export", "CSV", "Excel"],
                      scenario:
                        "Task: Your data is outside R (e.g., in a CSV). You must learn how to import it.",
                    },
                    {
                      id: "p1s3ss5",
                      title: "Data Wrangling Verbs",
                      description:
                        "Using select, filter, mutate, arrange, summarize, group_by, and joins.",
                      why: "To perform the most common data manipulation tasks, you will use the core dplyr verbs like select, filter, and mutate.",
                      tags: ["dplyr", "Wrangling", "Joins"],
                      scenario:
                        "Goal: You need to perform common manipulations: subsetting, creating new columns, and summarizing.",
                    },
                    {
                      id: "p1s3ss6",
                      title: "The Pipe Operator",
                      description: "Chaining commands with %>% and |>.",
                      why: "To write clean, readable sequences of data manipulation steps, you will chain commands together using the pipe (%>% or |>).",
                      tags: ["Pipe", "Chaining"],
                      scenario:
                        "Problem: Your multi-step code is hard to read. You need to chain commands into a clean workflow.",
                    },
                    {
                      id: "p1s3ss7",
                      title: "Data Cleaning",
                      description:
                        "Techniques for handling missing data and outliers.",
                      why: "When your dataset contains errors or missing values, you need techniques to identify and correct these issues.",
                      tags: ["Cleaning", "Missing Data"],
                      scenario:
                        "Task: Your raw dataset is messy. You must apply your wrangling skills to clean it.",
                    },
                  ],
                },
                {
                  id: "p1s4",
                  title: "Object-Oriented Programming in R",
                  icon: "fa-cubes",
                  description:
                    "Understanding R's object-oriented programming systems and when to use them.",
                  why: "To build complex, scalable, and maintainable analytical systems, you need to learn how to structure your code around objects that combine data and functions.",
                  tags: ["OOP", "S3", "S4", "R6", "Classes"],
                  difficulty: "intermediate",
                  scenario:
                    "Your projects are becoming larger and more complex. You need to move beyond simple scripts and learn how to build maintainable, scalable systems.",
                  subsections: [
                    {
                      id: "p1s4ss1",
                      title: "R's OOP Systems",
                      description:
                        "Understanding S3, S4, R6, and Reference Classes.",
                      why: "To choose the right tool for the job, you need to understand the differences between R's various OOP systems (S3, S4, R6).",
                      tags: ["S3", "S4", "R6"],
                      scenario:
                        "Decision: You see that R has multiple OOP systems. You must understand their differences to choose the right one.",
                    },
                    {
                      id: "p1s4ss2",
                      title: "Core OOP Concepts",
                      description:
                        "Classes, methods, inheritance, encapsulation, polymorphism, abstraction, and composition.",
                      why: "To write effective object-oriented code, you must understand core concepts like classes, methods, and inheritance.",
                      tags: ["Classes", "Methods", "Inheritance"],
                      scenario:
                        "Goal: To write effective OOP code, you must understand the fundamental vocabulary (classes, methods, inheritance).",
                    },
                    {
                      id: "p1s4ss3",
                      title: "Design Patterns",
                      description:
                        "Common software design patterns applied in R.",
                      why: "To solve common software design problems in a reusable way, you can apply established design patterns in your R code.",
                      tags: ["Design Patterns"],
                      scenario:
                        "Problem: You face a common design challenge. You can solve it cleanly by applying a proven design pattern.",
                    },
                    {
                      id: "p1s4ss4",
                      title: "Use Cases for OOP",
                      description:
                        "When and why to use object-oriented programming in R.",
                      why: "To decide when to use OOP, you should know the types of problems it's best suited for, such as building a simulator or a complex modeling package.",
                      tags: ["Use Cases", "Best Practices"],
                      scenario:
                        "Decision: Is your project a one-off script or a reusable system? This will help you decide if you need OOP.",
                    },
                  ],
                },
                {
                  id: "p1s5",
                  title: "Mathematical Foundations",
                  icon: "fa-calculator",
                  description:
                    "Essential mathematical concepts that underpin data science and machine learning.",
                  why: "To truly understand how and why data models work, you need to grasp the underlying principles of statistics, linear algebra, and calculus that form the bedrock of machine learning.",
                  tags: [
                    "Statistics",
                    "Linear Algebra",
                    "Calculus",
                    "Probability",
                  ],
                  difficulty: "intermediate",
                  scenario:
                    "You know how to program, but to understand why models work, you need to learn the underlying mathematical principles.",
                  subsections: [
                    {
                      id: "p1s5ss1",
                      title: "Core Statistics",
                      description:
                        "Fundamental statistical concepts for data analysis.",
                      why: "To describe your data, make inferences about a larger population, and test hypotheses, you need a solid foundation in statistical concepts.",
                      tags: ["Statistics", "Probability", "Hypothesis Testing"],
                      scenario:
                        "Your goal is to move from simply observing data to making rigorous, evidence-based conclusions.",
                      topics: [
                        {
                          id: "p1s5ss1t1",
                          title: "Descriptive Statistics",
                          description:
                            "Measures of central tendency (mean, median, mode) and dispersion (variance, SD, IQR).",
                          why: "To summarize the key characteristics of your dataset, you'll calculate measures of central tendency (mean, median) and spread (variance, standard deviation).",
                          tags: ["Mean", "Median", "Standard Deviation"],
                          scenario:
                            'Task: To summarize the key features of your dataset (e.g., the "typical" value and how spread out the data is).',
                        },
                        {
                          id: "p1s5ss1t2",
                          title: "Probability Theory",
                          description:
                            "Events, sample spaces, conditional probability, and Bayes’ theorem.",
                          why: "To quantify uncertainty and understand the likelihood of events, you need to know the fundamentals of probability.",
                          tags: ["Probability", "Bayes Theorem", "Conditional"],
                          scenario:
                            "Goal: To quantify uncertainty and understand the likelihood of events, which is the foundation for all inference.",
                        },
                        {
                          id: "p1s5ss1t3",
                          title: "Probability Distributions",
                          description:
                            "Common distributions (Normal, Binomial, Poisson, t, F, Chi-Square, etc.).",
                          why: "To model real-world phenomena, you need to be familiar with common probability distributions (like the Normal or Binomial distribution).",
                          tags: [
                            "Normal",
                            "Binomial",
                            "Poisson",
                            "Distribution",
                          ],
                          scenario:
                            "Condition: You need to model a random process (e.g., coin flips, customer arrivals). You must choose the correct distribution.",
                        },
                        {
                          id: "p1s5ss1t4",
                          title: "Sampling & Resampling",
                          description:
                            "Techniques like random sampling, bootstrapping, and permutation tests.",
                          why: "When you can't analyze the entire population, you use sampling techniques to draw conclusions and use resampling (like bootstrapping) to estimate the uncertainty of your findings.",
                          tags: ["Sampling", "Bootstrap", "Resampling"],
                          scenario:
                            "Problem: You can't study the whole population. You need techniques to make reliable inferences from a smaller sample.",
                        },
                        {
                          id: "p1s5ss1t5",
                          title: "Statistical Significance",
                          description:
                            "Understanding z-scores, p-values, confidence intervals, effect size, and power.",
                          why: "To determine if an observed effect is real or just due to chance, you need to understand p-values, confidence intervals, and statistical power.",
                          tags: [
                            "P-Value",
                            "Confidence Interval",
                            "Significance",
                          ],
                          scenario:
                            "Decision: Is the effect you see in your sample a real phenomenon or just random chance?",
                        },
                        {
                          id: "p1s5ss1t6",
                          title: "Hypothesis Testing",
                          description:
                            "Null/alternative hypotheses, Type I/II errors, and test selection.",
                          why: "To formally test a claim or hypothesis about your data, you'll set up null/alternative hypotheses and use statistical tests to make a decision.",
                          tags: [
                            "Hypothesis Testing",
                            "Null Hypothesis",
                            "Errors",
                          ],
                          scenario:
                            "Task: To formally test a specific claim about your data using a structured scientific procedure.",
                        },
                        {
                          id: "p1s5ss1t7",
                          title: "Measuring Relationships",
                          description:
                            "Covariance and correlation (Pearson, Spearman, Kendall).",
                          why: "To quantify how two variables move together, you'll calculate their covariance and correlation.",
                          tags: ["Correlation", "Covariance", "Pearson"],
                          scenario:
                            "Goal: To quantify the direction and strength of the relationship between two variables.",
                        },
                        {
                          id: "p1s5ss1t8",
                          title: "Statistical Paradigms",
                          description:
                            "Contrasting descriptive vs. inferential statistics.",
                          why: "To understand the goal of your analysis, you must distinguish between simply describing data (descriptive) and making predictions about a larger group (inferential).",
                          tags: ["Descriptive", "Inferential", "Statistics"],
                          scenario:
                            "Decision: Are you describing the data you have, or trying to make inferences about a larger population?",
                        },
                      ],
                    },
                    {
                      id: "p1s5ss2",
                      title: "Linear Algebra",
                      description:
                        "Vector and matrix operations essential for machine learning.",
                      why: "To work with data in matrix form and understand the mechanics of many machine learning algorithms (like PCA and linear regression), you need to know linear algebra.",
                      tags: ["Linear Algebra", "Vectors", "Matrices", "PCA"],
                      scenario:
                        "You want to understand the mechanics of machine learning algorithms that represent data as tables (matrices) and features as arrays (vectors).",
                      topics: [
                        {
                          id: "p1s5ss2t1",
                          title: "Core Objects & Operations",
                          description:
                            "Vectors, matrices, dot/cross products, norms, and projections.",
                          why: "To perform fundamental calculations in machine learning, you must be comfortable with vector and matrix operations.",
                          tags: ["Vector", "Matrix", "Dot Product", "%*%"],
                          scenario:
                            "Task: To measure similarity or transform data, you must master vector and matrix operations.",
                        },
                        {
                          id: "p1s5ss2t2",
                          title: "Matrix Decompositions",
                          description:
                            "LU, QR, Cholesky, and Singular Value Decomposition (SVD).",
                          why: "To solve complex matrix problems and understand how algorithms work internally, you'll learn about techniques like SVD.",
                          tags: ["SVD", "LU", "QR", "Decomposition"],
                          scenario:
                            "Goal: To simplify complex matrices and uncover hidden structures in your data (e.g., for recommendation systems).",
                        },
                        {
                          id: "p1s5ss2t3",
                          title: "Eigen-analysis",
                          description:
                            "Eigenvalues, eigenvectors, diagonalization, and the spectral theorem.",
                          why: "To find the most important directions of variance in your data (the basis of PCA), you need to understand eigenvalues and eigenvectors.",
                          tags: ["Eigenvalue", "Eigenvector", "PCA"],
                          scenario:
                            "Goal: To find the principal axes of variation in your data, which is the core of Principal Component Analysis (PCA).",
                        },
                        {
                          id: "p1s5ss2t4",
                          title: "Linear Systems",
                          description:
                            "Solving systems of equations (`Ax = b`) and Gaussian elimination.",
                          why: "To find the coefficients in a linear regression model, you are effectively solving a system of linear equations (`Ax = b`).",
                          tags: ["Linear Equations", "solve()", "Gaussian"],
                          scenario:
                            "Problem: You need to solve a system of simultaneous equations, which is the basis for solving linear regression.",
                        },
                        {
                          id: "p1s5ss2t5",
                          title: "Vector Space Concepts",
                          description:
                            "Orthogonality and the Gram-Schmidt process.",
                          why: "To understand how features can be represented and compared in space, you'll learn concepts like orthogonality.",
                          tags: [
                            "Vector Space",
                            "Orthogonality",
                            "Gram-Schmidt",
                          ],
                          scenario:
                            "Goal: To understand how to create independent (orthogonal) features.",
                        },
                        {
                          id: "p1s5ss2t6",
                          title: "Similarity Measures",
                          description: "Calculating cosine similarity.",
                          why: "To measure how similar two documents or data points are, you can calculate their cosine similarity.",
                          tags: ["Cosine Similarity", "Similarity"],
                          scenario:
                            'Task: To calculate how "close" two data points are in a high-dimensional space.',
                        },
                        {
                          id: "p1s5ss2t7",
                          title: "Principal Component Analysis (PCA)",
                          description:
                            "Theory and application for feature extraction and data compression.",
                          why: "When you have too many correlated features, you can use PCA to reduce the dimensionality of your data while retaining most of the information.",
                          tags: ["PCA", "Dimensionality Reduction"],
                          scenario:
                            "Condition: You have too many correlated features and need to reduce the dimensionality of your dataset.",
                        },
                      ],
                    },
                    {
                      id: "p1s5ss3",
                      title: "Calculus & Optimization",
                      description:
                        "Calculus concepts and optimization methods for machine learning.",
                      why: "To understand how machine learning models 'learn' by minimizing errors, you need the tools of calculus and optimization.",
                      tags: ["Calculus", "Optimization", "Gradient Descent"],
                      scenario:
                        'Your goal is to understand how a machine learning model "learns" by finding the best parameters that minimize its error.',
                      topics: [
                        {
                          id: "p1s5ss3t1",
                          title: "Differentiation",
                          description:
                            "Derivatives, gradients, Jacobians, and Hessians.",
                          why: "To find the direction of steepest error decrease when training a model, you need to calculate derivatives and gradients.",
                          tags: ["Derivatives", "Gradient", "Calculus"],
                          scenario:
                            "Task: To find the direction of the steepest descent on an error surface, you must calculate the gradient.",
                        },
                        {
                          id: "p1s5ss3t2",
                          title: "Integration",
                          description:
                            "Definite/indefinite integrals and their application in probability.",
                          why: "To calculate the area under a probability distribution curve, you use integration.",
                          tags: ["Integration", "Integrals", "Probability"],
                          scenario:
                            "Goal: To find the area under a probability curve (e.g., to calculate a p-value), you need to use integration.",
                        },
                        {
                          id: "p1s5ss3t3",
                          title: "Function Approximation",
                          description: "Taylor series expansions.",
                          why: "To understand how complex functions can be approximated, you can use Taylor series expansions.",
                          tags: ["Taylor Series", "Approximation"],
                          scenario:
                            "Goal: To understand how complex functions can be approximated by simpler ones, which is key to how some models work.",
                        },
                        {
                          id: "p1s5ss3t4",
                          title: "Optimization Theory",
                          description:
                            "Loss functions, convexity, and finding local/global minima.",
                          why: "To train a model, you must define a loss function and find the model parameters that minimize it.",
                          tags: ["Optimization", "Loss Function", "Minima"],
                          scenario:
                            'Task: To "train" a model, you must define a loss function and find the parameters that minimize it.',
                        },
                        {
                          id: "p1s5ss3t5",
                          title: "Gradient Descent Algorithms",
                          description:
                            "Batch, stochastic, mini-batch, momentum, and learning rates.",
                          why: "To iteratively find the minimum of a loss function, you'll use gradient descent and its variants.",
                          tags: [
                            "Gradient Descent",
                            "Stochastic",
                            "Learning Rate",
                          ],
                          scenario:
                            'Task: You need an iterative algorithm to "walk" down the error surface to find the minimum.',
                        },
                        {
                          id: "p1s5ss3t6",
                          title: "Advanced Optimization",
                          description:
                            "Newton’s method, Lagrange multipliers, and constrained optimization.",
                          why: "For more complex optimization problems, you might need advanced techniques like Newton's method.",
                          tags: ["Newton Method", "Lagrange", "Constrained"],
                          scenario:
                            "Condition: Your optimization problem has constraints or is difficult to solve with simple gradient descent.",
                        },
                        {
                          id: "p1s5ss3t7",
                          title: "Modern Optimizers",
                          description:
                            "Backpropagation and stochastic methods (Adam, RMSprop, Adagrad).",
                          why: "To efficiently train deep neural networks, you will use modern optimizers like Adam and RMSprop.",
                          tags: [
                            "Adam",
                            "RMSprop",
                            "Adagrad",
                            "Backpropagation",
                          ],
                          scenario:
                            "Goal: To efficiently train deep neural networks, you need to use advanced, adaptive optimizers like Adam or RMSprop.",
                        },
                      ],
                    },
                  ],
                },
                {
                  id: "p1s6",
                  title: "Advanced R Programming",
                  icon: "fa-code-branch",
                  description:
                    "Advanced programming techniques for writing efficient and maintainable R code.",
                  why: "To write highly efficient, robust, and sophisticated R code, you need to move beyond the basics and master advanced concepts.",
                  tags: ["Advanced Programming", "Performance", "Debugging"],
                  difficulty: "advanced",
                  scenario:
                    "You've mastered the basics. Now you need to write more sophisticated, efficient, and robust R code.",
                  subsections: [
                    {
                      id: "p1s6ss1",
                      title: "Advanced Functional Programming",
                      description: "Closures and function factories.",
                      why: "To write more elegant and powerful functions that can even generate other functions, you'll explore closures and function factories.",
                      tags: ["Functional Programming", "Closures"],
                      scenario:
                        "Goal: To write more elegant functions that can even generate other functions, you'll explore closures and function factories.",
                    },
                    {
                      id: "p1s6ss2",
                      title: "Evaluation Model",
                      description: "Understanding lazy evaluation in R.",
                      why: "To understand exactly when and how R evaluates code (which can be surprising), you need to learn about its lazy evaluation model.",
                      tags: ["Lazy Evaluation"],
                      scenario:
                        'Problem: R\'s code evaluation can be surprising. You need to understand its "lazy evaluation" model to predict behavior.',
                    },
                    {
                      id: "p1s6ss3",
                      title: "Code Debugging & Profiling",
                      description:
                        "Using tools like browser(), traceback(), and profvis.",
                      why: "When your code gives an error or runs too slowly, you need tools to step through it (`browser()`) and identify performance bottlenecks (`profvis`).",
                      tags: ["Debugging", "Profiling"],
                      scenario:
                        "Problem: Your code has a bug or is running too slowly. You need tools to diagnose and fix it.",
                    },
                    {
                      id: "p1s6ss4",
                      title: "Performance Optimization",
                      description:
                        "Vectorization, memory management, and efficient data structures.",
                      why: "To make your code run faster, you'll learn techniques like vectorization and careful memory management.",
                      tags: ["Performance", "Vectorization"],
                      scenario:
                        "Goal: To make your code run significantly faster by using techniques like vectorization and better memory management.",
                    },
                    {
                      id: "p1s6ss5",
                      title: "Error Handling",
                      description:
                        "Implementing robust code with tryCatch() and custom error classes.",
                      why: "To make your code robust and prevent it from crashing on unexpected input, you will implement error handling with tryCatch().",
                      tags: ["Error Handling", "Robustness"],
                      scenario:
                        "Goal: To build robust code that doesn't crash on unexpected inputs, you must implement proper error handling.",
                    },
                    {
                      id: "p1s6ss6",
                      title: "Code Craftsmanship",
                      description:
                        "Writing readable, maintainable, and well-documented code.",
                      why: "To ensure your code is easy for others (and your future self) to read and maintain, you must learn principles of clean, well-documented code.",
                      tags: ["Code Quality", "Documentation"],
                      scenario:
                        "Goal: To ensure your code is maintainable and understandable by others, you must practice good code craftsmanship.",
                    },
                  ],
                },
                {
                  id: "p1s7",
                  title: "Professional Tools & Workflows",
                  icon: "fa-tools",
                  description:
                    "Essential tools and workflows for professional data science work.",
                  why: "To manage real-world data science projects effectively and collaborate with others, you need to master professional tools and workflows.",
                  tags: [
                    "Workflow",
                    "RMarkdown",
                    "Documentation",
                    "Automation",
                  ],
                  difficulty: "intermediate",
                  scenario:
                    "Your code works, but to succeed in a team, you need to use professional tools for collaboration, reporting, and automation.",
                  subsections: [
                    {
                      id: "p1s7ss1",
                      title: "RStudio Projects",
                      description:
                        "Managing self-contained analytical projects.",
                      why: "To keep all the files for a specific analysis (data, code, reports) self-contained and portable, you should always use an RStudio Project.",
                      tags: ["Projects", "Organization"],
                      scenario:
                        "Task: To keep all files for an analysis self-contained and portable, you must use an RStudio Project.",
                    },
                    {
                      id: "p1s7ss2",
                      title: "Dynamic Documents",
                      description:
                        "Using RMarkdown & Quarto for reports and presentations.",
                      why: "To create reports that automatically update when your data or code changes, you'll use RMarkdown or Quarto.",
                      tags: ["RMarkdown", "Quarto", "Reports"],
                      scenario:
                        "Goal: You need to create reports that automatically update when your data or code changes.",
                    },
                    {
                      id: "p1s7ss3",
                      title: "Literate Programming",
                      description: "Interactive development with R Notebooks.",
                      why: "To develop analyses in an interactive, exploratory way, you'll use R Notebooks, which blend code, output, and narrative.",
                      tags: ["Literate Programming", "Notebooks"],
                      scenario:
                        "Goal: To develop your analysis interactively, blending code, output, and narrative text.",
                    },
                    {
                      id: "p1s7ss4",
                      title: "Automation & Pipelines",
                      description:
                        "Building workflows with drake, targets, and Makefiles.",
                      why: "When your analysis has many steps and needs to be re-run frequently, you'll build automated pipelines with tools like `targets`.",
                      tags: ["Automation", "Pipelines", "targets"],
                      scenario:
                        "Condition: Your analysis has many steps and needs to be re-run often. You should build an automated pipeline.",
                    },
                    {
                      id: "p1s7ss5",
                      title: "Code Documentation",
                      description:
                        "Using roxygen2, READMEs, and inline comments.",
                      why: "To make your functions and packages usable by others, you must document them professionally using `roxygen2`.",
                      tags: ["Documentation", "roxygen2"],
                      scenario:
                        "Task: To make your code usable by others, you must document it professionally.",
                    },
                    {
                      id: "p1s7ss6",
                      title: "Professional Reporting",
                      description:
                        "Creating dashboards and interactive visualizations.",
                      why: "To share your results with non-technical stakeholders, you'll create interactive dashboards.",
                      tags: ["Dashboards", "Reporting"],
                      scenario:
                        "Goal: To share your results with non-technical stakeholders, you need to create dashboards.",
                    },
                    {
                      id: "p1s7ss7",
                      title: "Version Control Integration",
                      description: "Using Git and GitHub within RStudio.",
                      why: "To track changes to your code and collaborate with a team, you must integrate Git and GitHub directly into your RStudio workflow.",
                      tags: ["Git", "GitHub", "Version Control"],
                      scenario:
                        "Task: To track changes and collaborate, you must integrate Git/GitHub into your workflow.",
                    },
                    {
                      id: "p1s7ss8",
                      title: "Collaborative Workflows",
                      description: "Shared projects and cloud platforms.",
                      why: "To work effectively with a team on a shared codebase, you need to learn cloud-based collaborative workflows.",
                      tags: ["Collaboration", "Cloud"],
                      scenario:
                        "Goal: To work effectively on a shared codebase, you must learn team-based workflows.",
                    },
                  ],
                },
                {
                  id: "p1s8",
                  title: "Version Control with Git",
                  icon: "fa-git-alt",
                  description:
                    "Master version control for collaborative and reproducible data science.",
                  why: "To track the history of your project, collaborate with others without overwriting work, and revert to previous versions if something breaks, you must use a version control system like Git.",
                  tags: ["Git", "GitHub", "Version Control", "Collaboration"],
                  difficulty: "intermediate",
                  scenario:
                    "Your project folder is full of files like analysis_v2.R and analysis_final.R. You need a reliable system to track changes, revert mistakes, and collaborate.",
                  subsections: [
                    {
                      id: "p1s8ss1",
                      title: "Repository Management",
                      description:
                        "Initializing local repos and connecting to GitHub.",
                      why: "To start tracking a project, you need to initialize a local Git repository and connect it to a remote host like GitHub.",
                      tags: ["Repository", "GitHub"],
                      scenario:
                        "Task: To start tracking a project, you must initialize a Git repository and connect it to a remote host like GitHub.",
                    },
                    {
                      id: "p1s8ss2",
                      title: "The Git Workflow",
                      description:
                        "Staging, committing, branching, and merging.",
                      why: "To save changes to your project history, you must learn the core cycle of staging, committing, and creating branches for new features.",
                      tags: ["Workflow", "Branching", "Merging"],
                      scenario:
                        "Task: To save the history of your changes, you must learn the core cycle of staging, committing, and branching.",
                    },
                    {
                      id: "p1s8ss3",
                      title: "Remote Collaboration",
                      description: "Using push, pull, and managing remotes.",
                      why: "To share your changes with your team and get their updates, you'll use `push` and `pull`.",
                      tags: ["Collaboration", "Remote"],
                      scenario:
                        "Goal: To share your work with a team and get their updates, you need to use push and pull.",
                    },
                    {
                      id: "p1s8ss4",
                      title: "Team Contributions",
                      description: "Forks, pull requests, and code reviews.",
                      why: "To propose changes to a project you don't own, or to review code from teammates, you will use forks and pull requests.",
                      tags: ["Pull Requests", "Code Review"],
                      scenario:
                        "Task: To review a teammate's code or propose changes to a project, you must use pull requests.",
                    },
                    {
                      id: "p1s8ss5",
                      title: "History Management",
                      description:
                        "Reverting changes and resolving merge conflicts.",
                      why: "When you make a mistake or two developers edit the same file, you need to know how to revert changes and resolve merge conflicts.",
                      tags: ["History", "Conflicts"],
                      scenario:
                        "Problem: You made a mistake or have conflicting edits. You need to know how to revert changes and resolve conflicts.",
                    },
                  ],
                },
              ],
            },
            {
              id: "phase2",
              title: "Phase 2: Data Preparation",
              icon: "fa-database",
              description:
                "Real-world data is messy, incomplete, and not formatted for analysis. This phase focuses on the crucial steps of acquiring, cleaning, and transforming raw data into a structured, reliable format ready for modeling.",
              scenario:
                "Real-world data is messy and not ready for analysis. Your goal is to acquire, clean, and transform it into a structured format.",
              sections: [
                {
                  id: "p2s1",
                  title: "Data Acquisition",
                  icon: "fa-download",
                  description:
                    "Learn various methods for sourcing data from different locations.",
                  why: "Before you can analyze data, you must first get it into your R session from various sources like files, databases, or the web.",
                  tags: ["Data Import", "APIs", "Databases", "Web Scraping"],
                  difficulty: "basic",
                  scenario:
                    "Your data exists, but it's outside of your current R session. You need to choose the right tool to import it based on its source.",
                  subsections: [
                    {
                      id: "p2s1ss1",
                      title: "From Local Files",
                      description:
                        "Using readr (CSVs), readxl (Excel), and jsonlite (JSON).",
                      why: "When your data is in a common file format like a CSV or Excel spreadsheet, you'll use packages like `readr` and `readxl` to import it.",
                      tags: ["CSV", "Excel", "JSON", "readr"],
                      scenario:
                        "Condition: The data is in a file on your computer, like a CSV or Excel spreadsheet.",
                    },
                    {
                      id: "p2s1ss2",
                      title: "From SQL Databases",
                      description:
                        "Using DBI and odbc to query relational databases.",
                      why: "When your data resides in a corporate relational database, you need to use `DBI` to write SQL queries from R to retrieve it.",
                      tags: ["SQL", "DBI", "odbc", "Databases"],
                      scenario:
                        "Condition: The data is stored in a corporate relational database (e.g., SQL Server, PostgreSQL).",
                    },
                    {
                      id: "p2s1ss3",
                      title: "From NoSQL Databases",
                      description: "Using mongolite to query MongoDB.",
                      why: "If your data is stored in a flexible-schema database like MongoDB, you'll use a specific package like `mongolite` to access it.",
                      tags: ["NoSQL", "MongoDB", "mongolite"],
                      scenario:
                        "Condition: The data is in a flexible-schema database like MongoDB.",
                    },
                    {
                      id: "p2s1ss4",
                      title: "From the Web",
                      description:
                        "Accessing APIs with httr2 and web scraping with rvest.",
                      why: "To get data from a web service or scrape information from a website, you'll use `httr2` to interact with APIs and `rvest` for web scraping.",
                      tags: ["APIs", "httr2", "rvest", "Web Scraping"],
                      scenario:
                        "Condition: The data needs to be retrieved from a web API or scraped from a website.",
                    },
                  ],
                },
                {
                  id: "p2s2",
                  title: "Data Cleaning & Wrangling",
                  icon: "fa-broom",
                  description:
                    "Transform raw, messy data into a clean, tidy, and usable format.",
                  why: "Your raw imported data is rarely perfect. You need a systematic process to handle errors, missing values, and inconsistencies to make it trustworthy.",
                  tags: [
                    "Data Cleaning",
                    "janitor",
                    "Missing Data",
                    "Outliers",
                  ],
                  difficulty: "intermediate",
                  scenario:
                    "You have successfully imported the data, but it's raw and messy. You must now apply a series of steps to make it clean and trustworthy for analysis.",
                  subsections: [
                    {
                      id: "p2s2ss1",
                      title: "Automated Cleaning",
                      description:
                        "Using janitor for standardizing data frames.",
                      why: "To perform common initial cleaning tasks like standardizing column names, you can use the `janitor` package to save time.",
                      tags: ["janitor", "Automation"],
                      scenario:
                        "Task: Perform common initial cleaning tasks like standardizing column names.",
                    },
                    {
                      id: "p2s2ss2",
                      title: "Missing Value Treatment",
                      description:
                        "Identification, visualization, and imputation strategies (mice, missForest).",
                      why: "When your dataset has empty cells (`NA`s), you need to decide on a strategy: remove them, or fill them in (imputation) using a logical method.",
                      tags: ["Missing Data", "mice", "Imputation"],
                      scenario:
                        "Problem: Your dataset has empty cells (NA`s). You must decide on a strategy to handle them.",
                    },
                    {
                      id: "p2s2ss3",
                      title: "Outlier Detection & Treatment",
                      description:
                        "Using statistical methods (Z-score, IQR) to identify and handle outliers.",
                      why: "When some data points are extreme and may skew your analysis, you need to identify them using statistical rules and decide whether to remove or adjust them.",
                      tags: ["Outliers", "Z-score", "IQR"],
                      scenario:
                        "Problem: Some data points are extreme and may skew your analysis. You need to identify and handle them.",
                    },
                    {
                      id: "p2s2ss4",
                      title: "Data Type Conversion",
                      description:
                        "Ensuring columns are the correct numeric, character, factor, or date types.",
                      why: "If a column of numbers is accidentally read as text, you must explicitly convert it to the correct data type before you can perform mathematical operations.",
                      tags: ["Data Types", "Conversion"],
                      scenario:
                        "Problem: A column of numbers was read as text. You must convert it to the correct type.",
                    },
                    {
                      id: "p2s2ss5",
                      title: "Text Preprocessing",
                      description:
                        "Cleaning text with stringr, regex, and standardization.",
                      why: "To analyze free-form text data, you must first clean it by removing punctuation, converting to lowercase, and standardizing it using `stringr` and regular expressions.",
                      tags: ["Text Cleaning", "stringr", "Regex"],
                      scenario:
                        "Task: To analyze text data, you must first clean and standardize it.",
                    },
                    {
                      id: "p2s2ss6",
                      title: "High-Performance Wrangling",
                      description:
                        "Using data.table for speed and memory efficiency on large datasets.",
                      why: "When your dataset is too large to fit in memory or `dplyr` is too slow, you'll turn to the highly efficient `data.table` package.",
                      tags: ["data.table", "Performance"],
                      scenario:
                        "Condition: Your dataset is too large and dplyr is too slow. You need a more memory-efficient tool.",
                    },
                    {
                      id: "p2s2ss7",
                      title: "Data Reshaping",
                      description: "Pivoting between wide and long formats.",
                      why: "When your data is in a 'wide' format but your analysis requires a 'long' format (or vice-versa), you need to pivot your data.",
                      tags: ["Reshaping", "Pivot"],
                      scenario:
                        'Task: Your data is in a "wide" format but your analysis requires a "long" format (or vice-versa).',
                    },
                    {
                      id: "p2s2ss8",
                      title: "Best Practices",
                      description:
                        "Ensuring reproducible scripts and validating cleaning steps.",
                      why: "To ensure your cleaning process is reliable and can be repeated, you must write reproducible scripts and validate each transformation step.",
                      tags: ["Best Practices", "Validation"],
                      scenario:
                        "Goal: To ensure your data cleaning is trustworthy and reliable, you must adopt a workflow that is documented, reproducible, and includes checks to validate each transformation.",
                    },
                  ],
                },
                {
                  id: "p2s3",
                  title: "Feature Engineering",
                  icon: "fa-wrench",
                  description:
                    "Create new features from existing data to improve model performance.",
                  why: "To improve your model's predictive power, you need to creatively transform raw data into informative features that better represent the underlying patterns.",
                  tags: ["Feature Engineering", "Feature Selection", "recipes"],
                  difficulty: "advanced",
                  scenario:
                    "Your data is clean, but you believe you can improve your model's performance by creating more informative features from the raw variables.",
                  subsections: [
                    {
                      id: "p2s3ss1",
                      title: "Feature Creation",
                      description:
                        "Generating new features from existing data using domain knowledge.",
                      why: "When you believe combining or transforming existing columns will provide new, valuable information, you create new features (e.g., creating an 'age' feature from a 'date of birth' column).",
                      tags: ["Feature Creation", "Domain Knowledge"],
                      scenario:
                        "Goal: To provide new signals to your model by combining or transforming existing columns based on domain knowledge.",
                    },
                    {
                      id: "p2s3ss2",
                      title: "Feature Selection",
                      description:
                        "Applying filter, wrapper, and embedded methods (e.g., LASSO).",
                      why: "When you have too many features, some of which might be redundant or irrelevant, you need a systematic way to choose the most impactful ones to avoid overfitting.",
                      tags: ["Feature Selection", "LASSO", "Filter Methods"],
                      scenario:
                        "Problem: You have too many features, some of which are irrelevant. You need to select the most impactful ones.",
                    },
                    {
                      id: "p2s3ss3",
                      title: "Feature Transformation",
                      description:
                        "Applying mathematical transforms (log, Box-Cox, Yeo-Johnson).",
                      why: "If a feature's distribution is highly skewed, applying a mathematical transformation (like a logarithm) can help your model perform better.",
                      tags: ["Transformation", "Box-Cox", "Yeo-Johnson"],
                      scenario:
                        "Condition: A feature is highly skewed. You apply a mathematical transform to help your model perform better.",
                    },
                    {
                      id: "p2s3ss4",
                      title: "Interaction Features",
                      description:
                        "Creating features from ratios, products, and combinations of variables.",
                      why: "When you suspect the effect of one feature depends on the value of another, you can create interaction features (e.g., `feature_A * feature_B`).",
                      tags: ["Interactions", "Feature Combinations"],
                      scenario:
                        "Hypothesis: The effect of one feature depends on another. You create features to capture this interaction.",
                    },
                    {
                      id: "p2s3ss5",
                      title: "The recipes Package",
                      description:
                        "Building reusable preprocessing and feature engineering pipelines.",
                      why: "To build a reusable, step-by-step pipeline for all your preprocessing and feature engineering tasks, you will use the `recipes` package from the `tidymodels` ecosystem.",
                      tags: ["recipes", "Preprocessing", "Pipelines"],
                      scenario:
                        "Goal: To build a reusable, step-by-step pipeline for all your preprocessing and feature engineering tasks.",
                    },
                    {
                      id: "p2s3ss6",
                      title: "Dimensionality Reduction",
                      description:
                        "Using PCA, t-SNE, and UMAP to reduce the number of features.",
                      why: "When you have a very wide dataset with many correlated features, you can use techniques like PCA to reduce the number of features while preserving information.",
                      tags: [
                        "PCA",
                        "t-SNE",
                        "UMAP",
                        "Dimensionality Reduction",
                      ],
                      scenario:
                        "Problem: You have hundreds of correlated features. You need to reduce the number of variables while preserving information.",
                    },
                    {
                      id: "p2s3ss7",
                      title: "Best Practices",
                      description:
                        "Avoiding data leakage, documenting steps, and validating feature impact.",
                      why: "To prevent your model from 'cheating' by seeing information from the test set during training, you must be vigilant about avoiding data leakage.",
                      tags: ["Data Leakage", "Validation", "Documentation"],
                      scenario:
                        "Problem: Your model works perfectly on your test set but fails in production. You've likely made a critical error, like data leakage. You need to follow best practices to create robust and generalizable features.",
                    },
                  ],
                },
                {
                  id: "p2s4",
                  title: "Feature Scaling",
                  icon: "fa-balance-scale",
                  description:
                    "Scale features appropriately for machine learning models.",
                  why: "When your features are on vastly different scales (e.g., age in years and income in thousands), you need to rescale them so that models that are sensitive to distance (like k-NN or SVMs) treat them equally.",
                  tags: ["Scaling", "Standardization", "Normalization"],
                  difficulty: "advanced",
                  scenario:
                    "You are preparing data for a distance-based algorithm (like k-NN, SVM, or PCA), and your numeric features are on vastly different scales.",
                  subsections: [
                    {
                      id: "p2s4ss1",
                      title: "Standardization",
                      description:
                        "Rescaling to zero mean and unit variance (Z-score).",
                      why: "When you want to rescale features to have a mean of 0 and a standard deviation of 1, you apply Z-score scaling.",
                      tags: ["Standardization", "Z-score"],
                      scenario:
                        "Goal: Rescale features to have a mean of 0 and a standard deviation of 1. This is the most common scaling method.",
                    },
                    {
                      id: "p2s4ss2",
                      title: "Normalization",
                      description: "Rescaling to a fixed range (e.g., 0-1).",
                      why: "When you need to scale features to a specific range, typically 0 to 1, you use min-max normalization.",
                      tags: ["Normalization", "Min-Max"],
                      scenario:
                        "Goal: Rescale features to a specific range, usually between 0 and 1. Useful for algorithms that expect this range.",
                    },
                    {
                      id: "p2s4ss3",
                      title: "Robust Scaling",
                      description:
                        "Scaling using median and IQR to handle outliers.",
                      why: "If your data has significant outliers that would skew standard scaling, you should use a robust method based on the median and interquartile range (IQR).",
                      tags: ["Robust Scaling", "Median", "IQR"],
                      scenario:
                        "Condition: Your data has significant outliers that would skew standard scaling. You need a method that is robust to them.",
                    },
                    {
                      id: "p2s4ss4",
                      title: "Best Practices",
                      description:
                        "Scaling after splitting data to avoid data leakage.",
                      why: "To avoid data leakage, you must always calculate scaling parameters on the training data *only* and then apply that same transformation to the test data.",
                      tags: ["Best Practices", "Data Leakage"],
                      scenario:
                        "Task: To prevent your model from gaining an unfair advantage by 'peeking' at the test data, you must apply your scaling transformations in the correct, leak-proof order.",
                    },
                  ],
                },
                {
                  id: "p2s5",
                  title: "Categorical Data Encoding",
                  icon: "fa-tags",
                  description:
                    "Convert categorical variables into numerical representations.",
                  why: "Since most machine learning models require numerical input, you need a strategy to convert categorical text features (like 'color' or 'city') into numbers.",
                  tags: ["Encoding", "One-Hot Encoding", "Label Encoding"],
                  difficulty: "advanced",
                  scenario:
                    "Your dataset contains text-based categorical features, but your machine learning model requires all inputs to be numeric.",
                  subsections: [
                    {
                      id: "p2s5ss1",
                      title: "One-Hot Encoding",
                      description: "Creating binary columns for each category.",
                      why: "For nominal categorical features (where order doesn't matter), you create new binary (0/1) columns for each category.",
                      tags: ["One-Hot", "Binary", "Dummy Variables"],
                      scenario:
                        "Condition: Your categorical feature is nominal (the order of categories doesn't matter).",
                    },
                    {
                      id: "p2s5ss2",
                      title: "Label & Ordinal Encoding",
                      description: "Converting categories to integer codes.",
                      why: "For ordinal features (where order matters, e.g., 'Low,' 'Medium,' 'High'), you convert categories into ordered integers (e.g., 1, 2, 3).",
                      tags: ["Label Encoding", "Ordinal"],
                      scenario:
                        'Condition: Your categorical feature is ordinal (the order of categories matters, e.g., "Low", "Medium", "High").',
                    },
                    {
                      id: "p2s5ss3",
                      title: "Target Encoding",
                      description:
                        "Encoding based on the mean of the target variable (with leakage protection).",
                      why: "For high-cardinality features (many unique categories), you can encode them based on the average target value for that category, being careful to prevent data leakage.",
                      tags: ["Target Encoding", "Mean Encoding"],
                      scenario:
                        "Condition: Your feature has a very high number of categories, and one-hot encoding would create too many new columns.",
                    },
                    {
                      id: "p2s5ss4",
                      title: "Frequency Encoding",
                      description: "Encoding based on category frequency.",
                      why: "When the frequency of a category might be a useful signal, you can encode categories based on how often they appear in the data.",
                      tags: ["Frequency", "Count Encoding"],
                      scenario:
                        "Hypothesis: The frequency with which a category appears might be a useful predictive signal for the model.",
                    },
                    {
                      id: "p2s5ss5",
                      title: "Best Practices",
                      description:
                        "Avoiding the dummy variable trap and documenting choices.",
                      why: "When using one-hot encoding, you should be aware of the 'dummy variable trap' and how to avoid it to prevent multicollinearity.",
                      tags: ["Dummy Variable Trap", "Best Practices"],
                      scenario:
                        "Decision: You have multiple encoding strategies. You must choose the right one for your data and model, while avoiding common pitfalls like the dummy variable trap that can harm your model's performance.",
                    },
                  ],
                },
                {
                  id: "p2s6",
                  title: "Feature Binning",
                  icon: "fa-boxes",
                  description:
                    "Convert continuous variables into categorical bins.",
                  why: "To simplify a continuous feature, reduce the impact of outliers, or model a non-linear relationship, you can group the feature's values into a smaller number of 'bins' or intervals.",
                  tags: ["Binning", "Discretization"],
                  difficulty: "advanced",
                  scenario:
                    "You have a continuous numeric feature and you want to group its values into a smaller number of discrete bins.",
                  subsections: [
                    {
                      id: "p2s6ss1",
                      title: "Unsupervised Binning",
                      description:
                        "Creating bins of equal width or equal frequency (quantile).",
                      why: "When you want to create bins without looking at the target variable, you can create bins of equal width or bins with an equal number of data points in each (quantile binning).",
                      tags: ["Equal Width", "Quantile", "Unsupervised"],
                      scenario:
                        "Goal: Create bins based only on the feature's own properties, like creating bins of equal width or with an equal number of observations.",
                    },
                    {
                      id: "p2s6ss2",
                      title: "Supervised Binning",
                      description:
                        "Using the target variable to create optimal bins (e.g., via decision trees).",
                      why: "To create bins that are most informative for predicting a target variable, you can use a supervised method, such as a decision tree, to find the optimal split points.",
                      tags: ["Supervised", "Decision Trees", "Target"],
                      scenario:
                        "Goal: Create bins that are most informative for predicting a target variable, using the target to find optimal split points.",
                    },
                    {
                      id: "p2s6ss3",
                      title: "Custom Binning",
                      description:
                        "Creating bins based on domain knowledge or business rules.",
                      why: "When you have domain knowledge that suggests specific thresholds are important, you can create custom bins based on business rules.",
                      tags: ["Custom", "Domain Knowledge", "Business Rules"],
                      scenario:
                        "Condition: You have specific domain knowledge or business rules that define meaningful thresholds for creating bins.",
                    },
                    {
                      id: "p2s6ss4",
                      title: "Best Practices",
                      description:
                        "Balancing information loss with model performance.",
                      why: "When deciding on the number of bins, you need to balance the risk of losing important information (too few bins) against the risk of creating noise (too many bins).",
                      tags: ["Information Loss", "Performance"],
                      scenario:
                        "Decision: You need to choose the number of bins for your feature. Too few, and you lose valuable information. Too many, and you might just be adding noise. You must find the optimal balance.",
                    },
                  ],
                },
                {
                  id: "p2s7",
                  title: "Date/Time Engineering",
                  icon: "fa-calendar-alt",
                  description:
                    "Extract meaningful features from date and time data.",
                  why: "To use date and time information in a model, you must parse it correctly and extract meaningful features from it.",
                  tags: ["Date/Time", "lubridate", "Feature Engineering"],
                  difficulty: "advanced",
                  scenario:
                    "Your dataset contains date and time information that you need to make usable as features for a machine learning model.",
                  subsections: [
                    {
                      id: "p2s7ss1",
                      title: "Parsing & Manipulation",
                      description:
                        "Using lubridate to handle date/time strings.",
                      why: "When your dates are stored as text (e.g., '09-13-2025'), you need to use a package like `lubridate` to convert them into a proper date/time format.",
                      tags: ["lubridate", "Parsing", "Date/Time"],
                      scenario:
                        'Problem: Your dates are stored as text (e.g., "2025-09-17"). You must first convert them into a proper date/time object.',
                    },
                    {
                      id: "p2s7ss2",
                      title: "Feature Extraction",
                      description:
                        "Deriving features like year, month, weekday, or time-based differences.",
                      why: "To make dates useful for a model, you can extract features like the year, month, day of the week, or whether it's a holiday.",
                      tags: ["Feature Extraction", "Time Features"],
                      scenario:
                        "Task: You need to derive meaningful features from your date objects, such as the year, month, day of the week, or if it's a holiday.",
                    },
                    {
                      id: "p2s7ss3",
                      title: "Time Zone Management",
                      description:
                        "Converting between and handling time zones.",
                      why: "When working with data from different geographic locations, you must correctly handle and convert between time zones to avoid errors.",
                      tags: ["Time Zones", "Conversion"],
                      scenario:
                        "Condition: Your data comes from different geographic locations. You must correctly handle and convert between time zones to avoid errors.",
                    },
                    {
                      id: "p2s7ss4",
                      title: "Best Practices",
                      description:
                        "Ensuring consistent formats and documenting assumptions.",
                      why: "To ensure consistency in your analysis, you must establish a standard format for all date/time features and document any assumptions made about them.",
                      tags: ["Consistency", "Documentation"],
                      scenario:
                        "Goal: To create a robust time-based analysis, you must establish a consistent workflow for handling time zones, documenting formats, and avoiding look-ahead bias in your features.",
                    },
                  ],
                },
              ],
            },
            {
              id: "phase3",
              title: "Phase 3: Exploratory Data Analysis & Visualization",
              icon: "fa-search",
              description:
                "Before building predictive models, you must first explore and understand your dataset by summarizing its main characteristics, identifying patterns, visualizing relationships, and forming initial hypotheses. This is like a detective gathering clues before naming a suspect.",
              scenario:
                "Your data is now clean and structured. Before you build a model, you must first explore it to understand its patterns, find relationships, and form initial hypotheses.",
              sections: [
                {
                  id: "p3s1",
                  title: "Initial Data Exploration",
                  icon: "fa-search-plus",
                  description:
                    "Perform initial investigation of your dataset structure and characteristics.",
                  why: "When you first receive a dataset, your goal is to get a quick, high-level overview of its contents and quality.",
                  tags: ["EDA", "Summary Statistics", "Data Quality"],
                  difficulty: "basic",
                  scenario:
                    "You have just loaded a new dataset. Your first goal is to perform a series of checks to quickly understand its basic characteristics and quality.",
                  subsections: [
                    {
                      id: "p3s1ss1",
                      title: "Data Summarization",
                      description:
                        "Using summary(), skimr::skim() for descriptive statistics.",
                      why: "To get a quick statistical overview of each column, you use functions like `summary()` or `skimr::skim()`.",
                      tags: ["Summary", "skimr", "Descriptive Statistics"],
                      scenario:
                        "Task: To get a quick statistical overview of each column (e.g., mean, median, quartiles).",
                    },
                    {
                      id: "p3s1ss2",
                      title: "Structure Inspection",
                      description:
                        "Using str(), glimpse(), head(), tail() to view data structure.",
                      why: "To see the data types, column names, and a few sample rows, you use `str()`, `glimpse()`, and `head()`.",
                      tags: ["Structure", "glimpse", "Data Inspection"],
                      scenario:
                        "Task: To see the column names, data types, and a few sample rows to understand the data's layout.",
                    },
                    {
                      id: "p3s1ss3",
                      title: "Data Quality Assessment",
                      description:
                        "Checking for missing values, duplicates, and inconsistencies.",
                      why: "To identify potential problems that need cleaning, you systematically check for missing values, duplicate rows, and obvious inconsistencies.",
                      tags: ["Data Quality", "Missing Values", "Duplicates"],
                      scenario:
                        "Goal: To identify potential problems that need cleaning, like missing values or duplicates.",
                    },
                    {
                      id: "p3s1ss4",
                      title: "Relationship Analysis",
                      description:
                        "Using correlation matrices and scatterplots to find relationships.",
                      why: "To get a first look at how variables relate to each other, you'll create correlation matrices and scatterplots.",
                      tags: ["Correlation", "Relationships", "Scatterplots"],
                      scenario:
                        "Goal: To get a first look at how different variables might relate to each other.",
                    },
                    {
                      id: "p3s1ss5",
                      title: "Distribution Assessment",
                      description:
                        "Using histograms, density plots, and boxplots to understand variable distributions.",
                      why: "To understand the shape and spread of each variable, you'll use histograms, density plots, and boxplots.",
                      tags: ["Distributions", "Histograms", "Boxplots"],
                      scenario:
                        "Task: To understand the shape and spread of each individual variable.",
                    },
                  ],
                },
                {
                  id: "p3s2",
                  title: "Automated EDA Tools",
                  icon: "fa-robot",
                  description:
                    "Leverage automated tools to accelerate your initial exploration.",
                  why: "To speed up your initial exploration and generate a comprehensive report with a single command, you can use automated EDA packages.",
                  tags: ["Automation", "DataExplorer", "skimr", "Reports"],
                  difficulty: "intermediate",
                  scenario:
                    "You want to speed up the initial exploration process by generating a comprehensive HTML report of your entire dataset with just a few lines of code.",
                  subsections: [
                    {
                      id: "p3s2ss1",
                      title: "Automated Reporting",
                      description:
                        "Generating comprehensive EDA reports with packages like DataExplorer and dlookr.",
                      why: "To quickly generate a detailed HTML report summarizing the entire dataset, you can use packages like `DataExplorer`.",
                      tags: ["DataExplorer", "dlookr", "Automated Reports"],
                      scenario:
                        "Task: To generate a detailed report covering univariate and bivariate analysis, missing data, and correlations.",
                    },
                    {
                      id: "p3s2ss2",
                      title: "Data Profiling",
                      description:
                        "Quickly summarizing data types, missingness, and outliers.",
                      why: "To get a fast, high-level summary of data types and missingness, these tools provide an excellent starting point.",
                      tags: ["Data Profiling", "Summary"],
                      scenario:
                        "Goal: To get a fast, high-level summary of all variables, their types, and their quality metrics.",
                    },
                    {
                      id: "p3s2ss3",
                      title: "Best Practices",
                      description:
                        "Using automated EDA as a starting point for deeper investigation.",
                      why: "To ensure a thorough analysis, you should use automated EDA as a starting point to guide your deeper, manual investigations, not as a replacement for them.",
                      tags: ["Best Practices", "Investigation"],
                      scenario:
                        "Decision: Understanding that automated tools are a great starting point but should be used to guide, not replace, deeper manual investigation.",
                    },
                  ],
                },
                {
                  id: "p3s3",
                  title: "Hypothesis Testing & Statistical Inference",
                  icon: "fa-flask",
                  description:
                    "Apply formal statistical procedures to test hypotheses about your data.",
                  why: "To move from simply observing patterns to making statistically-backed claims about them, you need to use formal hypothesis testing.",
                  tags: [
                    "Hypothesis Testing",
                    "Statistical Inference",
                    "T-Tests",
                    "ANOVA",
                  ],
                  difficulty: "intermediate",
                  scenario:
                    "You've observed a pattern in your sample data. You must now use a formal statistical framework to determine if this pattern is a real effect or just due to random chance.",
                  subsections: [
                    {
                      id: "p3s3ss1",
                      title: "Hypothesis Formulation",
                      description: "Defining null and alternative hypotheses.",
                      why: "To formally test a belief, you must first state it as a testable null hypothesis (e.g., 'There is no difference between the means of Group A and Group B').",
                      tags: ["Null Hypothesis", "Alternative Hypothesis"],
                      scenario:
                        "Task: To formally test a belief, you must first state it as a testable null hypothesis (no effect) and an alternative hypothesis.",
                    },
                    {
                      id: "p3s3ss2",
                      title: "Common Statistical Tests",
                      description:
                        "Applying t-tests, ANOVA, and chi-square tests.",
                      why: "To compare groups or test for relationships, you will apply the appropriate statistical test, such as a t-test, ANOVA, or chi-square test.",
                      tags: ["T-Tests", "ANOVA", "Chi-Square"],
                      scenario:
                        "Decision: Based on your data types and question, you must choose the appropriate statistical test (e.g., t-test, ANOVA).",
                    },
                    {
                      id: "p3s3ss3",
                      title: "Interpreting Results",
                      description:
                        "Understanding p-values, confidence intervals, and effect sizes.",
                      why: "To make a conclusion, you must correctly interpret the p-value, confidence intervals, and effect size from your test's output.",
                      tags: [
                        "P-Values",
                        "Confidence Intervals",
                        "Effect Sizes",
                      ],
                      scenario:
                        "Task: To make a conclusion, you must correctly interpret the p-value, confidence intervals, and effect sizes from your test.",
                    },
                    {
                      id: "p3s3ss4",
                      title: "Test Assumptions & Diagnostics",
                      description:
                        "Validating assumptions like normality and homogeneity of variance.",
                      why: "To ensure your test results are valid, you must check that the underlying assumptions of the test (like normality) are met.",
                      tags: ["Assumptions", "Normality", "Diagnostics"],
                      scenario:
                        "Goal: To ensure your test results are valid, you must check that the underlying assumptions of the test are met.",
                    },
                    {
                      id: "p3s3ss5",
                      title: "Multiple Testing Correction",
                      description:
                        "Adjusting p-values using methods like Bonferroni or FDR.",
                      why: "When you run many hypothesis tests at once, you need to adjust your p-values to avoid an inflated risk of finding a false positive.",
                      tags: ["Multiple Testing", "Bonferroni", "FDR"],
                      scenario:
                        "Problem: You ran many tests at once, which increases the risk of a false positive. You need to adjust your p-values.",
                    },
                    {
                      id: "p3s3ss6",
                      title: "Best Practices",
                      description:
                        "Avoiding p-hacking and reporting effect sizes alongside p-values.",
                      why: "To conduct responsible science, you must avoid 'p-hacking' (cherry-picking results) and always report effect sizes to show the magnitude of a finding, not just its statistical significance.",
                      tags: ["P-Hacking", "Effect Sizes", "Best Practices"],
                      scenario:
                        "Goal: To conduct credible and ethical science, you must go beyond just finding a 'significant' p-value. You need to adopt practices that prevent cherry-picking and honestly report the magnitude and uncertainty of your findings.",
                    },
                  ],
                },
                {
                  id: "p3s4",
                  title: "Visualization Fundamentals",
                  icon: "fa-chart-line",
                  description:
                    "Create effective visualizations for data exploration and communication.",
                  why: "To explore your data and communicate your findings effectively, you need to master the art and science of data visualization.",
                  tags: ["Visualization", "ggplot2", "Charts", "Communication"],
                  difficulty: "intermediate",
                  scenario:
                    "Your goal is to understand your data and communicate your findings. The most powerful way to do this is by mastering the art and science of data visualization.",
                  subsections: [
                    {
                      id: "p3s4ss1",
                      title: "Principles of Effective Visualization",
                      description:
                        "Choosing the right plot, using color effectively, and avoiding misleading graphics.",
                      why: "To create plots that are clear, honest, and impactful, you must learn principles of good design, like choosing the right chart type and using color purposefully.",
                      tags: ["Design Principles", "Color", "Best Practices"],
                      scenario:
                        "Goal: To create plots that are clear, honest, and impactful, you must first learn the principles of good design.",
                    },
                    {
                      id: "p3s4ss2",
                      title: "Foundational Plot Types",
                      description:
                        "Bar charts, histograms, scatter plots, and line plots.",
                      why: "To answer the most common analytical questions, you need to be proficient in creating bar charts (for comparisons), histograms (for distributions), scatter plots (for relationships), and line plots (for trends over time).",
                      tags: [
                        "Bar Charts",
                        "Histograms",
                        "Scatter Plots",
                        "Line Plots",
                      ],
                      scenario:
                        "Task: To answer the most common analytical questions (comparisons, distributions, relationships), you need to master the basic chart types.",
                    },
                    {
                      id: "p3s4ss3",
                      title: "Distributional Plots",
                      description:
                        "Violin plots, heatmaps, Q-Q plots, and boxplots.",
                      why: "To gain a deeper understanding of a variable's distribution, you'll use more advanced plots like violin plots and Q-Q plots.",
                      tags: ["Violin Plots", "Heatmaps", "Q-Q Plots"],
                      scenario:
                        "Goal: To gain a deeper understanding of a variable's distribution, you'll need more advanced plots like violin plots or Q-Q plots.",
                    },
                    {
                      id: "p3s4ss4",
                      title: "Advanced & Custom Plots",
                      description:
                        "Waterfall charts, bubble charts, and network plots.",
                      why: "For specialized analytical needs, you may need to create less common plots like waterfall charts or network graphs.",
                      tags: [
                        "Waterfall Charts",
                        "Bubble Charts",
                        "Network Plots",
                      ],
                      scenario:
                        "Condition: For specialized analytical needs, you may need to create less common plots like network graphs.",
                    },
                    {
                      id: "p3s4ss5",
                      title: "Complex Visualizations",
                      description: "Geospatial maps and multi-layered plots.",
                      why: "To show geographic patterns or layer multiple types of information, you will create geospatial maps or complex multi-layered plots.",
                      tags: ["Geospatial", "Maps", "Complex Plots"],
                      scenario:
                        "Task: To show geographic patterns or layer multiple types of information, you need to create maps or complex multi-layered plots.",
                    },
                    {
                      id: "p3s4ss6",
                      title: "Best Practices",
                      description:
                        "Ensuring clarity through labels, legends, and accessibility considerations.",
                      why: "To ensure your audience can understand your plot, you must always include clear labels, a descriptive title, and an easy-to-read legend.",
                      tags: ["Accessibility", "Clarity", "Labels"],
                      scenario:
                        "Task: Your plot contains the right data, but it's hard to understand. You must add the final layer of polish—titles, clear labels, and thoughtful design—to turn it into an effective piece of communication.",
                    },
                  ],
                },
                {
                  id: "p3s5",
                  title: "Storytelling & Communication",
                  icon: "fa-book-open",
                  description:
                    "Transform your analysis into compelling narratives and reports.",
                  why: "Your analysis is only valuable if you can convince others of its meaning and importance. This requires weaving your findings into a compelling narrative.",
                  tags: [
                    "Storytelling",
                    "Communication",
                    "Reporting",
                    "Dashboards",
                  ],
                  difficulty: "intermediate",
                  scenario:
                    "Your analysis is technically correct, but its value is lost unless you can translate your findings into a compelling story that persuades your audience.",
                  subsections: [
                    {
                      id: "p3s5ss1",
                      title: "Narrative with Data",
                      description:
                        "Framing questions and guiding an audience through the analysis.",
                      why: "To make your analysis persuasive, you should frame it around a central question and guide your audience through the evidence step-by-step.",
                      tags: ["Narrative", "Storytelling", "Audience"],
                      scenario:
                        "Goal: To make your analysis persuasive, you must frame it around a central question and guide your audience through the evidence step-by-step.",
                    },
                    {
                      id: "p3s5ss2",
                      title: "Visual Storytelling",
                      description:
                        "Sequencing plots to build a compelling narrative.",
                      why: "To build a strong argument, you should carefully sequence your visualizations to build on one another and tell a coherent story.",
                      tags: ["Visual Storytelling", "Sequencing"],
                      scenario:
                        "Task: To build a strong argument, you need to carefully sequence your visualizations so they build on one another to tell a coherent story.",
                    },
                    {
                      id: "p3s5ss3",
                      title: "Reporting & Sharing",
                      description:
                        "Integrating plots into RMarkdown/Quarto and creating interactive dashboards.",
                      why: "To deliver your final story, you must integrate your plots and narrative into a polished report or an interactive dashboard.",
                      tags: ["RMarkdown", "Quarto", "Shiny", "Dashboards"],
                      scenario:
                        "Task: To deliver your final story, you must integrate your plots and narrative into a polished report or an interactive dashboard.",
                    },
                  ],
                },
              ],
            },
            {
              id: "phase4",
              title: "Phase 4: Modeling",
              icon: "fa-robot",
              description:
                "With a clean, well-understood dataset, the goal is now to select and apply the appropriate statistical or machine learning algorithms to uncover insights, make predictions, or discover hidden structures.",
              scenario:
                "Your data is clean and understood. The goal is now to select and apply the right algorithm to uncover insights, make predictions, or discover hidden structures.",
              sections: [
                {
                  id: "p4s1",
                  title: "Regression Modeling",
                  icon: "fa-chart-line",
                  description:
                    "Predict continuous numerical outcomes using various regression techniques.",
                  why: "When your goal is to predict a continuous numerical value (e.g., price, temperature, sales), you will use regression models.",
                  tags: [
                    "Regression",
                    "Linear Models",
                    "GLM",
                    "Regularization",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "Your primary goal is to predict a continuous numerical outcome. You need to select the appropriate regression technique for your data's characteristics.",
                  subsections: [
                    {
                      id: "p4s1ss1",
                      title: "Simple & Multiple Linear Regression",
                      description:
                        "Using lm(), interpreting coefficients, and validating assumptions.",
                      why: "To model a linear relationship between one or more predictor variables and a numerical outcome, you'll start with `lm()`.",
                      tags: ["Linear Regression", "lm()", "Coefficients"],
                      scenario:
                        "Condition: You assume a linear relationship between your predictors and the outcome.",
                    },
                    {
                      id: "p4s1ss2",
                      title: "Generalized Linear Models (GLM)",
                      description:
                        "Logistic, Poisson, and binomial regression using glm().",
                      why: "When your outcome is not normally distributed (e.g., counts or proportions), you'll use a GLM like Poisson or logistic regression.",
                      tags: ["GLM", "Logistic", "Poisson"],
                      scenario:
                        "Condition: Your outcome is not normally distributed (e.g., it's a count or a proportion).",
                    },
                    {
                      id: "p4s1ss3",
                      title: "Polynomial Regression",
                      description:
                        "Modeling non-linear trends with higher-order terms.",
                      why: "When the relationship between your predictor and outcome is curved, you can model it by adding polynomial terms (e.g., x^2) to a linear model.",
                      tags: ["Polynomial", "Non-linear"],
                      scenario:
                        "Condition: You observe a curved, non-linear trend in your data.",
                    },
                    {
                      id: "p4s1ss4",
                      title: "Regularized Regression",
                      description:
                        "Using Ridge (L2), Lasso (L1), and Elastic Net for variable selection and shrinkage.",
                      why: "When you have many predictors and want to prevent overfitting or perform automatic feature selection, you'll use Ridge (L2) or Lasso (L1) regression.",
                      tags: ["Ridge", "Lasso", "Elastic Net", "Regularization"],
                      scenario:
                        "Problem: You have many predictors and want to prevent overfitting or perform automatic feature selection.",
                    },
                    {
                      id: "p4s1ss5",
                      title: "Robust & Quantile Regression",
                      description:
                        "Handling outliers and modeling conditional quantiles.",
                      why: "If your data has outliers that violate the assumptions of linear regression, you can use robust regression. To predict a specific quantile instead of the mean, you use quantile regression.",
                      tags: ["Robust Regression", "Quantile Regression"],
                      scenario:
                        "Problem: Your data has significant outliers, or you need to predict a specific quantile instead of the mean.",
                    },
                    {
                      id: "p4s1ss6",
                      title: "Model Selection",
                      description: "Using stepwise selection methods.",
                      why: "To automatically select the best subset of predictors from a larger set, you can use methods like stepwise selection (though with caution).",
                      tags: ["Model Selection", "Stepwise"],
                      scenario:
                        "Task: You need a systematic way to choose the best subset of predictors from a larger set.",
                    },
                    {
                      id: "p4s1ss7",
                      title: "Diagnostics & Validation",
                      description:
                        "Analyzing residuals, leverage, influence, and VIF.",
                      why: "To check if your linear model is trustworthy, you must analyze its residuals and check for issues like multicollinearity (using VIF).",
                      tags: ["Diagnostics", "Residuals", "VIF"],
                      scenario:
                        "Task: You must check if your trained model is trustworthy and meets its statistical assumptions.",
                    },
                  ],
                },
                {
                  id: "p4s2",
                  title: "Classification",
                  icon: "fa-tags",
                  description:
                    "Predict discrete categories using various classification algorithms.",
                  why: "When your goal is to predict a categorical outcome (e.g., 'spam' or 'not spam', 'cat' or 'dog', customer 'churn' or 'no churn'), you will use classification models.",
                  tags: [
                    "Classification",
                    "Logistic Regression",
                    "Decision Trees",
                    "SVM",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "Your primary goal is to predict a discrete categorical outcome. You need to select the right algorithm for the job.",
                  subsections: [
                    {
                      id: "p4s2ss1",
                      title: "Logistic Regression",
                      description: "Binary and multiclass classification.",
                      why: "For a simple, interpretable baseline model for binary (Yes/No) classification, logistic regression is the standard starting point.",
                      tags: ["Logistic Regression", "Binary", "Multiclass"],
                      scenario:
                        "Goal: You need a simple, interpretable baseline model for a binary (Yes/No) outcome.",
                    },
                    {
                      id: "p4s2ss2",
                      title: "Tree-Based Methods",
                      description:
                        "Decision trees, random forests, and gradient boosting.",
                      why: "When you need a more complex, non-linear model that often provides higher accuracy, you will use decision trees, random forests, or gradient boosting.",
                      tags: [
                        "Decision Trees",
                        "Random Forest",
                        "Gradient Boosting",
                      ],
                      scenario:
                        "Goal: You need a more complex, non-linear model that often provides higher accuracy.",
                    },
                    {
                      id: "p4s2ss3",
                      title: "Discriminant Analysis",
                      description: "Linear (LDA) and Quadratic (QDA).",
                      why: "As a simple probabilistic alternative to logistic regression, you might use Linear (LDA) or Quadratic (QDA) Discriminant Analysis.",
                      tags: ["LDA", "QDA", "Discriminant Analysis"],
                      scenario:
                        "Goal: You need a simple probabilistic model, often used when the assumptions of logistic regression are met.",
                    },
                    {
                      id: "p4s2ss4",
                      title: "Support Vector Machines (SVM)",
                      description: "Finding optimal separating hyperplanes.",
                      why: "To find the optimal boundary (hyperplane) that separates different classes, you'll use an SVM, which is particularly effective in high-dimensional spaces.",
                      tags: ["SVM", "Support Vector Machines", "Hyperplanes"],
                      scenario:
                        "Goal: To find the optimal boundary that separates classes, especially in high-dimensional space.",
                    },
                    {
                      id: "p4s2ss5",
                      title: "Naive Bayes",
                      description:
                        "Probabilistic classification based on Bayes' theorem.",
                      why: "For text classification or when you have a very large number of features and need a fast, simple model, Naive Bayes is a good choice.",
                      tags: ["Naive Bayes", "Probabilistic", "Bayes"],
                      scenario:
                        "Condition: You are working with text classification or have a very high number of features and need a fast model.",
                    },
                    {
                      id: "p4s2ss6",
                      title: "k-Nearest Neighbors (k-NN)",
                      description: "Instance-based learning.",
                      why: "When you want a simple, instance-based model that classifies a new point based on the 'votes' of its closest neighbors, you'll use k-NN.",
                      tags: ["k-NN", "Instance-based", "Neighbors"],
                      scenario:
                        "Goal: You need a simple, non-parametric model that classifies new points based on their proximity to known data points.",
                    },
                    {
                      id: "p4s2ss7",
                      title: "Model Evaluation",
                      description:
                        "Using confusion matrix, ROC/AUC, precision, recall, and F1-score.",
                      why: "To understand how well your classification model is performing, you must use a confusion matrix and metrics derived from it, like precision, recall, and the ROC/AUC curve.",
                      tags: [
                        "Confusion Matrix",
                        "ROC",
                        "AUC",
                        "Precision",
                        "Recall",
                      ],
                      scenario:
                        "Task: After training a model, you must use the correct metrics (e.g., confusion matrix, ROC/AUC) to assess its performance.",
                    },
                    {
                      id: "p4s2ss8",
                      title: "Best Practices",
                      description:
                        "Handling class imbalance and using cross-validation.",
                      why: "If one class is much rarer than another (class imbalance), you need to use specific strategies during training and evaluation to build a useful model.",
                      tags: ["Class Imbalance", "Cross-Validation"],
                      scenario:
                        "Problem: One of your classes is much rarer than the other. You need to apply techniques to handle this class imbalance.",
                    },
                  ],
                },
                {
                  id: "p4s3",
                  title: "Clustering & Unsupervised Learning",
                  icon: "fa-project-diagram",
                  description:
                    "Discover hidden patterns in data without labeled outcomes.",
                  why: "When you don't have a specific outcome to predict but want to discover natural groupings or hidden structures within your data, you will use unsupervised learning.",
                  tags: [
                    "Clustering",
                    "Unsupervised Learning",
                    "k-Means",
                    "Hierarchical",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "You have a dataset with no predefined outcome variable. Your goal is to explore the data and discover natural, hidden groupings or structures within it.",
                  subsections: [
                    {
                      id: "p4s3ss1",
                      title: "k-Means Clustering",
                      description: "Partitioning data into k clusters.",
                      why: "To partition your data into a pre-specified number (k) of distinct, non-overlapping groups, k-Means is a fast and popular choice.",
                      tags: ["k-Means", "Clustering", "Partitioning"],
                      scenario:
                        "Goal: To partition your data into a specific number (k) of distinct, non-overlapping groups based on distance.",
                    },
                    {
                      id: "p4s3ss2",
                      title: "Hierarchical Clustering",
                      description:
                        "Building a hierarchy of clusters represented by a dendrogram.",
                      why: "When you don't know the number of clusters in advance and want to see a hierarchy of groupings, you'll use hierarchical clustering and visualize it with a dendrogram.",
                      tags: ["Hierarchical", "Dendrogram", "Agglomerative"],
                      scenario:
                        "Condition: You don't know the number of clusters beforehand and want to see a hierarchy of potential groupings.",
                    },
                    {
                      id: "p4s3ss3",
                      title: "Density-Based Clustering",
                      description: "Using methods like DBSCAN.",
                      why: "To find arbitrarily shaped clusters and identify noise points that don't belong to any group, you can use an algorithm like DBSCAN.",
                      tags: ["DBSCAN", "Density-based", "Clustering"],
                      scenario:
                        "Goal: To find arbitrarily shaped clusters and identify points that don't belong to any group (noise).",
                    },
                    {
                      id: "p4s3ss4",
                      title: "Model-Based Clustering",
                      description: "Using Gaussian Mixture Models.",
                      why: "When you believe your data comes from a mix of different underlying probability distributions, you can use Gaussian Mixture Models to find these groups.",
                      tags: ["Gaussian Mixture", "Model-based"],
                      scenario:
                        "Hypothesis: You believe your data is a mixture of several underlying probability distributions.",
                    },
                    {
                      id: "p4s3ss5",
                      title: "Cluster Validation",
                      description:
                        "Evaluating cluster quality with silhouette score, elbow method, and gap statistic.",
                      why: "To determine the optimal number of clusters or evaluate how good your clusters are, you'll use techniques like the elbow method or the silhouette score.",
                      tags: ["Validation", "Silhouette", "Elbow Method"],
                      scenario:
                        "Task: After running a clustering algorithm, you need a way to determine the optimal number of clusters and evaluate their quality.",
                    },
                    {
                      id: "p4s3ss6",
                      title: "Best Practices",
                      description:
                        "Standardizing features and interpreting cluster meaning.",
                      why: "Because clustering algorithms are often based on distance, you must standardize your features before applying them.",
                      tags: ["Standardization", "Interpretation"],
                      scenario:
                        "Task: You must properly prepare your data (e.g., by standardizing features) before clustering.",
                    },
                  ],
                },
                {
                  id: "p4s4",
                  title: "Association Rule Mining",
                  icon: "fa-link",
                  description:
                    "Discover interesting relationships between variables in large databases.",
                  why: "To discover interesting relationships or 'rules' between items in large datasets, such as what products are frequently purchased together in a supermarket.",
                  tags: ["Association Rules", "Market Basket", "Apriori"],
                  difficulty: "advanced",
                  scenario:
                    'You have a large set of transactional data (e.g., items bought together in a store). Your goal is to discover interesting "if-then" rules and relationships between items.',
                  subsections: [
                    {
                      id: "p4s4ss1",
                      title: "Apriori & Eclat Algorithms",
                      description:
                        "Finding frequent itemsets in transactional data.",
                      why: "To efficiently find sets of items that frequently occur together in transactional data, you'll use algorithms like Apriori.",
                      tags: ["Apriori", "Eclat", "Frequent Itemsets"],
                      scenario:
                        "Task: To efficiently find sets of items that frequently occur together in your transactional data.",
                    },
                    {
                      id: "p4s4ss2",
                      title: "Rule Evaluation",
                      description:
                        "Interpreting support, confidence, and lift.",
                      why: "To decide if a rule like 'If a customer buys bread, they are likely to buy milk' is meaningful, you will interpret metrics like support, confidence, and lift.",
                      tags: ["Support", "Confidence", "Lift"],
                      scenario:
                        "Task: After finding rules, you must evaluate their strength and importance using metrics like support, confidence, and lift.",
                    },
                    {
                      id: "p4s4ss3",
                      title: "Applications",
                      description:
                        "Market basket analysis and recommendation systems.",
                      why: "The classic application is Market Basket Analysis, but it can also be used for web usage mining or recommendation systems.",
                      tags: ["Market Basket", "Recommendations"],
                      scenario:
                        "Goal: To apply your findings to real-world problems like market basket analysis or building recommendation systems.",
                    },
                  ],
                },
                {
                  id: "p4s5",
                  title: "Ensemble Methods",
                  icon: "fa-layer-group",
                  description:
                    "Combine multiple models to improve predictive performance.",
                  why: "To improve predictive performance and create more robust models, you can combine the predictions of multiple individual models (weak learners) into one powerful model (strong learner).",
                  tags: ["Ensemble", "Bagging", "Boosting", "Stacking"],
                  difficulty: "advanced",
                  scenario:
                    "A single predictive model is not giving you the performance you need. Your goal is to improve accuracy and robustness by combining the predictions of multiple models.",
                  subsections: [
                    {
                      id: "p4s5ss1",
                      title: "Bagging",
                      description:
                        "Bootstrap aggregating, with a focus on random forests.",
                      why: "To reduce variance and overfitting, you will use bagging (Bootstrap Aggregating), with the most famous example being the Random Forest algorithm.",
                      tags: ["Bagging", "Random Forest", "Bootstrap"],
                      scenario:
                        "Goal: To reduce the variance of a model and prevent overfitting by training multiple models on different bootstrap samples of the data.",
                    },
                    {
                      id: "p4s5ss2",
                      title: "Boosting",
                      description:
                        "AdaBoost, Gradient Boosting, XGBoost, and LightGBM.",
                      why: "To build a powerful model by sequentially training models that correct the errors of their predecessors, you will use boosting algorithms like XGBoost and LightGBM.",
                      tags: ["Boosting", "XGBoost", "AdaBoost", "LightGBM"],
                      scenario:
                        "Goal: To build a powerful model by training a sequence of models, where each new model focuses on correcting the errors of the previous one.",
                    },
                    {
                      id: "p4s5ss3",
                      title: "Stacking & Blending",
                      description:
                        "Combining multiple models to improve performance.",
                      why: "To combine the predictions of several different types of models to get the best of all worlds, you use stacking.",
                      tags: ["Stacking", "Blending", "Meta-learning"],
                      scenario:
                        "Goal: To combine the predictions of several different types of models (e.g., a linear model and a tree-based model) to get the best of all worlds.",
                    },
                    {
                      id: "p4s5ss4",
                      title: "Voting Classifiers",
                      description: "Using majority vote to make predictions.",
                      why: "The simplest way to combine models is to have them 'vote' on the final prediction, using either a hard (majority) or soft (probability-weighted) vote.",
                      tags: ["Voting", "Majority Vote"],
                      scenario:
                        'Task: You need a simple way to combine the outputs of several classification models by letting them "vote" on the final outcome.',
                    },
                    {
                      id: "p4s5ss5",
                      title: "Best Practices",
                      description:
                        "Tuning ensemble parameters to avoid overfitting.",
                      why: "While powerful, ensemble models are prone to overfitting, so careful tuning of their parameters using cross-validation is essential.",
                      tags: ["Parameter Tuning", "Overfitting"],
                      scenario:
                        "Problem: Ensemble models are very powerful but can easily overfit. You must learn how to tune their parameters carefully.",
                    },
                  ],
                },
                {
                  id: "p4s6",
                  title: "Deep Learning & Neural Networks",
                  icon: "fa-brain",
                  description:
                    "Build neural networks for complex pattern recognition.",
                  why: "To tackle complex pattern recognition problems with very large datasets, such as image classification or natural language understanding, you will use neural networks.",
                  tags: [
                    "Deep Learning",
                    "Neural Networks",
                    "Keras",
                    "TensorFlow",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "You are tackling a complex, non-linear pattern recognition problem with a very large dataset, such as classifying images or understanding natural language.",
                  subsections: [
                    {
                      id: "p4s6ss1",
                      title: "Neural Network Basics",
                      description:
                        "Architecture, activation functions, and forward/backpropagation.",
                      why: "To understand how deep learning works, you must first learn the basic architecture of a neuron, layers, and the process of forward/backpropagation.",
                      tags: [
                        "Neural Networks",
                        "Backpropagation",
                        "Activation",
                      ],
                      scenario:
                        "Task: To begin, you must first understand the basic architecture of a neuron, layers, and how networks learn via backpropagation.",
                    },
                    {
                      id: "p4s6ss2",
                      title: "Feedforward Neural Networks (FNN)",
                      description: "Basic network structures.",
                      why: "For basic regression or classification tasks on tabular data, you can use a standard fully-connected feedforward network.",
                      tags: ["Feedforward", "FNN", "Basic Networks"],
                      scenario:
                        "Condition: Your task is a standard regression or classification problem on tabular data.",
                    },
                    {
                      id: "p4s6ss3",
                      title: "Convolutional Neural Networks (CNNs)",
                      description: "For image and spatial data.",
                      why: "When your data has a grid-like topology, such as an image, you will use CNNs, which are specialized for finding spatial patterns.",
                      tags: ["CNN", "Convolutional", "Images"],
                      scenario:
                        "Condition: Your input data has a grid-like structure, such as an image.",
                    },
                    {
                      id: "p4s6ss4",
                      title: "Recurrent Neural Networks (RNNs)",
                      description: "LSTMs and GRUs for sequential data.",
                      why: "For sequential data where order matters, like time series or text, you'll use RNNs (or more advanced variants like LSTMs) to capture temporal dependencies.",
                      tags: ["RNN", "LSTM", "GRU", "Sequential"],
                      scenario:
                        "Condition: Your data is sequential, where the order of information matters, like text or time series.",
                    },
                    {
                      id: "p4s6ss5",
                      title: "Training & Optimization",
                      description:
                        "Loss functions, optimizers, regularization, and dropout.",
                      why: "To train a neural network effectively, you must choose the right loss function, optimizer, and regularization techniques (like dropout) to prevent overfitting.",
                      tags: [
                        "Training",
                        "Optimization",
                        "Dropout",
                        "Regularization",
                      ],
                      scenario:
                        "Task: You need to select the right components (loss functions, optimizers) and techniques (regularization) to train your network effectively.",
                    },
                    {
                      id: "p4s6ss6",
                      title: "Best Practices",
                      description:
                        "Using data augmentation and early stopping.",
                      why: "When your image dataset is small, you can use data augmentation (e.g., rotating or flipping images) to artificially increase its size and improve model robustness.",
                      tags: ["Data Augmentation", "Early Stopping"],
                      scenario:
                        "Problem: Your neural network is overfitting. You need techniques like data augmentation or early stopping to mitigate this.",
                    },
                  ],
                },
                {
                  id: "p4s7",
                  title: "Time Series Analysis & Forecasting",
                  icon: "fa-chart-line",
                  description:
                    "Analyze temporal data and build forecasting models.",
                  why: "When your data is collected sequentially over time (e.g., daily stock prices, monthly sales) and you want to understand its patterns or predict future values, you'll use time series analysis.",
                  tags: ["Time Series", "Forecasting", "ARIMA", "prophet"],
                  difficulty: "advanced",
                  scenario:
                    "Your data consists of observations collected sequentially over time. Your goal is to understand its underlying patterns and/or predict future values.",
                  subsections: [
                    {
                      id: "p4s7ss1",
                      title: "Time Series Decomposition",
                      description:
                        "Identifying trend, seasonality, and residuals.",
                      why: "To understand the underlying structure of your time series, you first decompose it into its trend, seasonal, and residual components.",
                      tags: ["Decomposition", "Trend", "Seasonality"],
                      scenario:
                        "Task: Your first step is to break down the time series into its core components: trend, seasonality, and random noise.",
                    },
                    {
                      id: "p4s7ss2",
                      title: "Stationarity",
                      description:
                        "Testing for and achieving stationarity through differencing.",
                      why: "Before applying many time series models, you must check if the series is stationary and, if not, use techniques like differencing to make it so.",
                      tags: ["Stationarity", "Differencing"],
                      scenario:
                        "Problem: Many time series models assume the data is stationary. You must test for this and apply transformations if it is not.",
                    },
                    {
                      id: "p4s7ss3",
                      title: "Autoregressive Models",
                      description: "AR, MA, ARMA, ARIMA, and SARIMA.",
                      why: "To make forecasts based on the series' own past values and past errors, you'll use models from the ARIMA family.",
                      tags: ["ARIMA", "AR", "MA", "SARIMA"],
                      scenario:
                        "Goal: To make forecasts based on the series' own past values and past forecast errors.",
                    },
                    {
                      id: "p4s7ss4",
                      title: "Exponential Smoothing",
                      description: "Simple and Holt-Winters methods.",
                      why: "As a popular and effective alternative to ARIMA models, you can use exponential smoothing methods like Holt-Winters to capture trend and seasonality.",
                      tags: ["Exponential Smoothing", "Holt-Winters"],
                      scenario:
                        "Goal: You need a flexible and widely used forecasting method that captures trend and seasonality.",
                    },
                    {
                      id: "p4s7ss5",
                      title: "Anomaly Detection",
                      description:
                        "Identifying unusual points in time series data.",
                      why: "To identify unusual spikes or dips in your time series data that may represent important events, you'll use anomaly detection techniques.",
                      tags: ["Anomaly Detection", "Outliers"],
                      scenario:
                        "Task: To identify unusual spikes or dips in your time series that may represent important real-world events.",
                    },
                    {
                      id: "p4s7ss6",
                      title: "Best Practices",
                      description:
                        "Visualizing the series and using backtesting for validation.",
                      why: "To properly evaluate a forecasting model, you must use a method that respects the temporal order of the data, such as backtesting or a walk-forward validation.",
                      tags: ["Visualization", "Backtesting"],
                      scenario:
                        "Task: To evaluate your forecasting model correctly, you must use validation techniques that respect the temporal order of the data.",
                    },
                  ],
                },
                {
                  id: "p4s8",
                  title: "Natural Language Processing (NLP)",
                  icon: "fa-comments",
                  description:
                    "Analyze and understand text data using NLP techniques.",
                  why: "To enable computers to understand, interpret, and generate human language, you'll apply NLP techniques to text and speech data.",
                  tags: [
                    "NLP",
                    "Text Mining",
                    "Sentiment Analysis",
                    "Topic Modeling",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "Your data is unstructured human language (text). Your goal is to apply computational techniques to analyze, understand, and extract insights from this text.",
                  subsections: [
                    {
                      id: "p4s8ss1",
                      title: "Text Preprocessing",
                      description:
                        "Tokenization, stemming, lemmatization, and stopword removal.",
                      why: "Before you can model text, you must clean and standardize it through tokenization (splitting into words), stemming/lemmatization (reducing words to their root form), and stopword removal.",
                      tags: ["Tokenization", "Stemming", "Lemmatization"],
                      scenario:
                        "Task: Before analysis, you must clean and standardize your raw text data.",
                    },
                    {
                      id: "p4s8ss2",
                      title: "Feature Extraction",
                      description: "Bag-of-words, TF-IDF, and word embeddings.",
                      why: "To convert text into a numerical format that models can understand, you'll use techniques like bag-of-words, TF-IDF, or more advanced word embeddings.",
                      tags: ["Bag-of-words", "TF-IDF", "Word Embeddings"],
                      scenario:
                        "Problem: Your models need numeric input. You must convert the text into a numerical representation (e.g., TF-IDF or word embeddings).",
                    },
                    {
                      id: "p4s8ss3",
                      title: "Text Classification",
                      description:
                        "Applying models like Naive Bayes or deep learning to text.",
                      why: "To categorize text into predefined groups (e.g., sentiment analysis, topic labeling), you'll apply classification models to its numerical representation.",
                      tags: ["Text Classification", "Naive Bayes"],
                      scenario:
                        "Goal: To assign predefined categories to text documents (e.g., spam detection, topic labeling).",
                    },
                    {
                      id: "p4s8ss4",
                      title: "Topic Modeling",
                      description:
                        "Discovering latent topics with LDA and NMF.",
                      why: "To discover the latent topics or themes present in a large collection of documents without any pre-labeled data, you'll use unsupervised methods like LDA.",
                      tags: ["Topic Modeling", "LDA", "NMF"],
                      scenario:
                        "Goal: To discover the latent, underlying topics in a large collection of documents without any pre-labeled data.",
                    },
                    {
                      id: "p4s8ss5",
                      title: "Sentiment Analysis",
                      description: "Determining the sentiment of text.",
                      why: "To determine the emotional tone (positive, negative, neutral) of a piece of text, you will build a sentiment analysis model.",
                      tags: ["Sentiment Analysis", "Polarity"],
                      scenario:
                        "Goal: To determine the emotional tone (positive, negative, neutral) of a piece of text.",
                    },
                    {
                      id: "p4s8ss6",
                      title: "Sequence Models",
                      description: "Using RNNs and transformers for NLP tasks.",
                      why: "For more advanced NLP tasks that require understanding context and word order (like machine translation), you'll use sequence models like RNNs or transformers.",
                      tags: ["Sequence Models", "Transformers", "RNN"],
                      scenario:
                        "Condition: Your NLP task requires understanding the context and order of words (e.g., machine translation, text generation).",
                    },
                  ],
                },
                {
                  id: "p4s9",
                  title: "Survival Analysis",
                  icon: "fa-hourglass-half",
                  description:
                    "Analyze time-to-event data and model survival functions.",
                  why: "When your goal is to analyze the time until an event of interest occurs (e.g., time until equipment failure, time until customer churn, time until patient recovery), you will use survival analysis.",
                  tags: ["Survival Analysis", "Kaplan-Meier", "Cox Regression"],
                  difficulty: "advanced",
                  scenario:
                    'Your analysis is not about if an event will happen, but when. Your goal is to analyze "time-to-event" data, even when some events haven\'t happened yet.',
                  subsections: [
                    {
                      id: "p4s9ss1",
                      title: "Core Concepts",
                      description:
                        "Understanding survival and hazard functions.",
                      why: "To understand this field, you must first grasp the core ideas of survival and hazard functions, and the concept of 'censoring'.",
                      tags: ["Survival Function", "Hazard Function"],
                      scenario:
                        "Task: First, you must understand the key ideas of survival functions, hazard functions, and censored data.",
                    },
                    {
                      id: "p4s9ss2",
                      title: "Kaplan-Meier Estimator",
                      description: "Non-parametric analysis of survival data.",
                      why: "To get a non-parametric estimate of the survival probability over time, you will create a Kaplan-Meier curve.",
                      tags: ["Kaplan-Meier", "Non-parametric"],
                      scenario:
                        "Goal: To get a non-parametric estimate and visualization of the survival probability over time.",
                    },
                    {
                      id: "p4s9ss3",
                      title: "Cox Proportional Hazards Model",
                      description:
                        "Semi-parametric regression for survival data.",
                      why: "To assess the effect of several predictor variables on survival time, the Cox model is the most widely used semi-parametric regression method.",
                      tags: ["Cox Regression", "Proportional Hazards"],
                      scenario:
                        "Goal: To assess the effect of several predictor variables on the survival time.",
                    },
                    {
                      id: "p4s9ss4",
                      title: "Parametric Survival Models",
                      description:
                        "Assuming specific distributions for survival times.",
                      why: "If you have a reason to believe the survival times follow a specific statistical distribution, you can use a parametric model.",
                      tags: ["Parametric", "Distributions"],
                      scenario:
                        "Hypothesis: You believe the survival times follow a specific statistical distribution.",
                    },
                    {
                      id: "p4s9ss5",
                      title: "Competing Risks",
                      description:
                        "Modeling events that prevent the primary event of interest.",
                      why: "When subjects can experience different types of events that prevent the primary event from happening, you need to use competing risks analysis.",
                      tags: ["Competing Risks", "Multiple Events"],
                      scenario:
                        "Problem: Your subjects can experience different types of events that prevent the primary event from occurring.",
                    },
                    {
                      id: "p4s9ss6",
                      title: "Best Practices",
                      description:
                        "Checking proportional hazards assumption and handling censored data.",
                      why: "A key assumption of the Cox model is proportional hazards; you must test this assumption to ensure your model is valid.",
                      tags: ["Assumptions", "Censoring"],
                      scenario:
                        "Task: You must check the assumptions of your model (like proportional hazards) to ensure your results are valid.",
                    },
                  ],
                },
                {
                  id: "p4s10",
                  title: "Causal Inference",
                  icon: "fa-arrow-right",
                  description:
                    "Estimate cause-and-effect relationships from observational data.",
                  why: "When you want to go beyond correlation and understand the true causal effect of an intervention (e.g., 'Did our marketing campaign *cause* an increase in sales?'), you need the tools of causal inference.",
                  tags: ["Causal Inference", "Propensity Score", "IV", "DiD"],
                  difficulty: "advanced",
                  scenario:
                    "You see a strong correlation between two variables, but you need to go further and determine if one variable causes a change in the other, especially with observational data.",
                  subsections: [
                    {
                      id: "p4s10ss1",
                      title: "The Potential Outcomes Framework",
                      description:
                        "Contrasting experimental and observational data.",
                      why: "To think rigorously about causality, you must understand the framework of potential outcomes, which distinguishes between what happened and what *would have happened*.",
                      tags: ["Potential Outcomes", "Experimental"],
                      scenario:
                        "Task: To think rigorously about causality, you must first learn the framework for contrasting what happened with what would have happened.",
                    },
                    {
                      id: "p4s10ss2",
                      title: "Matching Methods",
                      description: "Propensity Score Matching.",
                      why: "In observational studies, you can use methods like Propensity Score Matching to create a comparable 'control' group for your 'treatment' group.",
                      tags: ["Propensity Score", "Matching"],
                      scenario:
                        "Problem: You can't run a randomized experiment. You need to create a comparable control group from observational data.",
                    },
                    {
                      id: "p4s10ss3",
                      title: "Quasi-Experimental Methods",
                      description:
                        "Instrumental Variables, Difference-in-Differences, and Regression Discontinuity.",
                      why: "To estimate causal effects in non-experimental settings, you can leverage specific study designs like Instrumental Variables or Difference-in-Differences.",
                      tags: ["IV", "DiD", "RD", "Quasi-experimental"],
                      scenario:
                        "Condition: You can't randomize, but your study has a specific design (e.g., a before-and-after comparison, a sharp cutoff) that you can leverage.",
                    },
                    {
                      id: "p4s10ss4",
                      title: "Mediation Analysis",
                      description: "Understanding causal pathways.",
                      why: "To understand the pathway through which a cause leads to an effect, you can perform a mediation analysis.",
                      tags: ["Mediation", "Causal Pathways"],
                      scenario:
                        "Goal: To understand the causal pathway through which a cause leads to an effect.",
                    },
                    {
                      id: "p4s10ss5",
                      title: "Best Practices",
                      description:
                        "Clearly stating assumptions and performing sensitivity analysis.",
                      why: "Causal claims rely heavily on untestable assumptions; you must clearly state these assumptions and perform sensitivity analyses to see how your conclusions would change if they were violated.",
                      tags: ["Assumptions", "Sensitivity Analysis"],
                      scenario:
                        "Task: You must clearly state the untestable assumptions your causal claim relies on and perform sensitivity analyses.",
                    },
                  ],
                },
                {
                  id: "p4s11",
                  title: "Bayesian Modeling",
                  icon: "fa-balance-scale-right",
                  description:
                    "Apply Bayesian methods for probabilistic modeling and inference.",
                  why: "When you want to incorporate prior knowledge into your model and get a full probability distribution for your estimates rather than a single point estimate, you will use Bayesian methods.",
                  tags: ["Bayesian", "MCMC", "brms", "rstanarm"],
                  difficulty: "advanced",
                  scenario:
                    "You want to move beyond traditional frequentist statistics to a framework where you can incorporate prior knowledge and get a full probability distribution for your model's parameters.",
                  subsections: [
                    {
                      id: "p4s11ss1",
                      title: "Bayesian Inference",
                      description:
                        "Priors, likelihood, posterior, and MCMC methods.",
                      why: "To perform Bayesian analysis, you must understand how to combine your prior beliefs with your data to form an updated belief (the posterior), often using MCMC methods.",
                      tags: ["Bayesian", "MCMC", "Posterior", "Prior"],
                      scenario:
                        "Task: You must understand the core logic of combining your prior beliefs with data (likelihood) to form an updated belief (posterior).",
                    },
                    {
                      id: "p4s11ss2",
                      title: "Bayesian Regression & Classification",
                      description:
                        "Applying Bayesian principles to standard models.",
                      why: "You can apply the Bayesian framework to standard models to get more intuitive outputs, like the probability that a coefficient is positive.",
                      tags: ["Bayesian Regression", "Bayesian Classification"],
                      scenario:
                        "Goal: To apply the Bayesian framework to standard models to get more intuitive outputs.",
                    },
                    {
                      id: "p4s11ss3",
                      title: "Hierarchical Models",
                      description:
                        "Modeling nested or grouped data structures.",
                      why: "When your data is grouped or has a nested structure (e.g., students within schools), hierarchical Bayesian models are a natural and powerful way to model it.",
                      tags: ["Hierarchical", "Multilevel", "Nested"],
                      scenario:
                        "Condition: Your data has a nested or grouped structure (e.g., students within schools) that you want to model appropriately.",
                    },
                    {
                      id: "p4s11ss4",
                      title: "Model Checking & Diagnostics",
                      description: "Assessing model fit and MCMC convergence.",
                      why: "To ensure your Bayesian model is reliable, you must perform posterior predictive checks and diagnose the convergence of your MCMC chains.",
                      tags: ["Model Checking", "Convergence", "Diagnostics"],
                      scenario:
                        "Task: You must perform checks to ensure your Bayesian model has converged and fits the data well.",
                    },
                  ],
                },
                {
                  id: "p4s12",
                  title: "Network Analysis",
                  icon: "fa-network-wired",
                  description:
                    "Analyze and visualize network and graph structures.",
                  why: "When your data represents relationships and connections between entities (e.g., a social network, a protein interaction network), you will use network analysis to understand its structure and dynamics.",
                  tags: [
                    "Network Analysis",
                    "Graph Theory",
                    "igraph",
                    "ggraph",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "Your data doesn't represent independent entities, but rather a set of entities and the relationships between them (e.g., a social network, a power grid).",
                  subsections: [
                    {
                      id: "p4s12ss1",
                      title: "Graph Theory Basics",
                      description: "Nodes, edges, and adjacency matrices.",
                      why: "To work with network data, you must understand the basic components: nodes (entities) and edges (connections).",
                      tags: ["Graph Theory", "Nodes", "Edges"],
                      scenario:
                        "Task: To work with network data, you must first understand its basic components: nodes and edges.",
                    },
                    {
                      id: "p4s12ss2",
                      title: "Network Metrics",
                      description:
                        "Centrality, clustering coefficient, and path length.",
                      why: "To characterize a network or identify important nodes, you will calculate metrics like centrality (who is most influential?) and clustering coefficient.",
                      tags: ["Centrality", "Clustering Coefficient"],
                      scenario:
                        "Goal: To characterize the network or identify the most important or influential nodes within it.",
                    },
                    {
                      id: "p4s12ss3",
                      title: "Community Detection",
                      description:
                        "Using algorithms like Louvain and Girvan-Newman.",
                      why: "To find distinct clusters or communities within a large network, you'll use algorithms like Louvain.",
                      tags: ["Community Detection", "Louvain"],
                      scenario:
                        "Goal: To find distinct clusters or communities within a large network.",
                    },
                    {
                      id: "p4s12ss4",
                      title: "Applications",
                      description:
                        "Analyzing social, biological, and information networks.",
                      why: "You can apply these techniques to understand social dynamics, biological pathways, or the flow of information.",
                      tags: ["Social Networks", "Applications"],
                      scenario:
                        "Task: To apply these techniques to understand real-world systems like social dynamics or information flows.",
                    },
                  ],
                },
                {
                  id: "p4s13",
                  title: "Uplift Modeling",
                  icon: "fa-chart-bar",
                  description:
                    "Model the incremental impact of treatments or interventions.",
                  why: "When you want to target an intervention (like a marketing offer) only to those individuals who will be positively impacted by it, avoiding those who would have bought anyway or would be annoyed by the offer.",
                  tags: ["Uplift Modeling", "Incremental", "Treatment Effects"],
                  difficulty: "advanced",
                  scenario:
                    "Your goal is not to predict who will convert, but to predict who will convert because of your marketing intervention, allowing you to target your campaign more effectively.",
                  subsections: [
                    {
                      id: "p4s13ss1",
                      title: "Incremental Response Models",
                      description:
                        "Estimating the causal effect of an intervention.",
                      why: "The goal is not to predict who will buy, but who will buy *because* they received the offer. You need to estimate the causal uplift of the intervention.",
                      tags: ["Incremental", "Causal Effect"],
                      scenario:
                        "Task: To estimate the incremental (causal) effect of an intervention for each individual.",
                    },
                    {
                      id: "p4s13ss2",
                      title: "Model Evaluation",
                      description: "Using the Qini curve and uplift curve.",
                      why: "To evaluate an uplift model, you cannot use standard classification metrics. Instead, you'll use tools like the Qini curve or uplift curve to measure the incremental gains.",
                      tags: ["Qini Curve", "Uplift Curve"],
                      scenario:
                        "Problem: Standard classification metrics don't work for uplift. You need specialized tools like the Qini curve.",
                    },
                    {
                      id: "p4s13ss3",
                      title: "Applications",
                      description:
                        "Optimizing marketing campaigns and personalized interventions.",
                      why: "This is critical for optimizing marketing campaigns, personalizing recommendations, and maximizing ROI on interventions.",
                      tags: ["Marketing", "Personalization"],
                      scenario:
                        "Goal: To optimize marketing campaigns and maximize return on investment.",
                    },
                  ],
                },
              ],
            },
            {
              id: "phase5",
              title: "Phase 5: Model Validation, Metrics & Ethics",
              icon: "fa-check-double",
              description:
                "After building a model, it is critical to rigorously evaluate its performance, ensure its predictions are reliable and unbiased, and understand the ethical implications of its use.",
              scenario:
                "You've built a model, but your work isn't done. You must now rigorously evaluate its performance, ensure it's fair and interpretable, and understand its ethical implications before it can be trusted.",
              sections: [
                {
                  id: "p5s1",
                  title: "Model Validation & Evaluation",
                  icon: "fa-clipboard-check",
                  description:
                    "Rigorously assess model performance using proper validation techniques.",
                  why: "To honestly assess how well your model will perform in the real world, you must test it on data it has never seen before.",
                  tags: [
                    "Cross-Validation",
                    "Model Selection",
                    "Overfitting",
                    "Hyperparameters",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "Your model performs perfectly on the data you used to train it. But how can you be sure it will generalize to new data in the real world?",
                  subsections: [
                    {
                      id: "p5s1ss1",
                      title: "Data Splitting Strategies",
                      description:
                        "Train/test split and the validation set approach.",
                      why: "The most basic way to validate a model is to split your data into a training set (for building the model) and a test set (for evaluating it).",
                      tags: ["Train-Test Split", "Validation Set"],
                      scenario:
                        "Task: The most basic way to validate a model is to split your data into a training set and a test set.",
                    },
                    {
                      id: "p5s1ss2",
                      title: "Cross-Validation",
                      description:
                        "k-fold, stratified k-fold, and leave-one-out (LOOCV).",
                      why: "To get a more robust estimate of model performance and reduce the impact of a 'lucky' or 'unlucky' train/test split, you will use k-fold cross-validation.",
                      tags: ["k-fold", "Stratified", "LOOCV"],
                      scenario:
                        "Goal: To get a more robust and reliable estimate of your model's performance by repeatedly splitting the data.",
                    },
                    {
                      id: "p5s1ss3",
                      title: "Resampling Methods",
                      description: "Bootstrapping for stability assessment.",
                      why: "To understand the stability and uncertainty of your model's performance estimates, you can use bootstrapping.",
                      tags: ["Bootstrap", "Resampling"],
                      scenario:
                        "Goal: To understand the stability and uncertainty of your model's performance estimates using techniques like bootstrapping.",
                    },
                    {
                      id: "p5s1ss4",
                      title: "Hyperparameter Tuning",
                      description:
                        "Grid search, random search, and Bayesian optimization.",
                      why: "To find the optimal settings (hyperparameters) for your model (e.g., the 'k' in k-NN), you will use systematic methods like grid search or random search.",
                      tags: ["Grid Search", "Bayesian Optimization"],
                      scenario:
                        "Task: To find the optimal settings (hyperparameters) for your chosen algorithm to maximize its performance.",
                    },
                    {
                      id: "p5s1ss5",
                      title: "Model Complexity",
                      description:
                        "Diagnosing and mitigating overfitting and underfitting.",
                      why: "You must diagnose whether your model is too simple (underfitting) or too complex (overfitting) and take steps to correct it.",
                      tags: ["Overfitting", "Underfitting", "Complexity"],
                      scenario:
                        "Problem: Your model is either too simple (underfitting) or too complex (overfitting). You need to diagnose and fix this.",
                    },
                    {
                      id: "p5s1ss6",
                      title: "The Bias-Variance Tradeoff",
                      description:
                        "Balancing model complexity and generalization error.",
                      why: "When building a model, you must understand the fundamental tradeoff between simple models (high bias, low variance) and complex models (low bias, high variance).",
                      tags: ["Bias-Variance", "Generalization"],
                      scenario:
                        "Decision: You must understand and balance the fundamental tradeoff between model simplicity (bias) and complexity (variance).",
                    },
                    {
                      id: "p5s1ss7",
                      title: "Model Selection Criteria",
                      description:
                        "Using information criteria like AIC and BIC.",
                      why: "When comparing different models, you can use information criteria like AIC or BIC, which penalize complexity.",
                      tags: ["AIC", "BIC", "Information Criteria"],
                      scenario:
                        "Task: When comparing different models, you can use information criteria like AIC or BIC, which penalize complexity.",
                    },
                    {
                      id: "p5s1ss8",
                      title: "Best Practices",
                      description:
                        "Using random seeds for reproducibility and stratified sampling.",
                      why: "To ensure your validation results are reproducible, you must always use a random seed. For classification, use stratified sampling to ensure class proportions are the same in your splits.",
                      tags: ["Reproducibility", "Random Seeds"],
                      scenario:
                        "Task: To ensure your results are reproducible and your data splits are fair, you must follow best practices like using random seeds.",
                    },
                  ],
                },
                {
                  id: "p5s2",
                  title: "Model Performance Metrics",
                  icon: "fa-tachometer-alt",
                  description:
                    "Quantify model performance using appropriate metrics for different tasks.",
                  why: "To quantify your model's performance, you must select and interpret the appropriate metrics for your specific business problem.",
                  tags: ["Metrics", "Performance", "Accuracy", "RMSE", "AUC"],
                  difficulty: "advanced",
                  scenario:
                    "You have a validated model, but you need to quantify its performance. You must select the right metric that aligns with your specific business goal.",
                  subsections: [
                    {
                      id: "p5s2ss1",
                      title: "Classification Metrics",
                      description:
                        "Accuracy, precision, recall, F1-score, ROC/AUC, and Cohen's Kappa.",
                      why: "Depending on the business goal, you will choose from metrics like accuracy, precision, recall, F1-score, or AUC.",
                      tags: [
                        "Accuracy",
                        "Precision",
                        "Recall",
                        "F1-Score",
                        "ROC",
                        "AUC",
                      ],
                      scenario:
                        "Condition: Your model predicts a category. You must choose a metric (e.g., accuracy, precision, recall, AUC) based on what kind of error is more costly.",
                    },
                    {
                      id: "p5s2ss2",
                      title: "Regression Metrics",
                      description:
                        "RMSE, MAE, R-squared, and Adjusted R-squared.",
                      why: "To measure the average prediction error of your regression model, you will use metrics like RMSE or MAE. To measure the proportion of variance explained, you'll use R-squared.",
                      tags: ["RMSE", "MAE", "R-squared"],
                      scenario:
                        "Condition: Your model predicts a continuous number. You need a metric (e.g., RMSE, MAE) to measure the average prediction error.",
                    },
                    {
                      id: "p5s2ss3",
                      title: "Clustering Metrics",
                      description:
                        "Silhouette score, Davies-Bouldin index, and Adjusted Rand Index.",
                      why: "To evaluate the quality of your clusters when you don't have ground truth labels, you'll use metrics like the Silhouette score.",
                      tags: ["Silhouette", "Davies-Bouldin", "Adjusted Rand"],
                      scenario:
                        "Condition: You have used an unsupervised clustering algorithm. You need a metric (e.g., Silhouette score) to evaluate the quality of the clusters.",
                    },
                    {
                      id: "p5s2ss4",
                      title: "Ranking & Recommendation Metrics",
                      description: "NDCG, MAP, and hit rate.",
                      why: "To evaluate a model that produces a ranked list (like search results), you'll use metrics like NDCG or MAP.",
                      tags: ["NDCG", "MAP", "Hit Rate"],
                      scenario:
                        "Condition: Your model produces a ranked list (like search results). You need specialized metrics (e.g., NDCG, MAP) to evaluate the ranking quality.",
                    },
                    {
                      id: "p5s2ss5",
                      title: "Best Practices",
                      description:
                        "Selecting and interpreting metrics in their business context.",
                      why: "The choice of metric is not just technical; it's a business decision. You must understand the context to select the metric that best aligns with the project's goals.",
                      tags: ["Business Context", "Interpretation"],
                      scenario:
                        "Decision: The choice of metric is a business decision, not just a technical one. You must select the metric that best reflects project success.",
                    },
                  ],
                },
                {
                  id: "p5s3",
                  title: "Explainable AI (XAI) & Interpretability",
                  icon: "fa-lightbulb",
                  description:
                    "Understand and explain how your models make decisions.",
                  why: "When you or your stakeholders need to understand *why* your complex 'black-box' model made a particular prediction, you must use XAI techniques.",
                  tags: [
                    "XAI",
                    "Interpretability",
                    "SHAP",
                    "LIME",
                    "Feature Importance",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "Your complex model is highly accurate, but it's a \"black box.\" To gain stakeholder trust and ensure it's making sensible decisions, you need to explain why it makes the predictions it does.",
                  subsections: [
                    {
                      id: "p5s3ss1",
                      title: "Feature Importance Methods",
                      description: "Permutation importance, SHAP, and LIME.",
                      why: "To find out which features had the biggest impact on the model's predictions overall, you'll use methods like permutation importance, SHAP, or LIME.",
                      tags: [
                        "Feature Importance",
                        "SHAP",
                        "LIME",
                        "Permutation",
                      ],
                      scenario:
                        "Goal: To find out which features had the biggest impact on the model's predictions overall.",
                    },
                    {
                      id: "p5s3ss2",
                      title: "Model Behavior Visualization",
                      description:
                        "Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) Plots.",
                      why: "To understand how the model's prediction changes as you vary a single feature, you can use Partial Dependence Plots (PDP).",
                      tags: ["PDP", "ICE", "Visualization"],
                      scenario:
                        "Goal: To understand how the model's prediction changes as you vary a single input feature.",
                    },
                    {
                      id: "p5s3ss3",
                      title: "Explanation Scope",
                      description:
                        "Understanding the difference between global and local explanations.",
                      why: "You must distinguish between explaining the model's overall behavior (global) and explaining a single, specific prediction (local).",
                      tags: ["Global", "Local", "Explanations"],
                      scenario:
                        "Decision: Do you need to explain the model's overall behavior (a global explanation) or a single, specific prediction (a local explanation)?",
                    },
                    {
                      id: "p5s3ss4",
                      title: "Model Transparency",
                      description:
                        'Contrasting interpretable "white-box" models with "black-box" models.',
                      why: "When interpretability is paramount, you may choose a simple, transparent 'white-box' model over a more complex 'black-box' model.",
                      tags: ["White-box", "Black-box", "Transparency"],
                      scenario:
                        'Decision: When interpretability is paramount, you may need to choose a simpler, more transparent "white-box" model over a more complex black-box one.',
                    },
                    {
                      id: "p5s3ss5",
                      title: "Best Practices",
                      description:
                        "Communicating the limitations of explanations.",
                      why: "Explanations are themselves models and have limitations. You must be careful to communicate these limitations and not overstate the certainty of the explanation.",
                      tags: ["Communication", "Limitations"],
                      scenario:
                        "Task: You must communicate the limitations of your explanations and not overstate their certainty.",
                    },
                  ],
                },
                {
                  id: "p5s4",
                  title: "Fairness, Bias & Ethics",
                  icon: "fa-balance-scale",
                  description:
                    "Ensure your models are fair, unbiased, and ethically sound.",
                  why: "To ensure your model does not unfairly discriminate against certain groups and to build responsible, trustworthy AI systems, you must proactively audit for and mitigate bias.",
                  tags: ["Ethics", "Fairness", "Bias", "Responsible AI"],
                  difficulty: "advanced",
                  scenario:
                    "Your model is being used to make decisions that affect people. You have a professional and ethical responsibility to ensure it is fair, unbiased, and protects user privacy.",
                  subsections: [
                    {
                      id: "p5s4ss1",
                      title: "Sources of Bias",
                      description:
                        "Identifying sampling, label, measurement, and algorithmic bias.",
                      why: "To address bias effectively, you must first identify where it's coming from—be it in the data collection, the labels, or the algorithm itself.",
                      tags: ["Sampling Bias", "Label Bias", "Algorithmic Bias"],
                      scenario:
                        "Task: To address bias, you must first identify where it's coming from—the data, the labels, or the algorithm itself.",
                    },
                    {
                      id: "p5s4ss2",
                      title: "Fairness Metrics",
                      description:
                        "Demographic parity, equalized odds, and disparate impact.",
                      why: "To mathematically measure whether your model's predictions are fair across different demographic groups, you'll use metrics like demographic parity or equalized odds.",
                      tags: [
                        "Demographic Parity",
                        "Equalized Odds",
                        "Disparate Impact",
                      ],
                      scenario:
                        "Task: To mathematically measure whether your model's predictions are fair across different demographic groups.",
                    },
                    {
                      id: "p5s4ss3",
                      title: "Bias Mitigation Strategies",
                      description:
                        "Pre-processing, in-processing, and post-processing methods.",
                      why: "If you find your model is biased, you can apply techniques to fix it either before, during, or after training.",
                      tags: [
                        "Bias Mitigation",
                        "Pre-processing",
                        "Post-processing",
                      ],
                      scenario:
                        "Problem: You've found that your model is biased. You need to apply techniques to fix it.",
                    },
                    {
                      id: "p5s4ss4",
                      title: "Privacy & Data Protection",
                      description:
                        "Anonymization techniques and differential privacy.",
                      why: "When working with sensitive user data, you must understand and apply techniques like anonymization or differential privacy to protect individuals' privacy.",
                      tags: [
                        "Privacy",
                        "Anonymization",
                        "Differential Privacy",
                      ],
                      scenario:
                        "Condition: You are working with sensitive user data. You must understand and apply techniques to protect individual privacy.",
                    },
                    {
                      id: "p5s4ss5",
                      title: "Responsible AI Principles",
                      description:
                        "Transparency, accountability, and human-in-the-loop systems.",
                      why: "To guide your work, you should adhere to core principles like transparency and accountability for the model's impact.",
                      tags: [
                        "Responsible AI",
                        "Transparency",
                        "Accountability",
                      ],
                      scenario:
                        "Goal: To guide your work, you should adhere to core principles like transparency and accountability for your model's impact.",
                    },
                    {
                      id: "p5s4ss6",
                      title: "Regulatory Compliance",
                      description:
                        "Awareness of regulations like GDPR and CCPA.",
                      why: "You must be aware of and comply with data privacy regulations like GDPR that affect how you can collect and use data.",
                      tags: ["GDPR", "CCPA", "Compliance"],
                      scenario:
                        "Task: You must be aware of and comply with data privacy regulations like GDPR.",
                    },
                    {
                      id: "p5s4ss7",
                      title: "Best Practices",
                      description:
                        "Incorporating ethical reviews and stakeholder engagement.",
                      why: "Ethical considerations should not be an afterthought. You should incorporate ethical reviews and engage with stakeholders throughout the entire project lifecycle.",
                      tags: ["Ethical Review", "Stakeholder Engagement"],
                      scenario:
                        "Goal: To ensure ethical considerations are central to your work, you must incorporate ethical reviews and stakeholder engagement.",
                    },
                  ],
                },
              ],
            },
            {
              id: "phase6",
              title: "Phase 6: Communication, Deployment & Production",
              icon: "fa-rocket",
              description:
                "A model's true value is realized only when its insights are effectively communicated to stakeholders and it is deployed into a production environment where it can be used to make real-world decisions automatically.",
              scenario:
                "Your model is built and validated. Now you must deliver its value by communicating insights to stakeholders and deploying it into a production environment for real-world use.",
              sections: [
                {
                  id: "p6s1",
                  title: "Data Science Communication & Storytelling",
                  icon: "fa-presentation",
                  description:
                    "Effectively communicate your insights and findings to diverse audiences.",
                  why: "To ensure your analytical work has an impact, you must be able to translate complex results into a clear, concise, and persuasive story for your audience.",
                  tags: [
                    "Communication",
                    "Storytelling",
                    "Presentation",
                    "Visualization",
                  ],
                  difficulty: "intermediate",
                  scenario:
                    "Your analysis is finished. Your goal now is to ensure this work has an impact by communicating your findings effectively to a specific audience.",
                  subsections: [
                    {
                      id: "p6s1ss1",
                      title: "Narrative Techniques",
                      description:
                        "Framing insights and connecting them to business goals.",
                      why: "To make your findings memorable and actionable, you should frame them within a narrative structure that connects directly to business goals.",
                      tags: ["Narrative", "Business Goals", "Framing"],
                      scenario:
                        "Goal: To make your findings memorable and actionable by framing them within a story that connects to business goals.",
                    },
                    {
                      id: "p6s1ss2",
                      title: "Visualization for Communication",
                      description:
                        "Creating effective charts, graphs, and infographics.",
                      why: "When creating charts for an audience, your goal is to explain, not to explore. You must create clean, simple visualizations that highlight the key message.",
                      tags: ["Communication", "Charts", "Infographics"],
                      scenario:
                        "Task: To create clean, simple visualizations that explain your key message, rather than explore the data.",
                    },
                    {
                      id: "p6s1ss3",
                      title: "Reproducible Reporting",
                      description:
                        "Using RMarkdown and Quarto for dynamic documents.",
                      why: "To create professional, shareable reports that can be easily updated, you will use RMarkdown and Quarto.",
                      tags: ["RMarkdown", "Quarto", "Reproducible"],
                      scenario:
                        "Task: To create professional, shareable reports (using R Markdown or Quarto) that can be easily updated.",
                    },
                    {
                      id: "p6s1ss4",
                      title: "Audience Engagement",
                      description:
                        "Tailoring presentations for technical and non-technical stakeholders.",
                      why: "To be effective, you must tailor your language, content, and level of technical detail to your specific audience.",
                      tags: ["Audience", "Engagement", "Stakeholders"],
                      scenario:
                        "Decision: You must tailor your language and technical detail to your audience (e.g., technical peers vs. executives).",
                    },
                    {
                      id: "p6s1ss5",
                      title: "Data Journalism",
                      description:
                        "Applying journalistic principles to data narratives.",
                      why: "To tell a powerful story with data, you can apply principles from journalism, such as finding the human element and providing context.",
                      tags: ["Data Journalism", "Journalism"],
                      scenario:
                        "Goal: To tell a powerful, human-centric story with your data by applying principles from journalism.",
                    },
                  ],
                },
                {
                  id: "p6s2",
                  title: "Interactive Applications & Dashboards",
                  icon: "fa-desktop",
                  description:
                    "Build interactive web applications and dashboards for data exploration.",
                  why: "When you want to empower non-technical users to explore data and model results on their own, you will build an interactive web application or dashboard.",
                  tags: ["Shiny", "Dashboards", "Interactive", "Web Apps"],
                  difficulty: "advanced",
                  scenario:
                    "You want to move beyond static reports and create a tool that allows stakeholders to explore your data and model results interactively.",
                  subsections: [
                    {
                      id: "p6s2ss1",
                      title: "Shiny Applications",
                      description:
                        "UI design, server logic, reactivity, and deployment.",
                      why: "To build a custom, fully interactive web app in R, you will use the Shiny framework, defining the user interface (UI) and the server-side logic.",
                      tags: ["Shiny", "UI", "Server", "Reactivity"],
                      scenario:
                        "Goal: To build a custom, fully interactive web application using only R code.",
                    },
                    {
                      id: "p6s2ss2",
                      title: "Dashboard Creation",
                      description:
                        "Using flexdashboard and shinydashboard with interactive widgets.",
                      why: "To create a professional-looking dashboard with pre-defined layouts and interactive widgets, you can use packages like `flexdashboard`.",
                      tags: ["flexdashboard", "shinydashboard", "Widgets"],
                      scenario:
                        "Task: To create a professional-looking dashboard with pre-defined layouts and interactive widgets.",
                    },
                    {
                      id: "p6s2ss3",
                      title: "User Experience (UX) Design",
                      description:
                        "Focusing on accessibility, usability, and responsive design.",
                      why: "To ensure your application is useful and easy to navigate, you must think about the user experience, focusing on clarity, simplicity, and accessibility.",
                      tags: ["UX", "Accessibility", "Responsive"],
                      scenario:
                        "Goal: To ensure your application is useful, intuitive, and easy for your audience to navigate.",
                    },
                    {
                      id: "p6s2ss4",
                      title: "Best Practices",
                      description:
                        "Writing modular, performant, and secure application code.",
                      why: "To build a high-quality Shiny app, you should write modular, performant code and follow best practices for security.",
                      tags: ["Modular", "Performance", "Security"],
                      scenario:
                        "Task: To build a high-quality application, you must write code that is modular, performant, and secure.",
                    },
                  ],
                },
                {
                  id: "p6s3",
                  title: "Reproducible Research & Documentation",
                  icon: "fa-file-alt",
                  description:
                    "Create reproducible research workflows and comprehensive documentation.",
                  why: "To ensure that your work is credible, transparent, and can be built upon by others, you must follow best practices for reproducibility and documentation.",
                  tags: [
                    "Reproducibility",
                    "Documentation",
                    "Workflows",
                    "Automation",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "Your goal is to package your entire analysis—including code, data, outputs, and documentation—in a way that is transparent, credible, and can be easily understood and reused by others.",
                  subsections: [
                    {
                      id: "p6s3ss1",
                      title: "Literate Programming",
                      description:
                        "Combining code, text, and output with RMarkdown/Quarto.",
                      why: "The core principle of reproducible research is to combine code, output, and narrative in a single document, which is the purpose of RMarkdown and Quarto.",
                      tags: ["Literate Programming", "RMarkdown", "Quarto"],
                      scenario:
                        "Task: To combine your code, its output, and your narrative text into a single, cohesive document.",
                    },
                    {
                      id: "p6s3ss2",
                      title: "Versioned & Parameterized Reports",
                      description: "Automating report generation.",
                      why: "To automate the generation of similar reports for different inputs (e.g., a monthly sales report for each region), you can create parameterized reports.",
                      tags: ["Parameterized", "Automation", "Reports"],
                      scenario:
                        "Goal: To automate the generation of similar reports for different inputs (e.g., a monthly report for each sales region).",
                    },
                    {
                      id: "p6s3ss3",
                      title: "Package & Project Documentation",
                      description:
                        "Using roxygen2, pkgdown, READMEs, and vignettes.",
                      why: "To make your work understandable and usable by others, you must provide clear documentation for your functions (`roxygen2`) and project (`README`).",
                      tags: ["roxygen2", "pkgdown", "Vignettes"],
                      scenario:
                        "Task: To make your functions and project usable by others, you must provide clear documentation.",
                    },
                    {
                      id: "p6s3ss4",
                      title: "Workflow Automation",
                      description:
                        "Building robust data pipelines with drake or targets.",
                      why: "For complex, multi-step analyses, using a pipeline tool like `targets` ensures that the entire workflow is reproducible and only re-runs the parts that have changed.",
                      tags: ["drake", "targets", "Pipelines"],
                      scenario:
                        "Condition: Your analysis is complex and multi-stepped. You need a pipeline tool to manage it reproducibly.",
                    },
                  ],
                },
                {
                  id: "p6s4",
                  title: "MLOps & Model Deployment",
                  icon: "fa-cogs",
                  description:
                    "Deploy models to production and manage the model lifecycle.",
                  why: "To move your model from your laptop to a production system where it can serve live predictions, you need to learn the principles of Machine Learning Operations (MLOps).",
                  tags: ["MLOps", "Deployment", "APIs", "Model Management"],
                  difficulty: "advanced",
                  scenario:
                    "You have a trained model that works well on your machine. Now you must make it available for other applications to use by deploying it as a production service.",
                  subsections: [
                    {
                      id: "p6s4ss1",
                      title: "Model Serialization & Serving",
                      description:
                        "Saving models (saveRDS) and creating APIs with plumber, vetiver, and pins.",
                      why: "The first step in deployment is to save your trained model and wrap it in a web API using a framework like `plumber` or `vetiver` so other applications can access it.",
                      tags: ["Serialization", "plumber", "vetiver", "pins"],
                      scenario:
                        "Task: The first step is to save your trained model object and wrap it in a web API so it can receive requests.",
                    },
                    {
                      id: "p6s4ss2",
                      title: "REST APIs",
                      description:
                        "Designing and documenting APIs with OpenAPI.",
                      why: "To create a standardized way for other services to request predictions from your model, you will design and document a REST API.",
                      tags: ["REST", "APIs", "OpenAPI"],
                      scenario:
                        "Task: To create a standardized way for other services to request predictions from your model, you must design a REST API.",
                    },
                    {
                      id: "p6s4ss3",
                      title: "Model Monitoring",
                      description:
                        "Tracking performance, detecting drift, and implementing logging.",
                      why: "Once a model is deployed, you must continuously monitor its performance to detect data drift or concept drift, which can degrade its accuracy.",
                      tags: ["Monitoring", "Drift Detection", "Logging"],
                      scenario:
                        "Problem: A model's performance can degrade over time. You must continuously monitor it to detect drift and other issues.",
                    },
                    {
                      id: "p6s4ss4",
                      title: "Model Lifecycle Management",
                      description: "Automating model retraining and updating.",
                      why: "You need an automated process for retraining your model on new data and deploying the updated version without disrupting the service.",
                      tags: ["Lifecycle", "Retraining", "Updating"],
                      scenario:
                        "Goal: To create an automated process for retraining your model on new data and deploying the updated version.",
                    },
                    {
                      id: "p6s4ss5",
                      title: "Best Practices",
                      description:
                        "Implementing testing, versioning, and rollback strategies.",
                      why: "A production-grade MLOps pipeline includes automated testing, clear versioning of models and data, and the ability to quickly roll back to a previous model version if something goes wrong.",
                      tags: ["Testing", "Versioning", "Rollback"],
                      scenario:
                        "Goal: To ensure your deployed model is reliable, you must implement testing, versioning, and rollback strategies.",
                    },
                  ],
                },
                {
                  id: "p6s5",
                  title: "Containerization & Cloud",
                  icon: "fa-cloud",
                  description:
                    "Use containers and cloud platforms for scalable deployment.",
                  why: "To ensure that your R application or model runs reliably in any environment (a teammate's computer, a cloud server) and can be easily scaled, you will use containerization and cloud platforms.",
                  tags: ["Docker", "Cloud", "AWS", "GCP", "Azure"],
                  difficulty: "advanced",
                  scenario:
                    "You need to ensure that your R application or deployed model runs reliably in any environment and can be easily scaled to handle more traffic.",
                  subsections: [
                    {
                      id: "p6s5ss1",
                      title: "Docker for R",
                      description:
                        "Creating Dockerfiles, building images, and running containers.",
                      why: "To package your R code, its dependencies, and the system environment into a single, portable container, you will write a Dockerfile.",
                      tags: ["Docker", "Dockerfiles", "Containers"],
                      scenario:
                        "Task: To package your R code, its dependencies, and its system environment into a single, portable container.",
                    },
                    {
                      id: "p6s5ss2",
                      title: "Cloud Platforms",
                      description:
                        "Deploying R workloads on RStudio Cloud, Posit Connect, AWS, GCP, and Azure.",
                      why: "To deploy and manage your R workloads at scale, you will use cloud services like Posit Connect or general-purpose platforms like AWS, GCP, or Azure.",
                      tags: [
                        "RStudio Cloud",
                        "Posit Connect",
                        "AWS",
                        "GCP",
                        "Azure",
                      ],
                      scenario:
                        "Goal: To deploy and manage your R workloads at scale using cloud services like AWS, GCP, or Azure.",
                    },
                    {
                      id: "p6s5ss3",
                      title: "Orchestration",
                      description:
                        "Introduction to Kubernetes for scaling and load balancing.",
                      why: "When you need to manage many containers and automatically handle scaling and load balancing, you'll be introduced to orchestration tools like Kubernetes.",
                      tags: ["Kubernetes", "Scaling", "Load Balancing"],
                      scenario:
                        "Condition: You need to manage many containers and automatically handle scaling and load balancing.",
                    },
                    {
                      id: "p6s5ss4",
                      title: "Cloud Data Management",
                      description:
                        "Using cloud databases and object storage securely.",
                      why: "For production systems, you will learn to use cloud-native databases and secure object storage (like Amazon S3).",
                      tags: ["Cloud Storage", "Databases", "Security"],
                      scenario:
                        "Task: For a production system, you need to use cloud-native databases and secure object storage for your data.",
                    },
                  ],
                },
                {
                  id: "p6s6",
                  title: "CI/CD & Automation",
                  icon: "fa-sync",
                  description:
                    "Implement continuous integration and deployment for data science projects.",
                  why: "To automate the process of testing and deploying your code, reducing manual errors and increasing the speed and reliability of your updates, you will implement CI/CD pipelines.",
                  tags: ["CI/CD", "GitHub Actions", "Automation", "Testing"],
                  difficulty: "advanced",
                  scenario:
                    "You want to automate the process of testing and deploying your code to reduce manual errors and increase the speed and reliability of updates to your application or model.",
                  subsections: [
                    {
                      id: "p6s6ss1",
                      title: "Continuous Integration (CI)",
                      description:
                        "Using GitHub Actions, GitLab CI, or Travis CI for automated testing.",
                      why: "Every time you push new code, a CI service like GitHub Actions will automatically run all your tests to ensure you haven't broken anything.",
                      tags: ["CI", "GitHub Actions", "GitLab CI", "Testing"],
                      scenario:
                        "Goal: To automatically run all your tests every time new code is pushed, ensuring nothing has been broken.",
                    },
                    {
                      id: "p6s6ss2",
                      title: "Continuous Deployment (CD)",
                      description: "Building automated deployment pipelines.",
                      why: "If all the CI tests pass, a CD pipeline can automatically deploy your updated application or model to the production environment.",
                      tags: ["CD", "Deployment", "Pipelines"],
                      scenario:
                        "Goal: To automatically deploy your application to production after it has successfully passed all tests.",
                    },
                    {
                      id: "p6s6ss3",
                      title: "Automated Reporting & Monitoring",
                      description: "Scheduling reports and setting up alerts.",
                      why: "You can use CI/CD to automatically re-run an analysis and regenerate a report on a schedule, and set up alerts for any issues.",
                      tags: ["Automation", "Scheduling", "Alerts"],
                      scenario:
                        "Task: To schedule reports to run automatically and to set up alerts for any issues in your production system.",
                    },
                    {
                      id: "p6s6ss4",
                      title: "Best Practices",
                      description:
                        "Implementing versioning, rollbacks, and notifications.",
                      why: "A mature CI/CD workflow includes versioning, rollback strategies, and notifications to keep the team informed of the status of builds and deployments.",
                      tags: ["Versioning", "Rollbacks", "Notifications"],
                      scenario:
                        "Goal: To implement a mature pipeline that includes versioning, rollbacks, and notifications to keep the team informed.",
                    },
                  ],
                },
                {
                  id: "p6s7",
                  title: "Collaboration & Professional Practice",
                  icon: "fa-handshake",
                  description:
                    "Work effectively in teams and follow professional data science practices.",
                  why: "To work effectively as part of a data science team and contribute to large, long-running projects, you need to adopt professional software development and project management practices.",
                  tags: [
                    "Collaboration",
                    "Team Work",
                    "Professional Practice",
                    "Open Science",
                  ],
                  difficulty: "advanced",
                  scenario:
                    "You are working as part of a data science team. To be effective, you must adopt professional practices for project management and collaborative software development.",
                  subsections: [
                    {
                      id: "p6s7ss1",
                      title: "Team Workflows",
                      description:
                        "Using pull requests, code reviews, and issue tracking.",
                      why: "To manage code contributions in a structured way, your team will use a workflow centered on pull requests, code reviews, and issue tracking.",
                      tags: ["Pull Requests", "Code Review", "Issue Tracking"],
                      scenario:
                        "Task: To manage code contributions in a structured way, your team will use a workflow centered on pull requests and code reviews.",
                    },
                    {
                      id: "p6s7ss2",
                      title: "Project Management Methodologies",
                      description: "Applying Kanban and Agile principles.",
                      why: "To organize the team's work and deliver projects on time, you may use Agile or Kanban methodologies.",
                      tags: ["Kanban", "Agile", "Project Management"],
                      scenario:
                        "Goal: To organize the team's work and deliver projects on time, you can apply Agile or Kanban principles.",
                    },
                    {
                      id: "p6s7ss3",
                      title: "Open Science & Sharing",
                      description:
                        "Contributing to open-source and sharing reproducible research.",
                      why: "To contribute to the broader community and build a professional portfolio, you can contribute to open-source packages or share your research in a reproducible format.",
                      tags: ["Open Science", "Open Source", "Sharing"],
                      scenario:
                        "Goal: To contribute to the broader community and build your professional portfolio, you can contribute to open-source or share reproducible research.",
                    },
                    {
                      id: "p6s7ss4",
                      title: "Professional Standards",
                      description:
                        "Adhering to team workflows, transparency, and ethical guidelines.",
                      why: "As a professional data scientist, you are expected to adhere to team workflows, write transparent and ethical code, and take responsibility for the impact of your work.",
                      tags: [
                        "Professional Standards",
                        "Transparency",
                        "Ethics",
                      ],
                      scenario:
                        "Task: As a professional, you are expected to adhere to team workflows, write transparent code, and follow ethical guidelines.",
                    },
                  ],
                },
              ],
            },
          ],
        };

        const pageContent = {
          p1s1: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-play-circle"></i> Introduction to R & Workflow</div>
                        <div class="scenario">Your goal is to start a data science project. The first step is to set up a robust and reproducible environment.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-globe"></i> The R Ecosystem</div>
                                    <div class="scenario"><strong>Decision:</strong> First, you must understand *why* R is a good choice. You need to know its history, strengths, and the community resources available.</div>
                                </div>
                            </div>
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-download"></i> R & RStudio Installation</div>
                                    <div class="scenario"><strong>Task:</strong> To start coding, you need to install the core software: the R language (the engine) and RStudio (the interface).</div>
                                </div>
                            </div>
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-desktop"></i> The RStudio IDE</div>
                                    <div class="scenario"><strong>Goal:</strong> To work efficiently, you must become familiar with your primary tool. This means learning the layout and features of the RStudio IDE.</div>
                                </div>
                            </div>
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-box-open"></i> R Packages</div>
                                    <div class="scenario"><strong>Condition:</strong> When base R doesn't have the specific function you need (e.g., to read an Excel file), you must extend its capabilities by using external packages.</div>
                                </div>
                            </div>
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-folder-tree"></i> Project Organization</div>
                                    <div class="scenario"><strong>Task:</strong> To prevent future chaos, you must establish a clean and logical folder structure for your project's files (data, scripts, reports).</div>
                                </div>
                            </div>
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-sync-alt"></i> Reproducibility</div>
                                    <div class="scenario"><strong>Goal:</strong> To ensure your work is credible and can be verified by others (and your future self), you must adopt core principles for making your analysis reproducible.</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s2: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                 <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-code"></i> Core R Programming</div>
                        <div class="scenario">Your environment is set up. Now you must learn the actual R language to instruct the computer.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-spell-check"></i> Syntax & Data Types</div>
                                    <div class="scenario"><strong>Decision:</strong> To store and work with information, you first need to understand R's grammar and the different structures it uses to hold data.</div>
                                </div>
                            </div>
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-sitemap"></i> Control Flow & Functions</div>
                                    <div class="scenario"><strong>Condition:</strong> When your code needs to make decisions or repeat actions, you must learn to control its execution flow and package reusable logic into functions.</div>
                                </div>
                            </div>
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-cubes"></i> Packages & Environments</div>
                                    <div class="scenario"><strong>Goal:</strong> To become a proficient programmer, you must learn to manage external code (packages) and understand how R manages its own variables (environments).</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s2ss1: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-spell-check"></i> Syntax & Data Types</div>
                        <div class="scenario">Your goal is to store information in R. You need to choose the correct structure for the job.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-star"></i> Fundamentals</div><div class="scenario"><strong>Task:</strong> To store a single piece of information, you use a variable.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-braille"></i> Atomic Data Types</div><div class="scenario"><strong>Condition:</strong> If you need to store items that are all the *same type*, you use an atomic vector.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-table"></i> Composite Data Types</div><div class="scenario"><strong>Condition:</strong> If you need a table where columns can be *different types*, you use a data frame.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-exchange-alt"></i> Type Handling</div><div class="scenario"><strong>Problem:</strong> R misinterpreted a data type on import. You must learn how to fix it.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-search-location"></i> Subsetting & Indexing</div><div class="scenario"><strong>Goal:</strong> You only need a specific piece of your data. You must learn how to access it.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-exclamation-triangle"></i> Special Values</div><div class="scenario"><strong>Problem:</strong> Your data has missing values (<code>NA</code>) and calculations are failing. You need to learn how to handle them.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s2ss2: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-sitemap"></i> Control Flow & Functions</div>
                        <div class="scenario">You know R's syntax. Now you want to write smarter, more efficient, and reusable code.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-code-branch"></i> Conditional Logic & Loops</div><div class="scenario"><strong>Condition:</strong> Your code needs to perform an action *only if* a condition is met, or it needs to *repeat* an action.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-tachometer-alt"></i> Vectorized Operations</div><div class="scenario"><strong>Goal:</strong> Your loop is too slow. You need a faster, "R-like" way to perform the operation on a whole vector at once.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-box"></i> Function Definition</div><div class="scenario"><strong>Problem:</strong> You are writing the same code repeatedly. You need to package it into a reusable function.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-cogs"></i> Advanced Functions</div><div class="scenario"><strong>Goal:</strong> You need to create a one-off function to pass to another function, or build a "function factory".</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-industry"></i> Functional Programming</div><div class="scenario"><strong>Goal:</strong> You want to replace a complex loop with a clean, readable pipeline of operations.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s2ss3: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-cubes"></i> Packages & Environments</div>
                        <div class="scenario">Your goal is to manage the complexity of your R projects and extend R's built-in capabilities.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-tools"></i> Package Management</div><div class="scenario"><strong>Task:</strong> You need a tool not built into R. You must learn how to find, install, and manage this external code.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-paint-brush"></i> The Tidyverse Ecosystem</div><div class="scenario"><strong>Goal:</strong> You want to adopt a modern, intuitive, and consistent workflow for data science.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-language"></i> Programming Paradigms</div><div class="scenario"><strong>Decision:</strong> You need to understand the different styles of R code (Base R vs. Tidyverse) to choose the right approach.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-search"></i> Environments & Scoping</div><div class="scenario"><strong>Problem:</strong> Your variables seem to have unexpected values. You must understand how R finds them.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-magic"></i> Metaprogramming</div><div class="scenario"><strong>Goal:</strong> You want to build your own functions that work with the same clean syntax as Tidyverse functions.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s3: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-table"></i> Data Structures & Manipulation</div>
                        <div class="scenario">You have raw data. Now you need to get it into the right shape and format for analysis.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="far fa-file-alt"></i> Data Frames & Tibbles</div><div class="scenario"><strong>Task:</strong> Your primary goal is to work with tabular (spreadsheet-like) data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-sitemap"></i> Lists & Nested Data</div><div class="scenario"><strong>Condition:</strong> Your data isn't flat, it's hierarchical (like JSON). You must use lists.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-tags"></i> Factors & Categorical Data</div><div class="scenario"><strong>Problem:</strong> Your plot's axes are in the wrong (alphabetical) order. You must manage factor levels.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-exchange-alt"></i> Data Import & Export</div><div class="scenario"><strong>Task:</strong> Your data is outside R (e.g., in a CSV). You must learn how to import it.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-wrench"></i> Data Wrangling Verbs</div><div class="scenario"><strong>Goal:</strong> You need to perform common manipulations: subsetting, creating new columns, and summarizing.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-link"></i> The Pipe Operator</div><div class="scenario"><strong>Problem:</strong> Your multi-step code is hard to read. You need to chain commands into a clean workflow.</div></div></div>
                             <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-broom"></i> Data Cleaning</div><div class="scenario"><strong>Task:</strong> Your raw dataset is messy. You must apply your wrangling skills to clean it.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s4: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-cubes"></i> Object-Oriented Programming in R</div>
                        <div class="scenario">Your projects are becoming larger and more complex. You need to move beyond simple scripts and learn how to build maintainable, scalable systems.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-layer-group"></i> R's OOP Systems</div><div class="scenario"><strong>Decision:</strong> You see that R has multiple OOP systems. You must understand their differences to choose the right one.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-book-open"></i> Core OOP Concepts</div><div class="scenario"><strong>Goal:</strong> To write effective OOP code, you must understand the fundamental vocabulary (classes, methods, inheritance).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-drafting-compass"></i> Design Patterns</div><div class="scenario"><strong>Problem:</strong> You face a common design challenge. You can solve it cleanly by applying a proven design pattern.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-bullseye"></i> Use Cases for OOP</div><div class="scenario"><strong>Decision:</strong> Is your project a one-off script or a reusable system? This will help you decide if you need OOP.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s5: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-calculator"></i> Mathematical Foundations</div>
                        <div class="scenario">You know how to program, but to understand *why* models work, you need to learn the underlying mathematical principles.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-chart-bar"></i> Core Statistics</div><div class="scenario"><strong>Goal:</strong> To describe data and make inferences from a sample, you must understand statistics.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-square-root-alt"></i> Linear Algebra</div><div class="scenario"><strong>Condition:</strong> The models you use represent data as vectors and matrices. You must learn linear algebra to understand them.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-infinity"></i> Calculus & Optimization</div><div class="scenario"><strong>Goal:</strong> To understand how a model "learns" from data by minimizing an error function, you need calculus and optimization.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s6: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-code-branch"></i> Advanced R Programming</div>
                        <div class="scenario">You've mastered the basics. Now you need to write more sophisticated, efficient, and robust R code.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-cogs"></i> Advanced Functional Programming</div><div class="scenario"><strong>Goal:</strong> To write more elegant functions that can even generate other functions, you'll explore closures and function factories.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-clock"></i> Evaluation Model</div><div class="scenario"><strong>Problem:</strong> R's code evaluation can be surprising. You need to understand its "lazy evaluation" model to predict behavior.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-bug"></i> Code Debugging & Profiling</div><div class="scenario"><strong>Problem:</strong> Your code has a bug or is running too slowly. You need tools to diagnose and fix it.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-rocket"></i> Performance Optimization</div><div class="scenario"><strong>Goal:</strong> To make your code run significantly faster by using techniques like vectorization and better memory management.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-shield-alt"></i> Error Handling</div><div class="scenario"><strong>Goal:</strong> To build robust code that doesn't crash on unexpected inputs, you must implement proper error handling.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-pencil-ruler"></i> Code Craftsmanship</div><div class="scenario"><strong>Goal:</strong> To ensure your code is maintainable and understandable by others, you must practice good code craftsmanship.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s7: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-tools"></i> Professional Tools & Workflows</div>
                        <div class="scenario">Your code works, but to succeed in a team, you need to use professional tools for collaboration, reporting, and automation.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-box"></i> RStudio Projects</div><div class="scenario"><strong>Task:</strong> To keep all files for an analysis self-contained and portable, you must use an RStudio Project.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-file-invoice"></i> Dynamic Documents</div><div class="scenario"><strong>Goal:</strong> You need to create reports that automatically update when your data or code changes.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-book"></i> Literate Programming</div><div class="scenario"><strong>Goal:</strong> To develop your analysis interactively, blending code, output, and narrative text.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-robot"></i> Automation & Pipelines</div><div class="scenario"><strong>Condition:</strong> Your analysis has many steps and needs to be re-run often. You should build an automated pipeline.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-file-signature"></i> Code Documentation</div><div class="scenario"><strong>Task:</strong> To make your code usable by others, you must document it professionally.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-chart-pie"></i> Professional Reporting</div><div class="scenario"><strong>Goal:</strong> To share your results with non-technical stakeholders, you need to create dashboards.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fab fa-git-alt"></i> Version Control Integration</div><div class="scenario"><strong>Task:</strong> To track changes and collaborate, you must integrate Git/GitHub into your workflow.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-users"></i> Collaborative Workflows</div><div class="scenario"><strong>Goal:</strong> To work effectively on a shared codebase, you must learn team-based workflows.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s8: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fab fa-git-alt"></i> Version Control with Git</div>
                        <div class="scenario">Your project folder is full of files like <code>analysis_v2.R</code> and <code>analysis_final.R</code>. You need a reliable system to track changes, revert mistakes, and collaborate.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-folder-plus"></i> Repository Management</div><div class="scenario"><strong>Task:</strong> To start tracking a project, you must initialize a Git repository and connect it to a remote host like GitHub.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-code-commit"></i> The Git Workflow</div><div class="scenario"><strong>Task:</strong> To save the history of your changes, you must learn the core cycle of staging, committing, and branching.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-cloud-upload-alt"></i> Remote Collaboration</div><div class="scenario"><strong>Goal:</strong> To share your work with a team and get their updates, you need to use <code>push</code> and <code>pull</code>.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-code-branch"></i> Team Contributions</div><div class="scenario"><strong>Task:</strong> To review a teammate's code or propose changes to a project, you must use pull requests.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title"><i class="fas fa-history"></i> History Management</div><div class="scenario"><strong>Problem:</strong> You made a mistake or have conflicting edits. You need to know how to revert changes and resolve conflicts.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s5ss1: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-chart-bar"></i> Core Statistics</div>
                        <div class="scenario">Your goal is to move from simply observing data to making rigorous, evidence-based conclusions.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Descriptive Statistics</div><div class="scenario"><strong>Task:</strong> To summarize the key features of your dataset (e.g., the "typical" value and how spread out the data is).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Probability Theory</div><div class="scenario"><strong>Goal:</strong> To quantify uncertainty and understand the likelihood of events, which is the foundation for all inference.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Probability Distributions</div><div class="scenario"><strong>Condition:</strong> You need to model a random process (e.g., coin flips, customer arrivals). You must choose the correct distribution.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Sampling & Resampling</div><div class="scenario"><strong>Problem:</strong> You can't study the whole population. You need techniques to make reliable inferences from a smaller sample.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Statistical Significance</div><div class="scenario"><strong>Decision:</strong> Is the effect you see in your sample a real phenomenon or just random chance?</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Hypothesis Testing</div><div class="scenario"><strong>Task:</strong> To formally test a specific claim about your data using a structured scientific procedure.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Measuring Relationships</div><div class="scenario"><strong>Goal:</strong> To quantify the direction and strength of the relationship between two variables.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Statistical Paradigms</div><div class="scenario"><strong>Decision:</strong> Are you describing the data you have, or trying to make inferences about a larger population?</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s5ss2: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-square-root-alt"></i> Linear Algebra</div>
                        <div class="scenario">You want to understand the mechanics of machine learning algorithms that represent data as tables (matrices) and features as arrays (vectors).</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Core Objects & Operations</div><div class="scenario"><strong>Task:</strong> To measure similarity or transform data, you must master vector and matrix operations.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Matrix Decompositions</div><div class="scenario"><strong>Goal:</strong> To simplify complex matrices and uncover hidden structures in your data (e.g., for recommendation systems).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Eigen-analysis</div><div class="scenario"><strong>Goal:</strong> To find the principal axes of variation in your data, which is the core of Principal Component Analysis (PCA).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Linear Systems</div><div class="scenario"><strong>Problem:</strong> You need to solve a system of simultaneous equations, which is the basis for solving linear regression.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Vector Space Concepts</div><div class="scenario"><strong>Goal:</strong> To understand how to create independent (orthogonal) features.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Similarity Measures</div><div class="scenario"><strong>Task:</strong> To calculate how "close" two data points are in a high-dimensional space.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Principal Component Analysis (PCA)</div><div class="scenario"><strong>Condition:</strong> You have too many correlated features and need to reduce the dimensionality of your dataset.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s5ss3: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-infinity"></i> Calculus & Optimization</div>
                        <div class="scenario">Your goal is to understand how a machine learning model "learns" by finding the best parameters that minimize its error.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Differentiation</div><div class="scenario"><strong>Task:</strong> To find the direction of the steepest descent on an error surface, you must calculate the gradient.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Integration</div><div class="scenario"><strong>Goal:</strong> To find the area under a probability curve (e.g., to calculate a p-value), you need to use integration.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Function Approximation</div><div class="scenario"><strong>Goal:</strong> To understand how complex functions can be approximated by simpler ones, which is key to how some models work.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Optimization Theory</div><div class="scenario"><strong>Task:</strong> To "train" a model, you must define a loss function and find the parameters that minimize it.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Gradient Descent Algorithms</div><div class="scenario"><strong>Task:</strong> You need an iterative algorithm to "walk" down the error surface to find the minimum.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Advanced Optimization</div><div class="scenario"><strong>Condition:</strong> Your optimization problem has constraints or is difficult to solve with simple gradient descent.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Modern Optimizers</div><div class="scenario"><strong>Goal:</strong> To efficiently train deep neural networks, you need to use advanced, adaptive optimizers like Adam or RMSprop.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          phase2: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-database"></i> Phase 2: Data Preparation</div>
                        <div class="scenario">Real-world data is messy and not ready for analysis. Your goal is to acquire, clean, and transform it into a structured format.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Acquisition</div><div class="scenario"><strong>Task:</strong> Your first step is to get the data into your R session from its source (e.g., files, databases, web).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Cleaning & Wrangling</div><div class="scenario"><strong>Problem:</strong> The imported data has errors, missing values, and inconsistencies. You need a process to make it trustworthy.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Feature Engineering</div><div class="scenario"><strong>Goal:</strong> To improve model performance, you need to creatively transform raw variables into more informative features.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Feature Scaling</div><div class="scenario"><strong>Condition:</strong> Your numeric features have vastly different scales, which can bias certain algorithms. You must rescale them.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Categorical Data Encoding</div><div class="scenario"><strong>Problem:</strong> Your model requires numerical input, but your data contains text categories. You must convert them to numbers.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Feature Binning</div><div class="scenario"><strong>Goal:</strong> To simplify a continuous variable or capture non-linear effects, you can group its values into bins.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Date/Time Engineering</div><div class="scenario"><strong>Task:</strong> To use date/time information in a model, you must parse it and extract useful features like the month or day of the week.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p2s1: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-download"></i> Data Acquisition</div>
                        <div class="scenario">Your data exists, but it's outside of your current R session. You need to choose the right tool to import it based on its source.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">From Local Files</div><div class="scenario"><strong>Condition:</strong> The data is in a file on your computer, like a CSV or Excel spreadsheet.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">From SQL Databases</div><div class="scenario"><strong>Condition:</strong> The data is stored in a corporate relational database (e.g., SQL Server, PostgreSQL).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">From NoSQL Databases</div><div class="scenario"><strong>Condition:</strong> The data is in a flexible-schema database like MongoDB.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">From the Web</div><div class="scenario"><strong>Condition:</strong> The data needs to be retrieved from a web API or scraped from a website.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p2s2: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-broom"></i> Data Cleaning & Wrangling</div>
                        <div class="scenario">You have successfully imported the data, but it's raw and messy. You must now apply a series of steps to make it clean and trustworthy for analysis.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Automated Cleaning</div><div class="scenario"><strong>Task:</strong> Perform common initial cleaning tasks like standardizing column names.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Missing Value Treatment</div><div class="scenario"><strong>Problem:</strong> Your dataset has empty cells (<code>NA</code>s). You must decide on a strategy to handle them.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Outlier Detection & Treatment</div><div class="scenario"><strong>Problem:</strong> Some data points are extreme and may skew your analysis. You need to identify and handle them.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Type Conversion</div><div class="scenario"><strong>Problem:</strong> A column of numbers was read as text. You must convert it to the correct type.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Text Preprocessing</div><div class="scenario"><strong>Task:</strong> To analyze text data, you must first clean and standardize it.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">High-Performance Wrangling</div><div class="scenario"><strong>Condition:</strong> Your dataset is too large and <code>dplyr</code> is too slow. You need a more memory-efficient tool.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Reshaping</div><div class="scenario"><strong>Task:</strong> Your data is in a "wide" format but your analysis requires a "long" format (or vice-versa).</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p2s3: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-wrench"></i> Feature Engineering</div>
                        <div class="scenario">Your data is clean, but you believe you can improve your model's performance by creating more informative features from the raw variables.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Feature Creation</div><div class="scenario"><strong>Goal:</strong> To provide new signals to your model by combining or transforming existing columns based on domain knowledge.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Feature Selection</div><div class="scenario"><strong>Problem:</strong> You have too many features, some of which are irrelevant. You need to select the most impactful ones.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Feature Transformation</div><div class="scenario"><strong>Condition:</strong> A feature is highly skewed. You apply a mathematical transform to help your model perform better.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Interaction Features</div><div class="scenario"><strong>Hypothesis:</strong> The effect of one feature depends on another. You create features to capture this interaction.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">The <code>recipes</code> Package</div><div class="scenario"><strong>Goal:</strong> To build a reusable, step-by-step pipeline for all your preprocessing and feature engineering tasks.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Dimensionality Reduction</div><div class="scenario"><strong>Problem:</strong> You have hundreds of correlated features. You need to reduce the number of variables while preserving information.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p2s4: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-balance-scale"></i> Feature Scaling</div>
                        <div class="scenario">You are preparing data for a distance-based algorithm (like k-NN, SVM, or PCA), and your numeric features are on vastly different scales.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Standardization</div><div class="scenario"><strong>Goal:</strong> Rescale features to have a mean of 0 and a standard deviation of 1. This is the most common scaling method.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Normalization</div><div class="scenario"><strong>Goal:</strong> Rescale features to a specific range, usually between 0 and 1. Useful for algorithms that expect this range.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Robust Scaling</div><div class="scenario"><strong>Condition:</strong> Your data has significant outliers that would skew standard scaling. You need a method that is robust to them.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p2s5: `
             <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-tags"></i> Categorical Data Encoding</div>
                        <div class="scenario">Your dataset contains text-based categorical features, but your machine learning model requires all inputs to be numeric.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">One-Hot Encoding</div><div class="scenario"><strong>Condition:</strong> Your categorical feature is nominal (the order of categories doesn't matter).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Label & Ordinal Encoding</div><div class="scenario"><strong>Condition:</strong> Your categorical feature is ordinal (the order of categories matters, e.g., "Low", "Medium", "High").</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Target Encoding</div><div class="scenario"><strong>Condition:</strong> Your feature has a very high number of categories, and one-hot encoding would create too many new columns.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Frequency Encoding</div><div class="scenario"><strong>Hypothesis:</strong> The frequency with which a category appears might be a useful predictive signal for the model.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p2s6: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-boxes"></i> Feature Binning</div>
                        <div class="scenario">You have a continuous numeric feature and you want to group its values into a smaller number of discrete bins.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Unsupervised Binning</div><div class="scenario"><strong>Goal:</strong> Create bins based only on the feature's own properties, like creating bins of equal width or with an equal number of observations.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Supervised Binning</div><div class="scenario"><strong>Goal:</strong> Create bins that are most informative for predicting a target variable, using the target to find optimal split points.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Custom Binning</div><div class="scenario"><strong>Condition:</strong> You have specific domain knowledge or business rules that define meaningful thresholds for creating bins.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p2s7: `
             <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-calendar-alt"></i> Date/Time Engineering</div>
                        <div class="scenario">Your dataset contains date and time information that you need to make usable as features for a machine learning model.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Parsing & Manipulation</div><div class="scenario"><strong>Problem:</strong> Your dates are stored as text (e.g., "2025-09-17"). You must first convert them into a proper date/time object.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Feature Extraction</div><div class="scenario"><strong>Task:</strong> You need to derive meaningful features from your date objects, such as the year, month, day of the week, or if it's a holiday.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Time Zone Management</div><div class="scenario"><strong>Condition:</strong> Your data comes from different geographic locations. You must correctly handle and convert between time zones to avoid errors.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          phase3: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-search"></i> Phase 3: Exploratory Data Analysis & Visualization</div>
                        <div class="scenario">Your data is now clean and structured. Before you build a model, you must first explore it to understand its patterns, find relationships, and form initial hypotheses.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Initial Data Exploration</div><div class="scenario"><strong>Task:</strong> To get a quick, high-level overview of your new dataset's contents and quality.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Automated EDA Tools</div><div class="scenario"><strong>Goal:</strong> To accelerate your initial exploration by generating a comprehensive report with a single command.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Hypothesis Testing & Statistical Inference</div><div class="scenario"><strong>Problem:</strong> You've observed a pattern in your sample, but you need to determine if it's a real effect or just random chance.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Visualization Fundamentals</div><div class="scenario"><strong>Goal:</strong> To explore your data visually and communicate findings effectively, you must learn the principles of creating clear graphics.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Storytelling & Communication</div><div class="scenario"><strong>Task:</strong> Your analysis is done. Now you must weave your findings into a compelling narrative to convince others of its importance.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p3s1: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-search-plus"></i> Initial Data Exploration</div>
                        <div class="scenario">You have just loaded a new dataset. Your first goal is to perform a series of checks to quickly understand its basic characteristics and quality.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Summarization</div><div class="scenario"><strong>Task:</strong> To get a quick statistical overview of each column (e.g., mean, median, quartiles).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Structure Inspection</div><div class="scenario"><strong>Task:</strong> To see the column names, data types, and a few sample rows to understand the data's layout.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Quality Assessment</div><div class="scenario"><strong>Goal:</strong> To identify potential problems that need cleaning, like missing values or duplicates.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Relationship Analysis</div><div class="scenario"><strong>Goal:</strong> To get a first look at how different variables might relate to each other.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Distribution Assessment</div><div class="scenario"><strong>Task:</strong> To understand the shape and spread of each individual variable.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p3s2: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-robot"></i> Automated EDA Tools</div>
                        <div class="scenario">You want to speed up the initial exploration process by generating a comprehensive HTML report of your entire dataset with just a few lines of code.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Automated Reporting</div><div class="scenario"><strong>Task:</strong> To generate a detailed report covering univariate and bivariate analysis, missing data, and correlations.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Profiling</div><div class="scenario"><strong>Goal:</strong> To get a fast, high-level summary of all variables, their types, and their quality metrics.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Decision:</strong> Understanding that automated tools are a great starting point but should be used to guide, not replace, deeper manual investigation.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p3s3: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-flask"></i> Hypothesis Testing & Statistical Inference</div>
                        <div class="scenario">You've observed a pattern in your sample data. You must now use a formal statistical framework to determine if this pattern is a real effect or just due to random chance.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Hypothesis Formulation</div><div class="scenario"><strong>Task:</strong> To formally test a belief, you must first state it as a testable null hypothesis (no effect) and an alternative hypothesis.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Common Statistical Tests</div><div class="scenario"><strong>Decision:</strong> Based on your data types and question, you must choose the appropriate statistical test (e.g., t-test, ANOVA).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Interpreting Results</div><div class="scenario"><strong>Task:</strong> To make a conclusion, you must correctly interpret the p-value, confidence intervals, and effect sizes from your test.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Test Assumptions & Diagnostics</div><div class="scenario"><strong>Goal:</strong> To ensure your test results are valid, you must check that the underlying assumptions of the test are met.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Multiple Testing Correction</div><div class="scenario"><strong>Problem:</strong> You ran many tests at once, which increases the risk of a false positive. You need to adjust your p-values.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p3s4: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-chart-line"></i> Visualization Fundamentals</div>
                        <div class="scenario">Your goal is to understand your data and communicate your findings. The most powerful way to do this is by mastering the art and science of data visualization.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Principles of Effective Visualization</div><div class="scenario"><strong>Goal:</strong> To create plots that are clear, honest, and impactful, you must first learn the principles of good design.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Foundational Plot Types</div><div class="scenario"><strong>Task:</strong> To answer the most common analytical questions (comparisons, distributions, relationships), you need to master the basic chart types.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Distributional Plots</div><div class="scenario"><strong>Goal:</strong> To gain a deeper understanding of a variable's distribution, you'll need more advanced plots like violin plots or Q-Q plots.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Advanced & Custom Plots</div><div class="scenario"><strong>Condition:</strong> For specialized analytical needs, you may need to create less common plots like network graphs.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Complex Visualizations</div><div class="scenario"><strong>Task:</strong> To show geographic patterns or layer multiple types of information, you need to create maps or complex multi-layered plots.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p3s5: `
             <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-book-open"></i> Storytelling & Communication</div>
                        <div class="scenario">Your analysis is technically correct, but its value is lost unless you can translate your findings into a compelling story that persuades your audience.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Narrative with Data</div><div class="scenario"><strong>Goal:</strong> To make your analysis persuasive, you must frame it around a central question and guide your audience through the evidence step-by-step.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Visual Storytelling</div><div class="scenario"><strong>Task:</strong> To build a strong argument, you need to carefully sequence your visualizations so they build on one another to tell a coherent story.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Reporting & Sharing</div><div class="scenario"><strong>Task:</strong> To deliver your final story, you must integrate your plots and narrative into a polished report or an interactive dashboard.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          phase4: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-robot"></i> Phase 4: Modeling</div>
                        <div class="scenario">Your data is clean and understood. The goal is now to select and apply the right algorithm to uncover insights, make predictions, or discover hidden structures.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Regression Modeling</div><div class="scenario"><strong>Goal:</strong> To predict a continuous numerical value (e.g., price, temperature, sales).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Classification</div><div class="scenario"><strong>Goal:</strong> To predict a discrete category (e.g., 'spam' or 'not spam', 'churn' or 'no churn').</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Clustering & Unsupervised Learning</div><div class="scenario"><strong>Goal:</strong> To discover natural groupings in your data without any pre-labeled outcomes.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Association Rule Mining</div><div class="scenario"><strong>Goal:</strong> To find "if-then" relationships between items in large datasets (e.g., market basket analysis).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Ensemble Methods</div><div class="scenario"><strong>Goal:</strong> To improve predictive performance by combining multiple models into one stronger model.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Deep Learning & Neural Networks</div><div class="scenario"><strong>Condition:</strong> You are tackling a complex pattern recognition problem (e.g., image or text) with a very large dataset.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Time Series Analysis & Forecasting</div><div class="scenario"><strong>Condition:</strong> Your data is collected sequentially over time, and you need to predict future values.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Natural Language Processing (NLP)</div><div class="scenario"><strong>Task:</strong> To analyze, understand, or generate human language from text data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Survival Analysis</div><div class="scenario"><strong>Goal:</strong> To analyze the time until an event of interest occurs (e.g., equipment failure, customer churn).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Causal Inference</div><div class="scenario"><strong>Goal:</strong> To go beyond correlation and estimate the true causal effect of an intervention.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Bayesian Modeling</div><div class="scenario"><strong>Goal:</strong> To incorporate prior knowledge into your model and get a full probability distribution for your estimates.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Network Analysis</div><div class="scenario"><strong>Condition:</strong> Your data represents relationships and connections between entities (e.g., a social network).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Uplift Modeling</div><div class="scenario"><strong>Goal:</strong> To target an intervention only to those individuals who will be positively impacted by it.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s1: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-chart-line"></i> Regression Modeling</div>
                        <div class="scenario">Your primary goal is to predict a continuous numerical outcome. You need to select the appropriate regression technique for your data's characteristics.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Simple & Multiple Linear Regression</div><div class="scenario"><strong>Condition:</strong> You assume a linear relationship between your predictors and the outcome.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Generalized Linear Models (GLM)</div><div class="scenario"><strong>Condition:</strong> Your outcome is not normally distributed (e.g., it's a count or a proportion).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Polynomial Regression</div><div class="scenario"><strong>Condition:</strong> You observe a curved, non-linear trend in your data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Regularized Regression</div><div class="scenario"><strong>Problem:</strong> You have many predictors and want to prevent overfitting or perform automatic feature selection.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Robust & Quantile Regression</div><div class="scenario"><strong>Problem:</strong> Your data has significant outliers, or you need to predict a specific quantile instead of the mean.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Selection</div><div class="scenario"><strong>Task:</strong> You need a systematic way to choose the best subset of predictors from a larger set.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Diagnostics & Validation</div><div class="scenario"><strong>Task:</strong> You must check if your trained model is trustworthy and meets its statistical assumptions.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s2: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-tags"></i> Classification</div>
                        <div class="scenario">Your primary goal is to predict a discrete categorical outcome. You need to select the right algorithm for the job.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Logistic Regression</div><div class="scenario"><strong>Goal:</strong> You need a simple, interpretable baseline model for a binary (Yes/No) outcome.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Tree-Based Methods</div><div class="scenario"><strong>Goal:</strong> You need a more complex, non-linear model that often provides higher accuracy.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Discriminant Analysis</div><div class="scenario"><strong>Goal:</strong> You need a simple probabilistic model, often used when the assumptions of logistic regression are met.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Support Vector Machines (SVM)</div><div class="scenario"><strong>Goal:</strong> To find the optimal boundary that separates classes, especially in high-dimensional space.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Naive Bayes</div><div class="scenario"><strong>Condition:</strong> You are working with text classification or have a very high number of features and need a fast model.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">k-Nearest Neighbors (k-NN)</div><div class="scenario"><strong>Goal:</strong> You need a simple, non-parametric model that classifies new points based on their proximity to known data points.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Evaluation</div><div class="scenario"><strong>Task:</strong> After training a model, you must use the correct metrics (e.g., confusion matrix, ROC/AUC) to assess its performance.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Problem:</strong> One of your classes is much rarer than the other. You need to apply techniques to handle this class imbalance.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s3: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-project-diagram"></i> Clustering & Unsupervised Learning</div>
                        <div class="scenario">You have a dataset with no predefined outcome variable. Your goal is to explore the data and discover natural, hidden groupings or structures within it.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">k-Means Clustering</div><div class="scenario"><strong>Goal:</strong> To partition your data into a specific number (k) of distinct, non-overlapping groups based on distance.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Hierarchical Clustering</div><div class="scenario"><strong>Condition:</strong> You don't know the number of clusters beforehand and want to see a hierarchy of potential groupings.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Density-Based Clustering</div><div class="scenario"><strong>Goal:</strong> To find arbitrarily shaped clusters and identify points that don't belong to any group (noise).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model-Based Clustering</div><div class="scenario"><strong>Hypothesis:</strong> You believe your data is a mixture of several underlying probability distributions.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Cluster Validation</div><div class="scenario"><strong>Task:</strong> After running a clustering algorithm, you need a way to determine the optimal number of clusters and evaluate their quality.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Task:</strong> You must properly prepare your data (e.g., by standardizing features) before clustering.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s4: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-link"></i> Association Rule Mining</div>
                        <div class="scenario">You have a large set of transactional data (e.g., items bought together in a store). Your goal is to discover interesting "if-then" rules and relationships between items.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Apriori & Eclat Algorithms</div><div class="scenario"><strong>Task:</strong> To efficiently find sets of items that frequently occur together in your transactional data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Rule Evaluation</div><div class="scenario"><strong>Task:</strong> After finding rules, you must evaluate their strength and importance using metrics like support, confidence, and lift.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Applications</div><div class="scenario"><strong>Goal:</strong> To apply your findings to real-world problems like market basket analysis or building recommendation systems.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s5: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-layer-group"></i> Ensemble Methods</div>
                        <div class="scenario">A single predictive model is not giving you the performance you need. Your goal is to improve accuracy and robustness by combining the predictions of multiple models.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Bagging</div><div class="scenario"><strong>Goal:</strong> To reduce the variance of a model and prevent overfitting by training multiple models on different bootstrap samples of the data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Boosting</div><div class="scenario"><strong>Goal:</strong> To build a powerful model by training a sequence of models, where each new model focuses on correcting the errors of the previous one.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Stacking & Blending</div><div class="scenario"><strong>Goal:</strong> To combine the predictions of several different *types* of models (e.g., a linear model and a tree-based model) to get the best of all worlds.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Voting Classifiers</div><div class="scenario"><strong>Task:</strong> You need a simple way to combine the outputs of several classification models by letting them "vote" on the final outcome.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Problem:</strong> Ensemble models are very powerful but can easily overfit. You must learn how to tune their parameters carefully.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s6: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-brain"></i> Deep Learning & Neural Networks</div>
                        <div class="scenario">You are tackling a complex, non-linear pattern recognition problem with a very large dataset, such as classifying images or understanding natural language.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Neural Network Basics</div><div class="scenario"><strong>Task:</strong> To begin, you must first understand the basic architecture of a neuron, layers, and how networks learn via backpropagation.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Feedforward Neural Networks (FNN)</div><div class="scenario"><strong>Condition:</strong> Your task is a standard regression or classification problem on tabular data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Convolutional Neural Networks (CNNs)</div><div class="scenario"><strong>Condition:</strong> Your input data has a grid-like structure, such as an image.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Recurrent Neural Networks (RNNs)</div><div class="scenario"><strong>Condition:</strong> Your data is sequential, where the order of information matters, like text or time series.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Training & Optimization</div><div class="scenario"><strong>Task:</strong> You need to select the right components (loss functions, optimizers) and techniques (regularization) to train your network effectively.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Problem:</strong> Your neural network is overfitting. You need techniques like data augmentation or early stopping to mitigate this.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s7: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-chart-line"></i> Time Series Analysis & Forecasting</div>
                        <div class="scenario">Your data consists of observations collected sequentially over time. Your goal is to understand its underlying patterns and/or predict future values.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Time Series Decomposition</div><div class="scenario"><strong>Task:</strong> Your first step is to break down the time series into its core components: trend, seasonality, and random noise.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Stationarity</div><div class="scenario"><strong>Problem:</strong> Many time series models assume the data is stationary. You must test for this and apply transformations if it is not.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Autoregressive Models</div><div class="scenario"><strong>Goal:</strong> To make forecasts based on the series' own past values and past forecast errors.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Exponential Smoothing</div><div class="scenario"><strong>Goal:</strong> You need a flexible and widely used forecasting method that captures trend and seasonality.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Anomaly Detection</div><div class="scenario"><strong>Task:</strong> To identify unusual spikes or dips in your time series that may represent important real-world events.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Task:</strong> To evaluate your forecasting model correctly, you must use validation techniques that respect the temporal order of the data.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s8: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-comments"></i> Natural Language Processing (NLP)</div>
                        <div class="scenario">Your data is unstructured human language (text). Your goal is to apply computational techniques to analyze, understand, and extract insights from this text.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Text Preprocessing</div><div class="scenario"><strong>Task:</strong> Before analysis, you must clean and standardize your raw text data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Feature Extraction</div><div class="scenario"><strong>Problem:</strong> Your models need numeric input. You must convert the text into a numerical representation (e.g., TF-IDF or word embeddings).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Text Classification</div><div class="scenario"><strong>Goal:</strong> To assign predefined categories to text documents (e.g., spam detection, topic labeling).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Topic Modeling</div><div class="scenario"><strong>Goal:</strong> To discover the latent, underlying topics in a large collection of documents without any pre-labeled data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Sentiment Analysis</div><div class="scenario"><strong>Goal:</strong> To determine the emotional tone (positive, negative, neutral) of a piece of text.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Sequence Models</div><div class="scenario"><strong>Condition:</strong> Your NLP task requires understanding the context and order of words (e.g., machine translation, text generation).</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s9: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-hourglass-half"></i> Survival Analysis</div>
                        <div class="scenario">Your analysis is not about *if* an event will happen, but *when*. Your goal is to analyze "time-to-event" data, even when some events haven't happened yet.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Core Concepts</div><div class="scenario"><strong>Task:</strong> First, you must understand the key ideas of survival functions, hazard functions, and censored data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Kaplan-Meier Estimator</div><div class="scenario"><strong>Goal:</strong> To get a non-parametric estimate and visualization of the survival probability over time.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Cox Proportional Hazards Model</div><div class="scenario"><strong>Goal:</strong> To assess the effect of several predictor variables on the survival time.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Parametric Survival Models</div><div class="scenario"><strong>Hypothesis:</strong> You believe the survival times follow a specific statistical distribution.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Competing Risks</div><div class="scenario"><strong>Problem:</strong> Your subjects can experience different types of events that prevent the primary event from occurring.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Task:</strong> You must check the assumptions of your model (like proportional hazards) to ensure your results are valid.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s10: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-arrow-right"></i> Causal Inference</div>
                        <div class="scenario">You see a strong correlation between two variables, but you need to go further and determine if one variable *causes* a change in the other, especially with observational data.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">The Potential Outcomes Framework</div><div class="scenario"><strong>Task:</strong> To think rigorously about causality, you must first learn the framework for contrasting what happened with what *would have* happened.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Matching Methods</div><div class="scenario"><strong>Problem:</strong> You can't run a randomized experiment. You need to create a comparable control group from observational data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Quasi-Experimental Methods</div><div class="scenario"><strong>Condition:</strong> You can't randomize, but your study has a specific design (e.g., a before-and-after comparison, a sharp cutoff) that you can leverage.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Mediation Analysis</div><div class="scenario"><strong>Goal:</strong> To understand the causal pathway *through which* a cause leads to an effect.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Task:</strong> You must clearly state the untestable assumptions your causal claim relies on and perform sensitivity analyses.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s11: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-balance-scale-right"></i> Bayesian Modeling</div>
                        <div class="scenario">You want to move beyond traditional frequentist statistics to a framework where you can incorporate prior knowledge and get a full probability distribution for your model's parameters.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Bayesian Inference</div><div class="scenario"><strong>Task:</strong> You must understand the core logic of combining your prior beliefs with data (likelihood) to form an updated belief (posterior).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Bayesian Regression & Classification</div><div class="scenario"><strong>Goal:</strong> To apply the Bayesian framework to standard models to get more intuitive outputs.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Hierarchical Models</div><div class="scenario"><strong>Condition:</strong> Your data has a nested or grouped structure (e.g., students within schools) that you want to model appropriately.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Checking & Diagnostics</div><div class="scenario"><strong>Task:</strong> You must perform checks to ensure your Bayesian model has converged and fits the data well.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s12: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-network-wired"></i> Network Analysis</div>
                        <div class="scenario">Your data doesn't represent independent entities, but rather a set of entities and the relationships *between* them (e.g., a social network, a power grid).</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Graph Theory Basics</div><div class="scenario"><strong>Task:</strong> To work with network data, you must first understand its basic components: nodes and edges.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Network Metrics</div><div class="scenario"><strong>Goal:</strong> To characterize the network or identify the most important or influential nodes within it.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Community Detection</div><div class="scenario"><strong>Goal:</strong> To find distinct clusters or communities within a large network.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Applications</div><div class="scenario"><strong>Task:</strong> To apply these techniques to understand real-world systems like social dynamics or information flows.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p4s13: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-chart-bar"></i> Uplift Modeling</div>
                        <div class="scenario">Your goal is not to predict who will convert, but to predict who will convert *because* of your marketing intervention, allowing you to target your campaign more effectively.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Incremental Response Models</div><div class="scenario"><strong>Task:</strong> To estimate the incremental (causal) effect of an intervention for each individual.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Evaluation</div><div class="scenario"><strong>Problem:</strong> Standard classification metrics don't work for uplift. You need specialized tools like the Qini curve.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Applications</div><div class="scenario"><strong>Goal:</strong> To optimize marketing campaigns and maximize return on investment.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          phase5: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-check-double"></i> Phase 5: Model Validation, Metrics & Ethics</div>
                        <div class="scenario">You've built a model, but your work isn't done. You must now rigorously evaluate its performance, ensure it's fair and interpretable, and understand its ethical implications before it can be trusted.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Validation & Evaluation</div><div class="scenario"><strong>Goal:</strong> To honestly assess how well your model will perform on new, unseen data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Performance Metrics</div><div class="scenario"><strong>Task:</strong> To quantify your model's performance, you must select and interpret the appropriate metrics for your specific business problem.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Explainable AI (XAI) & Interpretability</div><div class="scenario"><strong>Problem:</strong> Your complex model is a "black box." You need to understand and explain *why* it makes the decisions it does.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Fairness, Bias & Ethics</div><div class="scenario"><strong>Goal:</strong> To ensure your model does not unfairly discriminate and to build a responsible, trustworthy AI system.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p5s1: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-clipboard-check"></i> Model Validation & Evaluation</div>
                        <div class="scenario">Your model performs perfectly on the data you used to train it. But how can you be sure it will generalize to new data in the real world?</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Splitting Strategies</div><div class="scenario"><strong>Task:</strong> The most basic way to validate a model is to split your data into a training set and a test set.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Cross-Validation</div><div class="scenario"><strong>Goal:</strong> To get a more robust and reliable estimate of your model's performance by repeatedly splitting the data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Resampling Methods</div><div class="scenario"><strong>Goal:</strong> To understand the stability and uncertainty of your model's performance estimates using techniques like bootstrapping.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Hyperparameter Tuning</div><div class="scenario"><strong>Task:</strong> To find the optimal settings (hyperparameters) for your chosen algorithm to maximize its performance.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Complexity</div><div class="scenario"><strong>Problem:</strong> Your model is either too simple (underfitting) or too complex (overfitting). You need to diagnose and fix this.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">The Bias-Variance Tradeoff</div><div class="scenario"><strong>Decision:</strong> You must understand and balance the fundamental tradeoff between model simplicity (bias) and complexity (variance).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Selection Criteria</div><div class="scenario"><strong>Task:</strong> When comparing different models, you can use information criteria like AIC or BIC, which penalize complexity.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Task:</strong> To ensure your results are reproducible and your data splits are fair, you must follow best practices like using random seeds.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p5s2: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-tachometer-alt"></i> Model Performance Metrics</div>
                        <div class="scenario">You have a validated model, but you need to quantify its performance. You must select the right metric that aligns with your specific business goal.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Classification Metrics</div><div class="scenario"><strong>Condition:</strong> Your model predicts a category. You must choose a metric (e.g., accuracy, precision, recall, AUC) based on what kind of error is more costly.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Regression Metrics</div><div class="scenario"><strong>Condition:</strong> Your model predicts a continuous number. You need a metric (e.g., RMSE, MAE) to measure the average prediction error.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Clustering Metrics</div><div class="scenario"><strong>Condition:</strong> You have used an unsupervised clustering algorithm. You need a metric (e.g., Silhouette score) to evaluate the quality of the clusters.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Ranking & Recommendation Metrics</div><div class="scenario"><strong>Condition:</strong> Your model produces a ranked list (like search results). You need specialized metrics (e.g., NDCG, MAP) to evaluate the ranking quality.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Decision:</strong> The choice of metric is a business decision, not just a technical one. You must select the metric that best reflects project success.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p5s3: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-lightbulb"></i> Explainable AI (XAI) & Interpretability</div>
                        <div class="scenario">Your complex model is highly accurate, but it's a "black box." To gain stakeholder trust and ensure it's making sensible decisions, you need to explain *why* it makes the predictions it does.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Feature Importance Methods</div><div class="scenario"><strong>Goal:</strong> To find out which features had the biggest impact on the model's predictions overall.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Behavior Visualization</div><div class="scenario"><strong>Goal:</strong> To understand how the model's prediction changes as you vary a single input feature.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Explanation Scope</div><div class="scenario"><strong>Decision:</strong> Do you need to explain the model's overall behavior (a global explanation) or a single, specific prediction (a local explanation)?</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Transparency</div><div class="scenario"><strong>Decision:</strong> When interpretability is paramount, you may need to choose a simpler, more transparent "white-box" model over a more complex black-box one.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Task:</strong> You must communicate the limitations of your explanations and not overstate their certainty.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p5s4: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-balance-scale"></i> Fairness, Bias & Ethics</div>
                        <div class="scenario">Your model is being used to make decisions that affect people. You have a professional and ethical responsibility to ensure it is fair, unbiased, and protects user privacy.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Sources of Bias</div><div class="scenario"><strong>Task:</strong> To address bias, you must first identify where it's coming from—the data, the labels, or the algorithm itself.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Fairness Metrics</div><div class="scenario"><strong>Task:</strong> To mathematically measure whether your model's predictions are fair across different demographic groups.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Bias Mitigation Strategies</div><div class="scenario"><strong>Problem:</strong> You've found that your model is biased. You need to apply techniques to fix it.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Privacy & Data Protection</div><div class="scenario"><strong>Condition:</strong> You are working with sensitive user data. You must understand and apply techniques to protect individual privacy.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Responsible AI Principles</div><div class="scenario"><strong>Goal:</strong> To guide your work, you should adhere to core principles like transparency and accountability for your model's impact.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Regulatory Compliance</div><div class="scenario"><strong>Task:</strong> You must be aware of and comply with data privacy regulations like GDPR.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Goal:</strong> To ensure ethical considerations are central to your work, you must incorporate ethical reviews and stakeholder engagement.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          phase6: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-rocket"></i> Phase 6: Communication, Deployment & Production</div>
                        <div class="scenario">Your model is built and validated. Now you must deliver its value by communicating insights to stakeholders and deploying it into a production environment for real-world use.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Science Communication & Storytelling</div><div class="scenario"><strong>Goal:</strong> To translate complex results into a clear, persuasive story that drives action.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Interactive Applications & Dashboards</div><div class="scenario"><strong>Goal:</strong> To empower non-technical users to explore data and model results on their own.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Reproducible Research & Documentation</div><div class="scenario"><strong>Task:</strong> To ensure your work is credible, transparent, and can be built upon by others.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">MLOps & Model Deployment</div><div class="scenario"><strong>Task:</strong> To move your model from your laptop to a production system where it can serve live predictions.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Containerization & Cloud</div><div class="scenario"><strong>Goal:</strong> To ensure your R application runs reliably and can be easily scaled in any environment.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">CI/CD & Automation</div><div class="scenario"><strong>Goal:</strong> To automate the testing and deployment of your code, reducing errors and increasing speed.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Collaboration & Professional Practice</div><div class="scenario"><strong>Task:</strong> To work effectively in a team, you must adopt professional project management and software development practices.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p6s1: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-presentation"></i> Data Science Communication & Storytelling</div>
                        <div class="scenario">Your analysis is finished. Your goal now is to ensure this work has an impact by communicating your findings effectively to a specific audience.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Narrative Techniques</div><div class="scenario"><strong>Goal:</strong> To make your findings memorable and actionable by framing them within a story that connects to business goals.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Visualization for Communication</div><div class="scenario"><strong>Task:</strong> To create clean, simple visualizations that explain your key message, rather than explore the data.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Reproducible Reporting</div><div class="scenario"><strong>Task:</strong> To create professional, shareable reports (using R Markdown or Quarto) that can be easily updated.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Audience Engagement</div><div class="scenario"><strong>Decision:</strong> You must tailor your language and technical detail to your audience (e.g., technical peers vs. executives).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Data Journalism</div><div class="scenario"><strong>Goal:</strong> To tell a powerful, human-centric story with your data by applying principles from journalism.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p6s2: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-desktop"></i> Interactive Applications & Dashboards</div>
                        <div class="scenario">You want to move beyond static reports and create a tool that allows stakeholders to explore your data and model results interactively.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Shiny Applications</div><div class="scenario"><strong>Goal:</strong> To build a custom, fully interactive web application using only R code.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Dashboard Creation</div><div class="scenario"><strong>Task:</strong> To create a professional-looking dashboard with pre-defined layouts and interactive widgets.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">User Experience (UX) Design</div><div class="scenario"><strong>Goal:</strong> To ensure your application is useful, intuitive, and easy for your audience to navigate.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Task:</strong> To build a high-quality application, you must write code that is modular, performant, and secure.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p6s3: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-file-alt"></i> Reproducible Research & Documentation</div>
                        <div class="scenario">Your goal is to package your entire analysis—including code, data, outputs, and documentation—in a way that is transparent, credible, and can be easily understood and reused by others.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Literate Programming</div><div class="scenario"><strong>Task:</strong> To combine your code, its output, and your narrative text into a single, cohesive document.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Versioned & Parameterized Reports</div><div class="scenario"><strong>Goal:</strong> To automate the generation of similar reports for different inputs (e.g., a monthly report for each sales region).</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Package & Project Documentation</div><div class="scenario"><strong>Task:</strong> To make your functions and project usable by others, you must provide clear documentation.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Workflow Automation</div><div class="scenario"><strong>Condition:</strong> Your analysis is complex and multi-stepped. You need a pipeline tool to manage it reproducibly.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p6s4: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-cogs"></i> MLOps & Model Deployment</div>
                        <div class="scenario">You have a trained model that works well on your machine. Now you must make it available for other applications to use by deploying it as a production service.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Serialization & Serving</div><div class="scenario"><strong>Task:</strong> The first step is to save your trained model object and wrap it in a web API so it can receive requests.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">REST APIs</div><div class="scenario"><strong>Task:</strong> To create a standardized way for other services to request predictions from your model, you must design a REST API.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Monitoring</div><div class="scenario"><strong>Problem:</strong> A model's performance can degrade over time. You must continuously monitor it to detect drift and other issues.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Model Lifecycle Management</div><div class="scenario"><strong>Goal:</strong> To create an automated process for retraining your model on new data and deploying the updated version.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Goal:</strong> To ensure your deployed model is reliable, you must implement testing, versioning, and rollback strategies.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p6s5: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-cloud"></i> Containerization & Cloud</div>
                        <div class="scenario">You need to ensure that your R application or deployed model runs reliably in any environment and can be easily scaled to handle more traffic.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Docker for R</div><div class="scenario"><strong>Task:</strong> To package your R code, its dependencies, and its system environment into a single, portable container.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Cloud Platforms</div><div class="scenario"><strong>Goal:</strong> To deploy and manage your R workloads at scale using cloud services like AWS, GCP, or Azure.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Orchestration</div><div class="scenario"><strong>Condition:</strong> You need to manage many containers and automatically handle scaling and load balancing.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Cloud Data Management</div><div class="scenario"><strong>Task:</strong> For a production system, you need to use cloud-native databases and secure object storage for your data.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p6s6: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-sync"></i> CI/CD & Automation</div>
                        <div class="scenario">You want to automate the process of testing and deploying your code to reduce manual errors and increase the speed and reliability of updates to your application or model.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Continuous Integration (CI)</div><div class="scenario"><strong>Goal:</strong> To automatically run all your tests every time new code is pushed, ensuring nothing has been broken.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Continuous Deployment (CD)</div><div class="scenario"><strong>Goal:</strong> To automatically deploy your application to production after it has successfully passed all tests.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Automated Reporting & Monitoring</div><div class="scenario"><strong>Task:</strong> To schedule reports to run automatically and to set up alerts for any issues in your production system.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Best Practices</div><div class="scenario"><strong>Goal:</strong> To implement a mature pipeline that includes versioning, rollbacks, and notifications to keep the team informed.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p6s7: `
            <div class="subsection">
                <h3 class="subsection-title">Decision Path</h3>
                <div class="mindmap-container">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-handshake"></i> Collaboration & Professional Practice</div>
                        <div class="scenario">You are working as part of a data science team. To be effective, you must adopt professional practices for project management and collaborative software development.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch"><div class="content"><div class="title">Team Workflows</div><div class="scenario"><strong>Task:</strong> To manage code contributions in a structured way, your team will use a workflow centered on pull requests and code reviews.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Project Management Methodologies</div><div class="scenario"><strong>Goal:</strong> To organize the team's work and deliver projects on time, you can apply Agile or Kanban principles.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Open Science & Sharing</div><div class="scenario"><strong>Goal:</strong> To contribute to the broader community and build your professional portfolio, you can contribute to open-source or share reproducible research.</div></div></div>
                            <div class="mindmap-branch"><div class="content"><div class="title">Professional Standards</div><div class="scenario"><strong>Task:</strong> As a professional, you are expected to adhere to team workflows, write transparent code, and follow ethical guidelines.</div></div></div>
                        </div>
                    </div>
                </div>
            </div>
          `,
          p1s1ss1: `
            <div class="subsection">
              <h3 class="subsection-title">A Legacy of Statistical Computing</h3>
              <p>R's journey began not in a corporate lab, but in academia, which has deeply influenced its philosophy and strengths. It was created as an open-source implementation of the <strong>S programming language</strong>, a powerful statistical environment developed at Bell Labs in the 1970s.</p>
              <ul class="prose-list">
                  <li><strong>The Beginning (1993):</strong> At the University of Auckland, New Zealand, professors Ross Ihaka and Robert Gentleman started developing R. Their goal was to create a free, accessible, and powerful statistical software environment for their students and the wider research community.</li>
                  <li><strong>Going Open Source (1995):</strong> R was released under a free software license. This pivotal decision opened the floodgates for a global community of statisticians, programmers, and domain experts to contribute, fix bugs, and add new functionality.</li>
                  <li><strong>Today:</strong> R is a mature, stable, and world-class platform for data science. It is maintained by the "R Core Team," an international group of dedicated developers, and its capabilities are constantly expanding thanks to its vibrant user community.</li>
              </ul>
            </div>

            <div class="subsection">
              <h3 class="subsection-title">Where R Shines: Key Use Cases</h3>
              <p>R's deep statistical roots and unparalleled visualization capabilities make it the preferred tool in many data-intensive fields.</p>
               <div class="decision-branches" style="align-items: flex-start;">
                    <div class="decision-branch">
                        <strong><i class="fas fa-university"></i> Academia and Research</strong>
                        <p>R is the undisputed standard for statistical research. Most new academic papers presenting a novel statistical method will release it as an R package, making cutting-edge techniques immediately available to the entire world.</p>
                    </div>
                    <div class="decision-branch">
                        <strong><i class="fas fa-dna"></i> Biotechnology & Pharmaceuticals</strong>
                        <p>The field of bioinformatics runs on R. The specialized <strong>Bioconductor</strong> project provides thousands of packages for analyzing genomic, transcriptomic, and other high-throughput biological data, crucial for drug discovery and clinical trials.</p>
                    </div>
                </div>
                <div class="decision-branches" style="align-items: flex-start; margin-top: 1rem;">
                    <div class="decision-branch">
                        <strong><i class="fas fa-chart-line"></i> Finance and Insurance</strong>
                        <p>Quantitative analysts ("quants") use R extensively for risk modeling, portfolio management, time-series forecasting of stock prices, and actuarial analysis. There are numerous packages dedicated to financial econometrics.</p>
                    </div>
                    <div class="decision-branch">
                        <strong><i class="fas fa-newspaper"></i> Data Journalism</strong>
                        <p>Leading news organizations like the BBC, The New York Times, and FiveThirtyEight use R to analyze complex datasets and produce the compelling, data-driven visualizations you see in their articles.</p>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">The Community Engine: CRAN & Bioconductor</h3>
                <p>The true power of R lies in its community, which contributes code through centralized, curated repositories. This ensures that the tools you download are reliable and work as expected.</p>
                <div class="mindmap-container" style="padding-top: 0;">
                    <div class="mindmap-root">
                        <div class="title"><i class="fas fa-users"></i> The R Community</div>
                        <div class="scenario">The R language is extended by a massive global community that shares its work through organized platforms.</div>
                    </div>
                    <div class="mindmap-branches-container">
                        <div class="mindmap-branches">
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-box-open"></i> CRAN</div>
                                    <div class="scenario"><strong>The Comprehensive R Archive Network</strong> is the primary repository. It's like an official, quality-controlled "App Store" for R, hosting over 18,000 free packages for almost any task imaginable.</div>
                                </div>
                            </div>
                            <div class="mindmap-branch">
                                <div class="content">
                                    <div class="title"><i class="fas fa-microscope"></i> Bioconductor</div>
                                    <div class="scenario">A specialized repository focused on packages for the analysis of high-throughput genomic data. It's the standard for bioinformatics and computational biology.</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Finding Help</h3>
                <div class="scenario-content">
                    <p>The R community is famously active and helpful. When you get stuck, you're not alone! Beyond official documentation, fantastic resources include:</p>
                    <ul class="prose-list">
                        <li><strong>Stack Overflow:</strong> The go-to site for specific coding questions. Make sure to create a <a href="https://stackoverflow.com/help/minimal-reproducible-example" target="_blank">minimal reproducible example</a> (a "reprex") to get the best answers.</li>
                        <li><strong>R-bloggers:</strong> A central hub that aggregates blog posts and tutorials from hundreds of R experts around the world.</li>
                        <li><strong>Posit Community (formerly RStudio Community):</strong> A friendly forum for discussions, questions, and announcements related to R and the Tidyverse.</li>
                    </ul>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>A key skill for an R user is discovering new tools. Let's explore the vast library of packages available.</p>
                    <ul class="prose-list">
                        <li>Go to the <a href="https://cran.r-project.org/web/views/" target="_blank"><strong>CRAN Task Views</strong></a> page. This page curates packages by topic (e.g., "Finance", "Machine Learning", "Social Sciences").</li>
                        <li>Find a Task View that matches your personal or professional interests.</li>
                        <li>Read the description and identify two packages from that list that seem particularly interesting or useful to you. Click on one to read its reference manual. What is the main purpose of the package you chose?</li>
                    </ul>
                </div>
            </div>
          `,
          p1s1ss2: `
            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="overview">What are R and RStudio?</button>
                    <button class="tab-button" data-tab="windows">Install on Windows</button>
                    <button class="tab-button" data-tab="macos">Install on macOS</button>
                    <button class="tab-button" data-tab="linux">Install on Linux</button>
                </div>

                <div id="overview" class="tab-pane active">
                    <h4 class="subsection-title">The Engine and the Dashboard</h4>
                    <p style="margin-bottom: 1.5rem;">Before installing, it's crucial to understand the difference between R and RStudio. They are two separate pieces of software that work together.</p>
                    <div class="decision-branches" style="align-items: flex-start;">
                        <div class="decision-branch">
                            <strong><i class="fab fa-r-project"></i> R: The Engine</strong>
                            <p>R is the programming language itself. It's the powerful engine that executes your commands, performs statistical calculations, and generates plots. You can use R by itself through a simple command-line terminal, but it's not very user-friendly.</p>
                        </div>
                        <div class="decision-branch">
                            <strong><i class="fas fa-desktop"></i> RStudio: The Dashboard</strong>
                            <p>RStudio is an Integrated Development Environment (IDE). Think of it as a sophisticated dashboard or cockpit for the R engine. It provides a text editor for your scripts, a console to see output, tools for managing plots and packages, and much more. It makes writing and managing R code significantly easier.</p>
                        </div>
                    </div>
                    <div class="scenario-content" style="margin-top: 1.5rem; border-color: var(--warning);">
                        <p><strong>Crucial Rule:</strong> You must install <strong>R *before*</strong> you install RStudio. RStudio needs the R engine to be present on your system before it can function.</p>
                    </div>
                </div>

                <div id="windows" class="tab-pane">
                    <h4 class="subsection-title">Step-by-Step Guide for Windows</h4>
                    <ul class="prose-list">
                        <li><strong>Step 1: Install R.</strong> Go to the <a href="https://cran.r-project.org/bin/windows/base/" target="_blank">CRAN Windows download page</a>. Click the large "Download R-X.X.X for Windows" link at the top. Run the downloaded <code>.exe</code> file. It is safe to accept all the default settings during installation. </li>
                        <li><strong>Step 2: Install RStudio.</strong> Go to the <a href="https://posit.co/download/rstudio-desktop/" target="_blank">Posit RStudio Desktop download page</a>. The website should automatically detect you are on Windows and recommend the correct installer. Download and run the <code>.exe</code> file. Again, the default settings are perfect.</li>
                        <li><strong>Step 3: Launch RStudio.</strong> Once both are installed, you only need to open RStudio (not R itself). It will automatically find your R installation.</li>
                    </ul>
                </div>
                
                <div id="macos" class="tab-pane">
                   <h4 class="subsection-title">Step-by-Step Guide for macOS</h4>
                    <ul class="prose-list">
                        <li><strong>Step 1: Install R.</strong> Go to the <a href="https://cran.r-project.org/bin/macosx/" target="_blank">CRAN macOS download page</a>. Based on your Mac's processor (Apple Silicon M1/M2/M3 or Intel), download the appropriate <code>.pkg</code> file. The latest version usually covers both. Run the installer, accepting the default settings. </li>
                        <li><strong>Step 2: Install RStudio.</strong> Go to the <a href="https://posit.co/download/rstudio-desktop/" target="_blank">Posit RStudio Desktop download page</a>. The website will recommend the correct version for macOS. Download the <code>.dmg</code> file. Open it and drag the RStudio icon into your Applications folder.</li>
                        <li><strong>Step 3: Launch RStudio.</strong> Open RStudio from your Applications folder. It will automatically detect your R installation.</li>
                    </ul>
                </div>

                <div id="linux" class="tab-pane">
                    <h4 class="subsection-title">Installation via Terminal (Debian/Ubuntu)</h4>
                    <p>For Debian-based systems like Ubuntu, it's best to use the terminal for installation.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-bash"># First, update your package list
sudo apt update -qq
# Install helper packages
sudo apt install --no-install-recommends software-properties-common dirmngr
# Add the CRAN repository key
wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc
# Add the CRAN repository to your sources list
sudo add-apt-repository "deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/"
# Finally, install R
sudo apt install --no-install-recommends r-base</code></pre>
                    </div>
                    <p>After installing R, go to the <a href="https://posit.co/download/rstudio-desktop/" target="_blank">Posit RStudio Desktop download page</a> to get the <code>.deb</code> file for your version of Ubuntu and install it using your system's software installer or with <code>sudo dpkg -i [filename].deb</code>.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">First Launch & Verification</h3>
                <p>Once you open RStudio for the first time, you should see an interface with several panes. The most important one initially is the <strong>Console</strong>, which typically appears on the left. You should see text indicating the version of R that is running.</p>
                <p>To verify that everything is working, type the following command into the console and press Enter:</p>
                <div class="code-container">
                    <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                    <pre><code class="language-r">print("Hello, R!")</code></pre>
                </div>
                <p>You should see the output <code>[1] "Hello, R!"</code> printed back to you. This confirms your R engine is working correctly through the RStudio interface. </p>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's confirm your setup and install your first package.</p>
                    <ol class="prose-list">
                        <li>In the RStudio Console, type <code>10 + 5</code> and press Enter. You should see the answer <code>[1] 15</code>.</li>
                        <li>Now, install one of the most famous R packages for plotting, <code>ggplot2</code>. Type this command into the console and press Enter: <code>install.packages("ggplot2")</code>.</li>
                        <li>You will see a lot of text as R downloads and installs the package from CRAN. As long as you don't see an "ERROR" message at the end, you have successfully installed your first R package!</li>
                    </ol>
                </div>
            </div>
          `,
          p1s1ss3: `
            <style>
                .pane-grid { display: flex; flex-wrap: wrap; gap: 1rem; }
                .pane-card { flex: 1 1 45%; background: var(--flow-bg); border-radius: var(--border-radius); padding: 1.5rem; border: 1px solid var(--border-color); }
                .pane-card h5 { font-size: 1.1rem; color: var(--secondary); margin-bottom: 0.5rem; }
            </style>
            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="panes">The Four Panes</button>
                    <button class="tab-button" data-tab="projects">Working with R Projects</button>
                    <button class="tab-button" data-tab="settings">Settings & Customization</button>
                </div>

                <div id="panes" class="tab-pane active">
                    <h4 class="subsection-title">Your Data Science Cockpit</h4>
                    <p style="margin-bottom: 1.5rem;">RStudio is organized into four main panes by default. Understanding the role of each one is the key to an efficient workflow. </p>
                    <div class="pane-grid">
                        <div class="pane-card">
                            <h5><i class="fas fa-file-code"></i> 1. Source Editor (Top-Left)</h5>
                            <p>This is your primary workspace for writing code. You'll write your R scripts (<code>.R</code> files) and other documents like R Markdown (<code>.Rmd</code>) here. It's a text editor with syntax highlighting and code completion to help you write code faster and with fewer errors.</p>
                        </div>
                        <div class="pane-card">
                            <h5><i class="fas fa-cogs"></i> 2. Environment & History (Top-Right)</h5>
                            <p>The <strong>Environment</strong> tab shows you all the objects (variables, data frames, functions) currently active in your R session. The <strong>History</strong> tab keeps a record of every command you've ever run, which is useful for retracing your steps.</p>
                        </div>
                        <div class="pane-card">
                            <h5><i class="fas fa-terminal"></i> 3. Console (Bottom-Left)</h5>
                            <p>The Console is your direct line to the R "engine." You can type commands here for quick tests and see immediate results. When you run code from the Source Editor, the commands and their output are displayed here.</p>
                        </div>
                        <div class="pane-card">
                            <h5><i class="fas fa-folder-open"></i> 4. Files, Plots, & Packages (Bottom-Right)</h5>
                            <p>This is a multi-tool pane. The <strong>Files</strong> tab lets you browse your computer's file system. The <strong>Plots</strong> tab is where your visualizations appear. The <strong>Packages</strong> tab allows you to see and manage your installed packages. The <strong>Help</strong> tab displays documentation.</p>
                        </div>
                    </div>
                </div>

                <div id="projects" class="tab-pane">
                    <h4 class="subsection-title">The Single Most Important Habit</h4>
                    <p>Using R Projects (<code>.Rproj</code> files) is arguably the most important best practice for any R user. A project keeps all the files for a specific analysis (data, scripts, reports) together in one self-contained directory.</p>
                    <p><strong>Analogy:</strong> Think of an R Project as a dedicated lab notebook and workspace for a single experiment. Everything you need is inside, and it doesn't get mixed up with your other experiments.</p>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1rem;">Why is this so critical?</h5>
                    <ul class="prose-list">
                        <li><strong>Automatic Working Directory:</strong> When you open an <code>.Rproj</code> file, RStudio automatically sets the working directory to that project's folder. This solves the #1 problem for beginners: file paths. You can write <code>read.csv("data/my_data.csv")</code> and it will just work, without needing to use <code>setwd()</code>.</li>
                        <li><strong>Self-Contained & Portable:</strong> You can zip the entire project folder and send it to a collaborator, or move it to another computer, and it will work perfectly without any changes to the code.</li>
                        <li><strong>Clean Workspace:</strong> It encourages you to keep each analysis separate, preventing variables from one project from accidentally affecting another.</li>
                    </ul>
                     <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1rem;">How to Create a Project:</h5>
                     <p>From the RStudio menu, simply go to <strong>File > New Project... > New Directory > New Project</strong>, and give your project folder a name.</p>
                </div>
                
                <div id="settings" class="tab-pane">
                   <h4 class="subsection-title">Make RStudio Your Own</h4>
                    <p>You can customize RStudio to fit your preferences. Access the settings via the menu: <strong>Tools > Global Options...</strong>.</p>
                    <ul class="prose-list">
                        <li><strong>Appearance:</strong> This is the most popular setting to change. You can select different editor themes (both light and dark), change the font, and adjust the font size. A good dark theme like "Tomorrow Night Bright" or "Cobalt" can be easier on the eyes for long coding sessions. </li>
                        <li><strong>Pane Layout:</strong> Don't like the default four-pane layout? You can change it! Many experienced developers prefer to have their Console on the top-right, next to the Source Editor.</li>
                        <li><strong>Code:</strong> In the "Code" section, under the "Editing" tab, there are many helpful options. Consider enabling "Soft-wrap R source files" to prevent long lines of code from running off the screen.</li>
                        <li><strong>General:</strong> It's good practice to uncheck "Restore .RData into workspace at startup" and set "Save workspace to .RData on exit" to "Never". This ensures you start every session with a clean, empty environment, which improves reproducibility.</li>
                    </ul>
                </div>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Keyboard Shortcuts</h3>
                <div class="scenario-content">
                    <p>Learning keyboard shortcuts is the fastest way to speed up your coding. Here are two of the most essential ones to learn immediately:</p>
                    <ul class="prose-list">
                        <li><strong>Run Current Line/Selection:</strong> With your cursor on a line of code in the Source Editor, press <code>Ctrl + Enter</code> (Windows/Linux) or <code>Cmd + Enter</code> (macOS). This sends the code to the Console to be executed.</li>
                        <li><strong>Insert Assignment Operator:</strong> Press <code>Alt + -</code> (Windows/Linux) or <code>Option + -</code> (macOS). This will instantly type the assignment operator <code><-</code> with spaces around it. You will use this hundreds of times a day!</li>
                    </ul>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's put your knowledge of the IDE into practice.</p>
                    <ol class="prose-list">
                        <li>Create a new R Project on your Desktop called <code>my_first_project</code>.</li>
                        <li>Inside RStudio, create a new R Script (File > New File > R Script) and save it as <code>analysis.R</code>.</li>
                        <li>In your script, type <code>car_data <- mtcars</code>. Run this line using the keyboard shortcut. Observe that <code>car_data</code> now appears in your <strong>Environment</strong> pane.</li>
                        <li>On the next line, type <code>plot(car_data$wt, car_data$mpg)</code>. Run this line. Your first plot should appear in the <strong>Plots</strong> pane!</li>
                        <li>Go to <strong>Tools > Global Options > Appearance</strong> and change your editor theme to one you like.</li>
                    </ol>
                </div>
            </div>
          `,
          p1s1ss4: `
            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="what-is-a-package">What is an R Package?</button>
                    <button class="tab-button" data-tab="lifecycle">The Package Lifecycle</button>
                    <button class="tab-button" data-tab="base-packages">"Base" R Packages</button>
                </div>

                <div id="what-is-a-package" class="tab-pane active">
                    <h4 class="subsection-title">Extending R's Capabilities</h4>
                    <p style="margin-bottom: 1.5rem;">An R package is a standardized collection of reusable R functions, documentation about how to use them, and sometimes sample datasets, all bundled together for easy sharing and use.</p>

                    <div class="scenario-content" style="border-color: var(--primary);">
                        <h5 style="font-size: 1.1rem; color: var(--primary); margin-bottom: 0.5rem;">The Smartphone Analogy 📱</h5>
                        <p>The best way to think about R and its packages is to compare it to a new smartphone:</p>
                        <ul class="prose-list">
                            <li><strong>Base R</strong> is like your phone's operating system and its default apps (Phone, Camera, Calculator). It's powerful and can do many essential things right out of the box.</li>
                            <li><strong>R Packages</strong> are like the extra apps you download from an app store. If you want to use social media or edit photos, you need to find and install a specific app. Similarly, if you want to read an Excel file or build a machine learning model in R, you install a specific package.</li>
                            <li><strong>CRAN</strong> (The Comprehensive R Archive Network) is R's official app store.</li>
                        </ul>
                    </div>
                </div>

                <div id="lifecycle" class="tab-pane">
                    <h4 class="subsection-title">Find, Install, Load, Update</h4>
                    <p>Managing packages involves a simple four-step lifecycle. You only need to install a package once, but you must load it every time you start a new R session where you want to use it.</p>
                    <ol class="prose-list">
                        <li><strong>Find:</strong> The best place to discover packages is on the <a href="https://cran.r-project.org/web/views/" target="_blank">CRAN Task Views</a> page or by simply searching online (e.g., "R package for financial analysis").</li>
                        <li><strong>Install:</strong> Use the <code>install.packages()</code> function to download and install a package from CRAN onto your computer. This is a one-time action.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Let's install 'readxl' to read Microsoft Excel files
install.packages("readxl")</code></pre>
                            </div>
                        </li>
                        <li><strong>Load:</strong> Use the <code>library()</code> function to load the package into your current R session. This makes all of its functions available for you to use. You must do this in every new script or R session.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Now we load 'readxl' to make its functions ready to use
library(readxl)

# We can now use functions from the package, like read_excel()
# (This line would give an error if you hadn't loaded the library first!)</code></pre>
                            </div>
                        </li>
                        <li><strong>Update:</strong> Over time, package authors release new versions with bug fixes and new features. You can update all your installed packages at once using <code>update.packages()</code>.</li>
                    </ol>
                </div>

                <div id="base-packages" class="tab-pane">
                    <h4 class="subsection-title">Powerful from the Start</h4>
                    <p>Your "new phone" isn't empty; it comes with essential apps pre-installed. Similarly, your R installation comes with a set of fundamental packages that are automatically loaded every time you start R. You don't need to use <code>library()</code> for these.</p>
                     <ul class="prose-list">
                        <li><strong><code>base</code>:</strong> Contains the most fundamental functions of the R language, like <code>c()</code>, <code>list()</code>, <code>mean()</code>, and the operators <code>+</code>, <code>-</code>, etc.</li>
                        <li><strong><code>stats</code>:</strong> Provides a huge variety of statistical functions, such as linear models (<code>lm()</code>), t-tests (<code>t.test()</code>), and random number generation (<code>rnorm()</code>).</li>
                        <li><strong><code>graphics</code>:</strong> Includes functions for creating basic plots, like <code>plot()</code>, <code>hist()</code> (histograms), and <code>boxplot()</code>.</li>
                        <li><strong><code>utils</code>:</strong> A collection of utility functions, including the package management functions like <code>install.packages()</code> and help functions.</li>
                    </ul>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Dependencies</h3>
                <div class="scenario-content">
                    <p>Have you ever noticed that when you run <code>install.packages("some_package")</code>, R often installs several other packages as well? This is normal! Many packages rely on functions from other packages to work. These are called <strong>dependencies</strong>. R's package manager is smart enough to identify and install all the necessary dependencies for you automatically.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>A common task in data science is working with dates and times. Base R can handle dates, but a specialized package makes it much easier.</p>
                    <ol class="prose-list">
                        <li><strong>Find a package:</strong> Search online for "R package for dates". You will quickly discover the highly popular <strong><code>lubridate</code></strong> package.</li>
                        <li><strong>Install it:</strong> In your RStudio Console, run <code>install.packages("lubridate")</code>.</li>
                        <li><strong>Load it:</strong> Now, load it into your session with <code>library(lubridate)</code>.</li>
                        <li><strong>Use it:</strong> The <code>now()</code> function from <code>lubridate</code> gets the current date and time. Run <code>now()</code> in your console and see the result! You've just used your first community-contributed package.</li>
                    </ol>
                </div>
            </div>
          `,
          p1s1ss5: `
            <div class="subsection">
                <h3 class="subsection-title">Why Bother with Organization?</h3>
                <p style="margin-bottom: 1.5rem;">You've started a new analysis. At first, it's simple: one script, one data file. Soon, you have <code>code_v2.R</code>, <code>final_code.R</code>, <code>final_code_REALLY_final.R</code>, <code>data.csv</code>, and <code>cleaned_data.csv</code> all in one messy folder. When you return to it in two months, you have no idea where to start.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                     <p><strong>Good project organization is the single best way to be kind to your most important collaborator: your future self.</strong> A logical structure makes your work understandable, reproducible, and professional.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">A Battle-Tested Project Structure</h3>
                <p>There are many ways to organize a project, but most effective structures follow a similar logic. Here is a highly recommended and scalable template for almost any data analysis project. </p>
                <div class="code-container">
                    <pre><code class="language-bash">my_analysis_project/
├── my_analysis_project.Rproj
├── data/
│   ├── raw/
│   │   └── original_data.csv
│   └── processed/
│       └── cleaned_data.rds
├── R/
│   ├── 01_load_data.R
│   ├── 02_clean_data.R
│   └── 03_analyze_and_plot.R
├── reports/
│   ├── final_report.html
│   └── final_report.Rmd
└── README.md
</code></pre>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">The Role of Each Component</h3>
                <ul class="prose-list">
                    <li><strong><code>my_analysis_project.Rproj</code></strong>: The RStudio Project file. This is the heart of your project. You should always start your work by clicking this file.</li>
                    <li><strong><code>data/</code></strong>: The folder for all data.
                        <ul>
                            <li><strong><code>raw/</code></strong>: This is for the original, untouched data. Treat this folder as <strong>read-only</strong>. Never edit files here. This ensures you can always trace your work back to the source.</li>
                            <li><strong><code>processed/</code></strong>: When you clean or modify your raw data, save the clean version here. Your analysis scripts will read data from this folder.</li>
                        </ul>
                    </li>
                    <li><strong><code>R/</code></strong> (or <code>scripts/</code>): This folder contains all your R scripts.
                        <ul>
                            <li><strong>Pro Tip:</strong> Numbering your scripts (e.g., <code>01_...</code>, <code>02_...</code>) is a powerful way to enforce a logical workflow. It tells anyone (including you) the exact order in which to run the analysis.</li>
                        </ul>
                    </li>
                    <li><strong><code>reports/</code></strong>: This folder is for all your outputs. Store your R Markdown files (<code>.Rmd</code>), their rendered HTML or PDF reports, and any plots or tables you save.</li>
                    <li><strong><code>README.md</code></strong>: A simple text file where you describe the project. What is its goal? Where did the data come from? What are the main findings? How does someone run your analysis?</li>
                </ul>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Always Use Relative Paths</h3>
                <div class="scenario-content">
                    <p>Because you are using an R Project, your working directory is always the project's root folder. This means you should <strong>never</strong> use absolute paths in your code. Always use paths relative to the project root.</p>
                    <p><strong>GOOD:</strong> This path works on anyone's computer.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># This path starts from the project's root folder. It is portable.
my_data <- read.csv("data/raw/original_data.csv")</code></pre>
                    </div>
                    <p><strong>BAD:</strong> This path will break on anyone else's computer (and probably your own, eventually).</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># This path is specific to your machine's file structure. It is not portable.
my_data <- read.csv("C:/Users/YourName/Desktop/my_analysis_project/data/raw/original_data.csv")</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's build this structure from scratch.</p>
                    <ul class="prose-list">
                        <li>Using the RStudio menu (<code>File > New Project...</code>), create a new, empty R Project on your computer named <code>my_structured_project</code>.</li>
                        <li>Using the "Files" pane in RStudio, click the "New Folder" button to create the following four directories inside your project: <code>data</code>, <code>R</code>, and <code>reports</code>.</li>
                        <li>Navigate into the <code>data</code> folder and create two more folders inside it: <code>raw</code> and <code>processed</code>.</li>
                        <li>Create a new R Script (<code>File > New File > R Script</code>). Add a comment like <code># 01: Data Loading Script</code> and save it inside your <code>R</code> folder with the name <code>01_load.R</code>.</li>
                        <li>Your professional project structure is now ready for an analysis!</li>
                    </ul>
                </div>
            </div>
          `,
          p1s1ss6: `
            <div class="subsection">
                <h3 class="subsection-title">The Goal of Scientific Analysis</h3>
                <p style="margin-bottom: 1.5rem;">You share your code and data with a colleague, confident in your results. They run your script but get a completely different answer, or worse, an error. This common, frustrating scenario is a failure of reproducibility.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                     <p><strong>Reproducibility</strong> is the ability for an independent person to take your exact data, code, and computational environment and arrive at the exact same results. It is the cornerstone of credible data analysis.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="why">Why It Matters: The Recipe Analogy</button>
                    <button class="tab-button" data-tab="how">The Reproducibility Toolkit</button>
                </div>

                <div id="why" class="tab-pane active">
                    <h4 class="subsection-title">Baking a Cake vs. Analyzing Data</h4>
                    <p>Think of your analysis as a baking recipe. </p>
                    <ul class="prose-list">
                        <li><strong>A good recipe</strong> lists precise ingredients (your <strong>data</strong>), exact step-by-step instructions (your <strong>code</strong>), and the required oven temperature (your <strong>software environment</strong>). Anyone following it should produce the same delicious cake (your <strong>results</strong>).</li>
                        <li><strong>A bad recipe</strong> might say "add some flour" or "bake until it looks done." This ambiguity leads to different, unreliable outcomes. A non-reproducible analysis is like a bad recipe.</li>
                    </ul>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Reproducibility is critical for:</h5>
                     <ul class="prose-list">
                        <li><strong>Collaboration:</strong> Allows teammates to verify, trust, and build upon your work without friction.</li>
                        <li><strong>Your Future Self:</strong> Ensures you can re-run your own analysis six months from now and get the same result.</li>
                        <li><strong>Credibility:</strong> Proves that your findings are not a fluke or a one-time accident. It is the foundation of scientific and professional integrity.</li>
                    </ul>
                </div>

                <div id="how" class="tab-pane">
                    <h4 class="subsection-title">Your Checklist for Reliable Work</h4>
                    <p>Achieving reproducibility involves using the tools and habits you've already started learning, plus a few new ones.</p>
                    <ul class="prose-list">
                        <li><strong>Use R Projects:</strong> This is non-negotiable. It keeps all your files together and solves the working directory problem.</li>
                        <li><strong>Use a Clear File Structure:</strong> A logical folder structure (<code>/data</code>, <code>/R</code>, <code>/reports</code>) allows others to easily follow the flow of your analysis.</li>
                        <li><strong>Comment Your Code:</strong> Write clean code and use comments to explain the <em>why</em> behind your decisions, not just the <em>what</em>.</li>
                        <li><strong>Manage Your Environment:</strong> Always start your R session with a clean slate. Avoid saving your workspace between sessions. A common practice is to start your main script with a line to clear all objects.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># This command removes all objects from your current environment
rm(list = ls())</code></pre>
                            </div>
                        </li>
                        <li><strong>Manage Package Versions with <code>renv</code>:</strong> What happens if a package you used is updated and it changes how a function works? Your old script might break! The <code>renv</code> package solves this by creating a private library for each project, saving the exact versions of the packages you used.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># To start using renv in a project (do this once)
renv::init()

# When you install or update packages, save the state
renv::snapshot()</code></pre>
                            </div>
                            <p>When a collaborator opens your project, they can run <code>renv::restore()</code> to install the exact same package versions you used.</p>
                        </li>
                    </ul>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Control the Randomness</h3>
                <div class="scenario-content">
                    <p>Many analyses involve an element of randomness (e.g., splitting data, bootstrapping, some machine learning models). If you run the script again, you'll get slightly different results. To make even random processes reproducible, always set a "seed" at the start of your script with <code>set.seed()</code>.</p>
                    <p>This function initializes the random number generator with a specific starting number. As long as the seed is the same, the sequence of "random" numbers will be identical every time.</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># By setting a seed, the "random" sample will be the same every time.
set.seed(42)
sample(1:100, 5) # Will always produce: 92 93 29 80 41</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's demonstrate the power of setting a seed.</p>
                    <ul class="prose-list">
                        <li>In a new R script, run the command <code>sample(letters, 5)</code> three times. Notice how you get a different set of 5 random letters each time.</li>
                        <li>Now, add <code>set.seed(123)</code> to the top of your script.</li>
                        <li>Run the entire script (including the <code>set.seed(123)</code> line) three times. Notice how you get the <strong>exact same</strong> set of "random" letters every single time. You have just performed a reproducible random process!</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss1t1: `
            <div class="subsection">
                <h3 class="subsection-title">The Building Blocks of Code</h3>
                <p style="margin-bottom: 1.5rem;">To give R instructions, you need to understand the three most basic building blocks of any script: how to store information (variables), how to store it (assignment), and how to explain your code to humans (comments).</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You are a data analyst at a grocery store. You have 15 apples in one basket and 22 in another. You need to calculate the total and save that number so you can use it later to figure out total sales. This simple task requires using variables, assignment, and comments.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="variables">Variables: Storing Information</button>
                    <button class="tab-button" data-tab="assignment">Assignment: Giving Variables a Value</button>
                    <button class="tab-button" data-tab="comments">Comments: Explaining Your Code</button>
                </div>

                <div id="variables" class="tab-pane active">
                    <h4 class="subsection-title">Creating Labeled Boxes</h4>
                    <p>A <strong>variable</strong> is a name that refers to a value. Think of it as a labeled box where you can store a piece of information, like a number or some text. By giving the value a name, you can easily refer back to it later in your code.</p>

                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Variable Naming Rules in R:</h5>
                    <ul class="prose-list">
                        <li>Variable names must start with a letter.</li>
                        <li>They can contain letters, numbers, the dot (<code>.</code>), and the underscore (<code>_</code>).</li>
                        <li>R is <strong>case-sensitive</strong>. This means <code>my_age</code> is a different variable from <code>My_Age</code>.</li>
                    </ul>
                    <div class="scenario-content" style="border-color: var(--success); margin-top: 1.5rem;">
                        <p><strong>Best Practice: Use snake_case.</strong> While you can use dots (<code>my.variable</code>) or camelCase (<code>myVariable</code>), the most common and readable style in the R community is <strong>snake_case</strong>, where words are separated by underscores (e.g., <code>total_apple_count</code>).</p>
                    </div>
                </div>

                <div id="assignment" class="tab-pane">
                   <h4 class="subsection-title">Putting Values in the Box</h4>
                   <p><strong>Assignment</strong> is the act of placing a value into a variable. In R, the standard assignment operator is <code><-</code>.</p>
                   <p>Think of it as an arrow pointing from the value on the right to the variable name on the left. It visually says, "Put this value into this box."</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Here, we assign the value 30 to the variable my_age
my_age <- 30</code></pre>
                    </div>
                    <p>Let's solve our grocery store problem:</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Create a variable for the first basket
basket_1 <- 15

# Create a variable for the second basket
basket_2 <- 22

# Calculate the sum and assign it to a new variable
total_apples <- basket_1 + basket_2

# To see the result, just type the variable's name
total_apples</code></pre>
                    </div>
                    <p>When you run the last line, R will print <code>[1] 37</code> to the console.</p>
                    <p><strong>Note:</strong> While a single equals sign (<code>=</code>) often works for assignment, <code><-</code> is the universal convention in R. Using it makes your code clearer and avoids potential confusion, as <code>=</code> is also used for other purposes.</p>
                </div>

                <div id="comments" class="tab-pane">
                    <h4 class="subsection-title">Notes for Humans</h4>
                    <p>A <strong>comment</strong> is text in your script that R will completely ignore. Its purpose is to explain your code to human readers (including your future self). In R, anything on a line that follows a hash symbol (<code>#</code>) is a comment.</p>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Good vs. Bad Comments</h5>
                    <p>Good code should be easy to read, and good comments should explain the <strong>why</strong>, not the <strong>what</strong>.</p>
                    <ul class="prose-list">
                        <li><strong>A less useful comment:</strong>
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Add 1 to x
x <- x + 1</code></pre>
                            </div>
                            <p>This isn't helpful because the code itself already says what it's doing.</p>
                        </li>
                        <li><strong>A better comment:</strong>
                             <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Increment the loop counter to include the final value
x <- x + 1</code></pre>
                            </div>
                            <p>This is much better because it explains the <em>purpose</em> or <em>intent</em> of the code.</p>
                        </li>
                    </ul>
                </div>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Essential Shortcuts</h3>
                <div class="scenario-content">
                    <p>Using keyboard shortcuts for common tasks will dramatically speed up your coding.</p>
                     <ul class="prose-list">
                        <li><strong>Insert Assignment Operator (<code><-</code>):</strong> <code>Alt + -</code> (Windows/Linux) or <code>Option + -</code> (macOS).</li>
                        <li><strong>Comment/Uncomment Lines:</strong> Select one or more lines of code and press <code>Ctrl + Shift + C</code> (Windows/Linux) or <code>Cmd + Shift + C</code> (macOS).</li>
                    </ul>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's calculate the final price of an item including sales tax.</p>
                    <ul class="prose-list">
                        <li>Create a new R script.</li>
                        <li>Create a variable named <code>item_price</code> and assign it the value <code>150</code>.</li>
                        <li>Create another variable named <code>tax_rate</code> and assign it the value <code>0.08</code>.</li>
                        <li>Above the <code>tax_rate</code> variable, add a comment that says: <code># 8% sales tax</code>.</li>
                        <li>Create a third variable, <code>total_price</code>, that stores the result of <code>item_price * (1 + tax_rate)</code>.</li>
                        <li>Finally, type <code>total_price</code> on the last line to print the result to the console when you run the script.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss1t2: `
            <div class="subsection">
                <h3 class="subsection-title">Storing Collections of Data</h3>
                <p style="margin-bottom: 1.5rem;">While variables are great for storing a single value, data analysis almost always involves working with collections of values. R has several fundamental data structures for this purpose, known as atomic data types because their elements are of a single, simple type.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> An analyst is tracking weekly cafe sales. They need to store the sales figures for each day of the week. They also need to store information about a single customer, including their name (text), age (number), and whether they are a loyalty member (TRUE/FALSE). These two tasks require different types of data structures.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="vectors">Vectors: The Foundation</button>
                    <button class="tab-button" data-tab="matrices">Matrices & Arrays: 2D and Beyond</button>
                    <button class="tab-button" data-tab="lists">Lists: The Mixed-Type Collector</button>
                </div>

                <div id="vectors" class="tab-pane active">
                    <h4 class="subsection-title">The Building Block of R</h4>
                    <p>The <strong>vector</strong> is the most basic data structure in R. It's a sequence of elements that must all be of the <strong>same type</strong> (e.g., all numbers, or all text).</p>
                    <p><strong>Analogy:</strong> Think of a single column in a spreadsheet. Every cell in that column contains the same kind of data. </p>
                    <p>You create vectors with the <code>c()</code> function, which stands for "combine" or "concatenate".</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># A numeric vector for daily sales
daily_sales <- c(450.50, 675.25, 512.00, 780.75, 940.10)

# A character vector for the days of the week
weekdays <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")

# A logical vector indicating if a sales goal was met
goal_met <- c(FALSE, TRUE, TRUE, TRUE, TRUE)</code></pre>
                    </div>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">The "One Type" Rule (Coercion)</h5>
                    <p>What happens if you try to mix types in a vector? R will force all elements to conform to the most flexible type, a process called <strong>coercion</strong>.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># R will convert the numbers 1 and 5 to text ("1" and "5")
mixed_vector <- c(1, "hello", 5)

# The result is a character vector: "1", "hello", "5"
mixed_vector</code></pre>
                    </div>
                </div>

                <div id="matrices" class="tab-pane">
                    <h4 class="subsection-title">Data in Rows and Columns</h4>
                    <p>A <strong>matrix</strong> is a two-dimensional collection of elements, all of the <strong>same type</strong>. It's like a vector that has been arranged into a specific number of rows and columns.</p>
                    <p><strong>Analogy:</strong> A matrix is like a chessboard or an entire spreadsheet table (as long as all cells contain the same type of data).</p>
                    <p>You create matrices with the <code>matrix()</code> function.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Create a matrix of sales data for 2 stores over 3 days
# The data fills the matrix column by column by default.
sales_data <- matrix(data = c(10, 15, 12, 20, 25, 22), nrow = 2, ncol = 3)

# View the matrix
sales_data</code></pre>
                    </div>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Arrays: More Than Two Dimensions</h5>
                    <p>An <strong>array</strong> is an extension of a matrix to three or more dimensions. While less common for beginners, they are useful for storing multi-dimensional data, such as sales data across multiple stores, days, and products.</p>
                </div>

                <div id="lists" class="tab-pane">
                   <h4 class="subsection-title">The Flexible Container</h4>
                   <p>A <strong>list</strong> is a special type of vector where each element can be a <strong>different type</strong>. A list can contain anything: numbers, text, vectors, matrices, and even other lists!</p>
                   <p><strong>Analogy:</strong> A list is like a toolbox or a filing cabinet. You can store a variety of different items together in one place. </p>
                   <p>You create lists with the <code>list()</code> function. It's excellent practice to name the elements in your list.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Creating a list to store information about our customer
customer_profile <- list(
  name = "John Smith",
  age = 42,
  loyalty_member = TRUE,
  past_purchases = c(25.50, 15.00, 52.75)
)

# View the list
customer_profile</code></pre>
                    </div>
                    <p>Lists are one of the most powerful and flexible data structures in R, perfect for storing complex, hierarchical information.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Inspecting Your Objects</h3>
                <div class="scenario-content">
                    <p>How can you tell what kind of data structure you're working with? R has several functions to help you inspect your objects. These are invaluable for debugging.</p>
                    <ul class="prose-list">
                        <li><code>class(x)</code>: Tells you the object's high-level type (e.g., "numeric", "matrix", "list").</li>
                        <li><code>typeof(x)</code>: Tells you the low-level storage type (e.g., "double", "character", "list").</li>
                        <li><code>str(x)</code>: Shows the <strong>str</strong>ucture of an object. This is one of the most useful functions in R for understanding any data object, especially lists!</li>
                    </ul>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Inspect the customer_profile list from the example above
str(customer_profile)</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's practice creating these fundamental data structures.</p>
                     <ul class="prose-list">
                        <li>Create a numeric vector named <code>product_prices</code> containing three prices: <code>99.99</code>, <code>49.50</code>, and <code>120.00</code>.</li>
                        <li>Create a character vector named <code>product_names</code> containing three corresponding names: <code>"Laptop"</code>, <code>"Mouse"</code>, and <code>"Keyboard"</code>.</li>
                        <li>Create a list named <code>product_1</code> that stores the information for the first product. It should have two named elements: <code>name</code> (containing "Laptop") and <code>price</code> (containing 99.99).</li>
                        <li>Use the <code>str()</code> function to inspect the structure of your <code>product_1</code> list.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss1t3: `
            <div class="subsection">
                <h3 class="subsection-title">Organizing Data in Tables</h3>
                <p style="margin-bottom: 1.5rem;">While atomic vectors are the building blocks, real-world data is rarely just a single sequence. It's usually tabular, with rows and columns, where each column can be a different data type. R provides powerful composite data types specifically for handling this kind of structured data.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> An online retailer has sales data. They have a list of product names (text), a list of prices (numbers), and a list of customer ratings (categories like "Good", "Poor"). To analyze this effectively, they need to combine these separate lists into a single, organized table.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="dataframe">Data Frames: R's Spreadsheet</button>
                    <button class="tab-button" data-tab="tibble">Tibbles: The Modern Data Frame</button>
                    <button class="tab-button" data-tab="factor">Factors: Handling Categories</button>
                </div>

                <div id="dataframe" class="tab-pane active">
                    <h4 class="subsection-title">The Workhorse of R</h4>
                    <p>A <strong>data frame</strong> is R's primary structure for storing tabular data. It's a two-dimensional object where columns can be of different types (numeric, character, etc.), but all columns must have the exact same length.</p>
                    <p><strong>Analogy:</strong> A data frame is the R equivalent of a single sheet in a spreadsheet program like Excel or Google Sheets. </p>
                    <p>Under the hood, a data frame is a list of vectors, where each vector is a column.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># First, create the individual vectors (columns)
product_names <- c("Laptop", "Mouse", "Keyboard")
prices <- c(999.99, 49.50, 120.00)
units_sold <- c(15, 82, 45)

# Now, combine them into a data frame
sales_df <- data.frame(product_names, prices, units_sold)

# Print the data frame to the console
sales_df</code></pre>
                    </div>
                </div>

                <div id="tibble" class="tab-pane">
                    <h4 class="subsection-title">A Nicer Way to Work with Tables</h4>
                    <p>A <strong>tibble</strong> is the modern, reimagined data frame from the <strong>Tidyverse</strong> (a collection of popular R packages for data science). For nearly all new analyses, you should prefer tibbles over traditional data frames.</p>
                     <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">What makes tibbles better?</h5>
                     <ul class="prose-list">
                        <li><strong>Smarter Printing:</strong> A tibble only prints the first 10 rows and the columns that fit on your screen, preventing you from accidentally flooding your console. It also helpfully displays the data type of each column (e.g., <code>&lt;dbl&gt;</code> for numbers, <code>&lt;chr&gt;</code> for characters).</li>
                        <li><strong>No Surprises:</strong> Traditional data frames sometimes automatically convert your text columns into factors (see next tab), which can cause unexpected issues. Tibbles never change your data types without being asked.</li>
                    </ul>
                    <p>You create tibbles using the <code>tibble()</code> function from the <code>tibble</code> package.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># You need to install and load the package first
# install.packages("tibble")
library(tibble)

# Create a tibble
sales_tbl <- tibble(
  product = c("Laptop", "Mouse", "Keyboard"),
  price = c(999.99, 49.50, 120.00),
  units_sold = c(15, 82, 45)
)

# Print the tibble and notice the cleaner output!
sales_tbl</code></pre>
                    </div>
                </div>

                <div id="factor" class="tab-pane">
                   <h4 class="subsection-title">Storing Categorical Data Efficiently</h4>
                   <p>A <strong>factor</strong> is a special data type used to store a variable that can only take on a limited, fixed number of values. These fixed values are called <strong>levels</strong>.</p>
                   <p><strong>Analogy:</strong> Think of the options in a dropdown menu, like T-shirt sizes ("Small", "Medium", "Large"). The set of possible values is predefined and limited.</p>
                   <p>Using factors is important because many statistical models and plotting functions in R are designed to treat categorical data correctly, and it can be more memory-efficient than storing text.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># A character vector of customer ratings
ratings_vector <- c("Good", "Poor", "Good", "Excellent", "Good", "Poor")

# Convert the vector to a factor
ratings_factor <- factor(ratings_vector)

# Print the factor and notice the "Levels" output
ratings_factor</code></pre>
                    </div>
                    <p>The output shows the data but also lists the unique levels: <code>"Excellent"</code>, <code>"Good"</code>, <code>"Poor"</code>. You can also create <strong>ordered factors</strong> for categories that have a natural ranking, like "Low", "Medium", "High".</p>
                </div>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: <code>glimpse()</code> your data</h3>
                <div class="scenario-content">
                    <p>While <code>str()</code> is great for inspecting any R object, the Tidyverse provides a more user-friendly alternative for data frames and tibbles: <code>glimpse()</code>. It prints a compact, transposed view of your data that's perfect for getting a quick look at all the columns, their data types, and a few sample values.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># glimpse() is in the dplyr package (part of the Tidyverse)
# install.packages("dplyr")
library(dplyr)

# Use glimpse on the tibble we created earlier
glimpse(sales_tbl)</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's create a small dataset about company employees.</p>
                     <ul class="prose-list">
                        <li>Make sure you have the <code>tibble</code> package loaded with <code>library(tibble)</code>.</li>
                        <li>Create three vectors: <code>employee_name</code> with names "Alice", "Bob", "Charlie", "David"; <code>department</code> with values "Sales", "HR", "IT", "Sales"; and <code>start_year</code> with values 2018, 2020, 2019, 2021.</li>
                        <li>Combine these vectors into a tibble called <code>employee_data</code>.</li>
                        <li>The <code>department</code> column is categorical. Overwrite the original column by converting it to a factor: <code>employee_data$department <- factor(employee_data$department)</code>.</li>
                        <li>Use <code>str(employee_data)</code> or <code>glimpse(employee_data)</code> to confirm that the <code>department</code> column is now a Factor.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss1t4: `
            <div class="subsection">
                <h3 class="subsection-title">Coercion vs. Conversion</h3>
                <p style="margin-bottom: 1.5rem;">R is a dynamically typed language, which means it's generally flexible about data types. However, this flexibility can sometimes lead to unexpected behavior. It's crucial to understand how R handles data types, both automatically (coercion) and manually (conversion).</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You've imported data from a CSV file. A column containing product ratings from 1 to 5 has a single accidental text entry like "N/A". Because of this one entry, R has read the entire column as text. When you try to calculate the average rating, you get an error. You need to understand why this happened and how to fix it.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="coercion">Implicit Coercion: R's Automatic Pilot</button>
                    <button class="tab-button" data-tab="conversion">Explicit Conversion: Taking Control</button>
                </div>

                <div id="coercion" class="tab-pane active">
                    <h4 class="subsection-title">The "One Type" Rule in Action</h4>
                    <p>As we learned with atomic vectors, all elements must be of the same type. When you try to combine different types, R automatically forces them into a single common type. This process is called <strong>implicit coercion</strong>.</p>
                    <p>R follows a strict hierarchy: <strong>Logical → Integer → Numeric → Character</strong>. When mixed, all elements are promoted to the most flexible type in the hierarchy.</p>
                    <p></p>
                    <ul class="prose-list">
                        <li><strong>Logical to Numeric:</strong> <code>TRUE</code> becomes <code>1</code> and <code>FALSE</code> becomes <code>0</code>.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># TRUE is coerced to 1, FALSE to 0
c(TRUE, FALSE, TRUE) + 10 # Result: 11 10 11</code></pre>
                            </div>
                        </li>
                        <li><strong>Numeric to Character:</strong> This is the most common form of coercion. Because anything can be represented as text, character is the "highest" type.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># The number 5 becomes the character "5"
c("apple", 5, "orange") # Result: "apple" "5" "orange"</code></pre>
                            </div>
                            <p>This is exactly what happened in our scenario: the one "N/A" text entry forced the entire ratings column to become character data.</p>
                        </li>
                    </ul>
                </div>

                <div id="conversion" class="tab-pane">
                    <h4 class="subsection-title">The <code>as.*()</code> Family of Functions</h4>
                    <p>When you want to intentionally change a data type, you use <strong>explicit conversion</strong> functions. This is how you fix data type problems. These functions almost all follow the pattern <code>as.newtype()</code>.</p>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Common Conversion Functions:</h5>
                     <ul class="prose-list">
                        <li><code>as.numeric()</code>: Converts to a number (with decimals).</li>
                        <li><code>as.integer()</code>: Converts to an integer (whole number).</li>
                        <li><code>as.character()</code>: Converts to text.</li>
                        <li><code>as.logical()</code>: Converts to <code>TRUE</code> or <code>FALSE</code>.</li>
                        <li><code>as.factor()</code>: Converts to a factor.</li>
                    </ul>
                    <p>Let's solve our scenario's problem:</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Our ratings column, read in as text
ratings_text <- c("5", "4", "N/A", "5", "3")
class(ratings_text) # "character"

# Let's try to convert it to numbers
ratings_numeric <- as.numeric(ratings_text)</code></pre>
                    </div>
                    <p>R will try its best to convert each element. When it encounters "N/A", it can't turn it into a number, so it will produce an <code>NA</code> (Not Available) value and show a warning message: <code>NAs introduced by coercion</code>. This is helpful because it flags the problematic data for you!</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># View the result
ratings_numeric # Result: 5  4 NA  5  3
class(ratings_numeric) # "numeric"</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Check Before You Convert</h3>
                <div class="scenario-content">
                    <p>Before you perform an operation, it's good practice to check if your data is the type you expect. R provides a family of <code>is.*()</code> functions for this purpose, like <code>is.numeric()</code>, <code>is.character()</code>, etc. These functions return <code>TRUE</code> or <code>FALSE</code>.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">my_data <- "123"

if (!is.numeric(my_data)) {
  print("Data is not numeric. Converting now...")
  my_data <- as.numeric(my_data)
}

class(my_data) # Now it's "numeric"</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Practice cleaning a messy vector.</p>
                    <ul class="prose-list">
                        <li>Create the following character vector: <code>messy_vector <- c("10.5", "20.1", "thirty", "40.3")</code>.</li>
                        <li>Use <code>class()</code> to confirm that it is a character vector.</li>
                        <li>Use <code>as.numeric()</code> to try and convert it. Notice the warning message and the <code>NA</code> value that is created for "thirty". Store this new vector in a variable called <code>clean_vector</code>.</li>
                        <li>Calculate the average of <code>clean_vector</code>. You'll get <code>NA</code> because of the missing value.</li>
                        <li>Read the help file for the mean function by typing <code>?mean</code> in your console. Discover the argument that removes NA values and use it to successfully calculate the average (Hint: <code>mean(clean_vector, na.rm = TRUE)</code>).</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss1t5: `
            <div class="subsection">
                <h3 class="subsection-title">Accessing Your Data</h3>
                <p style="margin-bottom: 1.5rem;">You will rarely want to work with your entire dataset at once. More often, you'll need to pull out specific pieces: a single value, a single row, a few columns, or rows that meet a certain condition. The process of selecting data is called <strong>subsetting</strong> or <strong>indexing</strong>.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You have a large table of product sales. Your manager asks two questions: "What is the price of the third product on our list?" and "Can you show me only the sales data for products that cost more than $100?" To answer these, you need to know how to subset your data.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="vectors">Vectors: Using <code>[ ]</code></button>
                    <button class="tab-button" data-tab="dataframes">Data Frames: Using <code>[ , ]</code> & <code>$</code></button>
                    <button class="tab-button" data-tab="lists">Lists: Using <code>[[ ]]</code> & <code>$</code></button>
                </div>

                <div id="vectors" class="tab-pane active">
                    <h4 class="subsection-title">Selecting from a Sequence</h4>
                    <p>The square brackets <code>[ ]</code> are used to subset vectors. There are three primary ways to do this:</p>
                     <ul class="prose-list">
                        <li><strong>By Index (Position):</strong> R is a <strong>1-indexed</strong> language, meaning the first element is at position 1.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r">prices <- c(25, 30, 15, 50, 45)
prices[3] # Selects the 3rd element. Result: 15</code></pre>
                            </div>
                        </li>
                        <li><strong>By Range of Indices:</strong> Select a "slice" of the vector.
                             <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r">prices[2:4] # Selects elements from the 2nd to the 4th. Result: 30 15 50</code></pre>
                            </div>
                        </li>
                         <li><strong>By a Logical Vector:</strong> This is the most powerful method. You provide a logical vector of <code>TRUE</code>/<code>FALSE</code> values, and R returns only the elements where the value is <code>TRUE</code>.
                              <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># First, create the logical vector:
prices > 35 # Result: FALSE FALSE FALSE  TRUE  TRUE

# Now, use it to subset:
prices[prices > 35] # Returns only the elements where the condition is TRUE. Result: 50 45</code></pre>
                            </div>
                        </li>
                    </ul>
                </div>

                <div id="dataframes" class="tab-pane">
                    <h4 class="subsection-title">Selecting from a Table</h4>
                    <p>Since data frames have two dimensions, you need to provide two indices inside the brackets: <code>my_df[row_index, column_index]</code>. Leaving an index blank selects all rows or columns.</p>
                     <ul class="prose-list">
                        <li><strong>Select Rows:</strong> <code>my_df[1, ]</code> selects the first row. <code>my_df[1:5, ]</code> selects the first five rows.</li>
                        <li><strong>Select Columns:</strong> <code>my_df[, 2]</code> selects the second column. You can also select columns by name: <code>my_df[, c("product", "price")]</code>.</li>
                        <li><strong>Select a Specific Cell:</strong> <code>my_df[3, 2]</code> selects the cell at the 3rd row, 2nd column.</li>
                        <li><strong>The <code>$</code> Operator:</strong> The easiest way to select a single column by name is with the <code>$</code> operator. This returns the column as a vector.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Create a sample tibble
sales_tbl <- tibble(product = c("A", "B", "C"), price = c(150, 80, 120))

# Select the 'price' column using $
sales_tbl$price # Result: 150  80 120</code></pre>
                            </div>
                        </li>
                        <li><strong>Logical Subsetting (Rows):</strong> This is extremely common. You use a logical condition on a column to select only the rows that match.
                             <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Select all rows where the price is greater than 100
sales_tbl[sales_tbl$price > 100, ]</code></pre>
                            </div>
                        </li>
                    </ul>
                </div>

                <div id="lists" class="tab-pane">
                   <h4 class="subsection-title">The Train Analogy: <code>[ ]</code> vs. <code>[[ ]]</code></h4>
                   <p>Subsetting lists introduces a critical new concept. There are two operators, and they do very different things.</p>
                   <p><strong>Analogy:</strong> Imagine your list is a train with several cars.</p>
                   <ul class="prose-list">
                       <li><code>[ ]</code> (single bracket) <strong>always returns a new, smaller train (a sub-list)</strong>. If you ask for <code>my_train[1]</code>, you get back a tiny train containing only the first car.</li>
                       <li><code>[[ ]]</code> (double bracket) <strong>extracts the contents *from* a single car</strong>. If you ask for <code>my_train[[1]]</code>, you reach inside the first car and pull out its contents (the passengers, luggage, etc.).</li>
                   </ul>
                   <p><strong>Rule of thumb: You will use <code>[[ ]]</code> most of the time to get an object *out* of a list to work with it.</strong> The <code>$</code> operator is a convenient shortcut for <code>[[ ]]</code> with named elements.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">customer_profile <- list(name = "John Smith", age = 42)

# Using single brackets returns a smaller LIST
result_1 <- customer_profile[1]
class(result_1) # "list"

# Using double brackets extracts the ELEMENT
result_2 <- customer_profile[[1]]
class(result_2) # "character"

# The $ operator is a shortcut for [[]] by name
customer_profile$name # "John Smith"</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Excluding Elements</h3>
                <div class="scenario-content">
                    <p>You can use a negative integer to exclude elements when subsetting. This is a handy shortcut for dropping a single row or column.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">my_vector <- c("a", "b", "c", "d")

# Return the vector WITHOUT the 2nd element
my_vector[-2] # Result: "a" "c" "d"

# For data frames, you can drop a column
# iris is a famous built-in dataset
head(iris[, -5]) # Returns the first few rows of iris without the 5th column</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>The <code>iris</code> dataset is built into R and is great for practicing. Let's subset it.</p>
                     <ul class="prose-list">
                        <li>Load the dataset as a tibble to make it easier to view: <code>library(tibble)</code> then <code>iris_tbl <- as_tibble(iris)</code>.</li>
                        <li>Select only the 25th row from <code>iris_tbl</code>.</li>
                        <li>Select the <code>Petal.Length</code> column using the <code>$</code> operator.</li>
                        <li>Select all rows where the <code>Species</code> is "virginica". (Hint: The condition is <code>iris_tbl$Species == "virginica"</code>).</li>
                        <li>Select rows 50 through 60, but only the <code>Sepal.Width</code> and <code>Species</code> columns.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss1t6: `
            <div class="subsection">
                <h3 class="subsection-title">Handling Imperfect Data</h3>
                <p style="margin-bottom: 1.5rem;">Real-world data is often messy. It can have missing entries, or you might perform a calculation that results in an impossible number. R has a set of special values to represent these cases.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You're analyzing survey data and need to calculate the average age of respondents. However, some people left the "age" field blank. When you try to calculate the mean of the age column in R, the result is simply <code>NA</code>, not a number. You need to understand what this means and how to get a valid average.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="na"><code>NA</code>: Not Available</button>
                    <button class="tab-button" data-tab="nan"><code>NaN</code>: Not a Number</button>
                    <button class="tab-button" data-tab="inf"><code>Inf</code>: Infinity</button>
                    <button class="tab-button" data-tab="handling">Detection & Handling</button>
                </div>

                <div id="na" class="tab-pane active">
                    <h4 class="subsection-title">The Placeholder for Missing Data</h4>
                    <p><code>NA</code> (Not Available) is R's primary way of representing a missing value. It's a placeholder for a piece of information that you don't have.</p>
                    <p><strong>Analogy:</strong> Think of an empty cell in a spreadsheet or an unanswered question on a form. The data should be there, but it's not. </p>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">The Contagious Nature of <code>NA</code></h5>
                    <p>Most functions in R are "contagious" with respect to <code>NA</code>. If any input to a function is <code>NA</code>, the output will also be <code>NA</code>. This is a safety feature: R won't give you a silently incorrect answer; instead, it tells you the result is unknown because some of the data was missing.</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># A vector of ages with one missing value
ages <- c(25, 30, NA, 45, 22)

# Trying to calculate the mean results in NA
mean(ages) # Result: NA
sum(ages)  # Result: NA</code></pre>
                    </div>
                </div>

                <div id="nan" class="tab-pane">
                    <h4 class="subsection-title">The Result of Impossible Math</h4>
                    <p><code>NaN</code> (Not a Number) is a special type of <code>NA</code> that represents an undefined or mathematically impossible result.</p>
                    <p>You typically won't type <code>NaN</code> yourself; it's the result of calculations that don't make sense, like dividing zero by zero.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># These calculations are mathematically undefined
0 / 0      # Result: NaN
sqrt(-1)   # Result: NaN (and a warning)</code></pre>
                    </div>
                </div>

                <div id="inf" class="tab-pane">
                   <h4 class="subsection-title">Representing the Unrepresentable</h4>
                   <p><code>Inf</code> (Infinity) is a value that represents infinity. It's the result of a calculation that produces a number too large for the computer to store, or a division by zero.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Dividing a positive number by zero gives infinity
10 / 0       # Result: Inf

# Dividing a negative number by zero gives negative infinity
-50 / 0      # Result: -Inf

# A number that is too large
10^2000      # Result: Inf</code></pre>
                    </div>
                </div>

                <div id="handling" class="tab-pane">
                    <h4 class="subsection-title">Finding and Dealing with Special Values</h4>
                    <p>You can't use standard comparison operators (like <code>==</code>) to check for these values. Instead, R provides a family of <code>is.*()</code> functions to test for them.</p>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Detection Functions:</h5>
                    <ul class="prose-list">
                        <li><code>is.na(x)</code>: The most common function. It returns <code>TRUE</code> for both <code>NA</code> and <code>NaN</code> values.</li>
                        <li><code>is.nan(x)</code>: Checks specifically for <code>NaN</code>.</li>
                        <li><code>is.infinite(x)</code>: Checks for <code>Inf</code> and <code>-Inf</code>.</li>
                        <li><code>is.finite(x)</code>: Returns <code>TRUE</code> for normal numbers, and <code>FALSE</code> for <code>NA</code>, <code>NaN</code>, and <code>Inf</code>.</li>
                    </ul>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Handling Missing Values:</h5>
                    <p>The two most common ways to handle missing values are:</p>
                    <ul class="prose-list">
                        <li><strong>Use the <code>na.rm = TRUE</code> argument:</strong> Many summary functions (like <code>mean()</code>, <code>sum()</code>, <code>max()</code>) have an argument that tells them to remove missing values before calculation. This is the direct solution to our scenario.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r">ages <- c(25, 30, NA, 45, 22)
mean(ages, na.rm = TRUE) # Result: 30.5</code></pre>
                            </div>
                        </li>
                        <li><strong>Filter them out:</strong> You can create a new, clean dataset by removing all rows that contain <code>NA</code> values.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># !is.na(ages) returns TRUE for all values that are NOT NA
clean_ages <- ages[!is.na(ages)]
clean_ages # Result: 25 30 45 22</code></pre>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: The "NA" String is Not <code>NA</code></h3>
                <div class="scenario-content">
                    <p>A very common pitfall is confusing R's special <code>NA</code> value with the character string <code>"NA"</code>. When you import data, missing values are often represented by text like "NA", "missing", or "999".</p>
                    <p>These are just text to R and won't be treated as missing values unless you explicitly tell R to convert them. Most data import functions (like <code>read.csv()</code>) have an <code>na.strings</code> argument for this purpose: <code>read.csv("my_data.csv", na.strings = "N/A")</code>.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's practice cleaning a vector with various special values.</p>
                     <ul class="prose-list">
                        <li>Create the following vector: <code>sensor_data <- c(5.2, 4.8, 1/0, 5.4, -1/0, 0/0, 5.1, NA)</code>.</li>
                        <li>Try to calculate the <code>mean()</code> of <code>sensor_data</code>. What is the result?</li>
                        <li>Use the <code>is.finite()</code> function to subset <code>sensor_data</code> and create a new vector called <code>clean_data</code> that contains only the normal, finite numbers.</li>
                        <li>Calculate the <code>mean()</code> of the new <code>clean_data</code> vector to get a valid result.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss2t1: `
            <div class="subsection">
                <h3 class="subsection-title">Making Your Code Smart</h3>
                <p style="margin-bottom: 1.5rem;">By default, R executes a script line by line from top to bottom. Control flow statements allow you to change this default behavior. You can make your code make decisions (conditional logic) or repeat actions (loops), which are fundamental for writing any non-trivial program.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> A marketing analyst needs to assign a label to every customer based on their number of purchases. If a customer has more than 10 purchases, they should be labeled "Loyal"; otherwise, they are "Standard". This requires the script to make a decision for each customer.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="if-else">Conditional Logic: <code>if</code> & <code>else</code></button>
                    <button class="tab-button" data-tab="for-loop"><code>for</code> Loops: Repeating for Each Item</button>
                    <button class="tab-button" data-tab="while-loop"><code>while</code> & <code>repeat</code> Loops</button>
                    <button class="tab-button" data-tab="break-next">Controlling Loops: <code>break</code> & <code>next</code></button>
                </div>

                <div id="if-else" class="tab-pane active">
                    <h4 class="subsection-title">Making Decisions in Code</h4>
                    <p>Conditional logic lets your code execute different blocks of code based on whether a condition is <code>TRUE</code> or <code>FALSE</code>.</p>
                    <p><strong>Analogy:</strong> Think of it as giving instructions: "If it is raining, take an umbrella. Otherwise, wear sunglasses."</p>
                    <ul class="prose-list">
                        <li>The <strong><code>if</code></strong> statement runs a block of code only if its condition is <code>TRUE</code>.</li>
                        <li>The <strong><code>else</code></strong> statement provides an alternative block to run if the condition is <code>FALSE</code>.</li>
                        <li><strong><code>else if</code></strong> lets you test multiple conditions in sequence.</li>
                    </ul>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Let's solve our customer scenario
purchases <- 12
customer_label <- "" # An empty variable to store the result

if (purchases > 10) {
  customer_label <- "Loyal"
} else {
  customer_label <- "Standard"
}

print(customer_label) # Result: "Loyal"</code></pre>
                    </div>
                </div>

                <div id="for-loop" class="tab-pane">
                    <h4 class="subsection-title">The Workhorse of Repetition</h4>
                    <p>A <strong><code>for</code> loop</strong> is used to repeat a block of code for each item in a sequence (like a vector or a list). It is the most common type of loop in R.</p>
                    <p><strong>Analogy:</strong> "For each of the 50 emails in my inbox, run the spam filter." You know exactly how many times the action needs to repeat.</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Let's calculate the final price for a vector of product costs
product_costs <- c(10.00, 25.50, 8.75, 42.00)
final_prices <- c() # Create an empty vector to store results

# The loop will run 4 times. In each run, 'cost' will hold one value from product_costs.
for (cost in product_costs) {
  price_with_tax <- cost * 1.08 # Add 8% tax
  final_prices <- c(final_prices, price_with_tax) # Add the result to our vector
}

print(final_prices)</code></pre>
                    </div>
                </div>

                <div id="while-loop" class="tab-pane">
                   <h4 class="subsection-title">Looping on a Condition</h4>
                   <p>A <strong><code>while</code> loop</strong> will continue to execute a block of code as long as its condition remains <code>TRUE</code>. It's useful when you don't know in advance how many times you need to repeat.</p>
                   <p><strong>Analogy:</strong> "While the car's gas tank is not full, keep pumping gas." The action stops only when the condition is no longer met.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># A simple counter
i <- 1
while (i <= 5) {
  print(paste("The number is", i))
  i <- i + 1 # CRITICAL! You must change the variable in the condition, or you'll get an infinite loop!
}</code></pre>
                    </div>
                     <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">The <code>repeat</code> Loop</h5>
                     <p>A <code>repeat</code> loop is a simpler but potentially more dangerous version that creates an infinite loop by default. It requires an explicit <code>break</code> statement inside to stop. It is used less frequently than <code>for</code> or <code>while</code> loops.</p>
                </div>
                
                <div id="break-next" class="tab-pane">
                    <h4 class="subsection-title">Fine-Tuning Your Loops</h4>
                    <p>You can further control the behavior of your loops with two key commands:</p>
                     <ul class="prose-list">
                        <li><strong><code>break</code></strong>: Immediately terminates the entire loop and the program continues with the next line of code after the loop. It's an "emergency stop".
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Stop the loop when you find the first number divisible by 7
for (i in 1:20) {
  if (i %% 7 == 0) {
    print(paste("Found it! The number is", i))
    break
  }
}</code></pre>
                            </div>
                        </li>
                        <li><strong><code>next</code></strong>: Immediately stops the current iteration and jumps to the start of the next iteration. It lets you "skip" certain items.
                             <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Process only odd numbers
for (i in 1:10) {
  if (i %% 2 == 0) { # If the number is even...
    next             # ...skip to the next iteration.
  }
  print(i) # This line is only reached for odd numbers
}</code></pre>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Vectorization is (Almost) Always Better than Loops</h3>
                <div class="scenario-content">
                    <p>In R, you should always look for a "vectorized" solution before writing a <code>for</code> loop. Vectorized operations apply an action to every element of a vector at once, and they are written in C code "under the hood," making them significantly faster and more concise than loops.</p>
                    <p>Remember the <code>for</code> loop we wrote to calculate tax? Here is the vectorized R way to do it:</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">product_costs <- c(10.00, 25.50, 8.75, 42.00)

# The vectorized way: faster, simpler, and easier to read.
final_prices <- product_costs * 1.08

print(final_prices)</code></pre>
                    </div>
                    <p>This is a fundamental concept in R. If you can do something to a whole vector at once, do it!</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's combine loops and conditional logic.</p>
                     <ul class="prose-list">
                        <li>Create a vector of numbers: <code>my_numbers <- c(15, -8, 0, 22, -4, 1, 0)</code>.</li>
                        <li>Write a <code>for</code> loop that iterates through each number in <code>my_numbers</code>.</li>
                        <li>Inside the loop, use an <code>if</code>/<code>else if</code>/<code>else</code> structure to check each number.</li>
                        <li>If the number is greater than 0, print "Positive".</li>
                        <li>If the number is less than 0, print "Negative".</li>
                        <li>If the number is exactly 0, print "Zero".</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss2t2: `
            <div class="subsection">
                <h3 class="subsection-title">Writing Fast, Efficient R Code</h3>
                <p style="margin-bottom: 1.5rem;">One of R's greatest strengths is its ability to perform vectorized operations. Vectorization is the process of applying an operation to an entire vector at once, rather than element by element in a loop. Understanding this concept is the key to writing code that is not only much faster but also more concise and easier to read.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You have a dataset with the price and quantity sold for thousands of items. You need to calculate the total revenue for each item (<code>price * quantity</code>). Writing a <code>for</code> loop to do this one item at a time would be slow and clunky. Vectorization lets you do it all in one swift, readable command.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">The Loop vs. The Vectorized Approach</h3>
                <p>Let's compare the two approaches. The goal is simple: add an 8% tax to a vector of prices.</p>
                <div class="decision-branches" style="align-items: flex-start;">
                    <div class="decision-branch">
                        <strong>The <code>for</code> Loop Way (Slow and Verbose)</strong>
                        <p>This approach requires you to set up an empty vector and manually add each result one by one. R has to interpret each step of the loop, which is inefficient.</p>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r">product_costs <- c(10.00, 25.50, 8.75, 42.00)
final_prices <- c() # 1. Create an empty vector

for (cost in product_costs) { # 2. Loop through each item
  price_with_tax <- cost * 1.08 # 3. Do the math
  final_prices <- c(final_prices, price_with_tax) # 4. Store the result
}
print(final_prices)</code></pre>
                        </div>
                    </div>
                    <div class="decision-branch">
                        <strong>The Vectorized Way (Fast and Concise)</strong>
                        <p>In the vectorized approach, R's underlying, highly optimized C code handles the looping invisibly. The operation is applied to the entire vector at once.</p>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r">product_costs <- c(10.00, 25.50, 8.75, 42.00)

# The entire operation happens in one line!
final_prices <- product_costs * 1.08

print(final_prices)</code></pre>
                        </div>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">Common Vectorized Operations</h3>
                <p>Most standard R operations and functions are already vectorized.</p>
                <ul class="prose-list">
                    <li><strong>Arithmetic:</strong> All basic arithmetic operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>^</code>) are vectorized. You can operate on two vectors or a vector and a single value.</li>
                    <li><strong>Logical Comparisons:</strong> All comparison operators (<code>></code>, <code><</code>, <code>==</code>, <code>!=</code>) are vectorized. They return a logical vector, which is perfect for subsetting.
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r">product_costs > 20 # Result: FALSE  TRUE FALSE  TRUE</code></pre>
                        </div>
                    </li>
                    <li><strong>Functions:</strong> A huge number of built-in functions are vectorized, such as <code>sqrt()</code>, <code>log()</code>, <code>abs()</code>, <code>toupper()</code>, and <code>paste()</code>.
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r">numbers <- c(4, 9, 16, 25)
sqrt(numbers) # Result: 2 3 4 5</code></pre>
                        </div>
                    </li>
                </ul>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Vectorized <code>if</code>/<code>else</code> with <code>ifelse()</code></h3>
                <div class="scenario-content">
                    <p>How do you apply conditional logic in a vectorized way? Instead of writing a <code>for</code> loop with an <code>if</code> statement inside, use the <code>ifelse()</code> function.</p>
                    <p>It takes three arguments: <code>ifelse(test_condition, value_if_true, value_if_false)</code>.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Let's label products based on price without a loop
prices <- c(150, 80, 120, 95)

# For each price, if it's > 100, label it "Premium", otherwise "Standard".
labels <- ifelse(prices > 100, "Premium", "Standard")

print(labels) # Result: "Premium" "Standard" "Premium" "Standard"</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>You are given sales data in two vectors. Your task is to perform all calculations in a vectorized way.</p>
                    <ul class="prose-list">
                        <li>Create two vectors: <code>revenue <- c(250, 310, 180, 400)</code> and <code>costs <- c(150, 220, 200, 280)</code>.</li>
                        <li>Calculate the profit for each sale (revenue - costs) and store it in a new vector called <code>profit</code>.</li>
                        <li>Calculate the profit margin for each sale (profit / revenue) and store it in a new vector called <code>margin</code>.</li>
                        <li>Create a logical vector called <code>is_highly_profitable</code> that is <code>TRUE</code> for sales where the profit was greater than 100 and <code>FALSE</code> otherwise.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss2t3: `
            <div class="subsection">
                <h3 class="subsection-title">Stop Repeating Yourself</h3>
                <p style="margin-bottom: 1.5rem;">As your scripts get more complex, you'll often find yourself writing the same or similar lines of code over and over. Functions are the solution. A <strong>function</strong> is a reusable, named block of code that performs a specific task. Creating functions is the key to writing efficient, readable, and less error-prone code.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> As an e-commerce analyst, you constantly need to calculate the final price of an item after applying an 8% sales tax and an optional discount. Typing out <code>price * 1.08 * (1 - discount)</code> every time is tedious. You need to create a reusable <code>calculate_price()</code> tool to do this automatically.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="anatomy">Anatomy of a Function</button>
                    <button class="tab-button" data-tab="arguments">Arguments: The Inputs</button>
                    <button class="tab-button" data-tab="return">Return Values: The Output</button>
                    <button class="tab-button" data-tab="scope">Scoping: Variable Visibility</button>
                </div>

                <div id="anatomy" class="tab-pane active">
                    <h4 class="subsection-title">The Blueprint of a Function</h4>
                    <p><strong>Analogy:</strong> A function is like a specialized appliance, such as a coffee maker. You put specific things in (water, coffee grounds), it performs a process (brewing), and you get a specific thing out (coffee). </p>
                    <p>All functions in R have three basic parts:</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># The basic syntax for creating a function
my_function_name <- function(argument1, argument2) {
  # This is the function body, where the work gets done
  # ...
  # ...
  return(some_result) # What the function gives back
}</code></pre>
                    </div>
                     <ul class="prose-list">
                        <li><strong>Name:</strong> You assign your function to a variable (e.g., <code>my_function_name</code>). This is how you will call it later.</li>
                        <li><strong>Arguments:</strong> The inputs to your function, listed inside <code>function(...)</code>. These are the values the function will work with.</li>
                        <li><strong>Body:</strong> The code inside the curly braces <code>{ }</code>. This is the set of instructions that the function will execute.</li>
                    </ul>
                </div>
                
                <div id="arguments" class="tab-pane">
                   <h4 class="subsection-title">Providing Inputs to Your Function</h4>
                   <p>Arguments (or parameters) are the variables you use to pass information into your function.</p>
                   <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Default Arguments</h5>
                   <p>You can make an argument optional by giving it a <strong>default value</strong>. If the user doesn't provide a value for that argument, the default will be used. This makes functions more flexible.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># We give tax_rate and discount default values
calculate_final_price <- function(price, tax_rate = 0.08, discount = 0) {
  final_price <- price * (1 + tax_rate) * (1 - discount)
  return(final_price)
}

# Calling the function in different ways:
# 1. Only provide the required argument
calculate_final_price(price = 100) # Uses both default values. Result: 108

# 2. Override one of the defaults by name
calculate_final_price(price = 100, discount = 0.10) # 10% discount. Result: 97.2</code></pre>
                    </div>
                </div>
                
                <div id="return" class="tab-pane">
                   <h4 class="subsection-title">Getting an Output</h4>
                   <p>The <strong>return value</strong> is the object that the function sends back after it has finished its job. There are two ways to return a value in R:</p>
                   <ul class="prose-list">
                       <li><strong>Implicit Return:</strong> By default, an R function automatically returns the result of the <strong>very last expression</strong> it evaluates in its body.</li>
                       <li><strong>Explicit Return:</strong> You can use the <code>return()</code> function to explicitly state what value should be returned. This is considered <strong>best practice</strong> as it makes your code clearer and allows you to return a value from the middle of the function if needed.</li>
                   </ul>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Our completed function using an explicit return
calculate_final_price <- function(price, tax_rate = 0.08, discount = 0) {
  final_price <- price * (1 + tax_rate) * (1 - discount)
  return(final_price) # Explicitly return the calculated value
}

# We can now store the output in a new variable
item_cost <- calculate_final_price(200, discount = 0.25) # 25% discount
print(item_cost) # Result: 162</code></pre>
                    </div>
                </div>

                <div id="scope" class="tab-pane">
                    <h4 class="subsection-title">The "What Happens in the Function, Stays in the Function" Rule</h4>
                    <p><strong>Scoping</strong> refers to the rules R uses to find a variable. For beginners, the most important rule is that variables created *inside* a function are temporary and exist only within that function's environment.</p>
                    <p><strong>Analogy:</strong> Think of it as the "What happens in Vegas, stays in Vegas" rule. Once the function finishes running, any variables it created internally are discarded.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">test_scope <- function() {
  # internal_var is created inside the function's environment
  internal_var <- "I only exist inside this function"
  print(internal_var)
}

# This works fine, it prints the message
test_scope()

# This will give an ERROR: "object 'internal_var' not found"
# because it doesn't exist outside the function.
print(internal_var)</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Write "Pure" Functions</h3>
                <div class="scenario-content">
                    <p>The best functions are "pure." A pure function is like a reliable machine: for a given set of inputs, it will <strong>always</strong> produce the same output, and it has no "side effects" (it doesn't change anything outside of itself). Writing pure functions makes your code much easier to test, debug, and reason about.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's write a function to calculate Body Mass Index (BMI).</p>
                     <ul class="prose-list">
                        <li>Write a function named <code>calculate_bmi</code>.</li>
                        <li>It should take two arguments: <code>weight_kg</code> and <code>height_m</code>.</li>
                        <li>Inside the function body, calculate the BMI using the formula: <code>bmi <- weight_kg / height_m ^ 2</code>.</li>
                        <li>Use <code>return()</code> to explicitly return the calculated <code>bmi</code> value.</li>
                        <li>Call your function for a person who weighs 75 kg and is 1.8 meters tall. Store the result in a variable and print it.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss2t4: `
            <div class="subsection">
                <h3 class="subsection-title">Programming with Functions</h3>
                <p style="margin-bottom: 1.5rem;">Beyond simply defining named functions, R allows for more advanced and flexible ways to create and use them. Understanding anonymous functions and closures will allow you to write more elegant and powerful code, especially when working with functions that take other functions as arguments (like the <code>lapply()</code> family).</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You need to apply a very simple, one-off calculation to every item in a list. Later, you find yourself needing to create a whole "family" of similar tax-calculating functions (one for VAT, one for GST, etc.). Writing a full, named function for each small task is repetitive and clutters your workspace. These advanced techniques provide a better way.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="anonymous">Anonymous Functions: Functions on the Fly</button>
                    <button class="tab-button" data-tab="closures">Closures: Function Factories</button>
                </div>

                <div id="anonymous" class="tab-pane active">
                    <h4 class="subsection-title">Functions Without a Name</h4>
                    <p>An <strong>anonymous function</strong> is a function that you define without assigning it to a name. It's created on-the-fly, used exactly once, and then immediately discarded.</p>
                    <p><strong>Analogy:</strong> It's like a disposable coffee stirrer. You need it for one simple task (stirring your coffee), so you use it and then throw it away. You don't need to get a permanent, named spoon from your drawer for such a quick, one-off job.</p>
                    <p>Their most common use is as an argument to functional programming tools like <code>lapply()</code>, which applies a function to each element of a list.</p>
                    
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">The Old Way vs. The Anonymous Way</h5>
                    <div class="decision-branches" style="align-items: flex-start;">
                        <div class="decision-branch">
                            <strong>With a Named Function (Verbose)</strong>
                            <p>Here, we have to define <code>square_it</code> separately just for this one use.</p>
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># 1. Define the named function
square_it <- function(x) {
  return(x^2)
}

# 2. Use it in lapply()
lapply(1:5, square_it)</code></pre>
                            </div>
                        </div>
                        <div class="decision-branch">
                            <strong>With an Anonymous Function (Concise)</strong>
                            <p>The function definition happens right inside the call to <code>lapply()</code>. It's cleaner and more direct.</p>
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># The function is defined right here and has no name
lapply(1:5, function(x) { x^2 })</code></pre>
                            </div>
                        </div>
                    </div>
                </div>

                <div id="closures" class="tab-pane">
                   <h4 class="subsection-title">Functions That Create Functions</h4>
                   <p>A <strong>closure</strong> is a powerful concept where you create a "factory" function that produces other, specialized functions. The functions created by the factory "remember" the environment and variables from when they were created.</p>
                   <p><strong>Analogy:</strong> Imagine a soda factory (the outer function). You can set its controls (arguments like <code>flavor</code>). When you run the factory, it produces a specific soda machine (the inner function) that is permanently set to that flavor. You can use the factory to create a "Cola Machine," a "Lemonade Machine," and so on.</p>
                   <p>This is useful when you need to generate many similar functions that only differ by one or two parameters.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># The "factory" function that creates our calculators
create_tax_calculator <- function(tax_rate) {
  
  # The "product" function that the factory returns
  # It "remembers" the value of tax_rate from its creation.
  tax_calculator <- function(price) {
    return(price * (1 + tax_rate))
  }
  
  return(tax_calculator)
}

# Use the factory to create two specialized calculator functions
calc_vat <- create_tax_calculator(0.20) # A calculator for 20% VAT
calc_gst <- create_tax_calculator(0.05) # A calculator for 5% GST

# Now use the new functions
calc_vat(100) # Result: 120
calc_gst(100) # Result: 105</code></pre>
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: The Tidyverse Shortcut for Anonymous Functions</h3>
                <div class="scenario-content">
                    <p>The <code>purrr</code> package (part of the Tidyverse) provides an even shorter, more elegant way to write anonymous functions using a special formula syntax with a tilde (<code>~</code>) and <code>.x</code> as the placeholder for the argument.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">library(purrr) # From the Tidyverse

# Base R way with lapply and a full anonymous function
lapply(1:5, function(x) { x * 10 })

# The purrr way with map() and the formula shortcut
map(1:5, ~ .x * 10)</code></pre>
                    </div>
                    <p>You will see this <code>~ .x</code> style very frequently in modern R code.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's practice both concepts.</p>
                     <ul class="prose-list">
                        <li>Create a list of numbers: <code>my_numbers <- list(10, 15, 20, 25)</code>.</li>
                        <li>Use <code>lapply()</code> with an **anonymous function** to subtract 5 from each number in <code>my_numbers</code>.</li>
                        <li>Create a **closure** (a function factory) called <code>create_power_function</code>. The parent function should take one argument, <code>exponent</code>. The child function it returns should take one argument, <code>base</code>, and should calculate <code>base ^ exponent</code>.</li>
                        <li>Use your factory to create two functions: <code>square_it</code> (with <code>exponent = 2</code>) and <code>cube_it</code> (with <code>exponent = 3</code>).</li>
                        <li>Call both <code>square_it(4)</code> and <code>cube_it(4)</code> to test your new functions.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss2t5:
            `
            <div class="subsection">
                <h3 class="subsection-title">Programming with an Assembly Line</h3>
                <p style="margin-bottom: 1.5rem;">Functional Programming (FP) is a style of coding that treats computation as the evaluation of functions. In R, this practically means using a family of functions to replace loops. Instead of manually iterating over items, you pass your data through a series of "verb" functions like <code>map()</code>, <code>filter()</code>, and <code>reduce()</code>.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You have a list where each element is a data frame of one month's sales. Your task is to: 1) Keep only the data frames for months with more than 100 sales, 2) Calculate the total revenue for each of those months, and 3) Sum those revenues to get a final grand total. An FP approach makes this multi-step process clean and readable.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="map"><code>map()</code>: Apply a Function to Each</button>
                    <button class="tab-button" data-tab="filter"><code>keep()</code> / <code>discard()</code>: Keep or Remove a Subset</button>
                    <button class="tab-button" data-tab="reduce"><code>reduce()</code>: Combine into One</button>
                    <button class="tab-button" data-tab="walk"><code>walk()</code>: For Actions (Side Effects)</button>
                </div>

                <div id="map" class="tab-pane active">
                    <h4 class="subsection-title">The Main Workstation</h4>
                    <p>The <code>map()</code> function (from the <code>purrr</code> package) is the FP equivalent of a <code>for</code> loop. It takes a list/vector and a function, applies the function to every element, and returns a new list of the results.</p>
                    <p><strong>Analogy:</strong> This is the main station on your assembly line. Every item that passes through gets the same operation performed on it (e.g., gets painted).</p>
                    <p>The base R equivalent is <code>lapply()</code>.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># You need the purrr package (part of the Tidyverse)
# install.packages("purrr")
library(purrr)

numbers <- c(1, 2, 3, 4, 5)

# Use map() to apply the sqrt() function to each number
# The ~ .x syntax is the purrr shortcut for an anonymous function
sqrt_numbers <- map(numbers, ~ sqrt(.x))

# map() always returns a list. If you know the output type, you can be more specific:
# map_dbl() returns a numeric (double) vector
sqrt_numbers_dbl <- map_dbl(numbers, ~ sqrt(.x))
print(sqrt_numbers_dbl)</code></pre>
                    </div>
                </div>

                <div id="filter" class="tab-pane">
                   <h4 class="subsection-title">The Quality Control Station</h4>
                   <p>The <code>keep()</code> and <code>discard()</code> functions (from <code>purrr</code>) let you filter a list based on a condition. <code>keep()</code> retains elements where the condition is TRUE, while <code>discard()</code> removes them.</p>
                   <p><strong>Analogy:</strong> This is the quality control station on your assembly line. It inspects each item and either keeps it or discards it based on a standard.</p>
                   <p>The base R equivalent is <code>Filter()</code>.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">numbers <- 1:10

# Keep only the elements that are even
even_numbers <- keep(numbers, ~ .x %% 2 == 0)

print(even_numbers)</code></pre>
                    </div>
                </div>
                
                <div id="reduce" class="tab-pane">
                   <h4 class="subsection-title">The Final Packaging Station</h4>
                   <p>The <code>reduce()</code> function (from <code>purrr</code>) takes a list and "reduces" it to a single value by repeatedly applying a function that takes two arguments (the accumulated result and the next element).</p>
                   <p><strong>Analogy:</strong> This is the final station that takes all the finished products and packs them into one big shipping box.</p>
                   <p>The base R equivalent is <code>Reduce()</code>.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">numbers <- c(1, 2, 3, 4, 5)

# Reduce the vector by applying the + operator between elements
# It calculates (((1+2)+3)+4)+5
total_sum <- reduce(numbers, ` +
            `) # Note the backticks around ` +
            `

print(total_sum) # Result: 15</code></pre>
                    </div>
                </div>

                <div id="walk" class="tab-pane">
                    <h4 class="subsection-title">The Action Station</h4>
                    <p>The <code>walk()</code> function (from <code>purrr</code>) is a special version of <code>map()</code>. You use it when you care about the *action* of a function (like printing to the console or saving a file), not the value it returns. It's used for "side effects."</p>
                    <p><strong>Analogy:</strong> A station on the assembly line that stamps each item with a "Passed Inspection" mark. You care about the action of stamping, not about getting a value back.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">file_names <- c("report_jan.pdf", "report_feb.pdf", "report_mar.pdf")

# Use walk() to print a status message for each file
# This is better than map(), because we don't want a list of messages back.
walk(file_names, ~ print(paste("Saving file:", .x)))</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Chaining with the Pipe</h3>
                <div class="scenario-content">
                    <p>The true power of functional programming is revealed when you chain these functions together using the pipe operator (<code>%>%</code> or <code>|></code>). This allows you to write complex data processing pipelines that read like a series of steps.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">library(dplyr) # For the pipe

# Let's find the sum of the squares of only the even numbers from 1 to 10
result <- 1:10 %>%
  keep(~ .x %% 2 == 0) %>% # Start with 1:10, then keep only the even numbers
  map_dbl(~ .x^2) %>%      # Then, square each of those
  reduce(` +
            `)              # Finally, sum them all up

print(result) # Result: 220</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's use these functions on a list of data.</p>
                     <ul class="prose-list">
                        <li>Create a list of numbers: <code>data_list <- list(10, -5, 8, -2, 0, 15)</code>.</li>
                        <li>Use <code>keep()</code> to create a new list called <code>positive_numbers</code> that contains only the numbers greater than 0.</li>
                        <li>Use <code>map_dbl()</code> on your <code>positive_numbers</code> list to calculate the natural logarithm of each number (using the <code>log()</code> function).</li>
                        <li>Use <code>reduce()</code> and the <code>+</code> function to find the sum of all the numbers in the original <code>data_list</code>.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss3t1: `
            <div class="subsection">
                <h3 class="subsection-title">Your Toolbox for R</h3>
                <p style="margin-bottom: 1.5rem;">You know that R's power comes from its vast ecosystem of packages. But to be an effective R user, you need to be comfortable with the full lifecycle of package management. This involves more than just installing them; it's about knowing how to load, update, and remove them to keep your R environment tidy and up-to-date.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You're starting a new analysis. You need to install a set of packages for data manipulation and visualization. You also know one of your old packages has an important new feature, so you need to update it. Finally, you want to remove an old package you no longer use. This requires mastering the four core package management actions.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="install">Installing Packages</button>
                    <button class="tab-button" data-tab="load">Loading Packages</button>
                    <button class="tab-button" data-tab="update">Updating Packages</button>
                    <button class="tab-button" data-tab="remove">Removing Packages</button>
                </div>

                <div id="install" class="tab-pane active">
                    <h4 class="subsection-title">Adding a New Tool to Your Toolbox</h4>
                    <p>Installation is a one-time setup that downloads a package from CRAN and installs it on your computer.</p>
                    <div class="decision-branches" style="align-items: flex-start;">
                        <div class="decision-branch">
                            <strong>Using a Function</strong>
                            <p>The <code>install.packages()</code> function is the standard way to install packages. You can install one or multiple at a time.</p>
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Install a single package
install.packages("dplyr")

# Install multiple packages at once
install.packages(c("ggplot2", "rmarkdown", "shiny"))</code></pre>
                            </div>
                        </div>
                        <div class="decision-branch">
                            <strong>Using the RStudio GUI</strong>
                            <p>In the "Packages" pane (bottom-right), click the "Install" button. A dialog box will appear where you can type the names of the packages you want. RStudio will automatically write and run the <code>install.packages()</code> command for you.</p>
                            <p></p>
                        </div>
                    </div>
                </div>

                <div id="load" class="tab-pane">
                   <h4 class="subsection-title">Taking a Tool Out to Use It</h4>
                   <p>Installing a package makes it available, but <strong>loading</strong> it with <code>library()</code> makes its functions ready to use in your current R session. You must load a package in every new R session where you want to use its functions.</p>
                    <div class="decision-branches" style="align-items: flex-start;">
                        <div class="decision-branch">
                            <strong>Using a Function</strong>
                            <p>The <code>library()</code> function is the standard way to load a package. Note that the package name is not in quotes.</p>
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Load the dplyr package to use its functions
library(dplyr)</code></pre>
                            </div>
                            <p><strong>Note:</strong> You may also see <code>require()</code>. It's similar, but <code>library()</code> is generally preferred in scripts because it will produce an error if the package is missing, stopping your script immediately. <code>require()</code> will only give a warning.</p>
                        </div>
                        <div class="decision-branch">
                            <strong>Using the RStudio GUI</strong>
                            <p>In the "Packages" pane, find the package you want to load in the list and click the checkbox next to its name. This is equivalent to running the <code>library()</code> command.</p>
                            <p></p>
                        </div>
                    </div>
                </div>
                
                <div id="update" class="tab-pane">
                   <h4 class="subsection-title">Keeping Your Tools Sharp</h4>
                   <p>Package developers regularly release updates with bug fixes, performance improvements, and new features. It's good practice to keep your packages updated.</p>
                    <div class="decision-branches" style="align-items: flex-start;">
                        <div class="decision-branch">
                            <strong>Using a Function</strong>
                            <p>The <code>update.packages()</code> function will check all your installed packages against CRAN and ask you which ones you want to update.</p>
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Check for all available package updates
update.packages()</code></pre>
                            </div>
                        </div>
                        <div class="decision-branch">
                            <strong>Using the RStudio GUI</strong>
                            <p>In the "Packages" pane, click the "Update" button. RStudio will show a list of all packages that have a newer version available on CRAN. You can select the ones you want to update and click "Install Updates".</p>
                            <p></p>
                        </div>
                    </div>
                </div>

                <div id="remove" class="tab-pane">
                    <h4 class="subsection-title">Cleaning Out Your Toolbox</h4>
                    <p>If you no longer need a package, you can remove it to save space and reduce clutter.</p>
                    <div class="decision-branches" style="align-items: flex-start;">
                        <div class="decision-branch">
                            <strong>Using a Function</strong>
                            <p>The <code>remove.packages()</code> function will uninstall a package from your computer.</p>
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Uninstall a package you no longer need
remove.packages("some_old_package")</code></pre>
                            </div>
                        </div>
                        <div class="decision-branch">
                            <strong>Using the RStudio GUI</strong>
                            <p>In the "Packages" pane, find the package you want to remove in the list. Click the small "x" icon in the same row as the package name to uninstall it.</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Where Do Packages Live?</h3>
                <div class="scenario-content">
                    <p>R installs packages into specific folders on your computer called "libraries". You can have multiple library locations. To see which folders R is using to store and find packages, run the <code>.libPaths()</code> function. This can be useful for debugging if R ever says it can't find a package you know you've installed.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># See your current library paths
.libPaths()</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's practice the full lifecycle with a fun, simple package called <code>cowsay</code>.</p>
                     <ul class="prose-list">
                        <li><strong>Install:</strong> In the RStudio console, run <code>install.packages("cowsay")</code>.</li>
                        <li><strong>Load:</strong> Now, load the package with <code>library(cowsay)</code>.</li>
                        <li><strong>Use:</strong> Call the main function from the package by running <code>say("Packages are fun!")</code> in your console. You should see an ASCII-art animal!</li>
                        <li><strong>Remove:</strong> Clean up by uninstalling the package. Run <code>remove.packages("cowsay")</code>.</li>
                        <li><strong>Verify:</strong> Try running <code>say("Packages are fun!")</code> again. You should now get an error, confirming the package and its functions have been removed.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss3t2: `
            <div class="subsection">
                <h3 class="subsection-title">A Dialect for Data Science</h3>
                <p style="margin-bottom: 1.5rem;">The <strong>Tidyverse</strong> is an opinionated collection of R packages designed for data science that share an underlying design philosophy, grammar, and data structures. It's not just a set of tools; it's a "dialect" of R that makes data analysis more intuitive, readable, and efficient.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You're trying to clean a messy dataset using base R. Your code involves nested functions, temporary variables, and confusing syntax. It works, but it's hard to read and even harder to explain to a colleague. The Tidyverse provides a cleaner, more expressive way to perform the exact same steps using a logical chain of operations that reads like a sentence.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">The Core Philosophy</h3>
                <p>All Tidyverse packages are designed to work together seamlessly based on a few core principles:</p>
                <ul class="prose-list">
                    <li><strong>Design for Humans:</strong> Functions and arguments have clear, consistent names, making code easier to write and read.</li>
                    <li><strong>Compose with Pipes:</strong> Operations are chained together using the pipe operator (<code>%>%</code> or <code>|></code>), allowing you to read a data analysis workflow from left to right.</li>
                    <li><strong>Reuse Tidy Data Structures:</strong> The primary data structure is the <strong>tibble</strong>, a modern data frame that is easier to work with.</li>
                    <li><strong>Embrace Functional Programming:</strong> It encourages using functions like <code>purrr::map()</code> instead of writing complex loops.</li>
                </ul>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">Meet the Core Packages</h3>
                <p>While the Tidyverse contains many packages, a few "core" packages form the backbone of most analyses. You can get them all at once by installing one single package.</p>
                <div class="decision-branches" style="align-items: flex-start;">
                    <div class="decision-branch">
                        <strong><i class="fas fa-chart-pie"></i> ggplot2</strong>
                        <p>A powerful and declarative system for creating elegant and complex data visualizations based on the "Grammar of Graphics".</p>
                    </div>
                    <div class="decision-branch">
                        <strong><i class="fas fa-wrench"></i> dplyr</strong>
                        <p>Provides a "grammar of data manipulation" with a set of consistent verbs (like <code>mutate()</code>, <code>filter()</code>, <code>select()</code>) to solve the most common data wrangling challenges.</p>
                    </div>
                </div>
                <div class="decision-branches" style="align-items: flex-start; margin-top: 1rem;">
                    <div class="decision-branch">
                        <strong><i class="fas fa-retweet"></i> tidyr</strong>
                        <p>Helps you create "tidy data" by making it easy to reshape your data's layout (e.g., from wide to long format with <code>pivot_longer()</code>).</p>
                    </div>
                    <div class="decision-branch">
                        <strong><i class="fas fa-file-import"></i> readr</strong>
                        <p>Provides a fast and friendly way to read rectangular data from delimited files like CSVs, TSVs, etc.</p>
                    </div>
                </div>
                 <div class="decision-branches" style="align-items: flex-start; margin-top: 1rem;">
                    <div class="decision-branch">
                        <strong><i class="fas fa-cogs"></i> purrr</strong>
                        <p>Enhances R's functional programming toolkit, providing a complete and consistent set of tools for working with functions and vectors.</p>
                    </div>
                    <div class="decision-branch">
                        <strong><i class="fas fa-table"></i> tibble</strong>
                        <p>The modern, user-friendly reimagining of the data frame that we've already learned about.</p>
                    </div>
                </div>
                 <div class="decision-branches" style="align-items: flex-start; margin-top: 1rem;">
                    <div class="decision-branch">
                        <strong><i class="fas fa-quote-right"></i> stringr</strong>
                        <p>Provides a cohesive set of functions for common string and text manipulation tasks, built on top of the <code>stringi</code> package.</p>
                    </div>
                    <div class="decision-branch">
                        <strong><i class="fas fa-tags"></i> forcats</strong>
                        <p>Provides a suite of useful tools for working with categorical variables (factors), which can often be tricky in R.</p>
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Understanding Name Conflicts</h3>
                <div class="scenario-content">
                    <p>When you run <code>library(tidyverse)</code>, you may see some "Conflicts" messages. Don't panic! This is normal. It happens because some Tidyverse packages have functions with the same name as functions in base R (e.g., both <code>dplyr</code> and base R's <code>stats</code> package have a <code>filter()</code> function).</p>
                    <p>The message is simply telling you that the Tidyverse version will be used by default (which is usually what you want). If you ever need to use the base R version specifically, you can do so with the <code>package::function()</code> syntax, like this: <code>stats::filter()</code>.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's install the Tidyverse and see how the packages work together.</p>
                     <ul class="prose-list">
                        <li>In your console, install the entire package suite with one command: <code>install.packages("tidyverse")</code>. This may take a few minutes as it's installing many packages.</li>
                        <li>Load the core packages into your R session by running <code>library(tidyverse)</code>. Observe the list of attached packages and any conflict messages.</li>
                        <li>The built-in <code>mtcars</code> dataset is a traditional data frame. You can see its Tidyverse equivalent, a tibble, by running <code>as_tibble(mtcars)</code>. Notice the cleaner print output.</li>
                        <li>Practice a simple pipe: Take the <code>iris</code> dataset, pipe it to the <code>glimpse()</code> function to see its structure, like this: <code>iris %>% glimpse()</code>. This single command uses the pipe from <code>magrittr</code> (loaded by <code>dplyr</code>) and a function from <code>tibble</code>, showing how the ecosystem works together.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss3t3: `
            <div class="subsection">
                <h3 class="subsection-title">Two Dialects of the Same Language</h3>
                <p style="margin-bottom: 1.5rem;">As you explore R, you'll notice that there are often multiple ways to accomplish the same task. The two dominant "dialects" or programming paradigms you'll encounter are <strong>Base R</strong> and the <strong>Tidyverse</strong>. Neither is inherently "better"—they are just different philosophies for how to approach a problem. Understanding both will make you a more versatile R programmer.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You need to answer a simple question from the built-in <code>mtcars</code> dataset: "What is the average horsepower (<code>hp</code>) for cars with exactly 6 cylinders (<code>cyl</code>)?" Let's solve this using both paradigms to see the differences in action.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">A Practical Comparison</h3>
                <p>Here's how each paradigm tackles our scenario. Notice the difference in the flow of the code.</p>
                <div class="decision-branches" style="align-items: flex-start;">
                    <div class="decision-branch">
                        <strong>The Base R Approach</strong>
                        <p>Base R often relies on creating intermediate variables or nesting functions inside one another. The data is typically passed as an argument to each function.</p>
                        <p><strong>Method 1: Using intermediate variables.</strong></p>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r"># Step 1: Subset the data frame to keep only 6-cylinder cars
six_cyl_cars <- mtcars[mtcars$cyl == 6, ]

# Step 2: Extract the horsepower column into a vector
hp_vector <- six_cyl_cars$hp

# Step 3: Calculate the mean of that vector
mean(hp_vector)</code></pre>
                        </div>
                        <p><strong>Method 2: Nesting functions.</strong> This is more concise but can become very hard to read as more steps are added.</p>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r"># Read from the inside out: subset mtcars, then extract hp, then get the mean
mean(mtcars[mtcars$cyl == 6, ]$hp)</code></pre>
                        </div>
                    </div>
                    <div class="decision-branch">
                        <strong>The Tidyverse Approach</strong>
                        <p>The Tidyverse uses the pipe operator (<code>%>%</code>) to chain operations together. The data starts at the top and flows downwards through a series of "verb" functions that are easy to read.</p>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r"># Load the library first
library(tidyverse)

# The code reads like a set of instructions:
mtcars %>%                              # Start with the mtcars dataset...
  filter(cyl == 6) %>%                # THEN, filter for 6-cylinder cars...
  summarise(avg_hp = mean(hp)) %>%    # THEN, summarise to get the mean horsepower...
  pull(avg_hp)                        # FINALLY, pull out the single value.
</code></pre>
                        </div>
                        <p>This linear, readable workflow is the primary reason many data scientists prefer the Tidyverse for data manipulation tasks.</p>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">Key Philosophical Differences Summarized</h3>
                 <ul class="prose-list">
                    <li><strong>Data Flow:</strong> Base R uses nested functions (inside-out) or temporary variables. The Tidyverse uses the pipe for a linear (top-to-bottom) flow.</li>
                    <li><strong>Readability:</strong> Tidyverse code is often considered more readable for complex, multi-step operations because it can be read like a sentence.</li>
                    <li><strong>Consistency:</strong> Tidyverse functions are designed to be highly consistent (e.g., the data is always the first argument). Base R functions, developed over many years by different authors, can sometimes have inconsistent argument orders and naming schemes.</li>
                     <li><strong>Dependencies:</strong> Base R is always available. The Tidyverse requires you to install and load an external package suite.</li>
                </ul>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: You Don't Have to Choose!</h3>
                <div class="scenario-content">
                    <p>The best R programmers are fluent in both dialects. Base R and the Tidyverse are not mutually exclusive; they work perfectly together. A common and highly effective workflow is to use the Tidyverse (<code>dplyr</code>, <code>tidyr</code>) for the initial data cleaning and preparation steps, and then pass the final, clean tibble to a powerful Base R statistical modeling function like <code>lm()</code> or <code>t.test()</code>. Use the best tool for the job!</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's solidify your understanding by solving one problem two ways. The goal: Using the built-in <code>iris</code> dataset, find the average <code>Sepal.Width</code> for the <code>versicolor</code> species.</p>
                     <ul class="prose-list">
                        <li><strong>Part 1 (Base R):</strong> Solve the problem using only Base R subsetting (<code>iris[...]</code>) and the <code>mean()</code> function.</li>
                        <li><strong>Part 2 (Tidyverse):</strong> Solve the same problem using a <code>tidyverse</code> pipe chain with <code>filter()</code> and <code>summarise()</code>.</li>
                        <li>Compare your two code blocks. Do they produce the same answer? Which one do you find easier to read?</li>
                    </ul>
                </div>
            </div>
          `,
          p1s2ss3t4: `
            <div class="subsection">
                <h3 class="subsection-title">How R Finds Your Variables</h3>
                <p style="margin-bottom: 1.5rem;">When you run a command like <code>print(x)</code>, how does R know where to find the object named <code>x</code>? The answer lies in its rules for <strong>scoping</strong> and its use of <strong>environments</strong>. Understanding these rules is crucial for writing functions that work predictably and for debugging your code when they don't.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You define a variable <code>x <- 10</code> in your script. You then write a function that creates its own variable, also called <code>x</code>, and sets it to <code>50</code>. After you run the function, what is the value of <code>x</code>? Is it 10 or 50? The answer depends entirely on R's scoping rules.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="environments">What is an Environment?</button>
                    <button class="tab-button" data-tab="scoping">The Scoping Rules</button>
                    <button class="tab-button" data-tab="functions">Functions Create Environments</button>
                </div>

                <div id="environments" class="tab-pane active">
                    <h4 class="subsection-title">Rooms for Storing Objects</h4>
                    <p>An <strong>environment</strong> in R is a container that stores a collection of named objects (variables, functions, etc.).</p>
                    <p><strong>Analogy:</strong> Think of environments as a set of nested rooms or Russian dolls. </p>
                    <ul class="prose-list">
                        <li><strong>The Global Environment:</strong> This is the main "room" you work in. Any variable you create in the R console lives here. You can see all its contents in the "Environment" pane in RStudio.</li>
                        <li><strong>Function Environments:</strong> Each time you call a function, R creates a new, temporary, smaller "room" just for that function to do its work. This room is created *inside* the room where the function was called.</li>
                    </ul>
                </div>
                
                <div id="scoping" class="tab-pane">
                   <h4 class="subsection-title">How R Searches for a Variable</h4>
                   <p><strong>Scoping</strong> is the set of rules R follows to find an object. R uses <strong>lexical scoping</strong>, which sounds complex, but the basic rule is simple and follows our "nested rooms" analogy.</p>
                   <p><strong>The Search Path:</strong> When R needs to find a variable, it follows these steps:</p>
                   <ol class="prose-list">
                       <li>It first looks for the variable in the <strong>current room</strong> (e.g., inside the function it's currently running).</li>
                       <li>If it can't find it there, it goes to the <strong>parent room</strong> (the room the function was called from) and looks there.</li>
                       <li>It continues this search, moving outwards from room to bigger room, until it reaches the Global Environment.</li>
                       <li>If it still can't find the object, it throws an "object not found" error.</li>
                   </ol>
                </div>

                <div id="functions" class="tab-pane">
                    <h4 class="subsection-title">Why Your Global Variables are Safe</h4>
                    <p>The most important rule of function environments is that they are generally self-contained. When you assign a value to a variable inside a function, R creates a <strong>new, local</strong> copy of that variable inside the function's "room". It does not modify variables with the same name that might exist in an outer environment.</p>
                    <p>This is a critical safety feature that prevents functions from having unexpected "side effects" on your global workspace.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># 1. We create 'x' in the Global Environment (the big room)
x <- 10

my_function <- function() {
  # 2. Inside this function's temporary room, we create a *new, local* variable
  #    that is also named 'x'. This one is different from the global 'x'.
  x <- 50
  print(paste("Inside the function, x is:", x))
}

# 3. When we run the function, it uses its own local x.
my_function()
# Output: [1] "Inside the function, x is: 50"

# 4. Back in the Global Environment, our original 'x' is unchanged!
print(paste("Outside the function, x is:", x))
# Output: [1] "Outside the function, x is: 10"</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Peeking Inside an Environment with <code>ls()</code></h3>
                <div class="scenario-content">
                    <p>How can you see what objects exist in the current environment? The <code>ls()</code> function is a simple tool that lists the names of all objects. If you run it in the console, it shows you everything in your Global Environment. If you place it inside a function, it will show you only the objects that have been created within that function's temporary environment. It's a great way to debug and explore how scoping works.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's test your understanding of scoping rules. Read the code below, and before you run it, predict what each <code>print()</code> statement will output.</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># 1. Define variables in the Global Environment
a <- "I am global"
b <- "I am also global"

scope_tester <- function() {
  # 2. A new local variable 'a' is created here
  a <- "I am local to the function"
  
  # 3. 'b' is not found locally, so R will look in the parent (Global) environment
  print(paste("Inside function, a is:", a))
  print(paste("Inside function, b is:", b))
}

# 4. Run the function
scope_tester()

# 5. Which 'a' and 'b' will be printed here?
print(paste("Outside function, a is:", a))
print(paste("Outside function, b is:", b))</code></pre>
                    </div>
                </div>
            </div>
          `,
          p1s2ss3t5: `
            <div class="subsection">
                <h3 class="subsection-title">The "Magic" Behind the Tidyverse</h3>
                <p style="margin-bottom: 1.5rem;">Have you ever wondered why Tidyverse code feels so clean? For example, when you write <code>filter(mtcars, cyl == 6)</code>, you can refer to the column <code>cyl</code> directly, without quotes and without typing <code>mtcars$cyl</code>. This is not normal R behavior! This "magic" is a powerful programming technique called <strong>Metaprogramming</strong>, and the specific version used in the Tidyverse is known as <strong>Non-Standard Evaluation (NSE)</strong>.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You've been using <code>dplyr</code> and you love how readable it is. Now, you want to write your own function that mimics this behavior. You want to create a function that takes a data frame and a column name (without quotes) and creates a summary. Understanding the basics of NSE is the key to making this possible.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="what-is-nse">What is NSE?</button>
                    <button class="tab-button" data-tab="how-it-works">How It Works</button>
                    <button class="tab-button" data-tab="curly-curly">Programming with <code>{{ }}</code></button>
                </div>

                <div id="what-is-nse" class="tab-pane active">
                    <h4 class="subsection-title">Standard vs. Non-Standard Evaluation</h4>
                    <p>To understand NSE, you first need to understand what's "standard."</p>
                    <ul class="prose-list">
                        <li><strong>Standard Evaluation (SE):</strong> This is normal R behavior. When you call a function, R immediately evaluates everything you pass to it. If you write <code>my_func(x)</code>, R immediately looks for the value of <code>x</code>.</li>
                        <li><strong>Non-Standard Evaluation (NSE):</strong> This is a special technique where a function does <em>not</em> immediately evaluate its arguments. Instead, it captures the code you typed, and then evaluates it later in a special context.</li>
                    </ul>
                    <p><strong>Analogy:</strong> Standard Evaluation is a strict assistant: you give it a note saying "call Bob", and it immediately dials Bob's number. Non-Standard Evaluation is a smart assistant: you give it a note saying "call Bob", and it first looks at the project folder on your desk (the data frame) to find Bob's specific number before dialing.</p>
                </div>
                
                <div id="how-it-works" class="tab-pane">
                   <h4 class="subsection-title">Capturing, Not Evaluating</h4>
                   <p>When you run <code>filter(mtcars, cyl == 6)</code>, here's what happens behind the scenes:</p>
                   <ol class="prose-list">
                       <li>The <code>filter</code> function sees the code <code>cyl == 6</code>.</li>
                       <li>Instead of immediately looking for a variable called <code>cyl</code> in your global environment, it <strong>captures</strong> that piece of code without evaluating it. This is sometimes called "quoting" the expression.</li>
                       <li>It then looks at the first argument, <code>mtcars</code>, and evaluates the captured expression <strong>inside the context of that data frame</strong>.</li>
                       <li>Inside <code>mtcars</code>, it finds a column named <code>cyl</code>, and the code can now be successfully executed.</li>
                   </ol>
                   <p>This is what allows you to write concise code like <code>filter(cyl > 4 & hp > 100)</code> without having to repeat <code>mtcars$</code> everywhere.</p>
                </div>

                <div id="curly-curly" class="tab-pane">
                   <h4 class="subsection-title">Building Your Own Tidyverse Functions</h4>
                   <p>While using NSE is easy, programming with it (creating your own functions that use NSE) used to be very complex. The modern Tidyverse, powered by the <code>rlang</code> package, makes it simple with the <strong>curly-curly</strong> or <strong>embrace</strong> operator: <code>{{ }}</code>.</p>
                   <p>The <code>{{ }}</code> operator is a signal to a Tidyverse function. It tells it: "Don't evaluate this argument yet. Instead, 'unquote' it and pass it down to the next Tidyverse function."</p>
                   <p>Let's build a function that calculates the average of a column:</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">library(dplyr)
library(rlang) # Powers the {{ }} operator

# We create a function that takes a data frame and a column name
# We "embrace" the column_name argument with {{ }}
calculate_average <- function(data, column_name) {
  data %>%
    summarise(mean_value = mean({{ column_name }}, na.rm = TRUE))
}

# Now we can use our function just like a real dplyr verb!
calculate_average(mtcars, hp)
calculate_average(iris, Sepal.Length)</code></pre>
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: The <code>.data</code> and <code>.env</code> Pronouns</h3>
                <div class="scenario-content">
                    <p>What if you have a global variable with the same name as a column? This can create ambiguity. To solve this, <code>dplyr</code> provides two "pronouns":</p>
                     <ul class="prose-list">
                        <li><code>.data$my_col</code>: Explicitly refers to the column named <code>my_col</code> in the data being processed.</li>
                        <li><code>.env$my_var</code>: Explicitly refers to the variable named <code>my_var</code> in your global environment.</li>
                    </ul>
                    <p>Using these pronouns can make your functions more robust and prevent unexpected behavior.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's create your own Tidyverse-style plotting function.</p>
                     <ul class="prose-list">
                        <li>Make sure you have <code>ggplot2</code> and <code>dplyr</code> loaded (<code>library(tidyverse)</code>).</li>
                        <li>Write a function called <code>create_scatterplot</code> that takes three arguments: <code>data</code>, <code>x_var</code>, and <code>y_var</code>.</li>
                        <li>Inside the function, use <code>ggplot()</code> to create a scatterplot. In your <code>aes()</code> mapping, use the curly-curly operator for both the x and y variables: <code>aes(x = {{ x_var }}, y = {{ y_var }})</code>.</li>
                        <li>Add <code>geom_point()</code> to your plot.</li>
                        <li>Test your function by calling it with the <code>iris</code> dataset: <code>create_scatterplot(iris, Sepal.Length, Petal.Length)</code>.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s3ss1: `
            <div class="subsection">
                <h3 class="subsection-title">The Analyst's Primary Tool</h3>
                <p style="margin-bottom: 1.5rem;">We've introduced data frames and tibbles as R's version of a spreadsheet. Now, we'll dive deeper into the practical, day-to-day tasks of an analyst: creating them from scratch, pulling out specific pieces of information, and adding new columns based on calculations.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You have several vectors containing product information: names, prices, and units sold. Your task is to combine them into a single table, add a new column for the total revenue per product, and then filter this table to show only the highest-grossing items.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="creation">Creation & Inspection</button>
                    <button class="tab-button" data-tab="subsetting">Subsetting Data Frames</button>
                    <button class="tab-button" data-tab="manipulation">Adding & Modifying Columns</button>
                </div>

                <div id="creation" class="tab-pane active">
                    <h4 class="subsection-title">Building Your Table</h4>
                    <p>You can create a tibble (the modern data frame) from individual vectors using the <code>tibble()</code> function. The key rule is that all vectors must have the same length.</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">library(tibble) # Or library(tidyverse)

# 1. Create your vectors
product_name <- c("Laptop", "Monitor", "Keyboard", "Mouse")
price <- c(1200, 350, 110, 45)
quantity_sold <- c(30, 55, 120, 250)

# 2. Combine them into a tibble, giving each column a name
sales_data <- tibble(
  product = product_name,
  unit_price = price,
  quantity = quantity_sold
)

print(sales_data)</code></pre>
                    </div>
                    <p>Once created, use <code>glimpse(sales_data)</code> or <code>str(sales_data)</code> to inspect the structure and data types of your new table.</p>
                </div>

                <div id="subsetting" class="tab-pane">
                   <h4 class="subsection-title">Extracting the Data You Need</h4>
                   <p>You can use the subsetting techniques we've already learned (<code>[]</code> and <code>$</code>) on data frames and tibbles. The Tidyverse also provides expressive "verb" functions that are often easier to read.</p>
                   <ul class="prose-list">
                        <li><strong>Select columns with <code>select()</code>:</strong>
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r">library(dplyr)
# Select only the product and quantity columns
sales_data %>% select(product, quantity)</code></pre>
                            </div>
                        </li>
                        <li><strong>Select rows with <code>slice()</code>:</strong>
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Select the 2nd and 4th rows
sales_data %>% slice(c(2, 4))</code></pre>
                            </div>
                        </li>
                        <li><strong>Filter rows with <code>filter()</code>:</strong> This is the most common way to select rows based on a condition.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Filter for products where more than 100 units were sold
sales_data %>% filter(quantity > 100)</code></pre>
                            </div>
                        </li>
                   </ul>
                </div>
                
                <div id="manipulation" class="tab-pane">
                    <h4 class="subsection-title">Creating New Information</h4>
                    <p>The most common data manipulation task is creating a new column based on the values in existing columns. The Tidyverse function for this is <code>mutate()</code>.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Use mutate() to add a new 'revenue' column
sales_data_with_revenue <- sales_data %>%
  mutate(revenue = unit_price * quantity)

print(sales_data_with_revenue)</code></pre>
                    </div>
                    <p>You can also use the Base R <code>$</code> operator to create or modify a column. This modifies the data frame in place.</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Create a new column for tax
sales_data$tax <- sales_data$unit_price * 0.08

# Overwrite an existing column
sales_data$product <- tolower(sales_data$product) # convert product names to lowercase</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's practice these skills on the built-in <code>iris</code> dataset.</p>
                     <ul class="prose-list">
                        <li>First, convert <code>iris</code> to a tibble: <code>iris_tbl <- as_tibble(iris)</code>.</li>
                        <li>Use <code>mutate()</code> to create a new column called <code>Petal.Area</code>, calculated as <code>Petal.Length * Petal.Width</code>.</li>
                        <li>Use <code>filter()</code> to keep only the rows where the <code>Species</code> is "virginica" AND the new <code>Petal.Area</code> is greater than 10.</li>
                        <li>Use <code>select()</code> to show only the <code>Species</code> and <code>Petal.Area</code> columns from your filtered result. (Hint: You can pipe all these steps together!).</li>
                    </ul>
                </div>
            </div>
          `,
          p1s3ss2: `
            <div class="subsection">
                <h3 class="subsection-title">Handling Complex and Hierarchical Data</h3>
                <p style="margin-bottom: 1.5rem;">While data frames are great for flat, rectangular data, the real world is often more complex. Data from web APIs (like JSON), statistical models, or web scraping often comes in a hierarchical or "nested" format. R's <strong>list</strong> is the perfect tool for handling this kind of data.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You've queried a movie database API and received data for two movies. The data is not a simple table; each movie's information is a collection of different data types: a title (text), a year (number), a list of genres (a vector of text), and a list of actors (another vector). You need to store this complex structure and extract the name of the first actor from the second movie.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="creating">Creating Nested Lists</button>
                    <button class="tab-button" data-tab="navigating">Navigating Nested Data</button>
                    <button class="tab-button" data-tab="extracting">Extracting Data with <code>purrr</code></button>
                </div>

                <div id="creating" class="tab-pane active">
                    <h4 class="subsection-title">Building a Complex Container</h4>
                    <p>You can create a nested list by simply putting lists inside other lists. This allows you to build data structures of arbitrary complexity that can mirror formats like JSON.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Create a list that contains two other lists
movie_data <- list(
  movie1 = list(
    title = "The Matrix",
    year = 1999,
    genres = c("Action", "Sci-Fi"),
    actors = c("Keanu Reeves", "Laurence Fishburne", "Carrie-Anne Moss")
  ),
  movie2 = list(
    title = "Forrest Gump",
    year = 1994,
    genres = c("Drama", "Romance"),
    actors = c("Tom Hanks", "Robin Wright", "Gary Sinise")
  )
)

# Use str() to see the complex, nested structure
str(movie_data)</code></pre>
                    </div>
                </div>

                <div id="navigating" class="tab-pane">
                   <h4 class="subsection-title">Drilling Down into Your Data</h4>
                   <p>To access elements deep inside a nested list, you chain the subsetting operators (<code>[[ ]]</code> and <code>$</code>) together. Each operator takes you one level deeper into the structure.</p>
                   <p>Let's solve our scenario: get the first actor from the second movie.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># We can do this step-by-step to see how it works:
# 1. Extract the second movie's list
movie2_list <- movie_data[[2]] 
# or by name: movie_data$movie2

# 2. From that list, extract the actors vector
actors_vector <- movie2_list$actors

# 3. From that vector, extract the first element
first_actor <- actors_vector[1]

print(first_actor) # Result: "Tom Hanks"

# Or, we can chain the commands together in one line:
movie_data[[2]]$actors[1]</code></pre>
                    </div>
                </div>
                
                <div id="extracting" class="tab-pane">
                    <h4 class="subsection-title">Extracting Elements in a "Tidy" Way</h4>
                    <p>What if you want to get the <strong>same piece of information</strong> from every element in a list? For example, getting the title of *every* movie. A <code>for</code> loop would work, but functional programming tools from <code>purrr</code> are much cleaner.</p>
                    <p>The <code>map_*()</code> family of functions is perfect for this. You tell it which element to extract, and it will do it for every item in the list and return the results in a simple atomic vector.</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">library(purrr)

# Extract the 'title' from every movie list. 
# map_chr() ensures the result is a simple character vector.
all_titles <- map_chr(movie_data, "title")

print(all_titles) # Result: "The Matrix" "Forrest Gump"

# You can even extract deeply nested elements
# Get the first actor from every movie
first_actors <- map_chr(movie_data, c("actors", 1))

print(first_actors) # Result: "Keanu Reeves" "Tom Hanks"</code></pre>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's practice creating and navigating a nested list.</p>
                     <ul class="prose-list">
                        <li>Create a list called <code>class_data</code>. It should contain two top-level elements: <code>student1</code> and <code>student2</code>.</li>
                        <li>Each student's element should be another list containing: a <code>name</code> (character), a <code>major</code> (character), and a vector of <code>grades</code> (numeric).</li>
                        <li>Using chained operators (<code>[[]]</code> and <code>$</code>), extract the major of the second student.</li>
                        <li>Using a <code>purrr::map_*()</code> function, extract the <code>name</code> of *every* student into a single character vector.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s3ss3: `
            <div class="subsection">
                <h3 class="subsection-title">Managing Categorical Variables Effectively</h3>
                <p style="margin-bottom: 1.5rem;">We've introduced factors as R's way of handling categorical data. However, working with them can be tricky. R's default behaviors, like ordering factor levels alphabetically, are often not what you want for analysis or plotting. The <code>forcats</code> package from the Tidyverse provides a suite of tools to solve these common problems.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You are analyzing survey data where users rated a product on a scale: "Poor", "Good", "Excellent". When you create a bar chart, the bars appear in alphabetical order ("Excellent", "Good", "Poor"), which is confusing. You need to reorder them logically. You also want to combine "Poor" and "Good" into a single "Sub-par" category to simplify the analysis.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="levels">Understanding Levels</button>
                    <button class="tab-button" data-tab="reordering">Reordering Levels</button>
                    <button class="tab-button" data-tab="recoding">Recoding & Combining Levels</button>
                </div>

                <div id="levels" class="tab-pane active">
                    <h4 class="subsection-title">The Order Beneath the Surface</h4>
                    <p>Every factor has a set of <strong>levels</strong>, which are the unique categories the factor can take. By default, R orders these levels alphabetically. You can inspect the levels of a factor using the <code>levels()</code> function.</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># Our survey data from the scenario
ratings <- factor(c("Good", "Poor", "Good", "Excellent", "Good", "Poor"))

# Let's inspect the factor and its levels
print(ratings)
levels(ratings) # Result: "Excellent" "Good" "Poor" (Alphabetical order)</code></pre>
                    </div>
                    <p>This alphabetical ordering is why plots of this data would look strange. We need to manually set a more logical order.</p>
                </div>
                
                <div id="reordering" class="tab-pane">
                   <h4 class="subsection-title">Setting a Logical Order</h4>
                   <p>The <code>forcats</code> package makes reordering factor levels straightforward. The most common function for this is <code>fct_relevel()</code>, which lets you move specific levels to the front.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">library(forcats) # Or library(tidyverse)

# Use fct_relevel() to specify the correct order
ratings_reordered <- fct_relevel(ratings, "Poor", "Good", "Excellent")

# Check the new levels
levels(ratings_reordered) # Result: "Poor" "Good" "Excellent" (Logical order)</code></pre>
                    </div>
                    <p>Now, any plot created with <code>ratings_reordered</code> will display the bars in the correct, intuitive sequence.</p>
                </div>

                <div id="recoding" class="tab-pane">
                    <h4 class="subsection-title">Modifying Categories</h4>
                    <p>Often you need to change the names of levels or combine several levels into one. <code>forcats</code> provides simple functions for this.</p>
                     <ul class="prose-list">
                        <li><strong>Recoding (Renaming) with <code>fct_recode()</code>:</strong> Lets you change the name of one or more levels.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Let's rename "Poor" to "Negative"
fct_recode(ratings, "Negative" = "Poor")</code></pre>
                            </div>
                        </li>
                        <li><strong>Collapsing (Combining) with <code>fct_collapse()</code>:</strong> This solves our scenario's second goal. It lets you group multiple old levels into a single new one.
                             <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Combine "Poor" and "Good" into a new "Sub-par" category
ratings_collapsed <- fct_collapse(ratings,
  "Sub-par" = c("Poor", "Good"),
  "Excellent" = "Excellent"
)

print(ratings_collapsed)</code></pre>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Reorder by Frequency with <code>fct_infreq()</code></h3>
                <div class="scenario-content">
                    <p>A very common and useful task is to order the bars on a bar chart from most frequent to least frequent. The <code>fct_infreq()</code> function does this automatically. You simply wrap your factor variable in this function inside your plotting code, and it will reorder the levels based on how often each one appears.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">✍️ Practice Exercise</h3>
                <div class="scenario-content">
                    <p>Let's practice managing factor levels with month names.</p>
                     <ul class="prose-list">
                        <li>Create a character vector with month abbreviations: <code>months_char <- c("Mar", "Jan", "Mar", "Feb", "Jan")</code>.</li>
                        <li>Convert it to a factor called <code>months_factor</code>. Use the <code>levels()</code> function to see the default alphabetical order.</li>
                        <li>Use <code>fct_relevel()</code> to reorder the levels chronologically: "Jan", "Feb", "Mar".</li>
                        <li>Check the levels again to confirm the new, correct order.</li>
                    </ul>
                </div>
            </div>
          `,
          p1s3ss4: `
            <div class="subsection">
                <h3 class="subsection-title">Getting Data Into and Out of R</h3>
                <p style="margin-bottom: 1.5rem;">Your analysis is useless without data. The first step in any project is importing data into your R session, and the last step is often exporting your results or cleaned datasets. The <code>readr</code> and <code>readxl</code> packages from the Tidyverse provide fast, modern, and reliable tools for these tasks.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You've been given a sales report as a CSV file (<code>sales.csv</code>) and additional product information in an Excel file (<code>products.xlsx</code>). You need to load both into R, perform some analysis, and then save your final, cleaned data table as an <code>.rds</code> file for yourself and as a new CSV file for a colleague.</p>
                </div>
            </div>

            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="readr">Reading Flat Files (e.g., CSV)</button>
                    <button class="tab-button" data-tab="readxl">Reading Excel Files</button>
                    <button class="tab-button" data-tab="export">Exporting Your Data</button>
                </div>

                <div id="readr" class="tab-pane active">
                    <h4 class="subsection-title">Using the <code>readr</code> Package</h4>
                    <p>The <code>readr</code> package provides the best tools for reading rectangular data like Comma-Separated Values (CSV) files. Its main function, <code>read_csv()</code>, is a significant improvement over base R's <code>read.csv()</code> because it's much faster, produces tibbles directly, and has smarter data type guessing.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">library(readr) # Or library(tidyverse)

# Assuming 'sales.csv' is in a 'data' subfolder of your R Project
sales_data <- read_csv("data/sales.csv")

# readr provides a helpful summary of the column types it guessed.
# You can override these guesses with the 'col_types' argument.</code></pre>
                    </div>
                    <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Common Arguments:</h5>
                     <ul class="prose-list">
                        <li><code>col_names = TRUE</code>: The default; assumes the first row is the header. Set to <code>FALSE</code> if there are no column names.</li>
                        <li><code>skip = n</code>: Skips the first <code>n</code> lines of the file, useful for files with introductory text.</li>
                        <li><code>na = c("NA", "missing", "-99")</code>: A vector of strings that should be treated as missing values (<code>NA</code>).</li>
                    </ul>
                </div>
                
                <div id="readxl" class="tab-pane">
                   <h4 class="subsection-title">Using the <code>readxl</code> Package</h4>
                   <p>To read data from Microsoft Excel files (<code>.xls</code> or <code>.xlsx</code>), the best tool is the <code>readxl</code> package. It has no external dependencies (like Java), making it easy to use.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># install.packages("readxl") # Install if you haven't already
library(readxl)

# Read the first sheet of the Excel file
product_info <- read_excel("data/products.xlsx")

# You can also specify a sheet by name or number
# product_info <- read_excel("data/products.xlsx", sheet = "Sheet1")
# product_info <- read_excel("data/products.xlsx", sheet = 2)</code></pre>
                    </div>
                </div>

                <div id="export" class="tab-pane">
                    <h4 class="subsection-title">Saving Your Work</h4>
                    <p>There are two main philosophies for saving data:</p>
                    <ul class="prose-list">
                        <li><strong>For Other R Users (<code>.rds</code> files):</strong> This is the best way to save any R object (a data frame, a model, a list, etc.) for later use in R. It saves an exact, compressed copy that perfectly preserves all R-specific data types, like factors with their levels.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Save the cleaned data frame
saveRDS(cleaned_data, "data/processed/cleaned_data.rds")

# To load it back into R later:
reloaded_data <- readRDS("data/processed/cleaned_data.rds")</code></pre>
                            </div>
                        </li>
                        <li><strong>For Sharing with Non-R Users (<code>.csv</code> files):</strong> If you need to share your data with someone using Excel or other software, save it as a CSV.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r">library(readr)
# Write the data frame to a new CSV file
write_csv(cleaned_data, "reports/final_data_for_sharing.csv")</code></pre>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
          `,
          p1s3ss5: `
            <div class="subsection">
                <h3 class="subsection-title">The Grammar of Data Manipulation</h3>
                <p style="margin-bottom: 1.5rem;">Data wrangling (or manipulation) is the process of cleaning, structuring, and enriching raw data into a desired format for analysis. The <code>dplyr</code> package provides a set of functions that act as "verbs" for data wrangling. These verbs are powerful, consistent, and easy to understand.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You are given a large dataset of airline flights. You need to answer a specific question: "For flights from JFK airport, what was the average arrival delay, ordered from worst to best carrier?" Answering this requires you to select rows and columns, create new variables, and collapse the data—all tasks for which <code>dplyr</code> has a verb.</p>
                </div>
            </div>
             <div class="subsection">
                <h3 class="subsection-title">Meet the Core Verbs</h3>
                <p>Most common data manipulation tasks can be solved with just five core <code>dplyr</code> verbs. We'll use the built-in <code>starwars</code> dataset for examples.</p>
                <ul class="prose-list">
                    <li><strong><code>filter()</code>: Pick rows based on their values.</strong>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r">library(dplyr)
# Keep only the droids
droids <- filter(starwars, species == "Droid")</code></pre>
                        </div>
                    </li>
                     <li><strong><code>arrange()</code>: Reorder rows.</strong>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r"># Arrange the droids by their height (tallest first)
arrange(droids, desc(height))</code></pre>
                        </div>
                    </li>
                     <li><strong><code>select()</code>: Pick columns by their names.</strong>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r"># Select only the name, height, and mass columns
select(droids, name, height, mass)</code></pre>
                        </div>
                    </li>
                     <li><strong><code>mutate()</code>: Create new columns with functions of existing columns.</strong>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r"># Create a new column for height in inches and BMI
mutate(droids, height_in = height * 0.39, bmi = mass / (height / 100)^2)</code></pre>
                        </div>
                    </li>
                     <li><strong><code>summarise()</code>: Collapse many values down to a single summary.</strong> This is almost always used with <code>group_by()</code>.
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r"># Calculate the average height and mass for each species
starwars %>%
  group_by(species) %>%
  summarise(
    avg_height = mean(height, na.rm = TRUE),
    avg_mass = mean(mass, na.rm = TRUE),
    n = n() # n() is a helper that counts the number of rows in each group
  )</code></pre>
                        </div>
                    </li>
                </ul>
            </div>
          `,
          p1s3ss6: `
            <div class="subsection">
                <h3 class="subsection-title">Connecting Your Verbs</h3>
                <p style="margin-bottom: 1.5rem;">You've learned the powerful <code>dplyr</code> verbs, but how do you connect them? You could save the result of each step to a new variable, but that's messy. You could also nest the function calls, but that's hard to read. The best solution is the <strong>pipe operator</strong>.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You need to perform a multi-step analysis: take a dataset, filter its rows, then select some columns, and finally arrange the result. The pipe allows you to write this sequence in a natural, readable way, just like a recipe.</p>
                </div>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">How the Pipe Works</h3>
                <p>The pipe, written as <code>%>%</code> (from the <code>magrittr</code> package, loaded with <code>dplyr</code>) or <code>|></code> (a new version in base R), is a powerful tool for making your code more readable. It takes the object on its left-hand side and passes it as the <strong>first argument</strong> to the function on its right-hand side.</p>
                <p>In other words, the expression <code>x %>% f(y)</code> is translated by R into <code>f(x, y)</code>.</p>
            </div>

            <div class="subsection">
                <h3 class="subsection-title">A Practical Comparison</h3>
                <p>Let's solve a simple problem three ways to see why the pipe is so revolutionary. Goal: Find the average height of all droids from the <code>starwars</code> dataset.</p>
                <div class="decision-branches" style="align-items: flex-start;">
                    <div class="decision-branch">
                        <strong>1. Intermediate Variables (Clunky)</strong>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r">droids <- filter(starwars, species == "Droid")
heights <- droids$height
mean(heights, na.rm = TRUE)</code></pre>
                        </div>
                    </div>
                    <div class="decision-branch">
                        <strong>2. Nested Functions (Unreadable)</strong>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r"># Read from the inside-out... hard!
mean(filter(starwars, species == "Droid")$height, na.rm = TRUE)</code></pre>
                        </div>
                    </div>
                </div>
                <div class="decision-branches" style="margin-top: 1rem;">
                     <div class="decision-branch">
                        <strong>3. The Pipe (Clear and Readable)</strong>
                        <p>This code reads like a set of instructions. "Take the starwars data, THEN filter it, THEN pull out the height column, THEN calculate the mean."</p>
                        <div class="code-container">
                            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                            <pre><code class="language-r">starwars %>%
  filter(species == "Droid") %>%
  pull(height) %>%
  mean(na.rm = TRUE)</code></pre>
                        </div>
                    </div>
                </div>
            </div>
             <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: The Pipe Shortcut</h3>
                <div class="scenario-content">
                    <p>You will be typing the pipe operator hundreds of times. Use the RStudio keyboard shortcut to make your life easier: <code>Ctrl + Shift + M</code> (Windows/Linux) or <code>Cmd + Shift + M</code> (macOS).</p>
                </div>
            </div>
          `,
          p1s3ss7: `
            <div class="subsection">
                <h3 class="subsection-title">Preparing Data for Analysis</h3>
                <p style="margin-bottom: 1.5rem;">Data cleaning is the process of detecting and correcting (or removing) corrupt, inaccurate, or irrelevant records from a dataset. It is arguably the most important—and often most time-consuming—part of data analysis. Using the <code>dplyr</code> verbs we've learned, we can tackle the most common cleaning tasks.</p>
                <div class="scenario-content" style="border-color: var(--primary);">
                    <p><strong>Scenario:</strong> You receive a raw export of user data. It's messy: some rows are exact duplicates, some have missing age information (<code>NA</code>), and there are clearly some data entry errors (e.g., an age of 999). You must clean this dataset before you can perform any meaningful analysis.</p>
                </div>
            </div>
            
            <div class="subsection">
                <div class="tabs">
                    <button class="tab-button active" data-tab="duplicates">Handling Duplicates</button>
                    <button class="tab-button" data-tab="missing">Handling Missing Data</button>
                    <button class="tab-button" data-tab="outliers">Basic Outlier Handling</button>
                </div>

                <div id="duplicates" class="tab-pane active">
                    <h4 class="subsection-title">Removing Redundant Rows</h4>
                    <p>The <code>dplyr::distinct()</code> function is the easiest way to find and remove duplicate rows from your data frame.</p>
                     <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r">library(tibble)
# A tibble with a duplicate row (row 1 and 3 are identical)
messy_data <- tibble(
  id = c(1, 2, 1, 4),
  name = c("A", "B", "A", "D")
)

# distinct() keeps only the unique rows
distinct(messy_data)</code></pre>
                    </div>
                </div>
                
                <div id="missing" class="tab-pane">
                   <h4 class="subsection-title">Dealing with <code>NA</code> Values</h4>
                   <p>You have a few options for dealing with missing data, and which you choose depends on your analysis goals.</p>
                   <ul class="prose-list">
                       <li><strong>Dropping Missing Values:</strong> If you're okay with losing the entire row of information, <code>tidyr::drop_na()</code> is a convenient way to remove all rows that contain an <code>NA</code> in any column.
                           <div class="code-container">
                               <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                               <pre><code class="language-r">library(tidyr)
data_with_na <- tibble(x = c(1, 2, NA), y = c("a", NA, "b"))
# drop_na() removes rows 2 and 3
drop_na(data_with_na)</code></pre>
                           </div>
                       </li>
                       <li><strong>Replacing Missing Values:</strong> Sometimes it's better to replace (or "impute") <code>NA</code>s with a specific value, like 0, the mean, or the median.
                            <div class="code-container">
                                <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                                <pre><code class="language-r"># Using tidyr::replace_na()
replace_na(data_with_na, list(x = 0, y = "unknown"))

# Using dplyr::mutate()
data_with_na %>%
  mutate(x = ifelse(is.na(x), mean(x, na.rm = TRUE), x))</code></pre>
                            </div>
                       </li>
                   </ul>
                </div>
                
                <div id="outliers" class="tab-pane">
                   <h4 class="subsection-title">Fixing Obvious Errors</h4>
                   <p>Outlier detection is a deep statistical topic, but for basic data cleaning, you're often just looking for obvious data entry errors. You can use <code>filter()</code> or <code>mutate()</code> to handle these.</p>
                   <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># A tibble with an impossible age
user_data <- tibble(user = c("A", "B", "C"), age = c(35, 999, 42))

# Method 1: Filter out the bad row
user_data %>% filter(age < 120)

# Method 2: Replace the bad value with NA so it can be handled later
user_data %>% mutate(age = ifelse(age > 120, NA, age))</code></pre>
                    </div>
                </div>
            </div>
             <div class="subsection">
                <h3 class="subsection-title">⭐ Pro Tip: Clean Your Column Names with <code>janitor</code></h3>
                <div class="scenario-content">
                    <p>Data often comes with messy column names (e.g., "First Name", "SALES (USD)", "date-of-birth"). The <code>janitor</code> package has a magical function, <code>clean_names()</code>, that instantly converts these into a clean, consistent, snake_case format. It's often the very first function you'll use in a cleaning script.</p>
                    <div class="code-container">
                        <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
                        <pre><code class="language-r"># install.packages("janitor")
library(janitor)
# Create a messy data frame
messy_df <- data.frame("First Name" = "John", "Last Name" = "Smith")
# clean_names() fixes it automatically!
clean_names(messy_df) # Columns become first_name and last_name</code></pre>
                    </div>
                </div>
            </div>
          `,
          p1s4ss1: `
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="s3">S3: The Informal Workhorse</button>
        <button class="tab-button" data-tab="s4">S4: The Formal System</button>
        <button class="tab-button" data-tab="r6">R6: The Encapsulated System</button>
    </div>

    <div id="s3" class="tab-pane active">
        <h4 class="subsection-title">The System You're Already Using</h4>
        <p><strong>S3</strong> is the original and most-used OOP system in R. It's an informal, flexible system based on a concept called <strong>generic functions</strong>. You've been using it since you started R, perhaps without realizing it.</p>
        <p><strong>Analogy:</strong> An S3 generic function like <code>summary()</code> is like a universal remote control. The "Summary" button sends out a general signal. The device it's pointed at—a TV, a stereo, a DVD player—interprets that signal differently. The button is the <strong>generic function</strong>, the specific action (like showing TV channels or audio settings) is the <strong>method</strong>, and the device type is the <strong>class</strong>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
            <pre><code class="language-r"># Create two objects with different classes
# The lm() function returns an object of class "lm"
model_fit <- lm(dist ~ speed, data = cars) # Has class "lm"
data_summary <- summary(cars) # Has class "table"

# 1. Using the generic 'print()' function on the model
# R sees the class is "lm" and invisibly calls a specific method: print.lm()
print(model_fit)

# 2. Using the same generic 'print()' function on the summary table
# R sees the class is "table" and calls a different method: print.table()
print(data_summary)
</code></pre>
        </div>
    </div>

    <div id="s4" class="tab-pane">
       <h4 class="subsection-title">For More Rigorous Code</h4>
       <p>The <strong>S4</strong> system is a more formal and rigid alternative to S3. It was designed for building large, robust software systems where strict validation of objects is critical.</p>
       <p><strong>Analogy:</strong> If S3 is a flexible workshop with general-purpose tools, S4 is a high-tech cleanroom for building sensitive electronics. Everything is formally defined, validated, and documented before it can be used, which prevents errors but requires more setup.</p>
       <ul class="prose-list">
            <li><strong>Key Difference:</strong> S4 requires formal class definitions (using <code>setClass()</code>) that specify the exact names and types of data ("slots") an object can contain.</li>
            <li><strong>When to Use:</strong> It is used extensively in the Bioconductor project for bioinformatics, where data structures are highly complex and must be standardized. For most data analysis, it's overkill.</li>
            <li><strong>Syntax:</strong> You can recognize S4 code by its use of <code>setClass()</code> and the <code>@</code> symbol to access slots (e.g., <code>my_object@slot_name</code>).</li>
       </ul>
    </div>
    
    <div id="r6" class="tab-pane">
       <h4 class="subsection-title">OOP Like Other Languages</h4>
       <p><strong>R6</strong> is the newest system and comes from a package. It behaves much more like OOP in popular languages like Python or Java, and its key feature is <strong>encapsulation</strong>.</p>
       <p><strong>Analogy:</strong> If S3 is a universal remote that acts *on* separate devices, an R6 object is like a modern coffee machine. The data (water, beans) and the functions (<code>brew()</code>, <code>clean()</code>) are all bundled together in one self-contained appliance. You tell the coffee machine to brew itself (<code>coffee_maker$brew()</code>).</p>
        <ul class="prose-list">
            <li><strong>Key Difference:</strong> Data and methods are "encapsulated" within the object itself. This prevents you from accidentally modifying an object's internal data in an invalid way.</li>
            <li><strong>When to Use:</strong> It's excellent for building applications with "stateful" objects, like a web API connection that needs to store your credentials, or a simulator where agents need to remember their history.</li>
       </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Know Your Focus</h3>
    <div class="scenario-content">
        <p>As a data scientist, your primary focus should be mastering <strong>S3</strong>. It's the language of base R and the Tidyverse, and you'll use it every day to interact with model objects and data frames. You only need to be able to *recognize* S4 and R6 when you encounter them in specialized packages.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Discovering available S3 methods is a key skill. The <code>methods()</code> function lets you see all the specific "attachments" a generic "multi-tool" function has.</p>
         <ul class="prose-list">
            <li>Run <code>methods(plot)</code> in your console. This will show you a long list of all the different types of objects the generic <code>plot()</code> function knows how to handle.</li>
            <li>Can you find <code>plot.lm</code> in the list? This is the method that creates the diagnostic plots for a linear model object.</li>
        </ul>
    </div>
</div>
`,
          p1s4ss2: `
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="class-object">Classes & Objects</button>
        <button class="tab-button" data-tab="method-poly">Methods & Polymorphism</button>
        <button class="tab-button" data-tab="inheritance">Inheritance</button>
        <button class="tab-button" data-tab="encapsulation">Encapsulation & Abstraction</button>
    </div>

    <div id="class-object" class="tab-pane active">
        <h4 class="subsection-title">Blueprints and Instances</h4>
        <ul class="prose-list">
            <li>A <strong>Class</strong> is a blueprint or a template for an object. It defines a common structure and a set of behaviors. For example, <code>"data.frame"</code> is a class.</li>
            <li>An <strong>Object</strong> is a specific instance created from that blueprint. The built-in <code>iris</code> dataset is an object of the class <code>"data.frame"</code>.</li>
        </ul>
        <p><strong>Analogy:</strong> A cookie cutter is the <strong>class</strong>. It defines the shape (the structure) of all cookies made with it. Each individual cookie you cut out is an <strong>object</strong>.</p>
         <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
            <pre><code class="language-r"># Let's create our own simple "analysis_result" class
# The underlying data is just a list
result_data <- list(
  metric_name = "Accuracy",
  value = 0.95,
  data_points = 150
)

# We make it an object by assigning it a class name
class(result_data) <- "analysis_result"

# Now, 'result_data' is an object of our new class
# It will now respond to methods written for the "analysis_result" class.
print(class(result_data))
</code></pre>
        </div>
    </div>
    
    <div id="method-poly" class="tab-pane">
       <h4 class="subsection-title">One Function, Many Forms</h4>
        <ul class="prose-list">
           <li>A <strong>Method</strong> is a function specialized for a particular class. S3 methods use the naming convention <code>generic_function.class_name()</code>.</li>
           <li><strong>Polymorphism</strong> is the principle that a generic function can have "many forms" (methods) and will automatically choose the correct one based on the object's class.</li>
       </ul>
       <p>Let's create a custom <code>print</code> method for our <code>"analysis_result"</code> class to provide a more informative, human-readable summary instead of the default list output.</p>
       <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
            <pre><code class="language-r"># Define a print method for objects of class "analysis_result"
# The name MUST be print.analysis_result
print.analysis_result <- function(x, ...) { # The ... is important to match the generic
  cat("## Analysis Result ##\n")
  cat("  Metric:", x$metric_name, "\n")
  cat("  Value:", paste0(x$value * 100, "%"), "\n")
  cat("---------------------\n")
}

# Now, when we simply type the object's name...
result_data

# ...R automatically finds and uses our custom print method!
</code></pre>
        </div>
    </div>

    <div id="inheritance" class="tab-pane">
       <h4 class="subsection-title">Building on Existing Work</h4>
       <p><strong>Inheritance</strong> is a mechanism where a new "child" class can be based on an existing "parent" class. The child automatically inherits all the methods and behaviors of its parent, allowing you to extend functionality without rewriting code.</p>
       <p><strong>Analogy:</strong> Think of Russian dolls. The smallest doll has its own unique features, but it's also a member of the set of all the larger dolls that contain it. It inherits the property of being "a doll" from its parents.</p>
       <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
            <pre><code class="language-r"># Our previous object just had one class
class(result_data)

# Let's create a new, more specific object
timed_result <- list(
  metric_name = "F1-Score",
  value = 0.88,
  data_points = 500,
  time_taken = 15.2 # A new piece of data
)

# We can make it inherit from our old class!
# The class vector is searched from left to right.
class(timed_result) <- c("timed_analysis_result", "analysis_result")

# When we print it, R looks for print.timed_analysis_result.
# It doesn't find one, so it moves to the parent class and finds print.analysis_result!
print(timed_result)
</code></pre>
        </div>
    </div>
    
     <div id="encapsulation" class="tab-pane">
        <h4 class="subsection-title">Bundling and Hiding Complexity</h4>
         <ul class="prose-list">
            <li><strong>Encapsulation:</strong> The bundling of data and the functions that operate on that data into a single, self-contained object. This is the core principle of the R6 system. It protects an object's internal data from being changed in unexpected ways.</li>
            <li><strong>Abstraction:</strong> Hiding complex implementation details behind a simple interface. When you call <code>lm()</code>, you are using abstraction. You don't need to know the complex linear algebra inside; you just provide the formula and data. This makes code easier to use and maintain.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Don't Break the Contract</h3>
    <div class="scenario-content">
        <p>When you write a new method for a generic function (like <code>print.my_class</code>), you should ensure your function accepts the same arguments as the original generic. The most common generic is <code>print(x, ...)</code>. Even if you don't use the <code>...</code> (dot-dot-dot) argument, including it in your method definition makes your function more robust and ensures it won't break if other code tries to pass extra arguments to it.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's practice creating a simple class and method.</p>
         <ul class="prose-list">
            <li>Create a list named <code>my_report</code> with two elements: a <code>title</code> (e.g., "Q3 Sales") and a data frame of <code>data</code> (you can use <code>head(mtcars)</code> for this).</li>
            <li>Assign this object the S3 class "quarterly_report".</li>
            <li>Write a custom <code>summary()</code> method for this class called <code>summary.quarterly_report</code>.</li>
            <li>This method should print the report's title and then show the structure of the data using the <code>str()</code> function.</li>
            <li>Test your work by running <code>summary(my_report)</code>.</li>
        </ul>
    </div>
</div>
`,
          p1s4ss3: `
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="what-are-patterns">What Are Design Patterns?</button>
        <button class="tab-button" data-tab="factory">Example: The Factory Pattern</button>
        <button class="tab-button" data-tab="strategy">Example: The Strategy Pattern</button>
    </div>

    <div id="what-are-patterns" class="tab-pane active">
        <h4 class="subsection-title">Not Reinventing the Wheel</h4>
        <p><strong>Design Patterns</strong> are general, reusable, and well-documented solutions to commonly occurring problems in software design. They aren't specific pieces of code, but rather templates or blueprints for how to structure your code to solve a problem elegantly.</p>
        <p><strong>Analogy:</strong> A design pattern is like a <strong>recipe</strong>. If you want to bake a cake, you don't have to re-invent the entire process from scratch. You follow a recipe—a proven, tested pattern—that tells you how to combine ingredients (your objects and data) in a specific way to get a consistent, successful outcome (a well-structured program).</p>
        <p>Learning them helps you write more maintainable and robust code because you are leveraging the collective experience of many programmers who have solved that same problem before.</p>
    </div>

    <div id="factory" class="tab-pane">
       <h4 class="subsection-title">Creating Objects Without the Mess</h4>
       <p>The <strong>Factory Pattern</strong> solves the problem of creating different types of related objects without exposing the complex creation logic to the user. This is perfect for the scenario where you need to generate different but related objects.</p>
       <p><strong>Analogy:</strong> A factory function is like a vending machine. You, the user, simply press a button ("A1" for a circle, "B2" for a rectangle). You don't need to know the complex internal mechanics of how the machine assembles and dispenses that specific item. The factory hides the complexity and just gives you the finished product.</p>
       <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
            <pre><code class="language-r"># This factory function creates different shape objects
shape_factory <- function(type, ...) {
  # Collect additional arguments
  args <- list(...)
  
  if (type == "circle") {
    shape <- list(radius = args$radius)
    class(shape) <- "circle"
  } else if (type == "rectangle") {
    shape <- list(width = args$width, height = args$height)
    class(shape) <- "rectangle"
  } else {
    stop("Unknown shape type specified.")
  }
  return(shape)
}

# Now, we use the clean factory interface to create our objects
my_circle <- shape_factory(type = "circle", radius = 5)
my_rectangle <- shape_factory(type = "rectangle", width = 4, height = 6)

str(my_circle)
str(my_rectangle)
</code></pre>
        </div>
    </div>
    
    <div id="strategy" class="tab-pane">
        <h4 class="subsection-title">Making Algorithms Interchangeable</h4>
        <p>The <strong>Strategy Pattern</strong> solves the problem of needing to switch between different algorithms or behaviors at runtime. You define a family of algorithms (the "strategies"), and your main function can accept any of them as an argument, making them interchangeable.</p>
        <p><strong>Analogy:</strong> This is like having a power drill (the main function) with interchangeable heads (the strategies). You can pop in a Phillips head, a flat head, or a hex head depending on the screw you're working with. The drill itself doesn't change, only the strategy it's using.</p>
       <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="far fa-copy"></i></button>
            <pre><code class="language-r"># 1. Define two different "strategies" for summarizing data
# These are just standard R functions
strategy_mean <- function(x) { mean(x, na.rm = TRUE) }
strategy_median <- function(x) { median(x, na.rm = TRUE) }

# 2. Create a main function that accepts a data vector and a summary strategy function
summarise_data <- function(data_vector, summary_strategy_func) {
  # The function doesn't know or care WHICH strategy it's using.
  # It just executes the function it was given.
  summary_strategy_func(data_vector)
}

my_data <- c(1, 2, 3, 4, 5, 100) # Vector with an outlier

# 3. Use the main function with different strategies to get different results
summarise_data(my_data, strategy_mean)   # Result is skewed by the outlier: 19.16
summarise_data(my_data, strategy_median) # The median is a more robust strategy here: 3.5
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Functions are First-Class Citizens</h3>
    <div class="scenario-content">
        <p>The Strategy Pattern is incredibly natural in R because R treats functions as "first-class citizens." This means you can do anything with a function that you can do with a variable: assign it to a new name, store it in a list, or, most importantly, pass it as an argument to another function. This is the foundation of functional programming in R.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's combine the Factory and Strategy patterns.</p>
         <ul class="prose-list">
            <li>Using the <code>shape_factory</code> from the example, create a circle and a rectangle.</li>
            <li>Create a generic function called <code>calculate_area <- function(x) UseMethod("calculate_area")</code>.</li>
            <li>Write two methods: <code>calculate_area.circle</code> (which calculates $$\\pi \\cdot r^2$$) and <code>calculate_area.rectangle</code> (which calculates width * height).</li>
            <li>Test your factory and methods by creating one of each shape and calculating their areas. You've now built a complete, polymorphic system!</li>
        </ul>
    </div>
</div>
`,
          p1s4ss4: `
<div class="subsection">
    <h3 class="subsection-title">Going Beyond One-Off Scripts</h3>
    <p>You don't need to use object-oriented programming for every simple analysis. A straightforward script is often the best tool for the job. However, as your projects grow in complexity or as you start building reusable tools, OOP becomes an invaluable approach for creating code that is organized, scalable, and easy to maintain.</p>
</div>
<div class="subsection">
    <h3 class="subsection-title">When to Use OOP in Data Science</h3>
    <p>Here are some common situations in R where an OOP approach is highly beneficial:</p>
    <div class="decision-branches" style="align-items: flex-start;">
        <div class="decision-branch">
            <strong><i class="fas fa-box-open"></i> Building R Packages</strong>
            <p>When you create a package, you're building a tool for others. Using an S3 or R6 system to define classes for your package's objects makes the package predictable and professional. For example, providing custom <code>print()</code>, <code>summary()</code>, and <code>plot()</code> methods for your objects is a standard best practice.</p>
        </div>
        <div class="decision-branch">
            <strong><i class="fas fa-cogs"></i> Creating Complex Simulations</strong>
            <p>If you are modeling a system with multiple "agents" that have their own state (data) and behaviors (functions), OOP is a natural fit. For example, in an economic simulation, each "household" or "firm" could be an object with its own properties and rules of interaction. R6 is often used for this.</p>
        </div>
    </div>
    <div class="decision-branches" style="align-items: flex-start; margin-top: 1rem;">
        <div class="decision-branch">
            <strong><i class="fas fa-cloud"></i> Wrapping Web APIs</strong>
            <p>When writing code to communicate with a web API, it's very common to create an R6 object that represents your connection. The object can safely store your credentials or API key (data) and have methods for each of the API's endpoints (e.g., <code>my_connection$get_users()</code>).</p>
        </div>
        <div class="decision-branch">
            <strong><i class="fas fa-clipboard-list"></i> Standardized Analytical Frameworks</strong>
            <p>If your team performs the same type of analysis repeatedly (e.g., a standard marketing report), you can create an S3 class for an "analysis" object. You can then define standard methods (<code>plot()</code>, <code>summarise()</code>) that work on this object, ensuring every report is consistent.</p>
        </div>
    </div>
</div>

<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Litmus Test for OOP</h3>
    <div class="scenario-content">
        <p>When deciding whether to use OOP, ask yourself this simple question: <strong>"Am I writing a one-off script, or am I building a *system*?"</strong></p>
        <p>A script is a sequence of commands to get a specific answer. A system is a collection of interacting components designed to be reused, maintained, and expanded over time. If you're building a system, even a small one, investing in an OOP structure will almost always pay off in the long run.</p>
    </div>
</div>

<div class="subsection">
    <h3 class="subsection-title">✍️ Thought Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise. You don't need to write the code, just think about the structure.</p>
        <p><strong>Task:</strong> Imagine you are tasked with creating a standardized framework for evaluating machine learning models. You want to train a model and then get its accuracy, precision, and recall.</p>
        <p>How could you use a single S3 class called <code>"ml_model"</code> and three generic functions (<code>train()</code>, <code>predict()</code>, and <code>evaluate()</code>) to organize this workflow? What would be the benefit of this approach compared to writing separate scripts for each model type?</p>
    </div>
</div>
`,
          p1s5ss1t1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A retail manager wants to understand last month's sales performance. They have a list of daily sales figures and need to summarize them to answer questions like: "What was a typical day's sales?" and "How consistent were the sales from day to day?" This requires summarizing the data's center and spread.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss1t1-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s5ss1t1-math">Mathematical Foundation</button>
        <button class="tab-button" data-tab="p1s5ss1t1-impl">Implementation in R</button>
        <button class="tab-button" data-tab="p1s5ss1t1-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss1t1-usecase">Use Case Example</button>
    </div>

    <div id="p1s5ss1t1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Summarizing Your Data</h4>
        <p>Descriptive statistics are used to summarize and describe the main features of a dataset. They are broken down into two main types:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong><i class="fas fa-bullseye"></i> Measures of Central Tendency</strong>
                <p>These tell you where the "center" or "typical" value of the data lies.</p>
                <ul class="prose-list">
                    <li><strong>Mean:</strong> The arithmetic average. Highly sensitive to outliers.</li>
                    <li><strong>Median:</strong> The middle value of a sorted dataset. Robust to outliers.</li>
                    <li><strong>Mode:</strong> The most frequently occurring value. Can be used for categorical data.</li>
                </ul>
            </div>
            <div class="decision-branch">
                <strong><i class="fas fa-ruler-horizontal"></i> Measures of Dispersion</strong>
                <p>These tell you how "spread out" or variable the data is.</p>
                 <ul class="prose-list">
                    <li><strong>Standard Deviation (SD):</strong> The average distance of data points from the mean. The most common measure of spread.</li>
                    <li><strong>Variance:</strong> The square of the standard deviation.</li>
                    <li><strong>Interquartile Range (IQR):</strong> The range of the middle 50% of the data. Robust to outliers.</li>
                </ul>
            </div>
        </div>
    </div>

    <div id="p1s5ss1t1-math" class="tab-pane">
        <h4 class="subsection-title">The Formulas Behind the Stats</h4>
        <div class="math-foundation">
            <p><strong>Mean ($\\bar{x}$):</strong> The sum of all values ($x_i$) divided by the count of values ($n$).</p>
            $$ \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} $$
        </div>
        <div class="math-foundation" style="margin-top: 1rem;">
            <p><strong>Sample Variance ($s^2$):</strong> The average of the squared differences from the mean. We divide by $n-1$ for an unbiased estimate of the population variance.</p>
            $$ s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1} $$
        </div>
         <div class="math-foundation" style="margin-top: 1rem;">
            <p><strong>Sample Standard Deviation ($s$):</strong> The square root of the variance. It's in the same units as the original data, making it more interpretable.</p>
            $$ s = \\sqrt{s^2} $$
        </div>
    </div>

    <div id="p1s5ss1t1-impl" class="tab-pane">
        <h4 class="subsection-title">Calculating in R</h4>
        <p>Base R has built-in functions for all major descriptive statistics. The <code>summary()</code> function is a great shortcut.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># Let's use the 'mpg' column from the built-in mtcars dataset
car_mpg <- mtcars$mpg

# Central Tendency
mean(car_mpg)
median(car_mpg)

# Dispersion
sd(car_mpg)
var(car_mpg)
IQR(car_mpg)
range(car_mpg)

# A great shortcut for the most common stats
summary(car_mpg)
</code></pre>
        </div>
    </div>
    
    <div id="p1s5ss1t1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Distribution</h4>
        <p>A histogram is the best way to visualize the distribution of a single numeric variable. We can overlay lines for the mean and median to see how they relate and to visually assess skewness.</p>
        <div class="plot-container">
            <div id="p1s5ss1t1-plot1" class="plotly-chart"></div>
        </div>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># R code to generate this visualization
library(ggplot2)
library(plotly)

# For reproducibility
set.seed(42)

# Generate right-skewed data
sales_data <- data.frame(sales = round(exp(rnorm(500, mean = 6, sd = 0.8))))

# Calculate stats
mean_sales <- mean(sales_data$sales)
median_sales <- median(sales_data$sales)

# Create the plot
p <- ggplot(sales_data, aes(x = sales)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "#3b82f6", alpha = 0.7) +
  geom_density(color = "black", size = 1) +
  geom_vline(aes(xintercept = mean_sales, color = "Mean"), linetype = "dashed", size = 1.2) +
  geom_vline(aes(xintercept = median_sales, color = "Median"), linetype = "dotted", size = 1.2) +
  scale_color_manual(name = "Statistic", values = c("Mean" = "#f59e0b", "Median" = "#10b981")) +
  labs(title = "Distribution of Daily Sales", x = "Daily Sales ($)", y = "Density") +
  theme_minimal()

ggplotly(p)
</code></pre>
        </div>
    </div>

    <div id="p1s5ss1t1-usecase" class="tab-pane">
        <h4 class="subsection-title">Solving the Manager's Problem</h4>
        <p>Let's apply these concepts to the retail manager's scenario. We'll generate some sample data representing daily sales, with a few exceptionally high-sales days (outliers).</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># Sample data: 30 days of sales
daily_sales <- c(
    450, 520, 480, 550, 490, 510, 470, 530, 460, 500,
    540, 490, 560, 2500, 480, 510, 530, 470, 500, 520,
    490, 550, 480, 510, 3100, 530, 490, 470, 520, 540
)

summary(daily_sales)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong></p>
            <ul class="prose-list">
                <li>The <strong>Mean</strong> is $696, but the <strong>Median</strong> is only $510. This large difference tells the manager that the average is being pulled up by a few very large values (the $2500 and $3100 days).</li>
                <li><strong>Conclusion for the manager:</strong> A "typical" day's sales are closer to the <strong>median of $510</strong>. The mean is misleading in this case.</li>
                <li>The <strong>Standard Deviation</strong> would be large, and the <strong>IQR</strong> would be relatively small, confirming that most days are clustered together but a few are very far away.</li>
            </ul>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Mean vs. Median - Which to Report?</h3>
    <div class="scenario-content">
        <p>The choice between mean and median depends entirely on the shape of your data's distribution. For a symmetric distribution (like a bell curve), the mean and median will be very similar, so either is fine. However, for a <strong>skewed distribution</strong> (like income, housing prices, or our sales data), the <strong>median is almost always a more representative measure</strong> of the "typical" value because it isn't affected by extreme outliers.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>The built-in <code>mtcars</code> dataset contains information about different car models.</p>
        <ul class="prose-list">
            <li>Use the <code>summary()</code> function on the horsepower column (<code>mtcars$hp</code>).</li>
            <li>Calculate the standard deviation and variance of the car weight column (<code>mtcars$wt</code>).</li>
            <li>Based on the mean and median of the <code>hp</code> column, would you say the data is skewed? If so, in which direction? (Hint: If mean > median, it's a positive or right skew).</li>
        </ul>
    </div>
</div>
`,
          p1s5ss1t2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A gaming company is analyzing a new feature. They know 10% of players are "VIPs". They also know that VIPs use the feature 60% of the time, while non-VIPs only use it 20% of the time. A random player just used the feature. What is the actual probability that this player is a VIP? Simply knowing they used the feature isn't enough; we need a formal way to update our beliefs with this new evidence.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss1t2-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s5ss1t2-types">Types of Probability</button>
        <button class="tab-button" data-tab="p1s5ss1t2-bayes">Bayes' Theorem</button>
        <button class="tab-button" data-tab="p1s5ss1t2-usecase">Use Case Example</button>
    </div>

    <div id="p1s5ss1t2-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Language of Uncertainty</h4>
        <p>Probability theory is the mathematical framework for quantifying uncertainty. It's built on a few core ideas:</p>
        <ul class="prose-list">
            <li><strong>Experiment:</strong> A procedure with a well-defined set of possible outcomes (e.g., rolling a six-sided die).</li>
            <li><strong>Sample Space (S):</strong> The set of *all* possible outcomes of an experiment (e.g., S = {1, 2, 3, 4, 5, 6}).</li>
            <li><strong>Event (A):</strong> A specific subset of outcomes we are interested in (e.g., the event of rolling an even number, A = {2, 4, 6}).</li>
        </ul>
        <p>The probability of an event A, denoted $P(A)$, is a number between 0 (impossible) and 1 (certain). For the die roll, $P(A) = 3/6 = 0.5$.</p>
    </div>

    <div id="p1s5ss1t2-types" class="tab-pane">
        <h4 class="subsection-title">Relationships Between Events</h4>
        <p><strong>Analogy:</strong> Think of a standard deck of 52 cards.</p>
        <ul class="prose-list">
            <li><strong>Marginal Probability $P(A)$:</strong> The probability of a single event. The probability of drawing a King is $P(\text{King}) = 4/52$.</li>
            <li><strong>Joint Probability $P(A \\cap B)$:</strong> The probability of two events happening at the same time. The probability of drawing a King that is also a Heart is $P(\text{King} \\cap \text{Heart}) = 1/52$.</li>
            <li><strong>Conditional Probability $P(A|B)$:</strong> The probability of event A happening *given that* we know event B has already happened. This is a crucial concept. The probability of drawing a King *given that we know the card is a Heart* is $P(\text{King} | \text{Heart}) = 1/13$.</li>
        </ul>
    </div>

    <div id="p1s5ss1t2-bayes" class="tab-pane">
        <h4 class="subsection-title">Updating Your Beliefs with Evidence</h4>
        <p>Bayes' Theorem is one of the most important formulas in data science. It describes how to update our belief in a hypothesis (A) after observing new evidence (B).</p>
        <div class="math-foundation">
            <p>The formula allows us to "reverse" the conditional probability:</p>
            $$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$
        </div>
        <div class="plot-description">
            <ul>
                <li>$P(A|B)$ is the <strong>Posterior</strong>: What we want to find. Our updated belief.</li>
                <li>$P(A)$ is the <strong>Prior</strong>: Our initial belief before seeing any evidence.</li>
                <li>$P(B|A)$ is the <strong>Likelihood</strong>: The probability of seeing the evidence if our hypothesis is true.</li>
                <li>$P(B)$ is the <strong>Evidence</strong>: The overall probability of observing the evidence.</li>
            </ul>
        </div>
    </div>
    
    <div id="p1s5ss1t2-usecase" class="tab-pane">
        <h4 class="subsection-title">Solving the Gaming Company's Problem</h4>
        <p>Let's use R as a calculator to apply Bayes' Theorem. We want to find $P(\text{VIP} | \text{Used Feature})$.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># Step 1: Define our known probabilities (Priors and Likelihoods)
p_vip <- 0.10
p_not_vip <- 1 - p_vip

p_used_given_vip <- 0.60
p_used_given_not_vip <- 0.20

# Step 2: Calculate the denominator, P(Used Feature), using the Law of Total Probability
# P(Used) = P(Used|VIP)*P(VIP) + P(Used|Not VIP)*P(Not VIP)
p_used <- (p_used_given_vip * p_vip) + (p_used_given_not_vip * p_not_vip)
# p_used = (0.60 * 0.10) + (0.20 * 0.90) = 0.06 + 0.18 = 0.24

# Step 3: Apply Bayes' Theorem
# P(VIP | Used) = [ P(Used | VIP) * P(VIP) ] / P(Used)
p_vip_given_used <- (p_used_given_vip * p_vip) / p_used

print(p_vip_given_used) # Result: 0.25
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> Even though a player used the feature, there is only a <strong>25% chance</strong> they are a VIP. Our initial belief (the prior, 10%) was updated by the evidence, but because non-VIPs are so much more common, they still make up the majority of feature users.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Probability vs. Likelihood</h3>
    <div class="scenario-content">
        <p>These terms are often used interchangeably in plain English, but in statistics, they mean different things. <strong>Probability</strong> refers to the chance of future data points given a fixed model (e.g., "What is the probability of heads with a fair coin?"). <strong>Likelihood</strong> refers to how well a fixed set of observed data is explained by different models (e.g., "Given I saw 8 heads in 10 flips, what is the likelihood the coin is fair vs. biased?").</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's use Bayes' theorem for a classic medical diagnosis problem.</p>
        <p><strong>Given Information:</strong></p>
        <ul class="prose-list">
            <li>A rare disease affects 1% of the population. ($P(\text{Disease}) = 0.01$).</li>
            <li>A test for the disease has 99% accuracy:
                <ul>
                    <li>If you have the disease, it correctly tests positive 99% of the time. ($P(\text{Positive} | \text{Disease}) = 0.99$).</li>
                    <li>If you don't have the disease, it correctly tests negative 99% of the time, which means it falsely tests positive 1% of the time. ($P(\text{Positive} | \text{No Disease}) = 0.01$).</li>
                </ul>
            </li>
        </ul>
        <p><strong>Question:</strong> A patient tests positive. What is the actual probability they have the disease, $P(\text{Disease} | \text{Positive})$? Use R to calculate the answer. You will be surprised by the result!</p>
    </div>
</div>
`,
          p1s5ss1t3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A call center manager knows they receive an average of 10 calls per hour. They need a way to model this process to answer critical staffing questions: What is the probability of being overwhelmed (e.g., getting 15 or more calls) or being overstaffed (getting 5 or fewer calls)? This requires choosing the right probability distribution.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss1t3-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s5ss1t3-discrete">Discrete Distributions</button>
        <button class="tab-button" data-tab="p1s5ss1t3-continuous">Continuous Distributions</button>
        <button class="tab-button" data-tab="p1s5ss1t3-impl">Implementation in R</button>
    </div>

    <div id="p1s5ss1t3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Mapping Outcomes to Probabilities</h4>
        <p>A <strong>probability distribution</strong> is a mathematical function that describes the likelihood of all possible outcomes for a random variable. It provides a model for a real-world process.</p>
        <ul class="prose-list">
            <li>For <strong>discrete</strong> variables (countable outcomes like coin flips), the distribution is called a <strong>Probability Mass Function (PMF)</strong>. It gives the probability of each specific outcome. The sum of all probabilities is 1.</li>
            <li>For <strong>continuous</strong> variables (measured outcomes like height), it's a <strong>Probability Density Function (PDF)</strong>. The total area under the curve is 1, and the area between two points gives the probability of an outcome falling in that range.</li>
        </ul>
    </div>

    <div id="p1s5ss1t3-discrete" class="tab-pane">
        <h4 class="subsection-title">Distributions for Countable Events</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Binomial Distribution</strong>
                <p>Models the number of "successes" in a fixed number ($n$) of independent trials, each with a probability of success ($p$).</p>
                <div class="plot-container">
                    <div id="p1s5ss1t3-plot1" class="plotly-chart"></div>
                </div>
                <div class="plot-description">
                    <p><strong>How to observe this plot:</strong> This shows the probability of getting a certain number of heads in 20 coin flips. The most likely outcome is 10 heads, and the probabilities decrease symmetrically as you move away from the center. The height of each bar represents the exact probability for that outcome.</p>
                </div>
            </div>
            <div class="decision-branch">
                <strong>Poisson Distribution</strong>
                <p>Models the number of events occurring in a fixed interval of time or space, given an average rate ($\lambda$).</p>
                <div class="plot-container">
                    <div id="p1s5ss1t3-plot2" class="plotly-chart"></div>
                </div>
                 <div class="plot-description">
                    <p><strong>How to observe this plot:</strong> This models our call center scenario (average of 10 calls/hour). The peak is at 10, the average rate. The distribution is skewed to the right; while you can't get fewer than 0 calls, it's possible (though increasingly unlikely) to get 20 or more.</p>
                </div>
            </div>
        </div>
    </div>
    
    <div id="p1s5ss1t3-continuous" class="tab-pane">
        <h4 class="subsection-title">Distributions for Measured Outcomes</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Normal Distribution</strong>
                <p>The classic symmetric "bell curve," defined by its mean ($\mu$) and standard deviation ($\sigma$). It describes many natural phenomena.</p>
                 <div class="plot-container">
                    <div id="p1s5ss1t3-plot3" class="plotly-chart"></div>
                </div>
                 <div class="plot-description">
                    <p><strong>How to observe this plot:</strong> This shows a standard normal distribution. The curve is perfectly symmetric around the mean of 0. The area under the curve between any two points on the x-axis represents the probability of an outcome falling within that range.</p>
                </div>
            </div>
            <div class="decision-branch">
                <strong>Exponential Distribution</strong>
                <p>Models the time *between* events in a Poisson process, defined by a rate parameter ($\lambda$). It's highly skewed to the right.</p>
                 <div class="plot-container">
                    <div id="p1s5ss1t3-plot4" class="plotly-chart"></div>
                </div>
                 <div class="plot-description">
                    <p><strong>How to observe this plot:</strong> This models the waiting time for the next call at our center. It shows that very short waiting times are most likely, and the probability of a very long wait decreases rapidly (exponentially). This is common in waiting-time scenarios.</p>
                </div>
            </div>
        </div>
    </div>
    
    <div id="p1s5ss1t3-impl" class="tab-pane">
        <h4 class="subsection-title">The "d-p-q-r" Functions in R</h4>
        <p>R provides a consistent set of four functions for every major distribution. Let's use the normal distribution (norm) as an example:</p>
        <ul class="prose-list">
            <li><strong><code>dnorm()</code> (density):</strong> The height of the PDF curve at a specific point.</li>
            <li><strong><code>pnorm()</code> (probability):</strong> The cumulative probability up to a point (the area under the curve to the left). This is extremely useful.</li>
            <li><strong><code>qnorm()</code> (quantile):</strong> The inverse of <code>pnorm()</code>. Given a probability, it finds the value.</li>
            <li><strong><code>rnorm()</code> (random):</strong> Generates random numbers from that distribution.</li>
        </ul>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># Let's model IQ scores (mean=100, sd=15)
# What is the probability of having an IQ of 130 or less?
pnorm(q = 130, mean = 100, sd = 15) # Result: ~0.977 (97.7%)

# What IQ score represents the 90th percentile?
qnorm(p = 0.90, mean = 100, sd = 15) # Result: ~119.2

# Generate 5 random IQ scores
rnorm(n = 5, mean = 100, sd = 15)
</code></pre>
        </div>
        <h4 class="subsection-title" style="margin-top: 1.5rem">Solving the Call Center Problem</h4>
        <p>We use the Poisson functions (<code>ppois</code>) to solve the manager's problem, where the average rate, lambda ($\lambda$), is 10.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># 1. Probability of being overstaffed (5 or fewer calls)
ppois(q = 5, lambda = 10) # Result: ~0.067 or 6.7%

# 2. Probability of being overwhelmed (15 or more calls)
# This is 1 minus the probability of 14 or fewer calls.
1 - ppois(q = 14, lambda = 10) # Result: ~0.083 or 8.3%
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Central Limit Theorem (CLT)</h3>
    <div class="scenario-content">
        <p>Why is the Normal distribution so special? The <strong>Central Limit Theorem</strong> is the reason. It states that if you take large enough random samples from *any* population (regardless of its original shape) and calculate the mean of each sample, the distribution of those sample means will be approximately Normal. This powerful idea is the foundation for most classical hypothesis testing.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>A student is taking a 20-question multiple-choice test. Each question has 4 options, so the probability of guessing correctly is 0.25. This is a Binomial problem.</p>
        <ul class="prose-list">
            <li>Use <code>dbinom()</code> to find the probability of guessing *exactly* 5 questions correctly. (Hint: <code>x=5</code>, <code>size=20</code>, <code>prob=0.25</code>).</li>
            <li>Use <code>pbinom()</code> to find the probability of guessing *5 or fewer* questions correctly.</li>
            <li>Use <code>pbinom()</code> with <code>lower.tail = FALSE</code> to find the probability of guessing *more than* 10 questions correctly.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualizations</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Plot 1: Binomial Distribution
binom_data <- data.frame(successes = 0:20, prob = dbinom(0:20, size = 20, prob = 0.5))
p1 <- ggplot(binom_data, aes(x = successes, y = prob)) +
  geom_bar(stat = "identity", fill = "#8b5cf6") +
  labs(title = "Binomial PMF (n=20, p=0.5)", x = "Number of Successes", y = "Probability") +
  theme_minimal()

# Plot 2: Poisson Distribution
poisson_data <- data.frame(events = 0:25, prob = dpois(0:25, lambda = 10))
p2 <- ggplot(poisson_data, aes(x = events, y = prob)) +
  geom_bar(stat = "identity", fill = "#ef4444") +
  labs(title = "Poisson PMF (λ=10)", x = "Number of Events", y = "Probability") +
  theme_minimal()

# Plot 3: Normal Distribution
p3 <- ggplot(data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dnorm, geom = "area", fill = "#3b82f6", alpha = 0.7) +
  labs(title = "Standard Normal PDF (μ=0, o=1)", x = "Value", y = "Density") +
  theme_minimal()

# Plot 4: Exponential Distribution
p4 <- ggplot(data.frame(x = c(0, 5)), aes(x)) +
  stat_function(fun = dexp, args = list(rate = 1), geom = "area", fill = "#f59e0b", alpha = 0.7) +
  labs(title = "Exponential PDF (λ=1)", x = "Time", y = "Density") +
  theme_minimal()

# To view them interactively:
# ggplotly(p1)
# ggplotly(p2)
# ggplotly(p3)
# ggplotly(p4)
</code></pre>
    </div>
</div>
`,
          p1s5ss1t4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A political pollster has surveyed 1,000 people to estimate a candidate's approval rating. The sample shows 52% approval. How confident can they be in this single number? Is the true approval rating in the entire country likely to be exactly 52%? Almost certainly not. They need a way to quantify the uncertainty of their estimate and provide a plausible range for the true population value.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss1t4-concepts">Sampling Concepts</button>
        <button class="tab-button" data-tab="p1s5ss1t4-impl">Sampling in R</button>
        <button class="tab-button" data-tab="p1s5ss1t4-bootstrap">Resampling: The Bootstrap</button>
        <button class="tab-button" data-tab="p1s5ss1t4-usecase">Use Case Example</button>
    </div>

    <div id="p1s5ss1t4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Population vs. Sample</h4>
        <ul class="prose-list">
            <li><strong>Population:</strong> The entire group you want to study (e.g., all 150 million voters in a country).</li>
            <li><strong>Sample:</strong> The specific, smaller group you collect data from (e.g., the 1,000 voters you actually survey).</li>
        </ul>
        <p><strong>Analogy:</strong> You're cooking a large pot of soup (the population). To check the seasoning, you take a single spoonful (the sample). If your sample is well-mixed (random), you can make a reliable inference about the entire pot.</p>
        <p>The goal of statistical inference is to use information from the sample to draw conclusions about the population. The key is that the sample must be random to be representative.</p>
    </div>

    <div id="p1s5ss1t4-impl" class="tab-pane">
        <h4 class="subsection-title">Taking a Random Sample</h4>
        <p>The <code>dplyr</code> package makes it easy to take a simple random sample from a data frame using <code>slice_sample()</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">library(dplyr)
# For reproducibility, always set a seed!
set.seed(42)

# The iris dataset has 150 rows (our 'population')
# Take a random sample of 30 rows without replacement
my_sample <- iris %>% slice_sample(n = 30)

# We can also sample with replacement
# This means the same row could be picked more than once
my_resample <- iris %>% slice_sample(n = 30, replace = TRUE)
</code></pre>
        </div>
    </div>
    
    <div id="p1s5ss1t4-bootstrap" class="tab-pane">
        <h4 class="subsection-title">Quantifying Uncertainty by Resampling</h4>
        <p>The <strong>Bootstrap</strong> is a powerful resampling method that lets you estimate the uncertainty of a statistic (like a mean, median, or correlation) by repeatedly sampling *from your own sample data*.</p>
        <p><strong>The Process:</strong></p>
        <ol class="prose-list">
            <li>Start with your original sample (e.g., the 1,000 voter surveys).</li>
            <li>Create a new "bootstrap sample" by drawing 1,000 surveys from the original set, *with replacement*. Some original surveys will be picked multiple times, some not at all.</li>
            <li>Calculate your statistic (e.g., the approval rating) on this new bootstrap sample and save it.</li>
            <li>Repeat steps 2 and 3 thousands of times.</li>
            <li>The distribution of your thousands of saved statistics gives you an estimate of the sampling distribution. You can then calculate a confidence interval from this distribution.</li>
        </ol>
    </div>
    
    <div id="p1s5ss1t4-usecase" class="tab-pane">
        <h4 class="subsection-title">Solving the Pollster's Problem</h4>
        <p>Let's simulate the pollster's data and use a bootstrap loop to calculate a 95% confidence interval for the approval rating.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># For reproducibility
set.seed(123)

# 1. Our original sample: 1000 voters, 520 approve (52%)
# We represent this as a vector of 1s (approve) and 0s (disapprove)
original_sample <- c(rep(1, 520), rep(0, 480))

# 2. Set up the bootstrap
n_resamples <- 5000
bootstrap_means <- numeric(n_resamples) # An empty vector to store results

# 3. The bootstrap loop
for (i in 1:n_resamples) {
  # Create a bootstrap sample by sampling with replacement
  resample <- sample(original_sample, size = length(original_sample), replace = TRUE)
  
  # Calculate the mean (approval rate) and store it
  bootstrap_means[i] <- mean(resample)
}

# 4. Calculate the 95% confidence interval
# This is the 2.5th and 97.5th percentiles of our bootstrap results
confidence_interval <- quantile(bootstrap_means, probs = c(0.025, 0.975))

print(confidence_interval)
# Result is approximately 49% to 55%
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation for the pollster:</strong> While the single best estimate is 52%, a plausible range for the true population approval rating is between 49% and 55%. The result is too close to call!</p>
        </div>
        <div class="plot-container">
            <div id="p1s5ss1t4-plot1" class="plotly-chart"></div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>infer</code> Package for Tidy Inference</h3>
    <div class="scenario-content">
        <p>Writing bootstrap loops is great for learning, but in practice, you should use a dedicated package. The <code>infer</code> package from the Tidyverse provides a clean, expressive grammar for performing statistical inference. It uses a pipe-based workflow with verbs like <code>specify()</code>, <code>hypothesize()</code>, <code>generate()</code>, and <code>calculate()</code> that makes bootstrapping and permutation tests much more straightforward.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>The median is often a difficult statistic to get a confidence interval for with traditional methods. The bootstrap makes it easy! Adapt the code from the Use Case example to find the 95% confidence interval for the <strong>median</strong> of the <code>Sepal.Length</code> column in the <code>iris</code> dataset.</p>
        <ul class="prose-list">
            <li>Your "original sample" will be the <code>iris$Sepal.Length</code> vector.</li>
            <li>In your bootstrap loop, instead of calculating the <code>mean()</code> of each resample, calculate the <code>median()</code>.</li>
            <li>Find the 95% confidence interval using the <code>quantile()</code> function on your results.</li>
        </ul>
    </div>
</div>
`,
          p1s5ss1t5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A marketing team runs an A/B test on a "Sign Up" button. The original button (A) had a 10% click-through rate. The new button (B) gets a 12% click-through rate in a sample of 1,000 users. Is this 2% lift a real improvement, or could it just be due to random luck in who saw which button? We need a way to measure the evidence against random chance.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss1t5-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s5ss1t5-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss1t5-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss1t5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Measuring Evidence Against Chance</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>p-value</strong>
                <p>The probability of observing a result as extreme as, or more extreme than, the one you got, *assuming the null hypothesis is true*. It's a measure of surprise.</p>
                <p><strong>Analogy:</strong> The p-value is the "Wow!" factor. A low p-value (e.g., 0.01) means your result is very surprising if there's truly no effect, which makes you think there *is* an effect.</p>
            </div>
            <div class="decision-branch">
                <strong>Significance Level ($\alpha$)</strong>
                <p>A pre-determined threshold for surprise. The most common level is $\alpha = 0.05$. If your p-value is less than $\alpha$, you declare the result "statistically significant."</p>
                <p>This is the "bar for being impressed." You decide beforehand that you'll only say "Wow!" if the p-value is below this bar.</p>
            </div>
        </div>
        <div class="decision-branches" style="align-items: flex-start; margin-top: 1rem;">
             <div class="decision-branch">
                <strong>Confidence Interval</strong>
                <p>A range of plausible values for the true population parameter, based on your sample data. A 95% confidence interval means that if you repeated the experiment many times, 95% of the intervals you calculate would contain the true value.</p>
                <p><strong>Analogy:</strong> It's like a fishing net. You've caught a sample (one fish), and you use it to cast a net where you're reasonably sure the *real* population fish is swimming.</p>
            </div>
            <div class="decision-branch">
                <strong>Effect Size & Power</strong>
                <p><strong>Effect Size</strong> measures the *magnitude* of the finding (e.g., the 2% lift). <strong>Statistical Power</strong> is the probability of detecting an effect if there truly is one. A small p-value doesn't mean the effect is large or important, just that it's unlikely to be zero.</p>
            </div>
        </div>
    </div>
    
    <div id="p1s5ss1t5-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the p-value</h4>
        <p>This plot shows the distribution of results we'd expect if there were no difference between the buttons (the null hypothesis). Our observed result (the test statistic) falls in the tail. The p-value is the shaded area under the curve that is even more extreme than our result. If this area is smaller than our significance level ($\alpha$), we reject the idea that there's no difference.</p>
        <div class="plot-container">
            <div id="p1s5ss1t5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The large curve represents all the outcomes possible due to random chance if the new button has no real effect. Our observed result is far to the right. The tiny red shaded area is the p-value. Because this area is so small (less than the $\alpha$ of 0.05), we conclude our result is not just random chance.</p>
        </div>
    </div>
    
    <div id="p1s5ss1t5-usecase" class="tab-pane">
        <h4 class="subsection-title">Solving the A/B Test Problem</h4>
        <p>We can use a simple proportion test in R to check if the 12% click rate is significantly different from the original 10%.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># In the sample of 1000 users for button B, 12% clicked.
# This means 120 successes (clicks) out of 1000 trials.
# The null hypothesis is that the true rate is 10% (p = 0.10).

prop.test(x = 120, n = 1000, p = 0.10, alternative = "greater")
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation of the R Output:</strong></p>
            <ul class="prose-list">
                <li>The test will produce a <strong>p-value of approximately 0.016</strong>.</li>
                <li>Since 0.016 is less than our chosen significance level of $\alpha = 0.05$, we <strong>reject the null hypothesis</strong>.</li>
                <li>The test will also provide a <strong>95% confidence interval</strong>. It will show a lower bound that is above 10%, giving us confidence the true click rate is indeed higher.</li>
                <li><strong>Conclusion for the manager:</strong> The evidence suggests the 2% lift is a real improvement and not just random chance. It is statistically significant, so we should launch the new button.</li>
            </ul>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: P-Hacking is Bad Science</h3>
    <div class="scenario-content">
        <p>Never run multiple different tests and only report the one that gives a significant p-value. This is called "p-hacking" and is a form of scientific dishonesty. The significance level $\alpha$ should be chosen *before* you run your analysis, and your testing plan should be clear from the start.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>A student takes a 100-question true/false test and gets 60 questions right. If they were just guessing, you'd expect them to get 50 right. Is their score of 60 significantly better than random guessing?</p>
        <ul class="prose-list">
            <li>Use the <code>prop.test()</code> function in R.</li>
            <li>What is your number of successes (<code>x</code>)? What is the number of trials (<code>n</code>)?</li>
            <li>What is the null hypothesis probability (<code>p</code>)?</li>
            <li>Based on the p-value, can you conclude the student was doing better than guessing?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Create data for a standard normal curve
norm_data <- data.frame(x = seq(-4, 4, length.out = 200))
norm_data$y <- dnorm(norm_data$x)

# Define critical values and our observed statistic
alpha <- 0.05
critical_value <- qnorm(1 - alpha)
observed_statistic <- 2.1 # Our example test result
p_value_data <- subset(norm_data, x >= observed_statistic)

p <- ggplot(norm_data, aes(x, y)) +
  geom_line() +
  geom_area(data = p_value_data, aes(x, y), fill = "#ef4444", alpha = 0.7) +
  geom_vline(xintercept = critical_value, linetype = "dashed", color = "#f59e0b", size = 1) +
  annotate("text", x = critical_value + 0.1, y = 0.2, label = paste0("α = ", alpha), hjust = 0, color = "#f59e0b") +
  annotate("text", x = observed_statistic + 0.6, y = 0.05, label = paste0("p-value\n", round(pnorm(observed_statistic, lower.tail = FALSE), 3)), color = "white") +
  labs(title = "P-value vs. Significance Level (α)", x = "Test Statistic", y = "Density") +
  theme_minimal()

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss1t6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A pharmaceutical company develops a new drug to lower blood pressure. They conduct a clinical trial where one group gets the drug and another gets a placebo. After a month, the drug group shows a lower average blood pressure. How can the company formally prove to regulators that this difference is a real effect of the drug and not just random variation? This requires a structured hypothesis test.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss1t6-concepts">The Steps of a Test</button>
        <button class="tab-button" data-tab="p1s5ss1t6-errors">Type I & Type II Errors</button>
        <button class="tab-button" data-tab="p1s5ss1t6-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss1t6-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Courtroom Analogy</h4>
        <p>A hypothesis test is like a criminal trial. You have a default assumption, and you need strong evidence to argue against it.</p>
        <ol class="prose-list">
            <li><strong>State the Hypotheses:</strong>
                <ul>
                    <li><strong>Null Hypothesis ($H_0$):</strong> The default assumption of "no effect" or "no difference." In the trial, this is "The defendant is innocent." For our drug, it's "The drug has no effect on blood pressure."</li>
                    <li><strong>Alternative Hypothesis ($H_A$):</strong> The claim we want to prove. This is "The defendant is guilty" or "The drug *does* lower blood pressure."</li>
                </ul>
            </li>
            <li><strong>Choose a Significance Level ($\alpha$):</strong> This is the standard of evidence required to reject the null (e.g., $\alpha = 0.05$). This is like setting the standard of "beyond a reasonable doubt."</li>
            <li><strong>Calculate a Test Statistic:</strong> A number calculated from your sample data that measures how far your result is from what the null hypothesis predicted.</li>
            <li><strong>Calculate the p-value:</strong> The probability of seeing a test statistic that extreme, assuming $H_0$ is true.</li>
            <li><strong>Make a Conclusion:</strong> If p-value < $\alpha$, you have enough evidence to "reject the null hypothesis." You conclude the alternative is likely true. Otherwise, you "fail to reject the null hypothesis" (which is not the same as proving it's true!).</li>
        </ol>
    </div>

    <div id="p1s5ss1t6-errors" class="tab-pane">
        <h4 class="subsection-title">The Two Ways to Be Wrong</h4>
        <p>Because we're dealing with probabilities, our conclusion can sometimes be wrong.</p>
        <div class="plot-container">
            <div id="p1s5ss1t6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The blue curve shows the world where the drug has no effect ($H_0$ is true). The orange curve shows a world where the drug *does* work ($H_A$ is true). The dashed line is our decision point ($\alpha$).</p>
            <ul>
                <li><strong>Type I Error ($\alpha$, red area):</strong> We reject the null when it was actually true. (We conclude the drug works when it doesn't). This is like convicting an innocent person.</li>
                <li><strong>Type II Error ($\beta$, yellow area):</strong> We fail to reject the null when it was actually false. (We conclude the drug doesn't work when it actually does). This is like letting a guilty person go free.</li>
            </ul>
        </div>
    </div>
    
    <div id="p1s5ss1t6-usecase" class="tab-pane">
        <h4 class="subsection-title">Analyzing the Clinical Trial Data</h4>
        <p>Let's simulate some data for our trial and run a two-sample t-test to compare the means of the two groups.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># For reproducibility
set.seed(123)

# H0: The mean blood pressure of the drug group is equal to the placebo group.
# HA: The mean blood pressure of the drug group is LESS THAN the placebo group.

# Generate simulated data
placebo_group <- rnorm(n = 50, mean = 140, sd = 10)
drug_group <- rnorm(n = 50, mean = 132, sd = 10) # Drug has a real effect

# Perform a two-sample t-test
# We specify 'less' because our alternative hypothesis is that the drug group's mean is lower.
t.test(drug_group, placebo_group, alternative = "less")
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation of the R Output:</strong></p>
            <ul class="prose-list">
                <li>The test will produce a very small <strong>p-value</strong> (e.g., $p < 0.001$).</li>
                <li><strong>Conclusion:</strong> Because our p-value is much smaller than our significance level of $\alpha = 0.05$, we <strong>reject the null hypothesis</strong>.</li>
                <li>We have strong statistical evidence to conclude that the new drug is effective at lowering blood pressure compared to the placebo.</li>
            </ul>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: State Your Hypotheses First!</h3>
    <div class="scenario-content">
        <p>The most important step of any formal test is to clearly state your null ($H_0$) and alternative ($H_A$) hypotheses *before* you look at the data or run the test. This prevents you from forming your hypothesis based on the results you see, which can lead to biased conclusions.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>The built-in <code>sleep</code> dataset shows the effect of two different soporific drugs (group 1 and group 2) on 10 patients. We want to know if there is a significant difference in the extra hours of sleep between the two groups.</p>
        <ul class="prose-list">
            <li>First, create two vectors, one for each group: group1 <- sleep$extra[sleep$group == 1] and group2 <- sleep$extra[sleep$group == 2].</li>
            <li>What is your Null Hypothesis ($H_0$)? (Hint: The mean difference is zero).</li>
            <li>What is your Alternative Hypothesis ($H_A$)? (Hint: The mean difference is not zero).</li>
            <li>Run a two-sample t-test: <code>t.test(group1, group2)</code>.</li>
            <li>Based on the p-value, do you reject or fail to reject the null hypothesis? Is there a significant difference between the two drugs?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Data for two overlapping normal curves
h0_data <- data.frame(x = seq(-4, 8, length.out = 200))
h0_data$y <- dnorm(h0_data$x, mean = 0, sd = 1.5)
h0_data$dist <- "H0 is True (No Effect)"

ha_data <- data.frame(x = seq(-4, 8, length.out = 200))
ha_data$y <- dnorm(ha_data$x, mean = 3, sd = 1.5)
ha_data$dist <- "HA is True (Real Effect)"

combined_data <- rbind(h0_data, ha_data)

# Decision boundary
decision_boundary <- qnorm(0.95, mean = 0, sd = 1.5)

# Shaded areas for errors
type1_error_data <- subset(h0_data, x >= decision_boundary)
type2_error_data <- subset(ha_data, x < decision_boundary)

p <- ggplot(combined_data, aes(x, y, color = dist)) +
  geom_line(size = 1) +
  geom_vline(xintercept = decision_boundary, linetype = "dashed", size = 1) +
  geom_area(data = type1_error_data, aes(x, y), fill = "#ef4444", alpha = 0.5, color = NA) +
  geom_area(data = type2_error_data, aes(x, y), fill = "#f59e0b", alpha = 0.5, color = NA) +
  annotate("text", x = decision_boundary + 0.2, y = 0.15, label = "Type I Error (α)", color = "#ef4444", hjust = 0) +
  annotate("text", x = decision_boundary - 0.2, y = 0.05, label = "Type II Error (β)", color = "#c2410c", hjust = 1) +
  labs(title = "Type I and Type II Errors in Hypothesis Testing", x = "Observed Result", y = "Probability Density", color = "True State of World") +
  scale_color_manual(values = c("H0 is True (No Effect)" = "#3b82f6", "HA is True (Real Effect)" = "#eab308")) +
  theme_minimal() +
  theme(legend.position = "bottom")

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss1t7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An ice cream shop owner tracks two variables every day: the high temperature and the total sales. They have a hunch that hotter days lead to more sales, but they want to go beyond a hunch. They need to formally measure the direction and strength of the relationship between temperature and sales.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss1t7-concepts">Covariance vs. Correlation</button>
        <button class="tab-button" data-tab="p1s5ss1t7-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss1t7-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss1t7-concepts" class="tab-pane active">
        <h4 class="subsection-title">Quantifying a Relationship</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Covariance</strong>
                <p>Measures the joint variability of two variables. A positive covariance means they tend to move in the same direction, while a negative one means they move in opposite directions.</p>
                <p><strong>Problem:</strong> The value of covariance is not standardized. A covariance of 100 might be strong for one dataset but weak for another, making it hard to interpret.</p>
                 <div class="math-foundation">$$ Cov(X, Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1} $$</div>
            </div>
            <div class="decision-branch">
                <strong>Correlation</strong>
                <p>The solution to covariance's problem. Correlation is a standardized measure that is always between <strong>-1 and +1</strong>, making it easy to interpret.</p>
                <ul class="prose-list">
                    <li><strong>+1:</strong> Perfect positive linear relationship.</li>
                    <li><strong>0:</strong> No linear relationship.</li>
                    <li><strong>-1:</strong> Perfect negative linear relationship.</li>
                </ul>
                <div class="math-foundation">$$ r = \\frac{Cov(X, Y)}{s_x s_y} $$</div>
            </div>
        </div>
    </div>
    
    <div id="p1s5ss1t7-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Relationship with a Scatter Plot</h4>
        <p>The best way to visualize the relationship between two continuous variables is a scatter plot. It allows you to see the direction, strength, and form (e.g., linear or curved) of the relationship at a glance.</p>
        <div class="plot-container">
            <div id="p1s5ss1t7-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> Each dot represents a day. The x-axis is the temperature, and the y-axis is the sales. You can clearly see an upward trend from left to right, indicating a <strong>positive relationship</strong>. The points are tightly clustered around the blue regression line, suggesting the relationship is <strong>strong</strong>. The pattern appears to be roughly linear.</p>
        </div>
    </div>
    
    <div id="p1s5ss1t7-usecase" class="tab-pane">
        <h4 class="subsection-title">Solving the Ice Cream Shop Problem</h4>
        <p>We can use the <code>cor()</code> function in R to calculate the Pearson correlation coefficient, which measures the strength of the *linear* relationship.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># For reproducibility
set.seed(42)

# Generate sample data that has a strong positive relationship
temperature <- seq(20, 35, length.out = 30) # Temp in Celsius
# Sales = base sales + temp effect + random noise
sales <- 50 + (temperature * 15) + rnorm(30, mean = 0, sd = 40)

# Calculate the Pearson correlation coefficient
correlation_coefficient <- cor(temperature, sales)

print(paste("The correlation is:", round(correlation_coefficient, 2)))
# Result: "The correlation is: 0.96"
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation for the owner:</strong> A correlation coefficient of **0.96** is extremely close to +1. This confirms their hunch quantitatively: there is a very strong, positive, linear relationship between daily temperature and ice cream sales. This information could be used to inform staffing and inventory decisions.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Correlation Does Not Imply Causation!</h3>
    <div class="scenario-content">
        <p>This is the most important mantra in statistics. Just because two variables are highly correlated does not mean that one *causes* the other. For example, ice cream sales and drownings are also positively correlated. This is not because ice cream causes drowning, but because a third, "lurking" variable—hot weather—causes an increase in both. Always be cautious when interpreting correlation.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the built-in <code>mtcars</code> dataset, explore the relationship between a car's weight and its fuel efficiency.</p>
        <ul class="prose-list">
            <li>Create a scatter plot with weight (<code>wt</code>) on the x-axis and miles per gallon (<code>mpg</code>) on the y-axis.</li>
            <li>Based on the plot, what is the direction of the relationship (positive or negative)? Does it look strong or weak?</li>
            <li>Use the <code>cor()</code> function to calculate the correlation coefficient between <code>mtcars$wt</code> and <code>mtcars$mpg</code>.</li>
            <li>Write a sentence interpreting your findings.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Use the same data from the R example
set.seed(42)
temp_sales_data <- data.frame(
  temperature = seq(20, 35, length.out = 30),
  sales = 50 + (seq(20, 35, length.out = 30) * 15) + rnorm(30, mean = 0, sd = 40)
)

p <- ggplot(temp_sales_data, aes(x = temperature, y = sales)) +
  geom_point(size = 3, color = "#ef4444", alpha = 0.8) +
  # Add a linear regression line without the confidence interval ribbon
  geom_smooth(method = "lm", se = FALSE, color = "#3b82f6", size = 1.2) +
  labs(
    title = "Ice Cream Sales vs. Temperature",
    x = "Temperature (°C)",
    y = "Daily Sales ($)"
  ) +
  theme_minimal()

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss1t8: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is given a complete dataset of last quarter's sales. The CEO asks two very different questions:
        <br>1. "Give me a summary of our top-selling products and regions from *last quarter*."
        <br>2. "Based on last quarter's data, can we predict our sales for *next quarter*?"
        <br>Answering the first question requires describing the data you have. Answering the second requires making a leap from the data you have to an unknown future. These two tasks represent the two fundamental paradigms of statistics.
        </p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss1t8-concepts">Two Goals of Analysis</button>
        <button class="tab-button" data-tab="p1s5ss1t8-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss1t8-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss1t8-concepts" class="tab-pane active">
        <h4 class="subsection-title">Describing vs. Predicting</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Descriptive Statistics</strong>
                <p>The practice of summarizing and organizing the main features of a dataset that you have. It focuses on the "what."</p>
                <p><strong>Analogy:</strong> Descriptive statistics is like taking a <strong>photograph</strong> of a crowd. It tells you exactly what was there at that moment: the number of people, the average height, the distribution of hair colors, etc. It doesn't say anything about a different crowd or what this crowd will do next.</p>
                <p><strong>Methods include:</strong> Mean, median, standard deviation, frequency tables, and visualizations like bar charts and histograms.</p>
            </div>
            <div class="decision-branch">
                <strong>Inferential Statistics</strong>
                <p>The practice of using data from a sample to make generalizations or predictions about a larger population or a future outcome. It focuses on the "what if" or "what's next."</p>
                <p><strong>Analogy:</strong> Inferential statistics is like being a <strong>detective</strong> who uses clues from the photograph (the sample) to make a conclusion about the entire population or to predict the crowd's future behavior. It involves a calculated leap from the known to the unknown.</p>
                <p><strong>Methods include:</strong> Hypothesis testing, confidence intervals, regression, and all forms of predictive modeling.</p>
            </div>
        </div>
    </div>
    
    <div id="p1s5ss1t8-viz" class="tab-pane">
        <h4 class="subsection-title">A Visual Representation</h4>
        <p>This diagram shows the core difference. The descriptive plot (left) summarizes past data precisely. The inferential plot (right) uses past data to build a model (the blue line) that makes a prediction about an unknown future value (the red dot).</p>
        <div class="plot-container">
            <div id="p1s5ss1t8-plot1" class="plotly-chart"></div>
        </div>
         <div class="plot-description">
            <p><strong>How to observe this diagram:</strong> The bar chart on the left is purely **descriptive**; it simply shows the measured sales for past months. The scatter plot on the right is **inferential**; it uses the pattern in the past data to create a trend line that extends into the future to make a prediction, quantifying the uncertainty with the shaded confidence interval.</p>
        </div>
    </div>
    
    <div id="p1s5ss1t8-usecase" class="tab-pane">
        <h4 class="subsection-title">Answering the CEO's Questions</h4>
        <p>Let's use R to address both of the CEO's requests with a sample dataset.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(tibble)

# Sample sales data for last quarter
sales_data <- tibble(
  month = c("Jan", "Jan", "Feb", "Feb", "Mar", "Mar"),
  product = c("A", "B", "A", "B", "A", "B"),
  sales = c(100, 150, 110, 140, 125, 160)
)

# Question 1: DESCRIPTIVE - Summarize last quarter
sales_data %>%
  group_by(product) %>%
  summarise(total_sales = sum(sales)) %>%
  arrange(desc(total_sales))

# Question 2: INFERENTIAL - Predict sales for Product A in April
# We need a numeric time variable to build a model
product_a_data <- sales_data %>%
  filter(product == "A") %>%
  mutate(time_step = c(1, 2, 3)) # Jan=1, Feb=2, Mar=3

# Build a simple linear model (inference)
model <- lm(sales ~ time_step, data = product_a_data)

# "Predict" the sales for April (time_step = 4)
predict(model, newdata = data.frame(time_step = 4))
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: EDA is Primarily Descriptive</h3>
    <div class="scenario-content">
        <p>Exploratory Data Analysis (EDA) is the phase where you immerse yourself in the data. The vast majority of EDA—creating summaries, histograms, bar charts, and scatter plots—falls under the umbrella of **descriptive statistics**. You are learning the story of the data you *have*. This descriptive phase is a crucial prerequisite before you can move on to the more complex task of building inferential models.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>iris</code> dataset, perform one descriptive and one inferential task.</p>
        <ul class="prose-list">
            <li><strong>Descriptive Task:</strong> Calculate the average <code>Petal.Length</code> for each of the three <code>Species</code>. Which species has the longest petals on average? (Hint: use <code>group_by()</code> and <code>summarise()</code>).</li>
            <li><strong>Inferential Task:</strong> Can <code>Petal.Length</code> be used to predict <code>Petal.Width</code>? Build a simple linear model using <code>lm(Petal.Width ~ Petal.Length, data = iris)</code>. Look at the coefficients of the model. Does it seem like there's a predictive relationship?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)
library(patchwork) # For combining plots

# Data for Descriptive Plot
desc_data <- data.frame(
  Month = c("Jan", "Feb", "Mar"),
  Sales = c(250, 250, 285)
)
p_desc <- ggplot(desc_data, aes(x = Month, y = Sales)) +
  geom_bar(stat = "identity", fill = "#10b981") +
  labs(title = "Descriptive: Past Sales", y = "Total Sales") +
  theme_minimal()

# Data for Inferential Plot
inf_data <- data.frame(
  Month = 1:3,
  Sales = c(250, 250, 285)
)
p_inf <- ggplot(inf_data, aes(x = Month, y = Sales)) +
  geom_point(size = 3, color = "#10b981") +
  # Extend the regression line into the future
  geom_smooth(method = "lm", se = TRUE, fullrange = TRUE, color="#3b82f6") +
  scale_x_continuous(limits = c(0.5, 4.5), breaks = 1:4, labels = c("Jan", "Feb", "Mar", "Apr (Pred)")) +
  labs(title = "Inferential: Predicting Future Sales", y = "Total Sales") +
  theme_minimal()

# This would combine them in R, but we render them separately in JS
# p_desc + p_inf
</code></pre>
    </div>
</div>
`,
          p1s5ss2t1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist at a streaming service represents each user as a collection of features: age, hours watched per week, and number of sci-fi movies rated. They need to represent this data mathematically to answer questions like: "Which two users are most similar in their viewing habits?" This requires representing users as vectors and using vector operations to measure similarity.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss2t1-concepts">Core Objects</button>
        <button class="tab-button" data-tab="p1s5ss2t1-ops">Core Operations</button>
        <button class="tab-button" data-tab="p1s5ss2t1-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss2t1-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss2t1-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Building Blocks of Linear Algebra</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Vectors</strong>
                <p>In data science, a vector is an ordered list of numbers representing the features of a single data point. Geometrically, it can be thought of as an arrow from the origin to a point in space, possessing both magnitude (length) and direction.</p>
                <p><strong>Analogy:</strong> A vector is a "data DNA" for one observation. The vector <code>[age, income, credit_score]</code> defines a customer in a 3-dimensional feature space.</p>
            </div>
            <div class="decision-branch">
                <strong>Matrices</strong>
                <p>A matrix is a rectangular grid of numbers, arranged in rows and columns. In data science, a matrix typically represents an entire dataset, where each row is an observation (a vector) and each column is a feature.</p>
                <p><strong>Analogy:</strong> A matrix is simply a spreadsheet or a data frame. It's a collection of observation vectors stacked on top of each other.</p>
            </div>
        </div>
    </div>

    <div id="p1s5ss2t1-ops" class="tab-pane">
        <h4 class="subsection-title">Manipulating Objects in Space</h4>
        <ul class="prose-list">
            <li><strong>Scalar Multiplication:</strong> Multiplying a vector by a single number (a scalar) scales its magnitude but doesn't change its direction.</li>
            <li><strong>Vector Addition:</strong> Adding two vectors is done element-wise, resulting in a new vector. Geometrically, it's like placing the tail of the second vector at the head of the first.</li>
            <li><strong>Dot Product:</strong> The sum of the products of corresponding elements of two vectors. It's a measure of how much two vectors point in the same direction. A dot product of 0 means they are orthogonal (perpendicular). $$\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i b_i$$</li>
            <li><strong>Matrix Multiplication (<code>%*%</code> in R):</strong> A complex operation central to transforming data. If you multiply a matrix of data by a transformation matrix, it can rotate, scale, or shear all the data points at once.</li>
        </ul>
    </div>

    <div id="p1s5ss2t1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Vectors in Feature Space</h4>
        <p>We can plot vectors as arrows from the origin. This helps us intuitively understand their direction and magnitude. Two vectors pointing in a similar direction are more similar to each other.</p>
        <div class="plot-container">
            <div id="p1s5ss2t1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This plot shows two users, Alice and Bob, in a 2D feature space. Alice's vector shows she watches many hours but few sci-fi films. Bob's vector is the opposite. The angle between their vectors is large, indicating their tastes are very different. A third user, Carol, has a vector pointing in a very similar direction to Alice's, suggesting they have similar viewing habits.</p>
        </div>
    </div>

    <div id="p1s5ss2t1-usecase" class="tab-pane">
        <h4 class="subsection-title">Finding Similar Users</h4>
        <p>Let's use R to represent our users and calculate their dot product. A higher dot product (for vectors of similar length) means they are more aligned.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># User vectors: [hours_watched, sci_fi_rated]
alice <- c(15, 2)
bob <- c(3, 12)
carol <- c(16, 3)

# The dot product is the sum of the element-wise products.
# R's matrix multiplication operator %*% on vectors performs the dot product.
dot_alice_bob <- alice %*% bob
dot_alice_carol <- alice %*% carol

print(paste("Alice-Bob Dot Product:", dot_alice_bob))   # Result: 69
print(paste("Alice-Carol Dot Product:", dot_alice_carol)) # Result: 246
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The dot product between Alice and Carol is much higher than between Alice and Bob. This mathematically confirms what we saw in the plot: Alice's and Carol's viewing habits are far more similar.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Cosine Similarity</h3>
    <div class="scenario-content">
        <p>The dot product is affected by the vector's magnitude (e.g., a power-user who watches a lot of everything will have high dot products with everyone). A better measure of pure similarity in *direction* is <strong>Cosine Similarity</strong>, which is just the dot product normalized by the magnitudes of the vectors. The result is always between -1 and 1, just like correlation. It's calculated as $$\\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\|\\vec{b}\\|}$$</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create two vectors in R representing two different products based on the features [price, rating]: <code>product_x <- c(100, 4.5)</code> and <code>product_y <- c(110, 4.7)</code>.</li>
            <li>Calculate the dot product between them using the <code>%*%</code> operator.</li>
            <li>Now, create a 2x2 matrix called <code>sales_data</code> where the first row is <code>product_x</code> and the second is <code>product_y</code>. (Hint: <code>matrix(c(100, 4.5, 110, 4.7), nrow=2, byrow=TRUE)</code>).</li>
            <li>Imagine a transformation matrix that applies a 10% discount: <code>discount_matrix <- matrix(c(0.9, 0, 0, 1), nrow=2)</code>. Multiply <code>sales_data %*% discount_matrix</code> to see the prices after the discount.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# User vectors
users <- data.frame(
  name = c("Alice", "Bob", "Carol"),
  hours_watched = c(15, 3, 16),
  sci_fi_rated = c(2, 12, 3),
  color = c("#ef4444", "#3b82f6", "#eab308")
)

p <- ggplot(users, aes(x = hours_watched, y = sci_fi_rated, color = name)) +
  geom_segment(aes(xend = 0, yend = 0), arrow = arrow(length = unit(0.3, "cm")), size = 1.2) +
  geom_text(aes(label = name), nudge_x = 1, nudge_y = 1, size = 4) +
  scale_x_continuous(limits = c(0, 20)) +
  scale_y_continuous(limits = c(0, 15)) +
  labs(
    title = "User Profiles as Vectors in Feature Space",
    x = "Hours Watched per Week",
    y = "Sci-Fi Movies Rated"
  ) +
  scale_color_manual(values = c("Alice" = "#ef4444", "Bob" = "#3b82f6", "Carol" = "#eab308")) +
  theme_minimal() +
  theme(legend.position = "none")

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss2t2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A recommendation engine like Netflix starts with a massive, sparse matrix of users and their movie ratings. To find latent features (like hidden genres) and predict how a user might rate a movie they haven't seen, working with this raw matrix is inefficient. The engine must first decompose it into simpler, more meaningful parts that represent user tastes and movie characteristics.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss2t2-concepts">The Idea of Decomposition</button>
        <button class="tab-button" data-tab="p1s5ss2t2-svd">Singular Value Decomposition (SVD)</button>
        <button class="tab-button" data-tab="p1s5ss2t2-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss2t2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Breaking Down Complexity</h4>
        <p><strong>Matrix Decomposition</strong> (or factorization) is the process of breaking a matrix into a product of simpler, constituent matrices. By understanding these constituent parts, we can often reveal hidden properties of the original data.</p>
        <p><strong>Analogy:</strong> It's like taking a complex food dish (the original matrix) and breaking it down into its core nutritional components (protein, carbs, fats). This decomposition tells you more about the dish's fundamental nature than the final product alone.</p>
        <p>While there are many types (LU, QR, Cholesky), the most important for data science is <strong>Singular Value Decomposition (SVD)</strong>.</p>
    </div>

    <div id="p1s5ss2t2-svd" class="tab-pane">
        <h4 class="subsection-title">The Workhorse of Data Science</h4>
        <p>SVD decomposes any matrix A into three other matrices:</p>
        <div class="math-foundation">$$ A = U \\Sigma V^T $$</div>
        <div class="plot-description">
            <ul>
                <li><strong>$U$:</strong> The "user-to-concept" matrix. Its columns are orthogonal and represent latent features of the rows (e.g., how much each user likes "action" vs. "comedy").</li>
                <li><strong>$\\Sigma$ (Sigma):</strong> A diagonal matrix of "singular values." These values represent the *strength* or importance of each latent concept. They are always positive and ordered from largest to smallest.</li>
                <li><strong>$V^T$ (V-transpose):</strong> The "movie-to-concept" matrix. Its columns are also orthogonal and represent how much each column of the original matrix relates to the latent concepts (e.g., how much each movie *is* an "action" vs. "comedy" movie).</li>
            </ul>
        </div>
        <div class="plot-container">
            <div id="p1s5ss2t2-plot1" class="plotly-chart"></div>
        </div>
    </div>
    
    <div id="p1s5ss2t2-usecase" class="tab-pane">
        <h4 class="subsection-title">Finding Latent Features in Ratings</h4>
        <p>Let's use R's <code>svd()</code> function on a tiny user-movie rating matrix.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># Matrix: Rows are users, columns are movies
# (SciFi: Alien, Star Wars; Action: Die Hard, John Wick)
ratings_matrix <- matrix(
  c(5, 5, 1, 2,   # User 1 loves SciFi
    1, 2, 5, 4,   # User 2 loves Action
    4, 5, 2, 1),  # User 3 loves SciFi, dislikes Action
  nrow = 3, byrow = TRUE
)

# Perform SVD
svd_result <- svd(ratings_matrix)

# The components:
svd_result$u  # User-to-concept matrix
svd_result$d  # Singular values (strength of concepts)
svd_result$v  # Movie-to-concept matrix
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong></p>
            <ul>
                <li>The first singular value in <code>svd_result$d</code> will be much larger than the others, indicating there's one dominant "concept."</li>
                <li>Looking at the first column of <code>svd_result$v</code>, you'll see two positive values and two negative values, successfully separating the SciFi from the Action movies. This is the "SciFi vs. Action" latent feature.</li>
                <li>Looking at the first column of <code>svd_result$u</code>, you'll see which users align with which side of that concept.</li>
            </ul>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: SVD for Dimensionality Reduction</h3>
    <div class="scenario-content">
        <p>The real power of SVD is that you can reconstruct an *approximation* of the original matrix using only the top few singular values and their corresponding vectors from U and V. This is the core idea behind many dimensionality reduction and data compression techniques, including Principal Component Analysis (PCA), which is a special application of SVD on a covariance matrix.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's see how well we can approximate our ratings matrix.</p>
        <ul class="prose-list">
            <li>Take the <code>svd_result</code> from the example above.</li>
            <li>Reconstruct the matrix using all concepts: <code>reconstructed_all <- svd_result$u %*% diag(svd_result$d) %*% t(svd_result$v)</code>. Check if it's very close to the <code>ratings_matrix</code>.</li>
            <li>Now, reconstruct an approximation using only the FIRST, most important concept. (Hint: use <code>svd_result$u[,1]</code>, <code>svd_result$d[1]</code>, and <code>svd_result$v[,1]</code>). The formula will be: <code>svd_result$u[,1] * svd_result$d[1] * t(svd_result$v[,1])</code>.</li>
            <li>Compare this new approximated matrix to the original. You'll see it has captured the main essence of the user preferences!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# This plot is conceptual, so we create it with annotations and shapes
p <- ggplot() +
  # Matrix A
  annotate("rect", xmin = 0, xmax = 2, ymin = 3, ymax = 5, fill = "#3b82f6", alpha = 0.7) +
  annotate("text", x = 1, y = 4, label = "A", size = 15, color = "white") +
  annotate("text", x = 1, y = 2.7, label = "m x n", size = 5) +
  
  # Equals sign
  annotate("text", x = 2.5, y = 4, label = "=", size = 15) +
  
  # Matrix U
  annotate("rect", xmin = 3, xmax = 4.5, ymin = 3, ymax = 5, fill = "#10b981", alpha = 0.7) +
  annotate("text", x = 3.75, y = 4, label = "U", size = 15, color = "white") +
  annotate("text", x = 3.75, y = 2.7, label = "m x k", size = 5) +

  # Sigma
  annotate("rect", xmin = 5, xmax = 6, ymin = 3.5, ymax = 4.5, fill = "#f59e0b", alpha = 0.7) +
  annotate("text", x = 5.5, y = 4, label = "Σ", size = 15, color = "white") +
  annotate("text", x = 5.5, y = 3.2, label = "k x k", size = 5) +

  # V-Transpose
  annotate("rect", xmin = 6.5, xmax = 8.5, ymin = 3.75, ymax = 4.75, fill = "#ef4444", alpha = 0.7) +
  annotate("text", x = 7.5, y = 4.25, label = "Vᵀ", size = 15, color = "white") +
  annotate("text", x = 7.5, y = 3.5, label = "k x n", size = 5) +
  
  coord_equal() +
  theme_void()

# Plotly conversion might not be perfect for a conceptual plot, but we'll wrap it
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss2t3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A biologist has a dataset with many highly correlated measurements for a set of birds (e.g., wingspan, beak length, leg length, tail length). They suspect these variables are all just different reflections of one underlying, unmeasured concept: "overall body size." To find the mathematical axis that best represents "body size," they need to find the principal eigenvector of their data's covariance matrix.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss2t3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s5ss2t3-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss2t3-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss2t3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Finding the Axes of a Transformation</h4>
        <p>Given a square matrix (which can represent a linear transformation), its <strong>eigenvectors</strong> are special vectors that do not change their direction when the transformation is applied to them. They only get stretched or shrunk.</p>
        <p>The corresponding <strong>eigenvalue</strong> is the scalar factor by which the eigenvector is stretched or shrunk.</p>
        <p><strong>Analogy:</strong> Imagine spinning a globe (the transformation). The <strong>eigenvector</strong> is the Earth's axis of rotation. A point on the axis will only move up or down along that line; its fundamental direction from the center never changes. The <strong>eigenvalue</strong> would be 1, as the distance from the center doesn't change.</p>
        <div class="math-foundation">
            <p>For a matrix $A$, a vector $\\vec{v}$ is an eigenvector if it satisfies:</p>
            $$ A\\vec{v} = \\lambda\\vec{v} $$
            <p>where $\\lambda$ (lambda) is the scalar eigenvalue.</p>
        </div>
    </div>
    
    <div id="p1s5ss2t3-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Principal Components</h4>
        <p>For a data covariance matrix, the eigenvectors are called the <strong>principal components</strong>. The eigenvector with the largest eigenvalue (the first principal component) always points in the direction of maximum variance in the data.</p>
        <div class="plot-container">
            <div id="p1s5ss2t3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The black dots show a cloud of correlated data points. The <strong>first eigenvector (red)</strong> points along the direction where the data is most spread out. It has the largest eigenvalue. The <strong>second eigenvector (blue)</strong> is perpendicular (orthogonal) to the first and points along the direction of the next highest variance. These vectors form the principal axes of the data.</p>
        </div>
    </div>
    
    <div id="p1s5ss2t3-usecase" class="tab-pane">
        <h4 class="subsection-title">Finding the "Body Size" Component</h4>
        <p>Let's use R's <code>eigen()</code> function to find the eigenvalues and eigenvectors of a covariance matrix.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># For reproducibility
set.seed(123)

# Create some correlated data for wingspan and beak length
wingspan <- rnorm(100, mean = 50, sd = 5)
beak_length <- wingspan * 0.3 + rnorm(100, mean = 0, sd = 2)
bird_data <- data.frame(wingspan, beak_length)

# 1. Calculate the covariance matrix of the data
cov_matrix <- cov(bird_data)
print(cov_matrix)

# 2. Perform eigen-analysis on the covariance matrix
eigen_result <- eigen(cov_matrix)

# The components:
print(eigen_result$values)   # The eigenvalues
print(eigen_result$vectors)  # The eigenvectors (as columns)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong></p>
            <ul>
                <li>You will see that the first eigenvalue in <code>eigen_result$values</code> is much larger than the second. This means the first principal component captures most of the total variance.</li>
                <li>The first column of <code>eigen_result$vectors</code> is the first eigenvector. Its values will both be positive, indicating that it represents a direction where both wingspan and beak length increase together. This is our "overall body size" component.</li>
            </ul>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Eigen-analysis is the Heart of PCA</h3>
    <div class="scenario-content">
        <p>Principal Component Analysis (PCA) is one of the most fundamental dimensionality reduction techniques. The process of PCA is nothing more than performing an eigen-analysis on the covariance (or correlation) matrix of your data. The eigenvectors are the principal components, and the eigenvalues tell you how much variance each component explains.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the built-in <code>iris</code> dataset, let's find the principal components for two of the measurements.</p>
        <ul class="prose-list">
            <li>Select just the <code>Sepal.Length</code> and <code>Sepal.Width</code> columns from the <code>iris</code> dataset.</li>
            <li>Calculate the covariance matrix for these two columns using <code>cov()</code>.</li>
            <li>Run the <code>eigen()</code> function on this 2x2 covariance matrix.</li>
            <li>Look at the eigenvalues. What percentage of the total variance does the first principal component explain? (Hint: <code>eigen_result$values[1] / sum(eigen_result$values)</code>).</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Use the same data from the R example
set.seed(123)
wingspan <- rnorm(100, mean = 50, sd = 5)
beak_length <- wingspan * 0.3 + rnorm(100, mean = 0, sd = 2)
bird_data <- data.frame(wingspan, beak_length)

# Center the data
centered_data <- as.data.frame(scale(bird_data, center = TRUE, scale = FALSE))
data_mean <- colMeans(bird_data)

# Get eigenvectors
eigen_result <- eigen(cov(bird_data))
eigenvectors <- eigen_result$vectors
eigenvalues <- eigen_result$values

# Prepare vectors for plotting
vec_data <- data.frame(
  x_start = data_mean[1],
  y_start = data_mean[2],
  x_end = data_mean[1] + eigenvectors[1, ] * sqrt(eigenvalues) * 2,
  y_end = data_mean[2] + eigenvectors[2, ] * sqrt(eigenvalues) * 2,
  component = c("PC1", "PC2")
)

p <- ggplot(bird_data, aes(x = wingspan, y = beak_length)) +
  geom_point(alpha = 0.6) +
  geom_segment(
    data = vec_data, aes(x = x_start, y = y_start, xend = x_end, yend = y_end, color = component),
    arrow = arrow(length = unit(0.3, "cm")), size = 1.5
  ) +
  scale_color_manual(values = c("PC1" = "#ef4444", "PC2" = "#3b82f6")) +
  labs(
    title = "Principal Components of Correlated Data",
    x = "Wingspan (cm)", y = "Beak Length (cm)", color = "Component"
  ) +
  coord_equal() +
  theme_minimal()

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss2t4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A small bakery sells two items: croissants and muffins. On Monday, they sell 3 croissants and 2 muffins for a total of $18. On Tuesday, they sell 1 croissant and 4 muffins for a total of $16. What is the individual price of a croissant and a muffin? This is a classic system of linear equations, the same type of problem that linear regression solves on a much larger scale.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss2t4-concepts">The $Ax = b$ Form</button>
        <button class="tab-button" data-tab="p1s5ss2t4-usecase">Solving with R</button>
    </div>

    <div id="p1s5ss2t4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Translating Equations to Matrices</h4>
        <p>We can represent our bakery problem with two equations, where $c$ is the price of a croissant and $m$ is the price of a muffin:</p>
        <p>$3c + 2m = 18$</p>
        <p>$1c + 4m = 16$</p>
        <p>Linear algebra provides a compact way to write this system in the form $Ax = b$:</p>
        <div class="math-foundation">$$ \\begin{bmatrix} 3 & 2 \\\\ 1 & 4 \\end{bmatrix} \\begin{bmatrix} c \\\\ m \\end{bmatrix} = \\begin{bmatrix} 18 \\\\ 16 \\end{bmatrix} $$</div>
        <div class="plot-description">
            <ul>
                <li><strong>A</strong> is the matrix of known coefficients (the quantities sold).</li>
                <li><strong>x</strong> is the vector of unknown variables we want to solve for (the prices).</li>
                <li><strong>b</strong> is the vector of known outcomes (the total revenues).</li>
            </ul>
        </div>
        <p>To solve for $x$, we need to "undo" the multiplication by A. We do this by multiplying both sides by the inverse of A, $A^{-1}$:</p>
        <p>$A^{-1}Ax = A^{-1}b \\implies Ix = A^{-1}b \\implies x = A^{-1}b$</p>
        <p><strong>Analogy:</strong> This is like solving the simple equation $5x = 10$. You divide both sides by 5 (or multiply by $5^{-1}$) to get $x=2$. Matrix inversion is the equivalent of division.</p>
    </div>

    <div id="p1s5ss2t4-usecase" class="tab-pane">
        <h4 class="subsection-title">Solving the Bakery Problem</h4>
        <p>In R, you should almost never calculate the inverse directly. Instead, the <code>solve()</code> function is optimized to solve the system $Ax = b$ efficiently and accurately.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># 1. Create the matrix of coefficients A
# R fills matrices by column by default
A <- matrix(c(3, 1, 2, 4), nrow = 2) 
print(A)

# 2. Create the vector of outcomes b
b <- c(18, 16)
print(b)

# 3. Use solve(A, b) to find the vector of unknowns x
# This tells R to solve for x in the equation Ax = b
prices <- solve(A, b)

print(prices)
# Result: [1] 4 3
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The result <code>[1] 4 3</code> is our vector $x = \\begin{bmatrix} c \\\\ m \\end{bmatrix}$. This tells us that the price of a croissant (<code>c</code>) is $4 and the price of a muffin (<code>m</code>) is $3.</p>
            <p><strong>Verification:</strong></p>
            <ul>
                <li>Day 1: $3*4 + 2*3 = 12 + 6 = 18$. Correct.</li>
                <li>Day 2: $1*4 + 4*3 = 4 + 12 = 16$. Correct.</li>
            </ul>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Foundation of Linear Regression</h3>
    <div class="scenario-content">
        <p>When you run <code>lm(y ~ x1 + x2)</code> in R, you are doing this exact same thing on a massive scale. Your response variable <code>y</code> is the vector <code>b</code>. Your predictor variables <code>x1</code> and <code>x2</code> form the matrix <code>A</code> (called the design matrix). The model coefficients that <code>lm()</code> calculates are the unknown vector <code>x</code>. Linear regression is a geometric problem of finding the best-fit solution to a (usually overdetermined) system of linear equations.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>You are buying supplies. You purchase 2 pens and 5 notebooks for a total of $29. Your colleague buys 3 pens and 1 notebook for $11.</p>
        <ul class="prose-list">
            <li>Write down the two linear equations for this problem.</li>
            <li>Construct the coefficient matrix A and the outcome vector b in R.</li>
            <li>Use <code>solve(A, b)</code> to find the price of a single pen and a single notebook.</li>
        </ul>
    </div>
</div>
`,
          p1s5ss2t5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist is building a model with two features that are highly correlated, like "years of education" and "annual income." This multicollinearity can make the model's coefficient estimates unstable and hard to interpret. To fix this, they want to transform the original features into a new, independent set of features. This requires understanding the concept of orthogonality in a vector space.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss2t5-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s5ss2t5-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss2t5-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss2t5-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Geometry of Data</h4>
        <ul class="prose-list">
            <li><strong>Vector Space:</strong> The collection of all possible vectors of a certain length. A dataset with 2 features "lives" in a 2D vector space ($R^2$), one with 10 features lives in a 10D space ($R^{10}$).</li>
            <li><strong>Basis Vectors:</strong> A set of vectors that can be combined (via addition and scalar multiplication) to describe *any* other vector in the space. The standard basis in 2D is <code>[(1, 0)]</code> and <code>[(0, 1)]</code>.</li>
            <li><strong>Orthogonality:</strong> This is the crucial concept. Two vectors are **orthogonal** if they are at a 90-degree angle to each other. In data science, this means the features are **uncorrelated** or statistically independent. The dot product of two orthogonal vectors is always 0.</li>
        </ul>
        <p><strong>Analogy:</strong> Think of the standard North-South and East-West directions on a map. They are **orthogonal basis vectors**. Any location can be described as a combination of the two, and moving North has zero impact on your East-West position. They are perfectly independent.</p>
    </div>

    <div id="p1s5ss2t5-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Independence</h4>
        <p>This plot shows two sets of basis vectors. The correlated (non-orthogonal) basis on the left represents features that contain redundant information. The orthogonal basis on the right represents a clean, independent feature space, which is what techniques like PCA aim to find.</p>
        <div class="plot-container">
            <div id="p1s5ss2t5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> On the left, the two vectors are at an acute angle, indicating they are not independent (correlated). On the right, the two vectors are at a perfect 90-degree angle, representing an ideal, uncorrelated feature set. Most feature engineering and dimensionality reduction techniques are trying to transform the left situation into the right one.</p>
        </div>
    </div>
    
    <div id="p1s5ss2t5-usecase" class="tab-pane">
        <h4 class="subsection-title">Checking for Orthogonality</h4>
        <p>We can check if two vectors are orthogonal by calculating their dot product. If it's zero, they are orthogonal.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># Two correlated feature vectors
education <- c(12, 16, 18)
income <- c(50, 90, 110)

# Two orthogonal vectors
v1 <- c(1, 1)
v2 <- c(-1, 1)

# The dot product tells us about their relationship
# A large positive value means they point in a similar direction (correlated)
education %*% income

# A dot product of 0 means they are orthogonal (uncorrelated)
v1 %*% v2
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The dot product of <code>education</code> and <code>income</code> will be a large positive number, confirming they are not orthogonal. The dot product of <code>v1</code> and <code>v2</code> is exactly 0, confirming their orthogonality. The process of creating orthogonal vectors from a non-orthogonal set is called the **Gram-Schmidt process**, which is a key algorithm in linear algebra.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Orthonormal Basis</h3>
    <div class="scenario-content">
        <p>An even better situation than an orthogonal basis is an **orthonormal basis**. This is a set of orthogonal vectors where each vector also has a magnitude (length) of 1. These are called unit vectors. Working with an orthonormal basis simplifies many mathematical calculations, and it's the type of basis that SVD and PCA produce.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's find a vector that is orthogonal to a given vector.</p>
        <ul class="prose-list">
            <li>Create a vector <code>v_a <- c(4, 3)</code>.</li>
            <li>Can you manually figure out a 2D vector <code>v_b</code> that would be orthogonal to <code>v_a</code>? (Hint: for a 2D vector <code>[(x, y)]</code>, the vector <code>[(-y, x)]</code> is always orthogonal). Create this vector in R.</li>
            <li>Prove that they are orthogonal by calculating their dot product in R. The result should be 0.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)
library(patchwork)

# Data for Correlated Vectors
df_corr <- data.frame(
  x = c(0, 0), y = c(0, 0),
  xend = c(3, 2), yend = c(1.5, 3),
  label = c("v1", "v2")
)
p_corr <- ggplot(df_corr) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend),
               arrow = arrow(length = unit(0.3, "cm")), size = 1.5, color = "#f59e0b") +
  geom_text(aes(x = xend, y = yend, label = label), nudge_x = 0.2, nudge_y = 0.2) +
  coord_equal(xlim = c(0, 4), ylim = c(0, 4)) +
  labs(title = "Non-Orthogonal (Correlated)") +
  theme_minimal()

# Data for Orthogonal Vectors
df_ortho <- data.frame(
  x = c(0, 0), y = c(0, 0),
  xend = c(3, 0), yend = c(0, 3),
  label = c("b1", "b2")
)
p_ortho <- ggplot(df_ortho) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend),
               arrow = arrow(length = unit(0.3, "cm")), size = 1.5, color = "#10b981") +
  geom_text(aes(x = xend, y = yend, label = label), nudge_x = 0.2, nudge_y = 0.2) +
  coord_equal(xlim = c(0, 4), ylim = c(0, 4)) +
  labs(title = "Orthogonal (Uncorrelated)") +
  theme_minimal()

# We can't use patchwork with plotly, so we'll just render one for the JS example.
# This is the R code to generate both side-by-side.
# p_corr + p_ortho
ggplotly(p_corr) # We will simulate the side-by-side plot in JS.
</code></pre>
    </div>
</div>
`,
          p1s5ss2t6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A text analysis system converts two documents into high-dimensional vectors, where each dimension represents the count of a specific word. Document A is a short summary of a topic, while Document B is a long, detailed article on the same topic. How can the system determine that these documents are thematically similar, even though their word counts (and thus vector magnitudes) are vastly different?</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss2t6-concepts">Cosine Similarity</button>
        <button class="tab-button" data-tab="p1s5ss2t6-math">The Formula</button>
        <button class="tab-button" data-tab="p1s5ss2t6-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss2t6-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss2t6-concepts" class="tab-pane active">
        <h4 class="subsection-title">Measuring Direction, Not Magnitude</h4>
        <p>In many data science applications (like text analysis or recommendation systems), we care more about the "direction" or "pattern" of a vector than its absolute size. Standard distance metrics like Euclidean distance can be misleading here.</p>
        <p><strong>Cosine Similarity</strong> is the solution. It measures the cosine of the angle between two vectors. This value represents the similarity in their direction, irrespective of their magnitude.</p>
        <p><strong>Analogy:</strong> Imagine two people pointing flashlights. Cosine similarity doesn't care how bright each flashlight is (the magnitude). It only cares about whether they are pointing in the same direction. Two flashlights pointing at the same spot have a similarity of 1, even if one is much brighter than the other.</p>
        <p>The value ranges from -1 (pointing in opposite directions) to 1 (pointing in the same direction), with 0 indicating orthogonality (no similarity).</p>
    </div>

    <div id="p1s5ss2t6-math" class="tab-pane">
        <h4 class="subsection-title">The Dot Product, Normalized</h4>
        <p>The formula for cosine similarity is derived directly from the dot product formula. It is the dot product of the two vectors divided by the product of their magnitudes (lengths).</p>
        <div class="math-foundation">$$ \\text{similarity} = \\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\|\\vec{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}} $$</div>
    </div>
    
    <div id="p1s5ss2t6-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Similarity</h4>
        <p>This plot shows three document vectors. Documents A and B are very similar in topic, so their vectors point in almost the same direction (a small angle $\theta$). Document C is on a different topic and points in a different direction. The length of the vectors (document length) doesn't affect their cosine similarity.</p>
        <div class="plot-container">
            <div id="p1s5ss2t6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The key is the angle between the vectors. The angle between Document A and Document B is very small, so their cosine similarity will be close to 1. The angle between A and C is large (close to 90 degrees), so their cosine similarity will be close to 0.</p>
        </div>
    </div>
    
    <div id="p1s5ss2t6-usecase" class="tab-pane">
        <h4 class="subsection-title">Comparing Document Similarity</h4>
        <p>Let's solve our scenario. We'll create simple word-count vectors for three documents and an R function to calculate their similarity.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># Let's define a function for cosine similarity
cosine_sim <- function(a, b) {
  dot_product <- a %*% b
  magnitude_a <- sqrt(sum(a^2))
  magnitude_b <- sqrt(sum(b^2))
  return(dot_product / (magnitude_a * magnitude_b))
}

# Word vectors for [R, data, analysis, sport, game]
doc_A <- c(5, 8, 6, 0, 1)   # Short doc about data analysis in R
doc_B <- c(25, 30, 28, 1, 2)  # Long doc about data analysis in R
doc_C <- c(0, 1, 2, 20, 22) # Doc about sports and games

# Calculate similarities
sim_AB <- cosine_sim(doc_A, doc_B)
sim_AC <- cosine_sim(doc_A, doc_C)

print(paste("Similarity(A, B):", round(sim_AB, 3))) # Result: ~0.99
print(paste("Similarity(A, C):", round(sim_AC, 3))) # Result: ~0.11
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The cosine similarity between Document A and B is extremely high (0.99), correctly identifying them as being about the same topic despite their different lengths. The similarity between A and C is very low, correctly identifying them as unrelated.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Curse of Dimensionality</h3>
    <div class="scenario-content">
        <p>Why not use standard Euclidean distance? In very high-dimensional spaces (like text analysis with thousands of words/dimensions), the concept of distance becomes less meaningful. The ratio between the nearest and farthest points approaches 1, meaning everything appears to be far away from everything else. Cosine similarity is much more robust in these high-dimensional settings.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's find the most similar user from a set of movie ratings.</p>
        <ul class="prose-list">
            <li>Create vectors for three users' ratings of the movies [Star Wars, Titanic, The Godfather]:
                <ul>
                    <li><code>user1 <- c(5, 1, 5)</code></li>
                    <li><code>user2 <- c(1, 5, 2)</code></li>
                    <li><code>user3 <- c(4, 2, 4)</code></li>
                </ul>
            </li>
            <li>Use the <code>cosine_sim</code> function from the example to calculate the similarity between user1 and user2.</li>
            <li>Calculate the similarity between user1 and user3.</li>
            <li>Which user is more similar to user1 in terms of taste?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Document vectors
docs <- data.frame(
  name = c("Doc A (Short)", "Doc B (Long)", "Doc C (Different)"),
  x = c(5, 10, 2),
  y = c(4, 8, 9),
  color = c("#3b82f6", "#0ea5e9", "#ef4444")
)

p <- ggplot(docs, aes(x = x, y = y, color = name)) +
  geom_segment(aes(xend = 0, yend = 0), arrow = arrow(length = unit(0.3, "cm")), size = 1.2) +
  geom_text(aes(label = name), nudge_x = 0.5, nudge_y = 0.5) +
  coord_equal(xlim = c(0, 12), ylim = c(0, 12)) +
  labs(
    title = "Document Similarity as Vector Angles",
    x = "Count of word 'data'",
    y = "Count of word 'analysis'"
  ) +
  scale_color_manual(values = docs$color) +
  theme_minimal() +
  theme(legend.position = "none")

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss2t7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A geneticist has data with expression levels for thousands of genes for each patient. It's impossible to visualize or directly model this high-dimensional data. They need a way to reduce the thousands of correlated gene features into a handful of key "meta-genes" (principal components) that capture the most important patterns of variation in the data, making it suitable for visualization and modeling.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss2t7-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s5ss2t7-steps">The PCA Workflow</button>
        <button class="tab-button" data-tab="p1s5ss2t7-viz">The Scree Plot</button>
        <button class="tab-button" data-tab="p1s5ss2t7-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss2t7-concepts" class="tab-pane active">
        <h4 class="subsection-title">Summarizing High-Dimensional Data</h4>
        <p><strong>Principal Component Analysis (PCA)</strong> is an unsupervised dimensionality reduction technique. Its goal is to transform a dataset with many correlated variables into a new, smaller set of uncorrelated variables called **principal components**.</p>
        <p>These new components are linear combinations of the original variables, and they are ordered so that the first component (PC1) captures the largest possible variance in the data, PC2 captures the second largest, and so on. The principal components are the eigenvectors of the data's covariance matrix.</p>
        <p><strong>Analogy:</strong> PCA is like taking a 3D object (your data) and casting its shadow onto a 2D wall (the reduced dimension). You carefully choose the angle of the light (the principal components) to ensure the shadow retains the most important shape and features of the original object.</p>
    </div>

    <div id="p1s5ss2t7-steps" class="tab-pane">
        <h4 class="subsection-title">The PCA Recipe</h4>
        <ol class="prose-list">
            <li><strong>Scale the Data:</strong> This is a critical first step. Because PCA is based on variance, features with larger scales will dominate the analysis. You must first standardize each feature to have a mean of 0 and a standard deviation of 1.</li>
            <li><strong>Compute the Covariance Matrix:</strong> Calculate the covariance matrix of the scaled data to understand how the variables move together.</li>
            <li><strong>Perform Eigen-analysis:</strong> Find the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors are your principal components, and the eigenvalues represent the amount of variance captured by each component.</li>
            <li><strong>Select Components:</strong> Sort the components by their eigenvalues and decide how many to keep. This is often done using a "scree plot."</li>
            <li><strong>Transform the Data:</strong> Project the original scaled data onto the selected principal components to get your new, lower-dimensional dataset.</li>
        </ol>
    </div>

    <div id="p1s5ss2t7-viz" class="tab-pane">
        <h4 class="subsection-title">Deciding How Many Components to Keep</h4>
        <p>A **scree plot** is the standard tool for choosing the number of principal components. It's a bar chart showing the amount of variance explained by each successive principal component.</p>
        <div class="plot-container">
            <div id="p1s5ss2t7-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> You look for the "elbow" in the plot—the point where adding another component provides diminishing returns (doesn't add much new information). In this example, the first two components (PC1 and PC2) capture the vast majority of the variance. After PC2, the curve flattens out. This indicates that we can likely reduce our data from four dimensions to just two while retaining over 95% of the original information.</p>
        </div>
    </div>
    
    <div id="p1s5ss2t7-usecase" class="tab-pane">
        <h4 class="subsection-title">Dimensionality Reduction on <code>iris</code></h4>
        <p>Let's use R's built-in <code>prcomp()</code> function, which handles all the PCA steps for us, on the famous <code>iris</code> dataset.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># 1. Select only the numeric columns from iris
iris_numeric <- iris[, 1:4]

# 2. Run prcomp(). The <code>scale. = TRUE</code> argument is crucial!
pca_result <- prcomp(iris_numeric, scale. = TRUE)

# 3. Inspect the results
summary(pca_result) # Shows the proportion of variance explained by each component

# The new, transformed data is in pca_result$x
head(pca_result$x)

# The principal components (eigenvectors) are in pca_result$rotation
print(pca_result$rotation)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The <code>summary()</code> output will show that PC1 alone explains ~73% of the variance, and PC1 and PC2 combined explain ~95.8%. This confirms that we can very effectively visualize this 4-dimensional dataset in 2D by plotting PC1 against PC2, which is a very common practice.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Interpreting Principal Components</h3>
    <div class="scenario-content">
        <p>How do you give a business meaning to a component like PC1? You look at the <code>rotation</code> matrix from the <code>prcomp()</code> output. This matrix contains the eigenvectors, which show the "loadings" or weights of the original variables on each component. If PC1 has large positive loadings for <code>Sepal.Length</code>, <code>Petal.Length</code>, and <code>Petal.Width</code>, you could interpret PC1 as a measure of "overall flower size."</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's perform PCA on the <code>mtcars</code> dataset.</p>
        <ul class="prose-list">
            <li>Select only the numeric columns of <code>mtcars</code>. (Be careful, some might not be suitable for PCA, but we'll use them for this exercise).</li>
            <li>Run <code>prcomp()</code> on the selected columns, making sure to set <code>scale. = TRUE</code>.</li>
            <li>Use the <code>summary()</code> function on your PCA result. How many components are needed to explain at least 85% of the total variance?</li>
            <li>Create a scree plot by making a bar chart of the "Proportion of Variance" from the summary's importance table. (Hint: <code>summary(pca_result)$importance[2,]</code>).</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Perform PCA on iris to get the variance explained
pca_result <- prcomp(iris[, 1:4], scale. = TRUE)
variance_explained <- summary(pca_result)$importance[2, ]
cumulative_variance <- summary(pca_result)$importance[3, ]

scree_data <- data.frame(
  Component = factor(1:4),
  Variance = variance_explained,
  Cumulative = cumulative_variance
)

# Create the scree plot
p <- ggplot(scree_data, aes(x = Component, y = Variance)) +
  geom_bar(stat = "identity", fill = "#3b82f6", alpha = 0.8) +
  geom_line(aes(y = Cumulative, group = 1), color = "#ef4444", size = 1.2) +
  geom_point(aes(y = Cumulative), color = "#ef4444", size = 3) +
  geom_text(aes(label = scales::percent(Cumulative, accuracy = 0.1), y = Cumulative), vjust = -0.5) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Scree Plot for Iris PCA",
    subtitle = "Variance Explained by Each Component",
    x = "Principal Component",
    y = "Proportion of Variance Explained"
  ) +
  theme_minimal()

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss3t1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> When training a machine learning model, we define a "loss function" that measures how wrong the model's predictions are. To improve the model, we must adjust its parameters (e.g., slope and intercept). To know *how* to adjust them—whether to increase or decrease each one to reduce the loss—we need to find the direction of the steepest descent on the loss surface. This is precisely what the derivative tells us.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss3t1-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s5ss3t1-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss3t1-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss3t1-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Mathematics of Change</h4>
        <ul class="prose-list">
            <li><strong>Derivative:</strong> For a function of one variable, the derivative at a point is the <strong>instantaneous rate of change</strong>, or the slope of the tangent line at that point. It tells us how the output (<code>y</code>) changes for a tiny change in the input (<code>x</code>).</li>
            <li><strong>Gradient ($\nabla$):</strong> The multi-dimensional generalization of the derivative. For a function with multiple inputs (like a model with many parameters), the gradient is a vector that points in the direction of the <strong>steepest ascent</strong>. The negative of the gradient points in the direction of the steepest descent.</li>
        </ul>
        <p><strong>Analogy:</strong> Imagine you are standing on a mountainside. The derivative is the exact slope of the ground beneath your feet in a single direction (e.g., North-South). The <strong>gradient</strong> is a magical compass that always points directly uphill, to the steepest possible path from where you stand.</p>
    </div>

    <div id="p1s5ss3t1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Slope</h4>
        <p>This plot shows a simple curve ($f(x) = x^2$). The derivative at any point on this curve is the slope of the tangent line at that point. At $x=-2$, the slope is negative (pointing down). At $x=0$, the slope is zero (flat). At $x=1.5$, the slope is positive (pointing up).</p>
        <div class="plot-container">
            <div id="p1s5ss3t1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The red line is tangent to the blue curve at $x=1.5$. Its slope represents the derivative at that specific point. You can see it's positive, meaning the function is increasing. The value of the derivative, $f'(1.5) = 2 \times 1.5 = 3$, tells us the exact steepness of that tangent line.</p>
        </div>
    </div>
    
    <div id="p1s5ss3t1-usecase" class="tab-pane">
        <h4 class="subsection-title">Symbolic Differentiation in R</h4>
        <p>While numerical approximation is more common in ML packages, R can perform symbolic differentiation on simple expressions using the <code>D()</code> function. This is useful for understanding the underlying mechanics.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r"># Define an expression for our function f(x) = x^3
my_expression <- expression(x^3, "x")

# Use D() to find the derivative with respect to 'x'
derivative <- D(my_expression, "x")

print(derivative)
# Result: 3 * x^2

# Now we can evaluate the slope at any point, e.g., x = 2
x <- 2
eval(derivative) # Result: 12
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> For our loss function, we would calculate the gradient with respect to all model parameters. The resulting vector tells us exactly how to "nudge" each parameter to most quickly reduce the model's error. This is the core engine of optimization algorithms like Gradient Descent.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Chain Rule</h3>
    <div class="scenario-content">
        <p>Neural networks are essentially deeply nested functions (a function inside a function inside a function...). To calculate the gradient of the final loss with respect to the earliest parameters, we need the <strong>chain rule</strong>. This is the fundamental calculus rule that allows us to find the derivative of composite functions. The famous "backpropagation" algorithm is just a clever and efficient way of applying the chain rule repeatedly through the network.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The formula for Mean Squared Error for a simple linear model with one data point $(x_i, y_i)$ and prediction $\hat{y}_i = mx_i + b$ is $Loss = (y_i - (mx_i + b))^2$.</li>
            <li>In R, create an expression for this loss function.</li>
            <li>Use the <code>D()</code> function to find the partial derivative of the Loss with respect to the slope <code>m</code>. This tells you how the loss changes as you change the slope.</li>
            <li>Use <code>D()</code> again to find the partial derivative with respect to the intercept <code>b</code>.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Function and its derivative
f <- function(x) { x^2 }
f_prime <- function(x) { 2*x }

# Point of tangency
x0 <- 1.5
y0 <- f(x0)
slope <- f_prime(x0)

# Tangent line function
tangent_line <- function(x) { slope * (x - x0) + y0 }

p <- ggplot(data.frame(x = seq(-3, 3, 0.1)), aes(x)) +
  stat_function(fun = f, aes(color = "f(x) = x²"), size = 1.2) +
  stat_function(fun = tangent_line, aes(color = "Tangent Line"), size = 1, linetype = "dashed") +
  geom_point(aes(x = x0, y = y0), size = 4, color = "#ef4444") +
  annotate("text", x = x0, y = y0 + 1, label = paste0("Slope = ", slope), color = "#ef4444") +
  scale_color_manual(name = "Function", values = c("f(x) = x²" = "#3b82f6", "Tangent Line" = "#ef4444")) +
  labs(title = "The Derivative as the Slope of the Tangent Line", x = "x", y = "f(x)") +
  theme_minimal()

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss3t2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has a Probability Density Function (PDF) representing the distribution of customer heights. A clothing company wants to know: "What proportion of our customers are between 170cm and 180cm tall?" To answer this, the analyst must calculate the total area under the PDF curve between these two values. This process is called integration.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss3t2-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s5ss3t2-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss3t2-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss3t2-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Area Under the Curve</h4>
        <ul class="prose-list">
            <li><strong>Integration:</strong> The reverse process of differentiation. If differentiation finds the slope, integration finds the accumulated area.</li>
            <li><strong>Definite Integral ($\int_{a}^{b} f(x) dx$):</strong> Calculates the exact area under the curve of a function $f(x)$ from a starting point $a$ to an ending point $b$. This is the primary use in probability.</li>
            <li><strong>Indefinite Integral ($\int f(x) dx$):</strong> Finds the general anti-derivative of a function.</li>
        </ul>
        <p><strong>Analogy:</strong> If your car's speedometer readings are a function of time, the <strong>derivative</strong> at any moment is your acceleration. The <strong>integral</strong> of your speed over an hour is the total distance you traveled—the total "area" accumulated under the speed curve.</p>
    </div>

    <div id="p1s5ss3t2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Probability as Area</h4>
        <p>This plot shows a normal distribution for customer heights. The shaded area between 170cm and 180cm represents the definite integral. The value of this area is the probability that a randomly selected customer will have a height within this range.</p>
        <div class="plot-container">
            <div id="p1s5ss3t2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The total area under the entire blue curve is 1 (or 100%). The red shaded region represents a fraction of that total area. By calculating this area, we are finding the proportion of the population that falls within our specific range of interest.</p>
        </div>
    </div>
    
    <div id="p1s5ss3t2-usecase" class="tab-pane">
        <h4 class="subsection-title">Numerical Integration in R</h4>
        <p>R's <code>pnorm()</code> function does this for us automatically for the normal distribution, but we can also use the general-purpose <code>integrate()</code> function to find the area under any function's curve.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# Let's solve the customer height problem
# Assume heights are normally distributed with mean=175cm and sd=7cm

# Method 1: The easy way, using the built-in probability function
# pnorm(180) gives area left of 180. pnorm(170) gives area left of 170.
# The difference is the area between them.
pnorm(180, mean = 175, sd = 7) - pnorm(170, mean = 175, sd = 7)
# Result: ~0.5328 or 53.3%

# Method 2: The general way, using numerical integration
# First, define the PDF function for our specific normal distribution
normal_pdf <- function(x) { dnorm(x, mean = 175, sd = 7) }

# Now, integrate this function from 170 to 180
integrate(normal_pdf, lower = 170, upper = 180)
# Result: 0.5328072 with absolute error < 5.9e-15
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: No Closed-Form Solution? No Problem!</h3>
    <div class="scenario-content">
        <p>The integral of the normal distribution's PDF does not have a simple, "closed-form" solution that can be written down with elementary functions. This is why computers are essential! Functions like <code>pnorm()</code> and <code>integrate()</code> use sophisticated numerical approximation algorithms to find the area with extremely high precision.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's find the area under a simple parabola, $f(x) = x^2$, from $x=0$ to $x=2$.</p>
        <ul class="prose-list">
            <li>Define a function in R: <code>my_func <- function(x) { x^2 }</code>.</li>
            <li>Use the <code>integrate()</code> function to find the definite integral of <code>my_func</code> from a <code>lower</code> bound of 0 to an <code>upper</code> bound of 2.</li>
            <li>The exact analytical answer is $8/3 \approx 2.667$. How close does R's numerical approximation get?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Define the curve data
curve_data <- data.frame(x = seq(150, 200, length.out = 200))
curve_data$y <- dnorm(curve_data$x, mean = 175, sd = 7)

# Define the shaded area data
area_data <- subset(curve_data, x >= 170 & x <= 180)

p <- ggplot(curve_data, aes(x, y)) +
  geom_line(color = "#3b82f6", size = 1) +
  geom_area(data = area_data, fill = "#ef4444", alpha = 0.7) +
  labs(
    title = "Probability as Area Under the Curve",
    subtitle = "Shaded Area = P(170 < Height < 180)",
    x = "Height (cm)",
    y = "Probability Density"
  ) +
  theme_minimal()

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss3t3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An engineer is programming a calculator on a simple chip that can only perform basic arithmetic (add, multiply, etc.). It can't compute complex functions like $sin(x)$ directly. To make it work, the engineer needs to create a simple polynomial function that is a very close approximation of $sin(x)$ near a specific value, like $x=0$. This is a perfect use case for a Taylor series approximation.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss3t3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s5ss3t3-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss3t3-usecase">Relevance to ML</button>
    </div>

    <div id="p1s5ss3t3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Approximating Complexity with Simplicity</h4>
        <p>A <strong>Taylor series</strong> is a way of representing any complex, smooth function as an infinite sum of simple polynomial terms. The key insight is that if you know the value of a function and all its derivatives at a single point, you can approximate its behavior in the neighborhood of that point.</p>
        <p><strong>Analogy:</strong> It's like creating a highly accurate portrait using only simple, straight pencil strokes. You start at one point (e.g., the tip of the nose). Your first stroke matches the direction (1st derivative). Your next stroke adds a slight curve to match the curvature (2nd derivative), and so on. The more strokes (terms) you add, the more accurate the portrait becomes around the starting point.</p>
        <div class="math-foundation">$$ f(x) \\approx f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\dots + \\frac{f^{(n)}(a)}{n!}(x-a)^n $$</div>
    </div>

    <div id="p1s5ss3t3-viz" class="tab-pane">
        <h4 class="subsection-title">Getting Closer with More Terms</h4>
        <p>This plot shows the function $sin(x)$ in blue. The other lines are Taylor polynomial approximations centered at $x=0$. Notice how as we add more terms to the polynomial (from 1st order to 7th order), the approximation becomes incredibly accurate over a wider and wider range.</p>
        <div class="plot-container">
            <div id="p1s5ss3t3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The green line (1st order) is just the tangent line and is only a good approximation very close to zero. The purple line (3rd order) starts to capture the curve's shape. The orange line (7th order) is almost indistinguishable from the true $sin(x)$ curve over a large interval.</p>
        </div>
    </div>
    
    <div id="p1s5ss3t3-usecase" class="tab-pane">
        <h4 class="subsection-title">Why This Matters for Machine Learning</h4>
        <p>While data scientists rarely code Taylor series by hand, the concept is the theoretical bedrock for many advanced optimization algorithms.</p>
        <ul class="prose-list">
            <li><strong>Gradient Descent</strong> is essentially a <strong>1st-order approximation</strong>. At each step, it uses the derivative (the linear slope) to decide which way to go.</li>
            <li><strong>Newton's Method</strong> (an advanced optimizer) is a <strong>2nd-order approximation</strong>. It uses both the first and second derivatives to create a quadratic (parabola) approximation of the loss function, allowing it to find the minimum much more quickly.</li>
        </ul>
        <p>Understanding function approximation helps you understand why and how different optimization algorithms work.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Maclaurin Series</h3>
    <div class="scenario-content">
        <p>A <strong>Maclaurin series</strong> is not a different concept, but simply a special case of the Taylor series where the approximation is centered around the point $a=0$. These are very common in physics and engineering for modeling behavior around an equilibrium point.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>The Taylor series for $e^x$ centered at $a=0$ is famously simple: $1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\dots$</p>
        <ul class="prose-list">
            <li>In R, calculate the true value of $e^1$ using the <code>exp(1)</code> function.</li>
            <li>Write an R expression to calculate the 4-term Taylor approximation for $e^1$ (i.e., where $x=1$). Use the <code>factorial()</code> function for the denominators.</li>
            <li>How close is your approximation to the true value? What happens if you add a 5th term?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Define the functions
sin_x <- function(x) { sin(x) }
taylor1 <- function(x) { x }
taylor3 <- function(x) { x - x^3 / factorial(3) }
taylor7 <- function(x) { x - x^3 / factorial(3) + x^5 / factorial(5) - x^7 / factorial(7) }

p <- ggplot(data.frame(x = seq(-pi, pi, 0.1)), aes(x)) +
  stat_function(fun = sin_x, aes(color = "sin(x)"), size = 1.5) +
  stat_function(fun = taylor1, aes(color = "1st Order"), linetype = "dashed") +
  stat_function(fun = taylor3, aes(color = "3rd Order"), linetype = "dotted") +
  stat_function(fun = taylor7, aes(color = "7th Order"), linetype = "dotdash") +
  scale_color_manual(
    name = "Function",
    values = c("sin(x)" = "#3b82f6", "1st Order" = "#10b981", "3rd Order" = "#8b5cf6", "7th Order" = "#f59e0b")
  ) +
  labs(title = "Taylor Series Approximation of sin(x)", y = "f(x)") +
  theme_minimal()

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s5ss3t4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> We are building a linear regression model to predict house prices. How do we find the single "best" line that fits our data points? We need to first define what "best" means by creating a <strong>loss function</strong> (e.g., one that measures the total squared vertical distance from each point to the line). Then, we must use an optimization algorithm to find the specific slope and intercept that make this total distance as small as possible.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss3t4-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s5ss3t4-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss3t4-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss3t4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Finding the Lowest Point</h4>
        <ul class="prose-list">
            <li><strong>Loss Function (or Cost Function):</strong> A function that maps a set of model parameters to a single number representing the "error" or "cost" of using those parameters. The goal of training is to minimize this function. The most common one for regression is Mean Squared Error (MSE).</li>
            <li><strong>Minimum (plural: Minima):</strong> The point in the parameter space where the loss function has its lowest value.</li>
            <li><strong>Global vs. Local Minimum:</strong> A <strong>global minimum</strong> is the absolute lowest point across the entire function. A <strong>local minimum</strong> is the lowest point in its immediate neighborhood, but not necessarily overall.</li>
            <li><strong>Convexity:</strong> A function is convex if the line segment between any two points on its graph lies on or above the graph. Convex functions are "bowl-shaped" and have only one global minimum, which makes optimization easy. Non-convex functions can have many local minima, which are tricky to escape.</li>
        </ul>
        <p><strong>Analogy:</strong> Optimization is like being a hiker in a thick fog trying to find the lowest point in a landscape. The loss function is the shape of the land. A <strong>convex</strong> landscape is one big, simple valley; any step downhill will eventually lead you to the bottom. A <strong>non-convex</strong> landscape is a mountain range with many small valleys (local minima) where you could get stuck, thinking you're at the bottom when the true lowest point is over the next ridge.</p>
    </div>

    <div id="p1s5ss3t4-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Loss Surface</h4>
        <p>This plot shows a non-convex loss surface for a model with two parameters. Our goal is to find the parameter values that correspond to the deep blue area, the global minimum. An optimization algorithm might get stuck in the shallower local minimum if it isn't sophisticated enough.</p>
        <div class="plot-container">
            <div id="p1s5ss3t4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The X and Y axes represent two model parameters, and the Z-axis (color) represents the model's error (loss). The goal of optimization is to find the (X, Y) coordinates of the lowest point. The deep valley on the right is the <strong>global minimum</strong>, the best possible solution. The smaller dip on the left is a <strong>local minimum</strong>, a good but not optimal solution.</p>
        </div>
    </div>
    
    <div id="p1s5ss3t4-usecase" class="tab-pane">
        <h4 class="subsection-title">Defining a Loss Function in R</h4>
        <p>Let's define the Mean Squared Error (MSE) loss function for a simple linear model. This function takes our data and a set of parameters (slope <code>m</code>, intercept <code>b</code>) and returns the total error.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# Sample data
x_vals <- c(1, 2, 3, 4, 5)
y_vals <- c(2, 4, 5, 4, 6)

# Define the MSE loss function
mse_loss <- function(params, x_data, y_data) {
  m <- params[1] # slope
  b <- params[2] # intercept
  
  # Calculate the model's predictions
  predictions <- m * x_data + b
  
  # Calculate the squared errors
  squared_errors <- (y_data - predictions)^2
  
  # Return the mean of the squared errors
  return(mean(squared_errors))
}

# Let's test our loss function with a guess for the parameters
# Guess 1: m=1, b=1
mse_loss(params = c(1, 1), x_data = x_vals, y_data = y_vals) # Result: 1.2

# Guess 2: m=0.8, b=2
mse_loss(params = c(0.8, 2), x_data = x_vals, y_data = y_vals) # Result: 0.84 (a better fit!)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The goal of a machine learning algorithm is to find the <code>params</code> vector that results in the lowest possible output from the <code>mse_loss</code> function. The process of finding these optimal parameters is called optimization.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Convexity is a Blessing</h3>
    <div class="scenario-content">
        <p>The loss functions for many classical machine learning models, like Linear Regression and Logistic Regression, are convex. This is a huge advantage because it guarantees that an optimization algorithm like Gradient Descent will find the single global minimum. The loss functions for complex models like neural networks are highly non-convex, which is why optimization is much more challenging in deep learning.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mse_loss</code> function and <code>x_vals</code>/<code>y_vals</code> data from the example, manually try three different pairs of parameters for <code>m</code> and <code>b</code>.</li>
            <li>Which of your guesses gives the lowest loss?</li>
            <li>The true optimal parameters found by <code>lm(y_vals ~ x_vals)</code> are an intercept of 2.2 and a slope of 0.7. Calculate the loss for these true parameters. Is it lower than all your guesses?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(plotly)

# Define a non-convex function for the surface plot
loss_surface_func <- function(x, y) {
  # Himmelblau's function is a classic non-convex test function
  # Shifted to have a clearer global minimum in the positive quadrant
  term1 <- (x^2 + y - 11)^2
  term2 <- (x + y^2 - 7)^2
  return(log1p(term1 + term2)) # Use log scale for better visualization
}

# Create a grid of x and y values
x_grid <- seq(-0, 6, length.out = 100)
y_grid <- seq(-0, 6, length.out = 100)

# Calculate z values (the loss) for the grid
z_grid <- outer(x_grid, y_grid, loss_surface_func)

fig <- plot_ly(x = ~x_grid, y = ~y_grid, z = ~z_grid, type = "surface") %>%
  layout(
    title = "Non-Convex Loss Surface",
    scene = list(
      xaxis = list(title = "Parameter 1"),
      yaxis = list(title = "Parameter 2"),
      zaxis = list(title = "Loss")
    )
  )

fig
</code></pre>
    </div>
</div>
`,
          p1s5ss3t5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> We have a loss function that represents a foggy valley, and our goal is to find the lowest point. We can't see the entire landscape at once, so we need a step-by-step algorithm. The most fundamental optimization algorithm is <strong>Gradient Descent</strong>, which iteratively takes small steps in the steepest downhill direction until it finds a minimum.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss3t5-concepts">The Core Algorithm</button>
        <button class="tab-button" data-tab="p1s5ss3t5-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss3t5-variants">Algorithm Variants</button>
        <button class="tab-button" data-tab="p1s5ss3t5-usecase">Use Case in R</button>
    </div>

    <div id="p1s5ss3t5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Walking Down the Hill</h4>
        <p>Gradient Descent is an iterative optimization algorithm for finding a local minimum of a differentiable function.</p>
        <p><strong>Analogy:</strong> It's the hiker's strategy in the foggy valley. At any given point, they can't see the whole valley, but they can feel the slope of the ground right under their feet. The strategy is simple:
        <ul class="prose-list">
            <li>Check the slope (calculate the <strong>gradient</strong>). The gradient points uphill.</li>
            <li>Take one small step in the exact opposite direction (the <strong>negative gradient</strong>).</li>
            <li>Repeat until the ground is flat (the gradient is zero).</li>
        </ul>
        <div class="math-foundation">
            <p>The update rule is: <strong>New Position = Old Position - (Learning Rate * Gradient)</strong></p>
            $$ \\theta_{new} = \\theta_{old} - \\eta \\nabla J(\\theta) $$
            <p>where $\\eta$ (eta) is the <strong>learning rate</strong>—the size of the step you take.</p>
        </div>
    </div>

    <div id="p1s5ss3t5-viz" class="tab-pane">
        <h4 class="subsection-title">The Path to the Minimum</h4>
        <p>This contour plot shows a loss surface from above, with darker blue being lower. The red line shows the path taken by the gradient descent algorithm. It starts at a random point and takes a series of steps, each one perpendicular to the contour lines, until it converges at the minimum.</p>
        <div class="plot-container">
            <div id="p1s5ss3t5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The path starts at the top left and moves progressively "downhill" towards the center of the contours. Notice how the steps get smaller as the slope gets flatter near the minimum. This demonstrates the iterative nature of the algorithm as it hones in on the optimal solution.</p>
        </div>
    </div>

    <div id="p1s5ss3t5-variants" class="tab-pane">
        <h4 class="subsection-title">How Much Data to Use for Each Step?</h4>
        <p>The main difference between variants of gradient descent is how much data is used to calculate the gradient at each step.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Batch Gradient Descent</strong>
                <p>Calculates the gradient using the <strong>entire training dataset</strong> for each step. This provides a very accurate gradient, but is incredibly slow and memory-intensive for large datasets.</p>
            </div>
            <div class="decision-branch">
                <strong>Stochastic Gradient Descent (SGD)</strong>
                <p>Calculates the gradient using only a <strong>single, randomly chosen data point</strong> for each step. It's much faster and less memory-intensive, but the steps are very noisy and erratic.</p>
            </div>
            <div class="decision-branch">
                <strong>Mini-Batch Gradient Descent</strong>
                <p>The best of both worlds and the standard approach in deep learning. It calculates the gradient using a <strong>small, random subset (a "mini-batch")</strong> of the data for each step. It's efficient like SGD but has a much smoother and more stable convergence path.</p>
            </div>
        </div>
    </div>
    
    <div id="p1s5ss3t5-usecase" class="tab-pane">
        <h4 class="subsection-title">Finding a Minimum by Hand</h4>
        <p>Let's use a simple <code>for</code> loop in R to implement gradient descent and find the minimum of the function $f(x) = x^2$. We know the minimum is at $x=0$. The derivative is $f'(x) = 2x$.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# Set hyperparameters
learning_rate <- 0.1
n_iterations <- 20
current_x <- 4 # Start at a random point

# Store the path for visualization
path <- numeric(n_iterations)

# The optimization loop
for (i in 1:n_iterations) {
  gradient <- 2 * current_x # Calculate the gradient at the current point
  current_x <- current_x - learning_rate * gradient # Take a step downhill
  path[i] <- current_x # Record the new position
  print(paste("Iteration", i, ": x =", round(current_x, 4)))
}
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> As you watch the output, you will see the value of <code>current_x</code> get progressively closer to 0 with each iteration, successfully finding the function's minimum.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Learning Rate is Crucial</h3>
    <div class="scenario-content">
        <p>The <strong>learning rate</strong> is the most important hyperparameter in training a model with gradient descent. If it's too small, training will take forever. If it's too large, the algorithm will be unstable and may "overshoot" the minimum and diverge entirely. A significant part of training deep learning models involves finding a good learning rate or using a "learning rate schedule" that starts large and gets smaller over time.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Take the R code from the Use Case example for finding the minimum of $f(x) = x^2$.</li>
            <li>Modify it to find the minimum of the function $f(x) = (x-5)^2$. What is the derivative of this function? (Hint: Use the chain rule, it's $2 \times (x-5)$). Where should the minimum be?</li>
            <li>Run your modified loop. Does it correctly converge to the new minimum?</li>
            <li>Experiment by changing the <code>learning_rate</code>. What happens if you set it to a very large value, like 1.1? What happens if you set it to a very small value, like 0.001?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(plotly)

# Define the loss function (a simple bowl shape)
loss_func <- function(x, y) { x^2 + y^2 }
x_grid <- seq(-10, 10, length.out = 50)
y_grid <- seq(-10, 10, length.out = 50)
z_grid <- outer(x_grid, y_grid, loss_func)

# Simulate the path of gradient descent
path_x <- c(9)
path_y <- c(9)
path_z <- c()
learning_rate <- 0.1
for(i in 1:15){
  grad_x <- 2 * path_x[i]
  grad_y <- 2 * path_y[i]
  path_x[i+1] <- path_x[i] - learning_rate * grad_x
  path_y[i+1] <- path_y[i] - learning_rate * grad_y
}
path_z <- loss_func(path_x, path_y)

fig <- plot_ly() %>%
  add_contour(x = ~x_grid, y = ~y_grid, z = ~z_grid,
              colorscale = "Blues", reversescale=TRUE) %>%
  add_trace(x = ~path_x, y = ~path_y, type = 'scatter', mode = 'lines+markers',
            line = list(color = '#ef4444', width = 3),
            marker = list(color = '#ef4444', size = 6)) %>%
  layout(
    title = "Path of Gradient Descent on a Contour Plot",
    xaxis = list(title = "Parameter 1"),
    yaxis = list(title = "Parameter 2"),
    showlegend = FALSE
  )

fig
</code></pre>
    </div>
</div>
`,
          p1s5ss3t6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An optimization algorithm is trying to find the minimum of a long, narrow, elliptical valley (a poorly conditioned loss function). Standard gradient descent will inefficiently zig-zag back and forth across the valley floor. Are there more powerful methods that can take a more direct route by considering the *curvature* of the loss surface, not just its slope?</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss3t6-newton">Second-Order Methods: Newton's Method</button>
        <button class="tab-button" data-tab="p1s5ss3t6-constrained">Constrained Optimization</button>
    </div>

    <div id="p1s5ss3t6-newton" class="tab-pane active">
        <h4 class="subsection-title">Using Curvature for Faster Convergence</h4>
        <p>While gradient descent uses the 1st derivative (the gradient), <strong>second-order methods</strong> use the 2nd derivative (called the <strong>Hessian matrix</strong> in multiple dimensions) to get a more complete picture of the loss surface's shape.</p>
        <p><strong>Newton's Method</strong> fits a quadratic approximation (a parabola or paraboloid) to the loss function at the current point and then jumps directly to the minimum of that approximation. This allows it to take much larger, more intelligent steps.</p>
        <p><strong>Analogy:</strong> Gradient descent is a hiker who only knows the slope. Newton's method is a skier who can feel both the slope and the *curvature* of the hill. The skier can use this extra information to take a much more direct and faster path to the bottom of the valley.</p>
        <div class="math-foundation">
            <p>The update rule is: $$ \\theta_{new} = \\theta_{old} - H^{-1} \\nabla J(\\theta) $$
            where $H^{-1}$ is the inverse of the Hessian matrix.</p>
        </div>
        <p><strong>Trade-off:</strong> For a model with N parameters, the Hessian is an N x N matrix. Calculating and inverting this matrix is extremely computationally expensive, making it impractical for large models like neural networks. This is why first-order methods (like gradient descent) are dominant in deep learning.</p>
    </div>

    <div id="p1s5ss3t6-constrained" class="tab-pane">
        <h4 class="subsection-title">Optimization with Rules</h4>
        <p>Sometimes, we need to find the minimum of a function subject to certain constraints. For example, "minimize costs, *subject to the constraint* that production must be at least 1,000 units."</p>
        <p><strong>Lagrange Multipliers</strong> are a powerful mathematical technique for solving such constrained optimization problems. They essentially convert a constrained problem into an unconstrained one by adding a new term to the objective function that penalizes violations of the constraint.</p>
        <p><strong>Data Science Application:</strong> The most famous application is in the derivation of <strong>Support Vector Machines (SVMs)</strong>. The goal of an SVM is to find the maximum margin (distance) between two classes, *subject to the constraint* that all data points must be on the correct side of that margin. Lagrange multipliers are used to solve this problem elegantly.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Quasi-Newton Methods</h3>
    <div class="scenario-content">
        <p>Since calculating the true Hessian is too expensive, a popular middle ground is found in <strong>Quasi-Newton methods</strong> (like BFGS). These algorithms cleverly *approximate* the inverse Hessian at each step using only information from the first-order gradients. They offer much of the speed advantage of Newton's method without the massive computational cost and are a common default optimizer for many classical machine learning models.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Thought Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise. You don't need to write the code, just think about the problem.</p>
        <ul class="prose-list">
            <li>Imagine a loss surface shaped like a perfect, round bowl. Would you expect Newton's method to be much faster than gradient descent here? (Hint: The curvature is the same everywhere).</li>
            <li>Now imagine a loss surface shaped like a very long, narrow, and steep canyon. In which direction is the gradient largest? In which direction do you *want* to travel? How might Newton's method, by correcting for curvature, help you travel in the right direction?</li>
        </ul>
    </div>
</div>
`,
          p1s5ss3t7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist is training a huge neural network with millions of parameters. The loss surface is highly non-convex with many ravines, plateaus, and local minima. Basic Stochastic Gradient Descent (SGD) is very slow to converge and its path is extremely noisy. How do modern deep learning frameworks make this process faster, more stable, and more reliable? They use <strong>adaptive optimizers</strong>.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s5ss3t7-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s5ss3t7-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s5ss3t7-usecase">Use in Practice</button>
    </div>

    <div id="p1s5ss3t7-concepts" class="tab-pane active">
        <h4 class="subsection-title">Smarter Ways to Walk Downhill</h4>
        <p>Modern optimizers build on SGD by incorporating ideas about the *history* of the gradients to adapt the learning process.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Momentum</strong>
                <p>This method adds a fraction of the previous update vector to the current one. It helps the optimizer accelerate in the correct direction and dampens oscillations.</p>
                <p><strong>Analogy:</strong> Instead of a hiker, Momentum is a <strong>heavy ball rolling down the hill</strong>. It builds up speed, allowing it to power through small bumps (local minima) and avoid getting stuck oscillating back and forth in narrow ravines.</p>
            </div>
            <div class="decision-branch">
                <strong>RMSprop / Adagrad</strong>
                <p>These methods maintain a per-parameter learning rate. They decrease the learning rate for parameters that have large gradients (to prevent overshooting) and increase it for parameters with small gradients.</p>
                <p><strong>Analogy:</strong> This is a hiker with <strong>smart, adaptive boots</strong>. On steep, rocky parts of the hill (high gradient), the boots shorten to take small, careful steps. On gentle, flat parts (low gradient), they lengthen to take large strides and cover ground faster.</p>
            </div>
        </div>
        <div class="decision-branches" style="align-items: flex-start; margin-top: 1rem;">
             <div class="decision-branch">
                <strong>Adam (Adaptive Moment Estimation)</strong>
                <p>The most popular and often default optimizer. It essentially combines the ideas of both Momentum and RMSprop. It keeps track of both an exponentially decaying average of past gradients (like momentum) and past squared gradients (like RMSprop).</p>
                <p><strong>Analogy:</strong> Adam is the best of both worlds: a <strong>heavy, rolling ball with smart, adaptive boots</strong>. It's fast, stable, and generally works very well across a wide range of problems.</p>
            </div>
        </div>
    </div>

    <div id="p1s5ss3t7-viz" class="tab-pane">
        <h4 class="subsection-title">Comparing Optimizer Paths</h4>
        <p>This contour plot shows the paths that different optimizers might take on a difficult loss surface. SGD oscillates wildly. Momentum dampens the oscillations and accelerates faster. Adam takes the most direct and efficient path to the minimum.</p>
        <div class="plot-container">
            <div id="p1s5ss3t7-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The goal is to get from the starting point to the blue minimum as quickly as possible. Notice how SGD (purple) overshoots and zig-zags. Momentum (orange) smooths this path out. Adam (green) takes a much more intelligent, direct route by adapting its step size and direction.</p>
        </div>
    </div>
    
    <div id="p1s5ss3t7-usecase" class="tab-pane">
        <h4 class="subsection-title">Using Optimizers in Keras</h4>
        <p>You almost never need to implement these algorithms yourself. They are built directly into deep learning frameworks like Keras (in R) or PyTorch.</p>
        <p>When you compile a model, you simply specify the name of the optimizer you want to use. <code>adam</code> is the most common choice.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(keras)

# Define a simple sequential model
model <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', input_shape = c(784)) %>%
  layer_dense(units = 10, activation = 'softmax')

# Compile the model, specifying the optimizer
model %>% compile(
  optimizer = 'adam', # Simply specify by name!
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

summary(model)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> By specifying <code>optimizer = 'adam'</code>, you are telling Keras to use this advanced algorithm to update all the model's weights and biases during training. This choice significantly impacts the speed and success of the model training process.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Start with Adam</h3>
    <div class="scenario-content">
        <p>When you're starting a new deep learning project, <strong>Adam</strong> is almost always the best first choice for an optimizer. It is robust, efficient, and generally requires less manual tuning of the learning rate compared to other optimizers. While other optimizers might achieve slightly better performance on specific tasks after extensive tuning, Adam provides a fantastic and reliable baseline.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise to test your understanding of the analogies.</p>
        <ul class="prose-list">
            <li>You are training a model and the loss is decreasing very, very slowly on a flat part of the loss surface. Which optimizer (SGD or Adam) would be better at speeding up progress here and why?</li>
            <li>You are training another model, and the loss is jumping up and down erratically and not converging. What is the most likely cause related to gradient descent? (Hint: See the Pro Tip in the Gradient Descent topic). Which optimizer (SGD or Adam) would be more stable in this situation?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(plotly)

# This is a conceptual visualization, so we will generate the paths manually
# to illustrate the characteristic behavior of each optimizer.

# Contour data for an elliptical valley
x_grid <- seq(-2, 2, length.out=50)
y_grid <- seq(-5, 5, length.out=50)
loss_func <- function(x, y) { x^2 + 0.1 * y^2 }
z_grid <- outer(x_grid, y_grid, loss_func)

# Paths
sgd_path <- list(x = c(-1.8, 1.8, -1.7, 1.7, -1.6, 1.6), y = c(4, 3, 2.8, 1.8, 1.6, 0.8))
momentum_path <- list(x = c(-1.8, 0.2, -0.4, 0.1, -0.1, 0), y = c(4, 2.5, 1.5, 0.8, 0.3, 0))
adam_path <- list(x = c(-1.8, -1, -0.4, -0.1, 0), y = c(4, 0.5, 0.2, 0.1, 0))

fig <- plot_ly() %>%
  add_contour(x = ~x_grid, y = ~y_grid, z = ~z_grid,
              colorscale = "Blues", reversescale=TRUE, showscale = FALSE,
              contours = list(coloring = 'lines')) %>%
  add_trace(x = ~sgd_path$x, y = ~sgd_path$y, type='scatter', mode='lines+markers', name='SGD', line=list(color='#a855f7')) %>%
  add_trace(x = ~momentum_path$x, y = ~momentum_path$y, type='scatter', mode='lines+markers', name='Momentum', line=list(color='#f59e0b')) %>%
  add_trace(x = ~adam_path$x, y = ~adam_path$y, type='scatter', mode='lines+markers', name='Adam', line=list(color='#10b981')) %>%
  layout(
    title="Comparing Optimizer Paths",
    xaxis=list(title="Parameter 1", zeroline=FALSE),
    yaxis=list(title="Parameter 2", zeroline=FALSE),
    legend=list(x=0.1, y=0.9)
  )

fig
</code></pre>
    </div>
</div>
`,
          p1s6ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist needs to create a series of functions to normalize data vectors. One function should perform z-score scaling, another should perform min-max scaling to a 0-1 range. Instead of writing two completely separate functions, they can write a single "scaler factory" that generates the correct, specialized scaling function on demand. This requires using closures.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s6ss1-concepts">Closures & Function Factories</button>
        <button class="tab-button" data-tab="p1s6ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s6ss1-usecase">Use Case in R</button>
    </div>

    <div id="p1s6ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Functions that Remember and Create</h4>
        <ul class="prose-list">
            <li><strong>Closure:</strong> A function that "remembers" the environment in which it was created. This means it has access to the variables that existed in its parent function, even after that parent function has finished running.</li>
            <li><strong>Function Factory:</strong> The most common application of closures. It's a function that you call not to get a value, but to get *another function*. The returned function is specialized based on the arguments you provided to the factory.</li>
        </ul>
        <p><strong>Analogy:</strong> A function factory is like a machine tool assembly line. You provide the factory with specifications (e.g., <code>method = "z-score"</code>). The factory uses these specs to build a custom, specialized tool (a z-score scaling function) and returns it to you. It can then build other different tools (like a min-max scaler) using the same factory process.</p>
    </div>

    <div id="p1s6ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Factory Process</h4>
        <p>This diagram shows how a single factory function can be used with different arguments to produce distinct, specialized functions, each with its own "enclosed" environment.</p>
        <div class="plot-container">
            <div id="p1s6ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The "Factory Function" on the left is the single piece of code you write. When you call it with the argument "z-score", it produces the specialized "Z-Score Scaler" function. This new function has "remembered" that its method is z-score. When called with "min-max", the same factory produces a different tool, the "Min-Max Scaler".</p>
        </div>
    </div>
    
    <div id="p1s6ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Building a Scaler Factory</h4>
        <p>Let's implement the data scaler factory. The outer function <code>create_scaler()</code> will return an inner function that does the actual work.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
create_scaler <- function(method = "z-score") {
  # This is the factory function. It takes the method as an argument.
  
  # This is the inner function that will be returned.
  # It has access to 'method' from its parent environment.
  scaler_function <- function(x) {
    if (method == "z-score") {
      # Z-score formula: (x - mean) / sd
      return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
    } else if (method == "min-max") {
      # Min-max formula: (x - min) / (max - min)
      return((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
    } else {
      stop("Unknown method.")
    }
  }
  
  return(scaler_function) # The factory returns the specialized function
}

# --- Use the factory to create our tools ---
z_scaler <- create_scaler(method = "z-score")
min_max_scaler <- create_scaler(method = "min-max")

# --- Use our new tools ---
my_data <- c(10, 20, 30, 40, 50)
z_scaler(my_data)
min_max_scaler(my_data)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use for Stateful Functions</h3>
    <div class="scenario-content">
        <p>Closures can also be used to create "stateful" functions that remember values from previous calls. A common example is creating a counter function. The factory creates the counter, and the returned function has access to a count variable in its enclosing environment that it can modify with each call, allowing it to remember its state.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a function factory called <code>create_power_function</code>.</li>
            <li>The factory should take one argument, <code>exponent</code>.</li>
            <li>It should return a function that takes a single argument, <code>base</code>, and calculates <code>base ^ exponent</code>.</li>
            <li>Use your factory to create two specialized functions: <code>square_it</code> (where <code>exponent = 2</code>) and <code>cube_it</code> (where <code>exponent = 3</code>).</li>
            <li>Test your new functions by running <code>square_it(5)</code> and <code>cube_it(5)</code>.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# This is a purely conceptual diagram
p <- ggplot() +
  # Factory
  annotate("rect", xmin = 0, xmax = 2, ymin = 3, ymax = 5, fill = "#3b82f6", alpha = 0.7) +
  annotate("text", x = 1, y = 4, label = "Factory\nFunction", color = "white", size=5) +
  # Products
  annotate("rect", xmin = 5, xmax = 7, ymin = 4.5, ymax = 6, fill = "#10b981", alpha = 0.7) +
  annotate("text", x = 6, y = 5.25, label = "Z-Score\nScaler", color = "white") +
  annotate("rect", xmin = 5, xmax = 7, ymin = 2, ymax = 3.5, fill = "#f59e0b", alpha = 0.7) +
  annotate("text", x = 6, y = 2.75, label = "Min-Max\nScaler", color = "white") +
  # Arrows
  geom_segment(aes(x = 2, y = 4.5, xend = 4.5, yend = 5.25), arrow = arrow(length = unit(0.3, "cm")), size = 1) +
  geom_segment(aes(x = 2, y = 3.5, xend = 4.5, yend = 2.75), arrow = arrow(length = unit(0.3, "cm")), size = 1) +
  # Arguments
  annotate("text", x = 3.25, y = 5.1, label = "Argument:\n'z-score'") +
  annotate("text", x = 3.25, y = 3, label = "Argument:\n'min-max'") +
  
  coord_equal(xlim = c(-0.5, 7.5), ylim = c(1, 7)) +
  theme_void()

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s6ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A programmer writes a function call: <code>my_plot(data = expensive_function())</code>. The <code>expensive_function()</code> has a bug and should cause an error. However, the code runs without crashing. The programmer then modifies <code>my_plot()</code> to actually use the <code>data</code> argument, and now it crashes as expected. This confusing behavior is a direct result of R's **lazy evaluation** model.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s6ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s6ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s6ss2-usecase">Use Case in R</button>
    </div>

    <div id="p1s6ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Don't Do Work Until You Have To</h4>
        <p><strong>Lazy evaluation</strong> is a core principle of R. It means that arguments passed to a function are not evaluated until the very moment they are needed inside that function's body. If an argument is never used, it is never evaluated.</p>
        <p>When you call a function, R doesn't immediately compute the value of each argument. Instead, it creates a "promise" for each one. A promise contains the expression you typed and the environment where it should be evaluated. Only when the code inside the function accesses the variable does R open up the promise and compute its value.</p>
        <p><strong>Analogy:</strong> Lazy evaluation is like a <strong>procrastinating assistant</strong>. You give them a to-do list (the function arguments). They don't start working on any task immediately. They only do a task (e.g., "calculate monthly sales") at the exact moment you ask for the final report that depends on that calculation. If you never ask for a report that needs a specific task, the assistant never does that task.</p>
    </div>

    <div id="p1s6ss2-viz" class="tab-pane">
        <h4 class="subsection-title">The Evaluation Path</h4>
        <p>This diagram illustrates the process. The arguments are passed as unevaluated "promises." The promise for <code>arg2</code> is never opened because its value is never requested inside the function body, so the error inside it is never triggered.</p>
        <div class="plot-container">
            <div id="p1s6ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The call to <code>my_func</code> passes two promises. Inside the function, only <code>arg1</code> is accessed, so its promise is evaluated, and the value <code>10</code> is used. The promise for <code>arg2</code>, which contains an error, is never accessed, so the code finishes successfully.</p>
        </div>
    </div>
    
    <div id="p1s6ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Demonstrating Laziness</h4>
        <p>Let's prove this behavior with a simple example that solves our initial scenario.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# This function only uses its first argument 'x'
lazy_func_1 <- function(x, y) {
  print("Inside lazy_func_1")
  print(x * 2)
}

# This function uses both arguments
lazy_func_2 <- function(x, y) {
  print("Inside lazy_func_2")
  print(x + y)
}

# --- Test 1: The unused argument is never evaluated ---
print("Running Test 1...")
# The stop() command is passed as 'y', but since 'y' is never used,
# the code runs without error!
lazy_func_1(x = 10, y = stop("This error will NOT be triggered!"))


# --- Test 2: Now the argument is evaluated ---
print("Running Test 2...")
# Here, 'y' is needed to perform the addition. R now evaluates the promise,
# runs stop(), and the entire script crashes with an error.
lazy_func_2(x = 10, y = stop("This error WILL be triggered!"))
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Forcing Evaluation with <code>force()</code></h3>
    <div class="scenario-content">
        <p>Occasionally, you might want to ensure a function argument is evaluated immediately, perhaps to catch errors early. You can do this by using the <code>force()</code> function. Simply calling <code>force(argument_name)</code> at the top of your function will cause R to evaluate that promise right away.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Write a function called <code>welcome_user</code> that takes two arguments: <code>user_name</code> and <code>current_time</code>.</li>
            <li>The function body should be: <code>if (!missing(user_name)) { print(paste("Hello,", user_name)) }</code>. The <code>missing()</code> function checks if an argument was provided.</li>
            <li>Call the function like this: <code>welcome_user(user_name = "Alice", current_time = Sys.time())</code>. This works.</li>
            <li>Now call it like this: <code>welcome_user(current_time = Sys.time())</code>. This also works.</li>
            <li>Why does the second call work without an error, even though you didn't provide a value for <code>user_name</code> which is used in the <code>paste()</code> function? (Hint: Think about when the <code>if</code> condition is checked).</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Purely conceptual diagram
p <- ggplot() +
  # Function Call
  annotate("rect", xmin=0, xmax=3, ymin=5, ymax=6, fill="#a855f7", alpha=0.7) +
  annotate("text", x=1.5, y=5.5, label="my_func(arg1=10, arg2=stop('ERR'))", color="white") +
  # Arrows to Promises
  geom_segment(aes(x=1, y=5, xend=1, yend=4), arrow = arrow(length = unit(0.2, "cm"))) +
  geom_segment(aes(x=2, y=5, xend=2, yend=4), arrow = arrow(length = unit(0.2, "cm"))) +
  # Promises
  annotate("rect", xmin=0.5, xmax=1.5, ymin=3, ymax=4, fill="#eab308", alpha=0.7) +
  annotate("text", x=1, y=3.5, label="Promise 1\n(expr = 10)") +
  annotate("rect", xmin=1.5, xmax=2.5, ymin=3, ymax=4, fill="#f43f5e", alpha=0.7) +
  annotate("text", x=2, y=3.5, label="Promise 2\n(expr = stop())") +
  # Function Body
  annotate("rect", xmin=4, xmax=7, ymin=2, ymax=6, fill="#3b82f6", alpha=0.7) +
  annotate("text", x=5.5, y=5, label="Function Body:\n\nreturn(arg1 * 2)", color="white", size=5) +
  # Arrow from Promise 1 to Body
  geom_curve(aes(x=1.5, y=3.5, xend=4, yend=4.5), curvature=-0.2, arrow = arrow(length = unit(0.2, "cm"))) +
  annotate("text", x=2.75, y=4.2, label="Accessed!") +
  # Arrow from Body to Output
  geom_segment(aes(x=7, y=4, xend=8, yend=4), arrow = arrow(length = unit(0.2, "cm"))) +
  # Output
  annotate("rect", xmin=8, xmax=9, ymin=3.5, ymax=4.5, fill="#10b981") +
  annotate("text", x=8.5, y=4, label="20", color="white", size=6) +
  # X over Promise 2
  annotate("text", x=2, y=2.5, label="Never Evaluated!", color="#f43f5e") +
  
  coord_equal(xlim=c(0,9.5), ylim=c(2,6.5)) +
  theme_void()

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s6ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst's complex R script is failing with an obscure error message that doesn't pinpoint the problem. Later, after fixing the bug, they find the script takes an hour to run. They need a set of tools to solve two distinct problems: 1) A way to pause the code and inspect its state to find the source of the error (<strong>Debugging</strong>). 2) A way to identify which specific lines of code are the slowest and causing the bottleneck (<strong>Profiling</strong>).</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s6ss3-debugging">Debugging: Finding Bugs</button>
        <button class="tab-button" data-tab="p1s6ss3-profiling">Profiling: Finding Bottlenecks</button>
        <button class="tab-button" data-tab="p1s6ss3-viz">Profiling Visualization</button>
    </div>

    <div id="p1s6ss3-debugging" class="tab-pane active">
        <h4 class="subsection-title">Pausing and Inspecting Your Code</h4>
        <p>When an error occurs, the most powerful tool is the interactive debugger. R's primary debugging tool is the <code>browser()</code> function.</p>
        <p><strong>Analogy:</strong> Placing <code>browser()</code> in your code is like setting a <strong>breakpoint</strong>. It's like pausing a movie. When R hits this line, it stops execution and gives you a special command prompt. You can then act like a detective: inspect the value of any object at that exact moment, run code line-by-line, and see exactly where things go wrong.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
buggy_function <- function(data) {
  # ... some complex code ...
  # I suspect the error is around here, so I'll set a breakpoint
  browser() 
  
  # The next line will cause an error if 'data' is not a number
  result <- log(data) 
  return(result)
}

# Call the function with bad data
buggy_function("this is not a number")
</code></pre>
        </div>
        <p>When you run this, your console will change to <code>Browse[1]></code>. You can then type commands like <code>ls()</code> to see objects, <code>print(data)</code> to inspect the variable, and <code>n</code> to execute the next line.</p>
    </div>

    <div id="p1s6ss3-profiling" class="tab-pane">
        <h4 class="subsection-title">Finding What's Slow</h4>
        <p>Your code works, but it's too slow. The first rule of optimization is: <strong>don't guess, measure</strong>. Profiling is the process of measuring which parts of your code are taking the most time and memory.</p>
        <p>The best tool for this in R is the <code>profvis</code> package, which provides an interactive visualization of your code's performance.</p>
        <p><strong>Analogy:</strong> Profiling is like a time-and-motion study on a factory assembly line. A consultant with a stopwatch (<code>profvis</code>) watches the entire process and creates a report showing that "Station 3: Painting" is taking 80% of the total time. This tells you exactly where to focus your efforts to speed up the line.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("profvis")
library(profvis)

# Wrap the code you want to profile in profvis()
profvis({
  # This part is fast
  x <- 1:1000
  y <- x^2
  
  # This part is deliberately slow (growing a vector in a loop)
  slow_vector <- c()
  for (i in 1:10000) {
    slow_vector <- c(slow_vector, i)
  }
  
  # This part is fast again
  plot(y)
})
</code></pre>
        </div>
    </div>

    <div id="p1s6ss3-viz" class="tab-pane">
        <h4 class="subsection-title">The Flame Graph</h4>
        <p><code>profvis</code> produces an interactive "flame graph." Time flows from left to right, and the call stack goes from bottom to top. Wider bars represent code that took longer to execute.</p>
        <div class="plot-container">
            <div id="p1s6ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This is a simplified representation of a flame graph. The wide red bar for the <code>for</code> loop immediately draws your attention. It shows that the vast majority of the execution time was spent inside this loop, specifically on the <code>c()</code> function used to grow the vector. This tells the analyst that vectorizing this loop is the single most important optimization they can make.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Using <code>traceback()</code> After an Error</h3>
    <div class="scenario-content">
        <p>If your script crashes and you didn't have <code>browser()</code> set up, you can run <code>traceback()</code> immediately after the error. It will print the "call stack"—the sequence of function calls that led to the error. This is a great "post-mortem" tool to help you figure out in which function the error occurred so you know where to place <code>browser()</code> for your next run.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's find a bug.</p>
        <ul class="prose-list">
            <li>Copy the <code>buggy_function</code> from the example into your script.</li>
            <li>Run <code>buggy_function(c(1, 4, 9, -5, 16))</code>.</li>
            <li>The code will pause in the browser. Type <code>data</code> and press Enter to inspect the input.</li>
            <li>Type <code>n</code> to execute the next line (<code>result <- log(data)</code>). What happens? (You'll get a warning because R can't take the log of a negative number).</li>
            <li>Type <code>Q</code> to exit the debugger. Now you know exactly where the bug is!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# This is a simplified, static representation of a profvis flame graph
# Data is conceptual to illustrate the point
prof_data <- data.frame(
  ymin = c(0, 1, 1),
  ymax = c(1, 2, 2),
  xmin = c(0, 10, 20),
  xmax = c(100, 20, 90),
  label = c("Total Runtime", "Fast Part", "Slow Loop"),
  fill = c("grey", "#10b981", "#ef4444")
)

p <- ggplot(prof_data, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill)) +
  geom_rect() +
  geom_text(aes(x = (xmin + xmax) / 2, y = (ymin + ymax) / 2, label = label), color = "white") +
  scale_fill_identity() +
  scale_x_continuous(name = "Time (ms)") +
  scale_y_continuous(name = "Call Stack", breaks = NULL) +
  theme_minimal() +
  labs(title = "Simplified Flame Graph")

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s6ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data simulation script uses a <code>for</code> loop to build a large data frame by adding one row at a time with <code>rbind()</code>. The script takes hours to run. The analyst needs to rewrite the code to be more idiomatic and efficient, reducing the runtime to mere minutes. This requires moving from a row-by-row approach to a vectorized or pre-allocated one.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s6ss4-concepts">Sources of Inefficiency</button>
        <button class="tab-button" data-tab="p1s6ss4-viz">Visualizing Performance Gains</button>
        <button class="tab-button" data-tab="p1s6ss4-usecase">Fixing the Slow Code</button>
    </div>

    <div id="p1s6ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Writing "R-like" Code</h4>
        <p>R is an interpreted language, so loops can be slow. High-performance R code almost always relies on two key principles:</p>
        <ul class="prose-list">
            <li><strong>Vectorization:</strong> As we've learned, applying an operation to a whole vector at once is much faster than looping through its elements, because the looping happens in R's fast, compiled (C/Fortran) internals.</li>
            <li><strong>Pre-allocation:</strong> The single most costly mistake is growing an object (like a vector or data frame) inside a loop. Each time you run <code>new_vector <- c(new_vector, new_value)</code>, R has to find a new block of memory and copy the *entire* object over. This is incredibly inefficient.</li>
        </ul>
        <p><strong>Analogy:</strong> Growing an object in a loop is like building a brick wall by making a separate trip to the hardware store for *every single brick*. <strong>Pre-allocation</strong> is like having the entire pallet of bricks delivered to your worksite at the start. You still have to place them one by one, but you avoid the thousands of costly trips to the store.</p>
    </div>

    <div id="p1s6ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Impact of Optimization</h4>
        <p>This bar chart compares the time taken to perform the same task using three different methods. The performance difference is not minor—it can be orders of magnitude.</p>
        <div class="plot-container">
            <div id="p1s6ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The y-axis is on a logarithmic scale, meaning each major grid line is 10 times larger than the one below it. The "Growing in Loop" method is thousands of times slower than the pre-allocated loop, which is itself slower than the fully vectorized solution. This visually demonstrates why avoiding loops that grow objects is the number one rule of performance optimization in R.</p>
        </div>
    </div>
    
    <div id="p1s6ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">From Hours to Seconds</h4>
        <p>Let's benchmark the "bad" way versus the "good" way of building a vector. We'll use <code>system.time()</code> to measure performance.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# Task: Create a vector of the first 100,000 squared numbers.
n <- 100000

# Method 1: The BAD way (growing in a loop)
system.time({
  result1 <- c()
  for (i in 1:n) {
    result1 <- c(result1, i^2)
  }
}) # This will be very slow!

# Method 2: The BETTER way (pre-allocating)
system.time({
  result2 <- numeric(n) # Pre-allocate a vector of the final size
  for (i in 1:n) {
    result2[i] <- i^2 # Fill in the values
  }
}) # Much faster!

# Method 3: The BEST way (vectorization)
system.time({
  result3 <- (1:n)^2
}) # Nearly instantaneous!
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use <code>data.table</code> for Big Data</h3>
    <div class="scenario-content">
        <p>When you are working with very large datasets (millions or billions of rows), even optimized <code>dplyr</code> code can become slow. The <code>data.table</code> package is a community-contributed package that provides an alternative, high-performance syntax for data frame manipulation. Its syntax is more concise and less readable at first, but for memory- and speed-critical operations, it is the undisputed champion in the R ecosystem.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Write a <code>for</code> loop that iterates from 1 to 10,000.</li>
            <li>Inside the loop, check if the number <code>i</code> is even (<code>i %% 2 == 0</code>).</li>
            <li>If it is even, add it to a vector called <code>even_numbers</code>. Use the "bad" growing method for this.</li>
            <li>Wrap your whole loop in <code>system.time()</code> to see how long it takes.</li>
            <li>Now, find a vectorized way to get the same result without a loop. (Hint: create the full sequence <code>1:10000</code> first, then use logical subsetting). Wrap this in <code>system.time()</code> and compare the performance.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Conceptual performance data (actual times will vary)
perf_data <- data.frame(
  Method = c("Growing in Loop", "Pre-allocated Loop", "Vectorized"),
  Time = c(3500, 25, 0.5) # Time in milliseconds
)

p <- ggplot(perf_data, aes(x = Method, y = Time, fill = Method)) +
  geom_bar(stat = "identity") +
  scale_y_log10(breaks = c(0.1, 1, 10, 100, 1000, 10000), labels = scales::comma) +
  labs(
    title = "Performance Comparison (Log Scale)",
    x = "Programming Method",
    y = "Execution Time (ms)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s6ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst writes a script to download and process 100 files from a web API. Occasionally, one of the files is corrupted or the network connection fails, causing the download function to error out. This single failure crashes the entire, long-running script. The analyst needs to make the code more robust so that it can handle an error for one file, log a message, and gracefully continue to the next one.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s6ss5-concepts">The <code>tryCatch()</code> Function</button>
        <button class="tab-button" data-tab="p1s6ss5-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s6ss5-usecase">Use Case in R</button>
    </div>

    <div id="p1s6ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Handling Unexpected Events</h4>
        <p>Robust code anticipates and handles errors gracefully. R's primary tool for this is the <code>tryCatch()</code> function. It allows you to "wrap" an expression that might fail and define specific actions to take if an error, warning, or other condition occurs.</p>
        <p><strong>Analogy:</strong> A <code>tryCatch()</code> block is like putting a <strong>safety net under a trapeze artist</strong>.
            <ul>
                <li>The main expression in <code>tryCatch({...})</code> is the artist's high-flying routine.</li>
                <li>The <code>error = function(e){...}</code> block is the safety net. If the artist slips (an error occurs), the net catches them, performs a specific action (e.g., the crowd applauds), and allows the show to go on.</li>
                <li>The <code>finally = {...}</code> block is like the artist taking a bow—it's an action that happens at the end, regardless of whether they completed the routine perfectly or fell into the net.</li>
            </ul>
        </p>
    </div>

    <div id="p1s6ss5-viz" class="tab-pane">
        <h4 class="subsection-title">Controlling the Execution Path</h4>
        <p>This flow diagram shows how <code>tryCatch()</code> alters the normal execution of code. Instead of crashing, an error diverts the program flow to the error-handling block, after which it can continue.</p>
        <div class="plot-container">
            <div id="p1s6ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> In the "Normal Flow," an error leads directly to a program crash. In the "Robust Flow," the code is wrapped in a <code>tryCatch</code> block. When the error occurs, execution is rerouted to the "Error Handler" block, which returns a safe value (like <code>NA</code>) and allows the program to continue to the "Next Step."</p>
        </div>
    </div>
    
    <div id="p1s6ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Making a Loop Bulletproof</h4>
        <p>Let's solve our analyst's problem. We'll create a list of inputs, one of which will cause an error. We'll loop through them, first the fragile way, then the robust way.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
inputs <- list(5, 10, "bad_input", 20, 25)
results <- list()

# --- Method 1: The FRAGILE way ---
# This loop will crash on the third element
for (i in 1:length(inputs)) {
  # results[[i]] <- log(inputs[[i]]) # Uncomment to see it crash
}

# --- Method 2: The ROBUST way with tryCatch() ---
for (i in 1:length(inputs)) {
  results[[i]] <- tryCatch({
      # --- The 'try' block: Code that might fail ---
      log(inputs[[i]])
    }, 
    error = function(e) {
      # --- The 'error' block: What to do if an error occurs ---
      # 'e' is an object containing the error message
      message(paste("Error processing input #", i, ":", e$message))
      return(NA) # Return a safe default value
    })
}

print(results)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: <code>try()</code> vs. <code>tryCatch()</code></h3>
    <div class="scenario-content">
        <p>R has a simpler function, <code>try()</code>, which is a shortcut for a basic <code>tryCatch()</code>. If you just want to suppress an error and continue, you can wrap your code in <code>try(..., silent=TRUE)</code>. However, <code>tryCatch()</code> is much more powerful and flexible because it allows you to define custom logic, log the specific error message, and handle different types of conditions like warnings.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a list of file names: <code>files <- c("a.csv", "b.csv", "c.csv")</code>.</li>
            <li>Write a function <code>read_file_safely(path)</code> that takes a file path as an argument.</li>
            <li>Inside the function, use <code>tryCatch()</code> to wrap a call to <code>read.csv(path)</code>.</li>
            <li>The <code>error</code> handler should print a message like "File not found!" and return <code>NULL</code>.</li>
            <li>Use <code>lapply()</code> to apply your <code>read_file_safely</code> function to the <code>files</code> list. Since none of these files exist, you should see your custom error message three times, and the final result will be a list of <code>NULL</code>s instead of a crash.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s6ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A junior data scientist presents their code for review. The code *works*—it produces the correct result. However, the variable names are cryptic (e.g., <code>x</code>, <code>dat2</code>, <code>tmp</code>), there are no comments, and the formatting is inconsistent. A senior data scientist explains that for code to be useful and trustworthy in a team environment, it's not enough for it to work; it must also be clean, readable, and maintainable.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s6ss6-concepts">Core Principles</button>
        <button class="tab-button" data-tab="p1s6ss6-comparison">Code Comparison</button>
    </div>

    <div id="p1s6ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">Writing Code for Humans</h4>
        <p>Code craftsmanship is the practice of writing code that is not only functional but also elegant, clear, and easy for other people (including your future self) to understand. It treats programming as a craft that values quality and maintainability.</p>
        <p><strong>Analogy:</strong> It's the difference between a rickety, slapped-together shed and a well-designed, professionally built house. Both might keep the rain out today, but only one is easy to live in, modify, and trust for years to come.</p>
        <ul class="prose-list">
            <li><strong>Use Meaningful Names:</strong> Variable and function names should be descriptive. <code>daily_sales_usd</code> is infinitely better than <code>x</code>.</li>
            <li><strong>Comment the "Why," Not the "What":</strong> Your code should be clear enough that it explains *what* it's doing. Comments should be reserved for explaining *why* a particular approach was taken or clarifying a complex business rule.</li>
            <li><strong>Be Consistent:</strong> Adopt a style guide (like the <a href="https://style.tidyverse.org/" target="_blank">Tidyverse style guide</a>) for formatting, naming, and syntax. Consistency makes code predictable and easier to read.</li>
            <li><strong>Don't Repeat Yourself (DRY):</strong> If you find yourself copying and pasting the same block of code, it's time to turn it into a reusable function.</li>
            <li><strong>Write Small, Pure Functions:</strong> Each function should do one thing and do it well. Small, focused functions are easier to test, debug, and understand than one giant, monolithic script.</li>
        </ul>
    </div>

    <div id="p1s6ss6-comparison" class="tab-pane">
        <h4 class="subsection-title">Before and After Refactoring</h4>
        <p>Let's look at a simple task—calculating discounted prices and filtering—written two ways.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Bad Code (Works, but is Unclear)</strong>
                <div class="code-container">
                    <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
                    <pre><code class="language-r">
# process data
tmp <- my_data
tmp$val2 <- tmp$val * 0.8 # apply discount
final_dat <- tmp[tmp$val2 > 50,]
</code></pre>
                </div>
            </div>
            <div class="decision-branch">
                <strong>Good Code (Clean and Maintainable)</strong>
                <div class="code-container">
                    <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
                    <pre><code class="language-r">
# This script calculates final prices and filters for high-value items.
# A standard 20% discount is applied to all initial prices.

# Define constants at the top
DISCOUNT_RATE <- 0.20
MINIMUM_PRICE_USD <- 50

# Use descriptive names
price_data_with_discount <- raw_price_data
price_data_with_discount$final_price_usd <- price_data_with_discount$initial_price_usd * (1 - DISCOUNT_RATE)

high_value_items <- price_data_with_discount[price_data_with_discount$final_price_usd > MINIMUM_PRICE_USD, ]
</code></pre>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use a Linter</h3>
    <div class="scenario-content">
        <p>A "linter" is a tool that automatically analyzes your code and flags potential errors and stylistic problems. The <code>lintr</code> package in R can be integrated with RStudio to provide real-time feedback on your code, highlighting inconsistent spacing, long lines, and other violations of your chosen style guide. It's like having an automated proofreader for your code, helping you build good habits.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Refactor a small piece of code.</p>
        <ul class="prose-list">
            <li>Here is a poorly written block of code:
                <div class="code-container">
                    <pre><code class="language-r">
data <- mtcars$wt
data2 <- data * 1000
m <- mean(data2)
s <- sd(data2)
print((data2 - m) / s)
</code></pre>
                </div>
            </li>
            <li>Rewrite this code following the principles of good code craftsmanship. Use clear variable names (e.g., <code>car_weight_kg</code>), add a comment explaining what the code's purpose is, and maybe even wrap it in a function called <code>calculate_z_scores</code>.</li>
        </ul>
    </div>
</div>
`,
          p1s7ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is working on two separate projects: a sales analysis and a customer churn model. They keep getting errors because a variable named <code>final_data</code> from the sales project is interfering with a different variable also named <code>final_data</code> in the churn project. They need a way to keep these projects completely isolated from each other.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s7ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s7ss1-usecase">How to Use Projects</button>
    </div>

    <div id="p1s7ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">A Self-Contained Workspace</h4>
        <p>An <strong>RStudio Project</strong> is the single most important organizational tool for any R user. It is represented by an <code>.Rproj</code> file. A project isolates all the files associated with a single analysis—data, code, reports, and plots—into one self-contained directory.</p>
        <p><strong>Analogy:</strong> An R Project is like a dedicated, self-contained <strong>lab bench</strong> for a single experiment. All your tools (code), samples (data), and notebooks (reports) for that experiment are kept on one bench. This prevents them from getting mixed up with the different experiment you're running on the next bench over. When you want to work on a project, you go to that bench by opening the <code>.Rproj</code> file.</p>
        <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1rem;">Key Benefits:</h5>
        <ul class="prose-list">
            <li><strong>Automatic Working Directory:</strong> It automatically sets the working directory to the project's root folder. This eliminates the need for <code>setwd()</code> and makes file paths simple and portable.</li>
            <li><strong>Clean Workspace:</strong> Each project maintains its own environment, preventing variable conflicts between analyses.</li>
            <li><strong>Portability:</strong> You can zip the entire project folder and send it to a colleague or move it to another computer, and all file paths (like <code>read.csv("data/my_data.csv")</code>) will work without any changes.</li>
        </ul>
    </div>
    
    <div id="p1s7ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Disorganized vs. Project-Based Workflow</h4>
        <p>This diagram shows the chaos of a disorganized folder compared to the clarity of using separate RStudio Projects for each analysis.</p>
        <div class="plot-container">
            <div id="p1s7ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The left side shows a common but messy setup where all files are mixed together, leading to confusion and broken file paths. The right side shows the clean, organized structure that RStudio Projects enable, where each analysis is a self-contained unit.</p>
        </div>
    </div>

    <div id="p1s7ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Creating and Using a Project</h4>
        <ol class="prose-list">
            <li><strong>Create a Project:</strong> In RStudio, go to <code>File > New Project... > New Directory > New Project</code>. Give your project a name (e.g., <code>sales_analysis</code>) and choose a location to store it.</li>
            <li><strong>Organize Files:</strong> Inside your new project folder, create subdirectories like <code>data/</code>, <code>R/</code>, and <code>reports/</code> to keep things tidy.</li>
            <li><strong>Use Relative Paths:</strong> Because the working directory is always the project's root, you can write clean, portable code to read your data.
                <div class="code-container">
                    <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
                    <pre><code class="language-r">
# This code is robust and portable because it's in a project.
# It looks for a 'data' folder inside the project's main directory.
sales_data <- read.csv("data/quarterly_sales.csv")
                    </code></pre>
                </div>
            </li>
            <li><strong>Close and Re-open:</strong> Always start your work by opening the <code>.Rproj</code> file. RStudio will restore your session exactly as you left it for that specific project.</li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Always Be in a Project</h3>
    <div class="scenario-content">
        <p>Make it a habit to create an RStudio Project for *everything*, even small, exploratory analyses. The tiny amount of time it takes to set up a project at the beginning will save you hours of confusion and debugging later on. It is the foundation of a reproducible workflow.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a new RStudio Project on your Desktop called <code>my_test_project</code>.</li>
            <li>Inside the project, using the RStudio "Files" pane, create a new folder named <code>data</code>.</li>
            <li>Create a new R script. In that script, write the line <code>write.csv(mtcars, file = "data/car_data.csv")</code>.</li>
            <li>Save the script as <code>save_data.R</code> in the main project directory and run it.</li>
            <li>Verify that the <code>car_data.csv</code> file appeared inside your <code>data</code> folder. You have just successfully used a relative path within a project!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s7ss2:
            `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A manager requests an updated sales report every Monday. Currently, the analyst reruns their R script, manually copies the summary numbers, saves each plot as a PNG, and pastes everything into a Word document. This process is tedious, time-consuming, and prone to copy-paste errors. If the raw data changes, the entire manual process must be repeated. <strong>Dynamic documents</strong> automate this entire workflow.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s7ss2-viz">The Workflow</button>
        <button class="tab-button" data-tab="p1s7ss2-usecase">A Simple Example</button>
    </div>

    <div id="p1s7ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Code, Text, and Output in One Place</h4>
        <p>A dynamic document, written in formats like <strong>R Markdown</strong> (<code>.Rmd</code>) or <strong>Quarto</strong> (<code>.qmd</code>), is a file that weaves together three components:</p>
        <ul class="prose-list">
            <li><strong>Narrative Text:</strong> Your explanations and analysis, written in plain Markdown.</li>
            <li><strong>Code Chunks:</strong> Live, executable chunks of R code that perform the analysis.</li>
            <li><strong>Output:</strong> The results of the code (plots, tables, summaries) which are embedded directly into the final document.</li>
        </ul>
        <p><strong>Analogy:</strong> A dynamic document is like a <strong>baking recipe that bakes its own cake and puts it on the page</strong>. The document contains the instructions (your code) and a reference to the ingredients (your data). When you click the "Knit" (or "Render") button, R executes the recipe and embeds the freshly baked cakes (your plots and results) directly into the final, beautifully formatted document (e.g., HTML, PDF, or Word).</p>
    </div>

    <div id="p1s7ss2-viz" class="tab-pane">
        <h4 class="subsection-title">From Source to Report</h4>
        <p>This diagram shows the process. You, the author, only ever edit the source <code>.Rmd</code> file. The final report is a reproducible artifact generated from that single source of truth.</p>
        <div class="plot-container">
            <div id="p1s7ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The process starts with your data and your R Markdown file, which contains both text and code. The "Knit" process in RStudio executes the code, captures the output (like plots and tables), and combines it with your text to produce a polished, standalone report that can be shared with anyone.</p>
        </div>
    </div>
    
    <div id="p1s7ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">A Basic R Markdown Document</h4>
        <p>Below is a simple but complete R Markdown document. You can create one in RStudio via <code>File > New File > R Markdown...</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>` +
            '<pre><code class="language-markdown">' +
            `---
title: "My First Dynamic Report"
author: "Data Scientist"
date: "2025-09-18"
output: html_document
---

## Introduction

This is an R Markdown document. We can write normal text here using Markdown formatting.

Now, we will include a chunk of R code to analyze the built-in <code>cars</code> dataset.

` +
            "``" +
            "`{r car-analysis}\n" +
            `# This is a code chunk. R will execute this code.
summary(cars)

# We can also create plots, which will be embedded in the final report.
plot(cars, main = "Speed vs. Stopping Distance")
` +
            "``" +
            "`" +
            `

## Conclusion

As we can see from the plot above, there is a clear positive relationship between a car's speed and its stopping distance. The summary table provides the key descriptive statistics.
` +
            "</code></pre>" +
            `</div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Quarto is the Future</h3>
    <div class="scenario-content">
        <p>While R Markdown is fantastic and widely used, its successor, <strong>Quarto</strong>, is the future of scientific and technical publishing. Quarto offers all the features of R Markdown but is language-agnostic (it works natively with R, Python, Julia, and Observable JS) and has many new features for creating books, websites, presentations, and more from a single source. If you're starting new projects today, it's highly recommended to start with Quarto.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>In RStudio, create a new R Markdown document (<code>File > New File > R Markdown...</code>).</li>
            <li>Give it a title and author, and choose HTML as the default output.</li>
            <li>Delete the default template content below the YAML header (the part between the <code>---</code>).</li>
            <li>Write a sentence of your own, like "This report analyzes the iris dataset."</li>
            <li>Insert a new R code chunk (<code>Ctrl+Alt+I</code> or <code>Cmd+Option+I</code>).</li>
            <li>Inside the chunk, type <code>summary(iris)</code>.</li>
            <li>Click the "Knit" button at the top of the script editor and observe how your text and the summary table are combined in the final HTML file.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s7ss3:
            `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is exploring a new dataset. Their R script is just a long series of commands with a few comments. When they revisit the script a month later to answer a follow-up question, they can't remember their original thought process, why they made certain data cleaning decisions, or what their initial hypotheses were. Their analysis is not a coherent narrative, just a list of instructions.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss3-concepts">The Core Philosophy</button>
        <button class="tab-button" data-tab="p1s7ss3-comparison">Comparison: Script vs. Notebook</button>
    </div>

    <div id="p1s7ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Programming as an Act of Explanation</h4>
        <p><strong>Literate Programming</strong> is a philosophy introduced by Donald Knuth. It argues that a program should be written as an essay for human beings, explaining its logic in natural language, with executable code snippets included as illustrations. The focus is on the *explanation* of the thought process, not just the final code.</p>
        <p><strong>R Notebooks</strong> are RStudio's interactive implementation of this philosophy. They allow you to work in an environment where you can easily interweave text, code, and output, creating a "lab notebook" that documents your analysis as you perform it.</p>
        <p><strong>Analogy:</strong> A traditional script is like a list of cooking instructions from a machine. A literate programming notebook is like a page from a <strong>chef's personal journal</strong>. It includes the recipe (the code), but also their notes on why they chose certain ingredients, observations they made during the process, and their thoughts on the final dish (the interpretation of the results).</p>
    </div>
    
    <div id="p1s7ss3-comparison" class="tab-pane">
        <h4 class="subsection-title">Two Ways of Working</h4>
        <p>This comparison shows how the same simple analysis can be represented in a standard R script versus an R Notebook. The notebook format encourages a narrative and captures the analytical thought process.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Traditional R Script (<code>.R</code>)</strong>
                <p>Focused purely on the code. Explanations are limited to comments. Output appears separately in the console or plots pane.</p>
                <div class="code-container">
                    <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
                    <pre><code class="language-r">
# Load libraries
library(dplyr)
library(ggplot2)

# Filter data
versicolor_data <- filter(iris, Species == "versicolor")

# Plot
ggplot(versicolor_data, aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point()

# Model
lm(Sepal.Width ~ Sepal.Length, data = versicolor_data)
</code></pre>
                </div>
            </div>
            <div class="decision-branch">
                <strong>R Notebook (<code>.Rmd</code>)</strong>
                <p>Interweaves formatted text with executable code chunks. Output is displayed inline, directly beneath the code that generated it, creating a logical, readable flow.</p>
                <div class="code-container">` +
            '<pre><code class="language-markdown">' +
            `### Analysis of Versicolor Species

First, we load the necessary libraries and filter the <code>iris</code> dataset to keep only the <code>versicolor</code> species.

` +
            "``" +
            "`{r setup-and-filter}\n" +
            `library(dplyr)
library(ggplot2)
versicolor_data <- filter(iris, Species == "versicolor")
` +
            "``" +
            "`" +
            `

Next, let's visualize the relationship between sepal length and width for this species.

` +
            "``" +
            "`{r plot-versicolor}\n" +
            `ggplot(versicolor_data, aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point()
` +
            "``" +
            "`" +
            `

The plot shows a slight positive relationship. We can quantify this by fitting a simple linear model.

` +
            "``" +
            "`{r model-versicolor}\n" +
            `lm(Sepal.Width ~ Sepal.Length, data = versicolor_data)
` +
            "``" +
            "`" +
            "</code></pre>" +
            `</div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Inline Code for Truly Dynamic Text</h3>
    <div class="scenario-content">
        <p>R Markdown and Notebooks allow you to insert small snippets of R code directly into your text. This is done by enclosing the code in backticks with an <code>r</code> at the start, like this: <code>` +
            "`r R_CODE`" +
            `</code>. This is incredibly powerful for making your narrative truly dynamic. For example, you can write: "The average sepal length was <code>` +
            "`r mean(iris$Sepal.Length)`" +
            `</code> cm." If your data changes, the number in your final rendered sentence will update automatically.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>In RStudio, create a new R Notebook (<code>File > New File > R Notebook</code>).</li>
            <li>In the first text chunk, write a sentence describing the <code>mtcars</code> dataset.</li>
            <li>In a code chunk, calculate the average miles per gallon (<code>mpg</code>) for cars with 4 cylinders versus 8 cylinders. (Hint: use <code>group_by(cyl)</code> and <code>summarise(avg_mpg = mean(mpg))</code>).</li>
            <li>In a new text chunk below, use inline R code to report the average MPG for 4-cylinder cars directly in your sentence. For example: "The average MPG for 4-cylinder cars was <code>` +
            "`r # YOUR CODE HERE`" +
            `</code>."</li>
        </ul>
    </div>
</div>
`,
          p1s7ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A complex analysis involves multiple, dependent R scripts: <code>01_download_data.R</code>, <code>02_clean_data.R</code>, <code>03_train_model.R</code>, and <code>04_generate_report.Rmd</code>. Running these manually is tedious. More importantly, if a raw data file from step 1 changes, the analyst must remember to manually re-run all subsequent steps. A <strong>pipeline tool</strong> can automate this entire workflow and intelligently re-run only the necessary parts.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s7ss4-viz">Visualizing the Pipeline</button>
        <button class="tab-button" data-tab="p1s7ss4-usecase">The <code>targets</code> Package</button>
    </div>

    <div id="p1s7ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Managing Dependencies</h4>
        <p>An automation pipeline tool views your entire analysis as a network of dependencies. Each step (a "target") is an object that depends on other objects or files. The tool builds a <strong>Directed Acyclic Graph (DAG)</strong> to map these relationships.</p>
        <p>When you run the pipeline, the tool checks which targets are "out of date"—meaning their dependencies (code or data) have changed since the last run. It then intelligently runs *only* the targets that are out of date, and any downstream targets that depend on them.</p>
        <p><strong>Analogy:</strong> A pipeline tool is like a <strong>smart general contractor building a house</strong>. They have a blueprint (the pipeline plan) of all dependencies (the foundation must be laid before the walls go up; the plumbing must be installed before the drywall). If you decide to change the kitchen faucet (one piece of "code"), the contractor knows they don't need to rebuild the entire foundation. They only need to re-do the specific plumbing work affected by that change.</p>
    </div>

    <div id="p1s7ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Analysis as a Graph</h4>
        <p>This diagram shows a simple analysis workflow visualized as a dependency graph. An arrow from A to B means that B depends on A. If <code>clean_data.R</code> changes, the pipeline knows it must re-run the <code>clean_data</code>, <code>model</code>, and <code>report</code> targets, but it can skip re-running the <code>raw_data</code> target.</p>
        <div class="plot-container">
            <div id="p1s7ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This graph shows the flow of dependencies. The final report depends on the model and a plot. The model depends on the cleaned data. The cleaned data depends on the raw data. This map of relationships allows the pipeline tool to make smart decisions about what to re-run when something changes.</p>
        </div>
    </div>
    
    <div id="p1s7ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">A Simple <code>_targets.R</code> File</h4>
        <p>The <code>targets</code> package is the modern, standard choice for building pipelines in R. You define your entire workflow in a single script, typically named <code>_targets.R</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# Load the necessary packages
library(targets)

# Load user-defined functions (best practice)
source("R/functions.R") 

# Define the pipeline
list(
  # Target 1: A file path to the raw data
  tar_target(
    name = raw_data_file,
    command = "data/raw_data.csv",
    format = "file" 
  ),
  # Target 2: A clean data frame, depends on the file from step 1
  tar_target(
    name = clean_data,
    command = process_data(raw_data_file)
  ),
  # Target 3: A plot, which depends on the clean data
  tar_target(
    name = my_plot,
    command = create_plot(clean_data)
  )
)
</code></pre>
        </div>
        <p>To run the pipeline, you simply open R and run <code>tar_make()</code> in the console. To visualize the dependency graph, you run <code>tar_visnetwork()</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Functions over Scripts</h3>
    <div class="scenario-content">
        <p>The best practice for building <code>targets</code> pipelines is to put all your actual analysis code into functions within a separate <code>R/</code> folder. The <code>_targets.R</code> file should only be used for "calling the plays"—connecting these functions together to define the pipeline steps. This keeps your pipeline definition clean and your analysis code modular and reusable.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise to think about dependencies.</p>
        <ul class="prose-list">
            <li>Imagine an analysis with these steps: <code>download_data</code>, <code>clean_data</code>, <code>model_A</code>, <code>model_B</code>, <code>compare_models</code>.</li>
            <li>Draw out the dependency graph for this pipeline. Which steps depend on which other steps?</li>
            <li>If you change the code for <code>model_A</code>, which steps need to be re-run by the pipeline tool?</li>
            <li>If you change the code for <code>clean_data</code>, which steps need to be re-run?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s7ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist writes a brilliant but complex function to calculate a custom metric. A colleague on their team wants to use it, but has no idea what arguments it takes, what data type each argument should be, or what the function returns. The function is effectively unusable without proper documentation.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss5-concepts">Levels of Documentation</button>
        <button class="tab-button" data-tab="p1s7ss5-usecase">Function Documentation with <code>roxygen2</code></button>
    </div>

    <div id="p1s7ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Explaining Your Work</h4>
        <p>Good documentation is essential for collaboration, reproducibility, and your own future sanity. There are several levels of documentation for an R project:</p>
        <ul class="prose-list">
            <li><strong>Comments:</strong> Inline notes to explain the "why" of a specific line of code.</li>
            <li><strong>Function Documentation:</strong> A structured explanation of what a function does, what its arguments are, and what it returns. The standard for this in R is <code>roxygen2</code>.</li>
            <li><strong><code>README.md</code>:</strong> A file in your project's root directory that gives a high-level overview of the project: its purpose, how to install and run the analysis, and where to find key files.</li>
            <li><strong>Vignettes:</strong> Long-form documents that act as a tutorial or guide for a package, showing how to use its functions together to solve a real problem.</li>
        </ul>
        <p><strong>Analogy:</strong> Think of an appliance. Inline comments are the small warning labels. The <strong><code>roxygen2</code> documentation</strong> is the technical manual that describes what each button and knob does. The <strong><code>README</code></strong> is the "Quick Start" guide. A <strong>vignette</strong> is a recipe book showing you how to use the appliance to cook full meals.</p>
    </div>
    
    <div id="p1s7ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">The <code>roxygen2</code> Standard</h4>
        <p><code>roxygen2</code> is a system that allows you to write documentation in special comments directly above your function definition. RStudio can then process these comments to automatically create the official R help files (the ones you see when you type <code>?function_name</code>).</p>
        <p>A <code>roxygen2</code> block starts with <code>#'</code>. Special tags starting with <code>@</code> are used to define different parts of the documentation.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Function Without Documentation</strong>
                <div class="code-container">
                    <pre><code class="language-r">
calc_bmi <- function(w, h) {
  if (!is.numeric(w) || !is.numeric(h)) {
    stop("Inputs must be numeric.")
  }
  return(w / h^2)
}
</code></pre>
                </div>
            </div>
            <div class="decision-branch">
                <strong>Same Function With <code>roxygen2</code> Docs</strong>
                 <div class="code-container">
                    <pre><code class="language-r">
#' Calculate Body Mass Index (BMI)
#'
#' This function calculates BMI based on a person's
#' weight in kilograms and height in meters.
#'
#' @param w A numeric value for weight in kilograms.
#' @param h A numeric value for height in meters.
#'
#' @return A numeric value representing the BMI.
#' @export
#'
#' @examples
#' calc_bmi(w = 75, h = 1.8)
calc_bmi <- function(w, h) {
  if (!is.numeric(w) || !is.numeric(h)) {
    stop("Inputs must be numeric.")
  }
  return(w / h^2)
}
</code></pre>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: RStudio Integration</h3>
    <div class="scenario-content">
        <p>RStudio has excellent integration with <code>roxygen2</code>. After writing a function, you can place your cursor inside it and go to <code>Code > Insert Roxygen Skeleton</code>. RStudio will automatically insert a template with <code>@param</code> tags for each of your function's arguments, saving you time and ensuring you don't miss any.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Write a simple function called <code>convert_fahr_to_celsius</code> that takes one argument, <code>temp_f</code>, and returns the temperature in Celsius. The formula is <code>(temp_f - 32) * 5 / 9</code>.</li>
            <li>Use the RStudio shortcut (<code>Code > Insert Roxygen Skeleton</code>) or manually write a <code>roxygen2</code> documentation block above your function.</li>
            <li>Fill in the title, description, the <code>@param temp_f</code> tag, the <code>@return</code> tag, and an <code>@examples</code> tag.</li>
        </ul>
    </div>
</div>
`,
          p1s7ss6:
            `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst presents their findings to a leadership team using a static PDF report with dense tables and standard plots. The executives have trouble grasping the key insights and want to explore the data themselves by asking "what if" questions (e.g., "What would these sales figures look like if we only consider the North region?"). An interactive dashboard would be a much more effective tool for this audience.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss6-concepts">Static vs. Interactive</button>
        <button class="tab-button" data-tab="p1s7ss6-viz">Anatomy of a Dashboard</button>
        <button class="tab-button" data-tab="p1s7ss6-usecase">Dashboarding with <code>flexdashboard</code></button>
    </div>

    <div id="p1s7ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">Empowering Your Audience</h4>
        <p>Professional reporting is about tailoring the presentation of your results to your audience. A technical report for fellow data scientists can be dense and detailed. A report for leadership should be high-level, visual, and ideally, interactive.</p>
        <p><strong>Analogy:</strong> A static report (like a PDF) is a <strong>photograph</strong> of your data. It presents one fixed view. An interactive dashboard is a <strong>video game</strong> based on your data. It allows the user to interact with the environment, turn knobs, press buttons, and see the world change in real-time.</p>
        <p>R has two primary frameworks for this:</p>
        <ul class="prose-list">
            <li><strong><code>flexdashboard</code>:</strong> An R Markdown-based package for creating simple, grid-based layouts. It's excellent for turning an existing R Markdown analysis into a clean, professional dashboard with minimal extra code.</li>
            <li><strong><code>shiny</code>:</strong> A powerful framework for building full, complex, and highly customized interactive web applications directly from R. It requires more setup but offers unlimited flexibility.</li>
        </ul>
    </div>

    <div id="p1s7ss6-viz" class="tab-pane">
        <h4 class="subsection-title">Common Dashboard Components</h4>
        <p>This diagram shows the typical layout of a professional dashboard, with interactive controls on the side and reactive outputs in the main panel.</p>
        <div class="plot-container">
            <div id="p1s7ss6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The key components are the <strong>input controls</strong> (like sliders and dropdowns) in the sidebar and the <strong>outputs</strong> (like plots and value boxes) in the main area. The magic of a dashboard is that changing an input control automatically re-runs the necessary R code and updates the outputs in real-time.</p>
        </div>
    </div>
    
    <div id="p1s7ss6-usecase" class="tab-pane">
        <h4 class="subsection-title">A Simple <code>flexdashboard</code> Example</h4>
        <p>You can create this dashboard from a single R Markdown file. The layout is controlled by Markdown headers.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>` +
            '<pre><code class="language-markdown">' +
            `---
title: "Iris Species Dashboard"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
---

` +
            "``" +
            "`{r setup, include=FALSE}\n" +
            `library(flexdashboard)
library(ggplot2)
library(plotly)
` +
            "``" +
            "`" +
            `

Column {data-width=350}
-----------------------------------------------------------------------

### Controls

` +
            "``" +
            "`{r}\n" +
            `# Shiny components make it interactive
shiny::sliderInput("petal_length_min", 
                   label = "Minimum Petal Length:",
                   min = min(iris$Petal.Length), 
                   max = max(iris$Petal.Length), 
                   value = min(iris$Petal.Length))
` +
            "``" +
            "`" +
            `

Column {data-width=650}
-----------------------------------------------------------------------

### Sepal Dimensions Plot

` +
            "``" +
            "`{r}\n" +
            `# Use shiny::renderPlotly for reactive plots
shiny::renderPlotly({
  
  # Filter the data based on the slider's input value
  filtered_iris <- iris[iris$Petal.Length >= input$petal_length_min, ]
  
  p <- ggplot(filtered_iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
    geom_point() +
    theme_minimal()
  
  ggplotly(p)
})
` +
            "``" +
            "`" +
            "</code></pre>" +
            `</div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use <code>bslib</code> for Theming</h3>
    <div class="scenario-content">
        <p>You can easily customize the look and feel of your Shiny apps and <code>flexdashboard</code> dashboards using the <code>bslib</code> package. It allows you to apply modern Bootstrap themes (like a dark mode) and change fonts and colors with just a few lines of code in your YAML header, making your reports look highly professional.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>In RStudio, install <code>flexdashboard</code> if you haven't already.</li>
            <li>Create a new R Markdown file and paste in the example code from the "Use Case" tab.</li>
            <li>Instead of clicking "Knit," click the small dropdown arrow next to it and select "Run Document." This will start a live Shiny session.</li>
            <li>Play with the slider. Watch how the plot updates in real time as you change the minimum petal length. You've just created your first interactive report!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Dashboard Layout Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s7ss7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> Two analysts are working on the same script. Analyst A makes a critical change. Analyst B, unaware, makes a different change to their own older copy of the file and saves it, completely overwriting Analyst A's work. They have no record of the previous version and no easy way to merge their changes. This common problem is solved by <strong>version control</strong>.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss7-concepts">Git & GitHub</button>
        <button class="tab-button" data-tab="p1s7ss7-viz">The Workflow</button>
        <button class="tab-button" data-tab="p1s7ss7-usecase">Using Git in RStudio</button>
    </div>

    <div id="p1s7ss7-concepts" class="tab-pane active">
        <h4 class="subsection-title">Tracking Changes Over Time</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Git</strong>
                <p>A free, open-source version control system. It's software that runs on your local machine and tracks every change you make to a folder of files (a "repository").</p>
                <p><strong>Analogy:</strong> Git is like an infinite "undo" button for your entire project, combined with a detailed logbook. It lets you take "snapshots" (commits) of your project at any time and easily revert back to any previous snapshot.</p>
            </div>
            <div class="decision-branch">
                <strong>GitHub</strong>
                <p>A web-based hosting service for Git repositories. It provides a central, remote location to store your project's history and includes powerful tools for collaboration.</p>
                <p><strong>Analogy:</strong> GitHub is like <strong>Google Docs for your code</strong>. It's the central place where everyone on your team can see the latest version of the project, discuss changes, and safely merge their work together.</p>
            </div>
        </div>
    </div>

    <div id="p1s7ss7-viz" class="tab-pane">
        <h4 class="subsection-title">Local vs. Remote</h4>
        <p>The core workflow involves synchronizing the project on your local computer with the central copy on GitHub.</p>
        <div class="plot-container">
            <div id="p1s7ss7-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> You do your work on your <strong>Local Computer</strong>. After making a set of changes, you <strong>commit</strong> them to your local Git history. To share these changes with your team, you <strong>push</strong> your commits to the central <strong>GitHub Repository</strong>. To get the latest changes from your teammates, you <strong>pull</strong> their commits from GitHub to your local machine.</p>
        </div>
    </div>
    
    <div id="p1s7ss7-usecase" class="tab-pane">
        <h4 class="subsection-title">The Daily Workflow in RStudio</h4>
        <p>RStudio has a built-in "Git" pane that makes version control seamless. A typical workflow looks like this:</p>
        <ol class="prose-list">
            <li><strong>Start of Day: Pull.</strong> Click the blue "Pull" arrow in the Git pane to get the latest changes from your team from GitHub.</li>
            <li><strong>Do Your Work:</strong> Edit your scripts, run analyses, and save your files as usual. As you save, you'll see your modified files appear in the Git pane.</li>
            <li><strong>Stage Your Changes:</strong> Check the "Staged" box next to the files whose changes you want to include in your next snapshot.</li>
            <li><strong>Commit Your Changes:</strong> Click "Commit." A new window will open. Write a clear, descriptive commit message explaining *what* you changed and *why*. Click "Commit." This creates a snapshot in your *local* history.</li>
            <li><strong>End of Day: Push.</strong> Click the green "Push" arrow to send your committed changes up to GitHub for your team to see.</li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Write Good Commit Messages</h3>
    <div class="scenario-content">
        <p>A commit message is a note to your future self and your team. A message like "updated script" is useless. A good commit message follows a simple pattern: a short, imperative summary (e.g., "Fix bug in calculation") followed by a blank line and a more detailed explanation if necessary. This creates a readable and valuable history of the project's evolution.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This exercise requires you to have Git installed and a GitHub account.</p>
        <ul class="prose-list">
            <li>Create a new, empty repository on GitHub.com.</li>
            <li>In RStudio, create a new Project: <code>File > New Project > Version Control > Git</code>.</li>
            <li>Paste the repository URL from GitHub into the dialog box and choose a location on your computer. RStudio will clone the repository.</li>
            <li>Create a new R script in your project, add the line <code>print("Hello, Git!")</code>, and save it.</li>
            <li>Follow the workflow: use the Git pane to Stage the new script, Commit it with a message, and Push it to GitHub.</li>
            <li>Go back to your repository on GitHub.com and refresh the page. You should see your new script there!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s7ss8: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A team of data scientists is working on a central project. If everyone pushes their changes directly to the main project branch, it could become unstable and break the analysis for everyone else. They need a structured, safe workflow that allows developers to work on new features in isolation and have their code reviewed by a teammate before it's merged into the official project.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss8-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s7ss8-viz">Branching Visualization</button>
        <button class="tab-button" data-tab="p1s7ss8-usecase">The Pull Request Workflow</button>
    </div>

    <div id="p1s7ss8-concepts" class="tab-pane active">
        <h4 class="subsection-title">Isolating and Reviewing Work</h4>
        <ul class="prose-list">
            <li><strong>Branching:</strong> A branch is an independent line of development. You can create a new branch from the main branch to work on a new feature or bug fix without affecting the stable, main version of the project.</li>
            <li><strong>Pull Request (PR):</strong> A pull request is a formal request to merge your changes from your feature branch into another branch (usually the main branch). It's a central place on GitHub to discuss the proposed changes.</li>
            <li><strong>Code Review:</strong> A process where a teammate reviews the code in your pull request. They can ask questions, suggest improvements, and must approve the changes before they can be merged. This is a critical practice for maintaining code quality.</li>
        </ul>
        <p><strong>Analogy:</strong> The <code>main</code> branch is the <strong>master copy</strong> of a book in a library. When you want to edit a chapter, you don't write directly in the master copy. You create a clean photocopy (you create a <strong>branch</strong>). You make all your edits on your photocopy. When you're done, you submit your edited chapter to the lead editor for review (you open a <strong>Pull Request</strong>). Only after the editor approves it are your changes carefully merged into the master copy.</p>
    </div>

    <div id="p1s7ss8-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Workflow</h4>
        <p>This diagram shows the lifecycle of a feature branch. It is created from <code>main</code>, work is done on it in isolation, and then it is merged back into <code>main</code> after a pull request and review.</p>
        <div class="plot-container">
            <div id="p1s7ss8-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The blue line represents the stable <code>main</code> branch. A new <code>feature</code> branch (orange) is created to work on a new feature. Several commits are made to this branch independently. Once the feature is complete, a Pull Request is opened, and upon approval, the feature branch is merged back into <code>main</code>, incorporating the new work into the official project history.</p>
        </div>
    </div>
    
    <div id="p1s7ss8-usecase" class="tab-pane">
        <h4 class="subsection-title">A Data Scientist's PR Workflow</h4>
        <ol class="prose-list">
            <li><strong>Create a Branch:</strong> In RStudio's Git pane, click the purple branch icon to create and switch to a new branch (e.g., <code>feature/add-new-plot</code>).</li>
            <li><strong>Do Your Work:</strong> Write your code, make your plot, and commit your changes to this new branch as usual. Push the branch to GitHub.</li>
            <li><strong>Open a Pull Request:</strong> On GitHub.com, you'll see a prompt to open a pull request for your new branch. Click it.</li>
            <li><strong>Describe Your Changes:</strong> Give the PR a clear title and write a description of what you did, why you did it, and how to test it. Tag a teammate to review it.</li>
            <li><strong>Code Review:</strong> Your teammate will review your code on GitHub, leave comments, and suggest changes. You can make more commits on your branch to address their feedback.</li>
            <li><strong>Merge:</strong> Once your teammate approves the PR, you (or a project lead) can click the "Merge Pull Request" button on GitHub. Your changes are now safely part of the <code>main</code> branch!</li>
            <li><strong>Clean Up:</strong> Pull the updated <code>main</code> branch to your local machine and delete the old feature branch.</li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Link Commits to Issues</h3>
    <div class="scenario-content">
        <p>Most professional projects use an issue tracker (like GitHub Issues) to manage tasks. You can automatically link your commits and pull requests to a specific issue. If you're working on issue #42, simply include "fixes #42" or "closes #42" in your commit message or PR description. When the PR is merged, GitHub will automatically close the corresponding issue, keeping your project management tidy.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This exercise builds on the previous one.</p>
        <ul class="prose-list">
            <li>In the RStudio project you created and linked to GitHub, create a new branch called <code>update-greeting</code>.</li>
            <li>On this new branch, change the text in your R script from <code>"Hello, Git!"</code> to <code>"Hello, Collaboration!"</code>.</li>
            <li>Stage, commit, and push this change *on the new branch*.</li>
            <li>Go to GitHub and open a pull request to merge <code>update-greeting</code> into your <code>main</code> branch.</li>
            <li>Since you're the only one on the project, you can review and merge your own pull request. Click the "Merge" button.</li>
            <li>Your <code>main</code> branch on GitHub is now updated!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s8ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is starting a brand new project. To ensure their work is tracked from the very beginning and can be easily backed up and shared, the first step is to create a Git repository ("repo") and link it to a remote host like GitHub. This establishes the foundation for version control.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s8ss1-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s8ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s8ss1-usecase">Getting Started in RStudio</button>
    </div>

    <div id="p1s8ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Project's Container</h4>
        <ul class="prose-list">
            <li><strong>Repository:</strong> A repository, or "repo," is a folder that Git is tracking. It contains all of your project's files and the entire history of every change made to them.</li>
            <li><strong>Local Repository:</strong> This is the repository that lives on your own computer, inside your project folder. It's where you do your day-to-day work and save your changes.</li>
            <li><strong>Remote Repository:</strong> This is a version of your repository hosted on a server, typically on a platform like GitHub. It acts as the central hub for collaboration and a backup of your work.</li>
        </ul>
        <p><strong>Analogy:</strong> A <strong>local repository</strong> is like a digital lab notebook on your computer's hard drive that automatically saves every version of your work. A <strong>remote repository</strong> on GitHub is the cloud-synced, master copy of that notebook, accessible to you and your collaborators from anywhere.</p>
    </div>

    <div id="p1s8ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Two Paths to a Local Repository</h4>
        <p>You can start a version-controlled project in two main ways: by creating a new project locally and linking it to a remote, or by copying an existing remote project to your local machine.</p>
        <div class="plot-container">
            <div id="p1s8ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> <strong>Path A (Cloning)</strong> is the most common workflow: an existing repository on GitHub is copied (cloned) to your local computer, automatically creating the local repo and linking it to the remote. <strong>Path B (Initializing)</strong> is for brand new projects: you start a project locally (<code>git init</code>) and then connect it to a new, empty repository on GitHub.</p>
        </div>
    </div>
    
    <div id="p1s8ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">The Recommended Workflow: Clone from GitHub</h4>
        <p>The easiest and most foolproof way to start a new project is to create the repository on GitHub first.</p>
        <ol class="prose-list">
            <li>Go to GitHub.com and create a new, empty repository. Be sure to initialize it with a README file.</li>
            <li>Copy the repository's URL (it will end in <code>.git</code>).</li>
            <li>In RStudio, go to <code>File > New Project... > Version Control > Git</code>.</li>
            <li>Paste the URL into the "Repository URL" field and choose a location on your computer to store the project.</li>
            <li>Click "Create Project." RStudio will clone the remote repository, create an <code>.Rproj</code> file, and set up the connection for you. Your local and remote repositories are now linked.</li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use HTTPS to Clone</h3>
    <div class="scenario-content">
        <p>When you copy the URL from GitHub, you'll see options for HTTPS and SSH. For beginners, it's almost always easier to use the <strong>HTTPS</strong> link. It may require you to enter your GitHub username and password (or a Personal Access Token) the first time you push, but it requires less initial machine setup than SSH keys.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This exercise requires a GitHub account.</p>
        <ul class="prose-list">
            <li>Create a new public repository on your GitHub account. Name it <code>r-git-test</code>.</li>
            <li>Initialize it with a README file by checking the box on GitHub.</li>
            <li>Follow the steps in the "Use Case" section to clone this repository to your computer as a new RStudio Project.</li>
            <li>Once the project opens, edit the <code>README.md</code> file inside RStudio by adding a new line of text. Save the file.</li>
            <li>You have now successfully set up a version-controlled project and made your first change! In the next section, we'll learn how to save that change.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram. The JS implementation will create it directly.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s8ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has made several related changes to their project: they cleaned a dataset, updated a script that uses the clean data, and fixed a typo in a plot title. They now need to save this coherent set of changes as a single, meaningful "snapshot" in the project's history. This requires understanding the core Git workflow of staging and committing.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s8ss2-concepts">The Three States</button>
        <button class="tab-button" data-tab="p1s8ss2-viz">Visualization of the Workflow</button>
        <button class="tab-button" data-tab="p1s8ss2-usecase">Workflow in RStudio</button>
    </div>

    <div id="p1s8ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Path of a Change</h4>
        <p>Every file in your project is in one of three states:</p>
        <ul class="prose-list">
            <li><strong>Working Directory:</strong> Any file you have saved with changes that Git hasn't been told about yet.</li>
            <li><strong>Staging Area:</strong> A "drafting" area where you place the specific changes you want to include in your *next* snapshot. This allows you to group related changes together, even if you've modified many files.</li>
            <li><strong>Repository (<code>.git</code> directory):</strong> The final, permanent history. When you "commit," the changes from the staging area are saved as a new snapshot in the repository.</li>
        </ul>
        <p><strong>Analogy: The Photo Shoot.</strong> The <strong>Working Directory</strong> is your messy photo studio, with props and lights moved all over the place. To prepare for a shot, you carefully arrange the subject and props on the stage—this is the <strong>Staging Area</strong> (achieved with the <code>git add</code> command). When everything is perfect, you take the picture—this is the **Commit** (<code>git commit</code>), which saves that perfectly staged scene as a permanent snapshot in your photo album (the repository).</p>
    </div>

    <div id="p1s8ss2-viz" class="tab-pane">
        <h4 class="subsection-title">From Edit to History</h4>
        <p>This diagram shows how files move between the three states using the core Git commands.</p>
        <div class="plot-container">
            <div id="p1s8ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> You start by editing files in your <strong>Working Directory</strong>. To prepare a commit, you use the <code>git add</code> command to move specific changes to the <strong>Staging Area</strong>. Once you are happy with your staged changes, you use <code>git commit</code> to permanently store that snapshot in your local <strong>Repository</strong>.</p>
        </div>
    </div>
    
    <div id="p1s8ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Committing Changes in RStudio</h4>
        <p>RStudio's Git pane makes this workflow visual and intuitive.</p>
        <ol class="prose-list">
            <li><strong>Make Changes:</strong> Edit and save one or more files in your project. You'll see them appear in the Git pane with a yellow "M" for "Modified."</li>
            <li><strong>Stage:</strong> For each file whose changes you want to include in the commit, check the "Staged" box next to it. This performs a <code>git add</code>.</li>
            <li><strong>Commit:</strong> Click the "Commit" button at the top of the Git pane.</li>
            <li><strong>Write a Message:</strong> A new window will open. Write a clear, concise commit message in the top-right box describing the change (e.g., "Fix bug in data cleaning script").</li>
            <li><strong>Save Commit:</strong> Click the "Commit" button in this new window. Your snapshot is now saved to your local repository. You'll see the files disappear from the Git pane because they are now up-to-date.</li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Commit-Related Changes Together</h3>
    <div class="scenario-content">
        <p>A commit should represent a single, logical change. If you fix a bug and also add a new, unrelated feature, you should make two separate commits. Use the staging area to your advantage: stage and commit the bug fix files first, then stage and commit the new feature files. This creates a clean, understandable project history that is much easier to navigate and, if necessary, to revert.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Continuing from the previous exercise:</p>
        <ul class="prose-list">
            <li>You should have a modified <code>README.md</code> file showing in your Git pane.</li>
            <li>Check the "Staged" box next to the <code>README.md</code> file.</li>
            <li>Click "Commit."</li>
            <li>In the commit message window, type "Update project description in README" and click the "Commit" button.</li>
            <li>Close the commit window. Your Git pane should now be empty, indicating that your working directory is clean. You have successfully made your first local commit!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram. The JS implementation will create it directly.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s8ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has made several important commits to their local repository, saving the snapshots of their work on their own computer. Now, they need to share this work with their team and also download the latest updates that their colleagues have made to the central project on GitHub.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s8ss3-concepts">Core Concepts</button>
        <button class="tab-button" data-tab="p1s8ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p1s8ss3-usecase">Daily Workflow</button>
    </div>

    <div id="p1s8ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Synchronizing Your Work</h4>
        <ul class="prose-list">
            <li><strong>Remote:</strong> A remote is a pointer to another copy of the repository that lives on a server. By convention, the primary remote repository (the one on GitHub you cloned from) is named <strong><code>origin</code></strong>.</li>
            <li><strong><code>git push</code>:</strong> This command sends your committed changes from your local repository up to the remote repository (e.g., GitHub). It uploads your work.</li>
            <li><strong><code>git pull</code>:</strong> This command fetches any new changes from the remote repository and merges them into your local repository. It downloads others' work.</li>
        </ul>
        <p><strong>Analogy:</strong> Think of GitHub as a shared Google Drive folder. Committing is like saving the file to your own computer's hard drive. To make it visible to others, you must upload it to the shared folder—this is a <strong><code>push</code></strong>. To get the latest version of a file that a colleague edited, you must refresh or sync the folder—this is a <strong><code>pull</code></strong>.</p>
    </div>

    <div id="p1s8ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Pushing and Pulling Changes</h4>
        <p>The core collaboration workflow involves keeping your local repository and the remote GitHub repository in sync.</p>
        <div class="plot-container">
            <div id="p1s8ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> Work is done locally and saved with commits. The <strong>Push</strong> command sends these local commits to the central GitHub repo. The <strong>Pull</strong> command brings commits made by collaborators from the GitHub repo down to your local machine, updating your project with their work.</p>
        </div>
    </div>
    
    <div id="p1s8ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Synchronizing in RStudio</h4>
        <p>RStudio's Git pane provides simple buttons for these crucial commands.</p>
        <ol class="prose-list">
            <li><strong>Pull (Blue Down Arrow):</strong> This should be the first thing you do when you start working on a collaborative project. It ensures you have all the latest updates from your team before you start making your own changes, which helps prevent conflicts.</li>
            <li><strong>Push (Green Up Arrow):</strong> After you have committed one or more changes to your local repository, click this button to send them to GitHub. If you see that the Push button is grayed out, it means you have no new local commits to share. If it has a number next to it, that's how many commits you are "ahead" of the remote.</li>
        </ol>
        <p>A simple and effective daily rhythm is: **Pull, Work (Commit, Commit, ...), Push.**</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: What is a "Merge Conflict"?</h3>
    <div class="scenario-content">
        <p>A merge conflict happens when you try to <code>pull</code> changes, but Git discovers that you and a colleague have both edited the *exact same lines* in the *exact same file*. Git doesn't know which version is correct, so it stops the process and asks you, the human, to resolve the conflict by choosing which code to keep. Pulling frequently helps to reduce the likelihood and severity of merge conflicts.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Continuing from the previous exercise:</p>
        <ul class="prose-list">
            <li>You should have one commit in your local repository that is not yet on GitHub. The "Push" button in your Git pane should indicate you are "1 commit ahead."</li>
            <li>Click the green "Push" button. You may be prompted for your GitHub credentials.</li>
            <li>After it succeeds, go to your repository on GitHub.com and refresh the page. You should see your commit message and the updated <code>README.md</code> file.</li>
            <li>Now, on GitHub.com, edit the <code>README.md</code> file directly on the website and commit the change there.</li>
            <li>Back in RStudio, click the blue "Pull" button. You should see the change you just made on GitHub appear in your local RStudio editor. You have successfully completed a full push/pull cycle!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram. The JS implementation will create it directly.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s8ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist wants to contribute a bug fix to a popular open-source R package. They don't have permission to push changes directly to the main project repository. They need a workflow that allows them to make a copy of the project, work on their fix, and then formally propose that their changes be incorporated into the official package.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s8ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s8ss4-viz">Visualization of the Workflow</button>
        <button class="tab-button" data-tab="p1s8ss4-usecase">The Workflow Steps</button>
    </div>

    <div id="p1s8ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Proposing Changes Safely</h4>
        <ul class="prose-list">
            <li><strong>Forking:</strong> A fork is a **personal copy** of someone else's repository that lives on your own GitHub account. You have full control over your fork and can make any changes you want without affecting the original project.</li>
            <li><strong>Pull Request (PR):</strong> A pull request is a formal request to the owners of the original ("upstream") repository to "pull" the changes from your fork and merge them into their project. It opens a discussion thread on GitHub where your changes can be reviewed and discussed.</li>
        </ul>
        <p><strong>Analogy:</strong> The original project is a popular, published book. <strong>Forking</strong> is like buying your own copy of the book. You can highlight it, write notes in the margins, and do whatever you want to your personal copy. A <strong>Pull Request</strong> is like sending a letter to the original author saying, "I found a typo on page 52 and corrected it in my copy. I think you should include this correction in the next official printing of the book."</p>
    </div>

    <div id="p1s8ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Fork-and-Pull Workflow</h4>
        <p>This is the standard model for open-source contribution and for working across different teams in a large organization.</p>
        <div class="plot-container">
            <div id="p1s8ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The process starts with the central **Upstream Repository**. You create a personal copy, your **Fork on GitHub**. You then **clone** your fork to your local machine to do the work. After committing and **pushing** changes to *your fork*, you open a **Pull Request** back to the original upstream repository, proposing your changes for inclusion.</p>
        </div>
    </div>
    
    <div id="p1s8ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Contributing to a Project</h4>
        <ol class="prose-list">
            <li><strong>Fork:</strong> On the GitHub page of the project you want to contribute to, click the "Fork" button in the top-right. This creates a copy under your account.</li>
            <li><strong>Clone:</strong> Clone *your fork* to your local machine as a new RStudio Project.</li>
            <li><strong>Branch:</strong> Create a new, descriptive branch for your changes (e.g., <code>fix/typo-in-docs</code>).</li>
            <li><strong>Work:</strong> Make your changes, then stage, commit, and push them to your fork on GitHub.</li>
            <li><strong>Open Pull Request:</strong> On your fork's GitHub page, click the "Contribute" button and then "Open pull request."</li>
            <li><strong>Describe and Submit:</strong> GitHub will show a comparison of your branch to the original project's main branch. Write a clear title and description for your proposed change and submit the pull request.</li>
            <li><strong>Discuss and Revise:</strong> The project maintainers will review your code, may ask for changes, and will ultimately decide whether to merge it.</li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Keep Your Fork Synced</h3>
    <div class="scenario-content">
        <p>While you are working on your fork, the original "upstream" repository might be updated by its maintainers. It's a good practice to periodically pull these changes from the upstream repo into your fork's main branch to keep it up-to-date. This is done by adding the original repo as a new remote (conventionally named <code>upstream</code>) and then running commands like <code>git pull upstream main</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's practice the first step of open-source contribution.</p>
        <ul class="prose-list">
            <li>Go to the GitHub repository for a popular R package you use, for example, <code>https://github.com/tidyverse/dplyr</code>.</li>
            <li>Click the "Fork" button in the top-right corner.</li>
            <li>You now have a personal copy of the entire <code>dplyr</code> repository under your own GitHub account! You can explore it, clone it, and make any changes you want without any fear of breaking the original project.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram. The JS implementation will create it directly.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p1s8ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst makes a commit but then realizes they introduced a serious bug. They need a safe way to undo that commit. Separately, a colleague pulls their work and Git reports a "merge conflict" because they both edited the same line of code. They need to know how to resolve this conflict and complete the merge.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s8ss5-revert">Undoing Commits: <code>revert</code></button>
        <button class="tab-button" data-tab="p1s8ss5-reset">Rewriting History: <code>reset</code></button>
        <button class="tab-button" data-tab="p1s8ss5-conflicts">Resolving Merge Conflicts</button>
    </div>

    <div id="p1s8ss5-revert" class="tab-pane active">
        <h4 class="subsection-title">The Safe Way to Undo</h4>
        <p>The <code>git revert</code> command is the safest way to undo a previous commit. It works by creating a **new commit** that contains the inverse of the changes from the commit you want to undo. It doesn't delete any history; it just adds a new entry that corrects a previous one.</p>
        <p><strong>Analogy:</strong> This is like publishing a <strong>newspaper correction</strong>. You don't try to find all the old newspapers and destroy them. Instead, you publish a new article that says, "In our previous story, we made an error. The correct information is..." This preserves the public record while correcting the mistake. This is the best method for undoing changes on a shared, public branch.</p>
        <div class="plot-container">
            <div id="p1s8ss5-plot1" class="plotly-chart"></div>
        </div>
    </div>

    <div id="p1s8ss5-reset" class="tab-pane">
        <h4 class="subsection-title">The Dangerous Way to Undo (Use with Caution!)</h4>
        <p>The <code>git reset</code> command is a more powerful and dangerous tool. It allows you to move the "HEAD" of your branch to a previous commit, effectively discarding any commits that came after it. It rewrites the project history.</p>
        <p><strong>Analogy:</strong> This is like getting out the <strong>shredder</strong>. You take the old newspapers, shred them, and pretend they never existed. This is very confusing for collaborators if you have already pushed the commits you are now resetting. <strong>Rule of thumb: Only use <code>git reset</code> on commits that you have not yet pushed to a shared repository.</strong></p>
    </div>
    
    <div id="p1s8ss5-conflicts" class="tab-pane">
        <h4 class="subsection-title">When Git Needs Help</h4>
        <p>A merge conflict occurs when Git cannot automatically merge two branches because they both have changes to the same lines in the same file. Git will stop the merge and insert conflict markers directly into the problematic file.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-text">
&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
This is the line from your current branch.
=======
This is the conflicting line from the other branch.
>>>>>>> other-branch-name
</code></pre>
        </div>
        <p>To resolve it, you must:</p>
        <ul class="prose-list">
            <li><strong>Open the file:</strong> Find the conflict markers.</li>
            <li><strong>Edit the file:</strong> Manually delete the lines you don't want (including the <code><<<<<<<</code>, <code>=======</code>, and <code>>>>>>>></code> markers) to leave the code in its final, correct state.</li>
            <li><strong>Stage and Commit:</strong> Save the file, stage it (<code>git add</code>), and make a new commit to finalize the merge.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Reverting a Revert</h3>
    <div class="scenario-content">
        <p>Since a <code>git revert</code> is just a regular commit, you can revert it just like any other commit! If you revert a change and then later realize the original change was correct after all, you can simply find the commit hash of the revert commit itself and run <code>git revert <revert_commit_hash></code> to undo your undo.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>In your test RStudio project, edit the <code>README.md</code> file again, adding a line like "This is a bad change."</li>
            <li>Stage and commit this change.</li>
            <li>In RStudio's Git pane, click the "History" button (it looks like a clock). You will see your commit history.</li>
            <li>Find the "bad" commit you just made. Click on it, then click "Revert." RStudio will create a new commit that undoes the change.</li>
            <li>Click "Commit" to finalize the revert. Check your <code>README.md</code> file—the bad change should now be gone!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram. The JS implementation will create it directly.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p2s1ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst receives monthly sales data as a standard CSV file (<code>sales.csv</code>) and supplementary product information in an Excel spreadsheet (<code>products.xlsx</code>). They need to import both of these local files into R to begin their analysis.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s1ss1-readr">Reading Flat Files with <code>readr</code></button>
        <button class="tab-button" data-tab="p2s1ss1-readxl">Reading Excel Files with <code>readxl</code></button>
    </div>

    <div id="p2s1ss1-readr" class="tab-pane active">
        <h4 class="subsection-title">Reading CSV and Delimited Files</h4>
        <p>The <code>readr</code> package, part of the Tidyverse, is the modern standard for reading flat text files. Its <code>read_csv()</code> function is a powerful replacement for base R's <code>read.csv()</code>.</p>
        <p><strong>Analogy:</strong> Data import functions are like **universal power adapters**. Your data exists in one format (a CSV-shaped plug), and you need to get it into your R session (the wall socket). <code>read_csv()</code> is the fast, reliable, and intelligent adapter for this job.</p>
        <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1rem;">Why is <code>read_csv()</code> better?</h5>
        <ul class="prose-list">
            <li><strong>Speed:</strong> It is often 10x faster than its base R counterpart.</li>
            <li><strong>Tibbles:</strong> It returns a tibble directly, which has better printing and subsetting behavior.</li>
            <li><strong>Consistency:</strong> It is more consistent in how it handles data types and does not automatically convert strings to factors.</li>
        </ul>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# Load the readr package (part of the Tidyverse)
library(readr)

# Read a CSV file located in the 'data' subfolder of your project
# readr will print a helpful summary of the column types it guessed.
sales_data <- read_csv("data/sales.csv")
            </code></pre>
        </div>
    </div>

    <div id="p2s1ss1-readxl" class="tab-pane">
        <h4 class="subsection-title">Importing from Spreadsheets</h4>
        <p>The <code>readxl</code> package is the best tool for reading data from Microsoft Excel files (<code>.xls</code> and <code>.xlsx</code>). It is also part of the Tidyverse ecosystem.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# You may need to install it first: install.packages("readxl")
library(readxl)

# By default, read_excel() reads the first sheet
product_data <- read_excel("data/products.xlsx")

# --- Common and useful arguments ---

# You can specify a sheet by its name or number
regional_sales <- read_excel("data/products.xlsx", sheet = "Regionals")
summary_data <- read_excel("data/products.xlsx", sheet = 2)

# You can skip the first N rows, useful for files with report headers
product_data_clean <- read_excel("data/products.xlsx", skip = 3)
            </code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Specifying Column Types</h3>
    <div class="scenario-content">
        <p>Sometimes <code>read_csv()</code> might guess a column type incorrectly (e.g., reading a ZIP code as a number instead of text). You can take full control with the <code>col_types</code> argument. The <code>cols()</code> helper function makes this easy. This is a very robust way to write import scripts.</p>
        <div class="code-container">
            <pre><code class="language-r">
read_csv(
  "data/my_data.csv",
  col_types = cols(
    customer_id = col_character(),
    purchase_date = col_date(format = "%Y-%m-%d"),
    amount = col_double(),
    is_member = col_logical()
  )
)
            </code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>R has a built-in dataset called <code>iris</code>. First, save it to your project's directory as a CSV file using the command: <code>write_csv(iris, "iris_data.csv")</code>.</li>
            <li>Now, use <code>read_csv()</code> to read <code>iris_data.csv</code> back into a new R object called <code>iris_from_file</code>.</li>
            <li>Use the <code>glimpse()</code> function from <code>dplyr</code> on both the original <code>iris</code> dataset and your new <code>iris_from_file</code> object. Are they identical?</li>
        </ul>
    </div>
</div>
`,
          p2s1ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A company's official sales records are not in flat files but in a large, central relational database (like PostgreSQL or SQL Server). An analyst needs to connect to this database directly from R, write a SQL query to extract only the sales data for the last quarter, and pull the results into an R data frame for analysis.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s1ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p2s1ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s1ss2-usecase">The <code>DBI</code> Workflow</button>
    </div>

    <div id="p2s1ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Communicating with Databases</h4>
        <p>To query databases from R, you typically need two packages working together:</p>
        <ul class="prose-list">
            <li><strong>A "Backend" or "Driver" Package:</strong> This package knows how to speak the specific language of your database (e.g., <code>RPostgres</code> for PostgreSQL, <code>RMariaDB</code> for MySQL, or <code>odbc</code> for many others).</li>
            <li><strong><code>DBI</code> (Database Interface):</strong> This package provides a consistent set of commands that you, the user, will use. It acts as a universal front-end.</li>
        </ul>
        <p><strong>Analogy:</strong> <code>DBI</code> is like a <strong>universal translator</strong>. The specific driver package (like <code>RPostgres</code>) is the <strong>language-specific module</strong> for that translator. You plug in the "PostgreSQL" module, and then you can use the universal translator's consistent commands (<code>dbConnect</code>, <code>dbGetQuery</code>) to communicate with the database, even if you don't know the low-level details of its specific dialect.</p>
    </div>

    <div id="p2s1ss2-viz" class="tab-pane">
        <h4 class="subsection-title">The Connection Architecture</h4>
        <p>This diagram shows how your R session uses the <code>DBI</code> and driver packages to communicate with an external database server.</p>
        <div class="plot-container">
            <div id="p2s1ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> Your R code interacts only with the consistent functions provided by the <code>DBI</code> package. <code>DBI</code> then translates your commands and sends them through the specific driver (like <code>odbc</code> or <code>RPostgres</code>) which handles the actual network communication with the SQL Database.</p>
        </div>
    </div>
    
    <div id="p2s1ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">A Complete Database Workflow</h4>
        <p>The workflow for any database connection has three essential steps: Connect, Query, Disconnect.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# Load the necessary packages
library(DBI)
library(RPostgres) # Example using a Postgres driver

# --- 1. Connect to the database ---
# Create a connection object. Credentials should never be hard-coded in a script!
# They are better stored in environment variables or a .Renviron file.
con <- dbConnect(
  RPostgres::Postgres(),
  dbname = "sales_db",
  host = "database.server.com",
  port = 5432,
  user = "analyst",
  password = rstudioapi::askForPassword("Database password")
)

# --- 2. Query the database ---
# Write your SQL query as a string
sql_query <- "SELECT product_id, sale_date, amount FROM sales WHERE sale_date >= '2025-07-01';"

# Use dbGetQuery() to send the query and get the results as a data frame
last_quarter_sales <- dbGetQuery(con, sql_query)

# --- 3. Disconnect from the database ---
# This is a critical step to release resources on the database server.
dbDisconnect(con)

# You can now work with the 'last_quarter_sales' data frame in R
head(last_quarter_sales)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Never Hard-Code Credentials</h3>
    <div class="scenario-content">
        <p>Never save passwords or secret keys directly in your R script. This is a major security risk. A good practice is to use the <code>rstudioapi::askForPassword()</code> function, which will prompt you for a password interactively without it being saved in your code. For automated scripts, the best practice is to store credentials in a <code>.Renviron</code> file, which R automatically loads but you should never commit to Git.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This exercise uses a temporary, in-memory database with the <code>RSQLite</code> package, so you don't need a real database server.</p>
        <ul class="prose-list">
            <li>Install and load the <code>RSQLite</code> package: <code>install.packages("RSQLite")</code>, <code>library(RSQLite)</code>.</li>
            <li>Create a connection: <code>con <- dbConnect(RSQLite::SQLite(), ":memory:")</code>.</li>
            <li>Write the built-in <code>mtcars</code> data frame into a new table in the database: <code>dbWriteTable(con, "cars_table", mtcars)</code>.</li>
            <li>Use <code>dbGetQuery()</code> to write and run a SQL query that selects all cars where the number of cylinders (<code>cyl</code>) is 6.</li>
            <li>Finally, remember to disconnect with <code>dbDisconnect(con)</code>.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram. The JS implementation will create it directly.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p2s1ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is working with user profile data from a web application. This data, which includes nested information like a list of user interests, is stored in a flexible, document-based NoSQL database like MongoDB. They need to connect to the MongoDB instance from R to retrieve and analyze this complex, hierarchical data.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s1ss3-concepts">SQL vs. NoSQL</button>
        <button class="tab-button" data-tab="p2s1ss3-usecase">The <code>mongolite</code> Workflow</button>
    </div>

    <div id="p2s1ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Structured vs. Flexible Data</h4>
        <p><strong>NoSQL</strong> databases arose from the need to handle large volumes of data that didn't fit neatly into the rigid, table-based structure of traditional SQL databases. The most common type is a "document store" like MongoDB.</p>
        <p><strong>Analogy:</strong> If a SQL database is a perfectly organized **filing cabinet** with uniform folders (tables) and structured forms (rows), a NoSQL document database is a collection of **digital scrapbooks**. Each scrapbook (document) can have a different structure, containing text, lists, and even other nested scrapbooks. This makes it highly flexible.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>SQL (Relational)</strong>
                <ul class="prose-list">
                    <li><strong>Schema:</strong> Rigid, predefined tables.</li>
                    <li><strong>Data Model:</strong> Rows and columns.</li>
                    <li><strong>Example:</strong> A table of <code>users</code> where every row must have an <code>id</code>, <code>name</code>, and <code>email</code>.</li>
                </ul>
            </div>
            <div class="decision-branch">
                <strong>NoSQL (Document)</strong>
                <ul class="prose-list">
                    <li><strong>Schema:</strong> Dynamic and flexible.</li>
                    <li><strong>Data Model:</strong> JSON-like documents.</li>
                    <li><strong>Example:</strong> A collection of <code>user</code> documents. One might have a name and email, while another also has a list of hobbies and a nested address document.</li>
                </ul>
            </div>
        </div>
    </div>
    
    <div id="p2s1ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Querying MongoDB with <code>mongolite</code></h4>
        <p>The <code>mongolite</code> package is the high-performance, standard R driver for MongoDB.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("mongolite")
library(mongolite)

# --- 1. Connect to the database ---
# The connection string URI holds all the connection info.
# Again, credentials should not be hard-coded.
MONGO_URI <- "mongodb+srv://user:password@cluster.mongodb.net/"

# Create a connection object to a specific collection in a database
user_collection <- mongo(
  collection = "users",
  db = "application_db",
  url = MONGO_URI
)

# --- 2. Query the collection ---
# Find all users who have 'R' in their list of interests
# The query is written in a JSON-like list format
query_string <- '{"interests": "R"}'
r_users <- user_collection$find(query = query_string)

# Find all users older than 30, and only return the 'name' and 'email' fields
query_age <- '{"age": {"$gt": 30}}'
fields_to_return <- '{"name": 1, "email": 1, "_id": 0}'
older_users <- user_collection$find(query = query_age, fields = fields_to_return)


# The connection object automatically manages the connection pool,
# so an explicit disconnect is not usually needed for simple queries.
# user_collection$disconnect()

head(r_users)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: From JSON to Tibble</h3>
    <div class="scenario-content">
        <p>When you get data from MongoDB, it often comes into R as a data frame where some columns are themselves lists (containing the nested data). The <code>tidyr</code> package, particularly the functions <code>unnest_longer()</code> and <code>unnest_wider()</code>, is essential for flattening this complex, hierarchical data into a tidy tibble that is easy to analyze.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>The <code>mongolite</code> package comes with a built-in sample dataset of diamonds.</p>
        <ul class="prose-list">
            <li>Load the <code>mongolite</code> package.</li>
            <li>Access the built-in data by creating a connection to it: <code>d <- mongo("diamonds", url = "mongodb://cloud.r-project.org:27018")</code>.</li>
            <li>Use the <code>d$count()</code> method to find out how many documents (diamonds) are in the collection.</li>
            <li>Write a query to find all diamonds where the <code>cut</code> is "Ideal" AND the <code>price</code> is greater than 15000. (Hint: The query string will be <code>'{"cut": "Ideal", "price": {"$gt": 15000}}'</code>).</li>
            <li>How many such diamonds did you find?</li>
        </ul>
    </div>
</div>
`,
          p2s1ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A financial analyst needs to get up-to-the-minute stock price data, which is provided by a public API in a structured JSON format. Separately, a marketing analyst wants to scrape the titles of the top 10 articles from a news website's homepage, which only exists as unstructured HTML. These two common data acquisition tasks require different tools and philosophies for interacting with the web.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s1ss4-concepts">API vs. Web Scraping</button>
        <button class="tab-button" data-tab="p2s1ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s1ss4-usecase">Use Cases in R</button>
    </div>

    <div id="p2s1ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Structured vs. Unstructured Web Data</h4>
        <p><strong>Analogy:</strong> Getting data from the web is like getting food from a restaurant.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>APIs (Application Programming Interface)</strong>
                <p>An API is a formal contract provided by a web service for accessing its data. You make a structured request, and it gives you a structured, predictable response (usually JSON).</p>
                <p>This is like ordering from a <strong>restaurant menu</strong>. You make a specific request ("I'll have item #5, the steak"), and you get a well-formed, predictable plate of food in return. This is the preferred, most reliable method.</p>
                <p><strong>Tool:</strong> The <code>httr2</code> package.</p>
            </div>
            <div class="decision-branch">
                <strong>Web Scraping</strong>
                <p>Web scraping is the process of extracting information directly from the HTML code of a website that does *not* provide an API. It is often brittle and can break if the website's layout changes.</p>
                <p>This is like going into the restaurant's kitchen and <strong>picking ingredients out of whatever bowls you can find</strong>. You can still make a meal, but it's messy, and your process will break if the chef ever decides to rearrange the kitchen.</p>
                <p><strong>Tool:</strong> The <code>rvest</code> package.</p>
            </div>
        </div>
    </div>

    <div id="p2s1ss4-viz" class="tab-pane">
        <h4 class="subsection-title">Two Paths for Web Data</h4>
        <p>This diagram shows the two distinct workflows for acquiring web data.</p>
        <div class="plot-container">
            <div id="p2s1ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The API path (top) is clean and direct: a request with <code>httr2</code> gets back structured JSON. The Web Scraping path (bottom) is more complex: <code>rvest</code> downloads the entire unstructured HTML page, from which you must then parse and extract the specific pieces of data you need.</p>
        </div>
    </div>
    
    <div id="p2s1ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Practical Examples</h4>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# --- Example 1: Calling an API with httr2 ---
# install.packages("httr2")
library(httr2)

# 1. Build the request
req <- request("https://api.publicapis.org/random")

# 2. Perform the request
resp <- req_perform(req)

# 3. Extract the body and parse the JSON into an R list
random_api_info <- resp_body_json(resp)

print(random_api_info$entries[[1]]$API)


# --- CORRECTED Example 2: Scraping a Web Page with rvest ---
# install.packages("rvest")
library(rvest)

# 1. Read the HTML from a stable URL like a Wikipedia page
webpage <- read_html("https://en.wikipedia.org/wiki/R_(programming_language)")

# 2. Use a CSS selector to target the main title.
# On Wikipedia, the main title is an <h1> tag with the id "firstHeading"
# The selector for an id is a hash symbol (#).
main_title <- webpage %>%
  html_element("#firstHeading") %>%
  html_text2()

print(main_title)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use a Selector Gadget</h3>
    <div class="scenario-content">
        <p>The hardest part of web scraping is finding the correct CSS selector to target the information you want. A browser extension called <strong>SelectorGadget</strong> makes this easy. You can install it in Chrome or Firefox, click on the elements you want to scrape on a webpage, and it will give you the precise CSS selector to use in your <code>rvest</code> code.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's use a simple, fun API.</p>
        <ul class="prose-list">
            <li>Use the <code>httr2</code> workflow from the example to make a request to the URL: <code>"https://catfact.ninja/fact"</code>.</li>
            <li>Parse the JSON response body into an R list.</li>
            <li>Inspect the list. Can you extract and print the cat fact, which is stored in an element named <code>fact</code>?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram. The JS implementation will create it directly.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p2s2ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst receives a data export from a legacy system. The column names are a mess: inconsistent capitalization ("Region", "region"), spaces ("Sales Rep"), special characters ("Profit ($)"), and duplicates. Manually renaming dozens of columns is not only tedious but also prone to typos, making the data difficult to work with programmatically.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The First-Pass Cleaning Workflow with <code>janitor</code></h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-magic"></i> Automated Initial Cleaning</div>
            <div class="scenario">The <code>janitor</code> package provides a set of powerful, single-function solutions to the most common initial data cleaning headaches.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-broom"></i> 1. <code>clean_names()</code></div>
                        <div class="scenario"><strong>Problem:</strong> Messy, inconsistent column names.<br><strong>Solution:</strong> Instantly converts all names to a consistent format (e.g., snake_case), making them easy to use in scripts.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-trash-alt"></i> 2. <code>remove_empty()</code></div>
                        <div class="scenario"><strong>Problem:</strong> Datasets often contain completely blank rows or columns from import errors.<br><strong>Solution:</strong> Removes any row or column that contains only missing (<code>NA</code>) values.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-clone"></i> 3. <code>get_dupes()</code></div>
                        <div class="scenario"><strong>Problem:</strong> The dataset might contain duplicate records.<br><strong>Solution:</strong> A safe way to inspect duplicate rows based on specific columns *before* deciding to remove them.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">Step-by-Step Example</h3>
    <p>Let's apply this workflow to a messy data frame. Notice how we can chain these functions together in a <code>dplyr</code> pipe for a powerful, readable cleaning process.</p>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# install.packages("janitor")
library(janitor)
library(dplyr)

# Create a data frame with various problems
messy_data <- data.frame(
  "First Name" = c("John", "Jane", NA, "John"),
  <code>Profit ($)</code> = c(150, 200, NA, 150),
  <code>hire-date</code> = c("2022-01-10", "2021-05-20", NA, "2022-01-10"),
  check.names = FALSE # Prevent R's default name cleaning
)
messy_data$<code>  </code> <- NA # Add a completely empty column

# --- Inspect the Duplicates First ---
# It's best practice to see what the duplicates are before taking action.
# This shows us the John record is a complete duplicate.
duplicates <- messy_data %>% get_dupes(<code>First Name</code>., <code>hire-date</code>)
print("--- Identified Duplicates ---")
print(duplicates)

# --- The Full Cleaning Pipeline ---
clean_data <- messy_data %>%
  clean_names() %>%            # Step 1: Cleans names to first_name, profit_usd, etc.
  remove_empty(c("rows", "cols")) %>% # Step 2: Removes the empty row and the empty column.
  distinct()                   # Step 3: From dplyr, removes the duplicate John row.

print("--- Final Clean Data ---")
print(clean_data)
        </code></pre>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: tabyl() for Quick Frequency Tables</h3>
    <div class="scenario-content">
        <p>Beyond cleaning, <code>janitor</code> has a fantastic function for exploratory analysis: <code>tabyl()</code>. It's a modern take on <code>table()</code> that produces a data frame with counts and percentages, and can easily create cross-tabulations. For example, <code>mtcars %>% tabyl(cyl, gear)</code> gives a clean, readable summary table of cylinder and gear counts.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>R's built-in <code>iris</code> dataset has column names that start with a capital letter and contain a dot (e.g., <code>Sepal.Length</code>).</li>
            <li>Use the <code>clean_names()</code> function on the <code>iris</code> dataset. What format do the names become?</li>
            <li>Create a new data frame by adding a duplicate row to <code>iris</code>: <code>iris_with_dupes <- rbind(iris, iris[1,])</code>.</li>
            <li>Use <code>get_dupes(iris_with_dupes)</code> to identify and inspect this duplicate row.</li>
        </ul>
    </div>
</div>
`,
          p2s2ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A user survey dataset has many missing values (<code>NA</code>s) for the 'income' column. Simply deleting these rows would throw away valuable information from other completed columns. The analyst needs a systematic strategy to understand the pattern of missingness and choose an appropriate method to handle it, such as replacing the missing values with plausible estimates (imputation).</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s2ss2-concepts">Understanding Missingness</button>
        <button class="tab-button" data-tab="p2s2ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s2ss2-treatment">Treatment Strategies & Trade-offs</button>
    </div>

    <div id="p2s2ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Why is the Data Missing?</h4>
        <p>Understanding the potential mechanism of missingness helps you choose the right treatment. Briefly, there are three main types:</p>
        <ul class="prose-list">
            <li><strong>Missing Completely at Random (MCAR):</strong> The missingness has no relationship with any data, observed or unobserved. This is the ideal but rare case. (e.g., a survey respondent accidentally skips a question).</li>
            <li><strong>Missing at Random (MAR):</strong> The missingness *can* be explained by other observed variables in the dataset. (e.g., men are less likely to answer a question about depression than women; the missingness depends on the 'gender' variable). This is the key assumption for most advanced imputation methods.</li>
            <li><strong>Missing Not at Random (MNAR):</strong> The missingness is related to the unobserved value itself. (e.g., people with very high incomes are less likely to report it). This is the hardest case to handle and may require domain expertise to model.</li>
        </ul>
        <p><strong>Analogy:</strong> Missing data is like **potholes in a road**. Are they random (MCAR)? Do they only appear in the heavy-truck lane (MAR)? Or do they only appear where the road foundation itself is weakest, something you can't see (MNAR)?</p>
    </div>
    
    <div id="p2s2ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Patterns of Missingness</h4>
        <p>The <code>naniar</code> package provides excellent tools for visualizing missing data. An Upset plot is a powerful way to see which combinations of variables are often missing together.</p>
        <div class="plot-container">
            <div id="p2s2ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The bar chart at the bottom shows the total count of missing values for each variable. The main bar chart at the top shows the frequency of specific combinations of missingness. The dots below it indicate which variables are part of that combination. Here, we see that <code>Ozone</code> being missing by itself is the most common pattern, and <code>Ozone</code> and <code>Solar.R</code> being missing together is the next most common.</p>
        </div>
    </div>

    <div id="p2s2ss2-treatment" class="tab-pane">
        <h4 class="subsection-title">Deletion vs. Imputation</h4>
        <p>There are two main strategies for handling missing values:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Deletion (Removal)</strong>
                <p><strong>Pros:</strong> Simple, fast, and introduces no new assumptions about the data.</p>
                <p><strong>Cons:</strong> Can discard a huge amount of useful information if other columns in those rows were complete. Can introduce bias if the data is not MCAR.</p>
                <div class="code-container">
                    <pre><code class="language-r">
library(tidyr)
# Listwise deletion: removes any row with at least one NA
clean_data <- original_data %>% drop_na()
                    </code></pre>
                </div>
            </div>
            <div class="decision-branch">
                <strong>Imputation (Filling In)</strong>
                <p><strong>Pros:</strong> Preserves all observations, leading to more statistical power.</p>
                <p><strong>Cons:</strong> Introduces assumptions (e.g., that the mean is a good substitute) and can artificially reduce the true variance of the data if not done carefully.</p>
                 <div class="code-container">
                    <pre><code class="language-r">
# Simple Imputation (Median)
df %>% mutate(income = replace_na(income, median(income, na.rm=TRUE)))

# Advanced Imputation (Recommended)
# install.packages("mice")
library(mice)
imputed_model <- mice(original_data, m = 1, method = 'pmm', seed = 123)
completed_data <- complete(imputed_model, 1)
                    </code></pre>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Impute After Splitting Data</h3>
    <div class="scenario-content">
        <p>When building a machine learning model, you must perform imputation *after* splitting your data into training and testing sets. You should learn the imputation model (e.g., using <code>mice</code>) from the training data *only* and then use that fitted model to transform both the training and testing sets. This prevents "data leakage," where information from the test set influences your training process, leading to an overly optimistic evaluation of your model's performance.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>The built-in <code>airquality</code> dataset has missing values.</p>
        <ul class="prose-list">
            <li>Install and load the <code>naniar</code> package.</li>
            <li>Run <code>gg_miss_var(airquality)</code> to create a bar chart of missingness per column.</li>
            <li>Run <code>gg_miss_upset(airquality)</code> to reproduce the plot from the visualization tab.</li>
            <li>Use <code>dplyr</code>'s <code>mutate</code> and <code>tidyr</code>'s <code>replace_na</code> to replace the missing <code>Ozone</code> values with the median of the <code>Ozone</code> column.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# install.packages("naniar")
library(ggplot2)
library(plotly)
# In a real scenario, you would just run: naniar::gg_miss_upset(airquality)
# This code simulates the key parts of that plot for this educational display.

# Data for the combination matrix
combo_data <- data.frame(
  x = c(1, 1, 2, 2, 3),
  y = c(1, 2, 1, 2, 1),
  variable = c("Solar.R", "Ozone", "Solar.R", "Ozone", "Ozone"),
  is_missing = c(FALSE, TRUE, TRUE, TRUE, TRUE)
)
combo_labels <- data.frame(
  combo_name = c("Solar.R Only", "Both Missing", "Ozone Only"),
  x = c(1, 2, 3),
  y = 0.5
)

p_matrix <- ggplot() +
  geom_point(data = combo_data, aes(x = x, y = y, color = is_missing), size = 6) +
  geom_text(data = combo_labels, aes(x=x, y=y, label=variable), hjust=0) +
  scale_color_manual(values = c("TRUE" = "black", "FALSE" = "grey80")) +
  theme_minimal() +
  labs(x = "", y = "") +
  theme(legend.position = "none", axis.text = element_blank())

# Data for the top plot (missingness per combination)
miss_per_combo <- data.frame(
  combo_name = c("Ozone Only", "Both Missing", "Solar.R Only"),
  n_miss = c(35, 2, 5)
)
p_top <- ggplot(miss_per_combo, aes(x=combo_name, y=n_miss)) +
    geom_bar(stat="identity", fill="grey40") +
    labs(x="", y="Combination Count") +
    theme_minimal() +
    theme(axis.text.x = element_blank())

# The JS will combine these conceptually. We show the more important one.
ggplotly(p_top)
</code></pre>
    </div>
</div>
`,
          p2s2ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A dataset of transaction amounts contains a value of $9,999,999. This is clearly a data entry error or a placeholder, not a real sale. This extreme value, or <strong>outlier</strong>, will dramatically skew summary statistics like the mean and can ruin a machine learning model. It must be identified and handled appropriately.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s2ss3-concepts">Detection Methods</button>
        <button class="tab-button" data-tab="p2s2ss3-viz">Visualization with Boxplots</button>
        <button class="tab-button" data-tab="p2s2ss3-usecase">Treatment in R</button>
    </div>

    <div id="p2s2ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Identifying Extreme Values</h4>
        <p>An outlier is a data point that differs significantly from other observations. They can be legitimate rare events or errors.</p>
        <p><strong>Analogy:</strong> Outliers are like a single person **shouting in a library**. They don't represent the typical "volume" of the room. If you were to calculate the average noise level including the shout, you'd get a completely misleading result. You need to identify that shout and decide if it's a valid part of your study or an error to be handled.</p>
        <p>Common detection methods include:</p>
        <ul class="prose-list">
            <li><strong>Visualization:</strong> Using boxplots or scatterplots to visually identify points that fall far from the main data cluster.</li>
            <li><strong>IQR Rule:</strong> A robust statistical rule defining an outlier as any point outside the range: $[Q1 - 1.5 \times IQR, Q3 + 1.5 \times IQR]$.</li>
            <li><strong>Z-Score:</strong> For normally distributed data, any point with a Z-score (number of standard deviations from the mean) greater than 3 is often considered an outlier.</li>
        </ul>
    </div>
    
    <div id="p2s2ss3-viz" class="tab-pane">
        <h4 class="subsection-title">The Power of the Boxplot</h4>
        <p>A boxplot is the single best visualization for identifying potential outliers in a single numeric variable.</p>
        <div class="plot-container">
            <div id="p2s2ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The central box represents the middle 50% of the data, from the 1st Quartile (Q1) to the 3rd Quartile (Q3). The line inside is the median. The "whiskers" extend to the highest and lowest data points that are still within the 1.5 * IQR boundary. Any individual dots plotted beyond the whiskers, like the two shown here, are flagged by the boxplot as potential outliers.</p>
        </div>
    </div>

    <div id="p2s2ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Handling Outliers Programmatically</h4>
        <p>Once identified, you have several options for treating outliers based on your domain knowledge.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>1. Removal</strong>
                <p>If you are confident the outlier is a data entry error, the simplest approach is to remove it. This can be done by filtering based on the IQR rule.</p>
            </div>
            <div class="decision-branch">
                <strong>2. Capping (Winsorizing)</strong>
                <p>Capping involves replacing the outlier with the nearest "non-outlier" value (i.e., the value at the edge of the whisker). This reduces the outlier's influence without throwing away the entire row.</p>
            </div>
             <div class="decision-branch">
                <strong>3. Imputation</strong>
                <p>You can treat the outlier as a missing value and then use an imputation technique to replace it.</p>
            </div>
        </div>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
set.seed(42)
# Create data with some outliers
house_prices <- c(rnorm(100, mean = 250000, sd = 50000), 10, 9999999)

# 1. Calculate the IQR and outlier bounds
Q1 <- quantile(house_prices, 0.25)
Q3 <- quantile(house_prices, 0.75)
IQR_val <- IQR(house_prices)
lower_bound <- Q1 - 1.5 * IQR_val
upper_bound <- Q3 + 1.5 * IQR_val

# --- Treatment Strategy 1: Removal ---
# Filter to keep only the values within the bounds
prices_no_outliers <- house_prices[house_prices > lower_bound & house_prices < upper_bound]
cat("Number of rows after removal:", length(prices_no_outliers), "\n")

# --- Treatment Strategy 2: Capping (Winsorizing) ---
# Replace outliers with the boundary value
prices_capped <- if_else(house_prices > upper_bound, upper_bound, house_prices)
prices_capped <- if_else(prices_capped < lower_bound, lower_bound, prices_capped)
summary(prices_capped)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Trade-offs:</strong> Removing outliers is simple but loses information. Capping preserves the data points but reduces the variance of the data. The best choice depends on whether the outliers are believed to be errors or legitimate but extreme values.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Investigate, Don't Blindly Delete</h3>
    <div class="scenario-content">
        <p>Not all outliers are errors. A very high transaction amount could be a legitimate whale of a customer, or it could be fraud. An extreme reading from a sensor could indicate a critical system failure. Before you remove an outlier, always try to investigate its source. It might be the most important data point in your entire dataset.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>The <code>Ozone</code> column in the <code>airquality</code> dataset has some high outliers.</p>
        <ul class="prose-list">
            <li>Create a boxplot of <code>airquality$Ozone</code>. Do you see any points flagged as outliers?</li>
            <li>Calculate the upper bound for outliers using the 1.5 * IQR rule.</li>
            <li>Use <code>filter()</code> to create a new data frame that excludes any rows where <code>Ozone</code> is above this upper bound. How many rows did you remove?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(plotly)

# Use the same data as the use case
set.seed(42)
house_prices <- c(rnorm(100, mean = 250000, sd = 50000), 10, 550000, 560000)
df <- data.frame(prices = house_prices)

# Plotly automatically identifies outliers in its boxplot trace
fig <- plot_ly(df, y = ~prices, type = "box", 
               boxpoints = "outliers", name = "House Prices") %>%
    layout(
        title = "Boxplot with Outlier Detection",
        yaxis = list(title = "Price ($)")
    )
fig
</code></pre>
    </div>
</div>
`,
          p2s2ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst imports a CSV where a column of product IDs (e.g., "00123") has been incorrectly read as a number (123), losing the important leading zeros. In another column, a date ("2025-09-18") has been read as simple text, preventing date-based calculations. The analyst must explicitly convert these columns to their correct data types.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s2ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p2s2ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s2ss4-usecase">Use Case in R</button>
    </div>

    <div id="p2s2ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Ensuring Correct Data Representation</h4>
        <p>Every column in an R data frame has a specific type (e.g., numeric, character, logical, factor, date). If a column is assigned the wrong type during import, it can lead to incorrect calculations or errors. Data type conversion is the process of explicitly changing a column from one type to another.</p>
        <p><strong>Analogy:</strong> Data type conversion is like using a **currency exchange**. You have a value in one format (e.g., numeric "dollars") and you need to convert it to another (e.g., character "yen"). You must use the correct exchange booth (the right <code>as.*()</code> function) to perform the conversion correctly.</p>
        <p>The key is to first inspect your data types using <code>glimpse()</code> or <code>str()</code>, and then use the appropriate function to fix any issues.</p>
    </div>

    <div id="p2s2ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Conversion Workflow</h4>
        <p>This diagram shows the typical workflow for correcting data types in a data frame using functions from the Tidyverse.</p>
        <div class="plot-container">
            <div id="p2s2ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The process starts with an "Input Tibble" that has columns with incorrect types. This tibble is passed to the <code>dplyr::mutate()</code> function. Inside <code>mutate()</code>, we apply the correct <code>as.*()</code> conversion function to each problematic column, resulting in an "Output Tibble" where all columns have their proper types.</p>
        </div>
    </div>
    
    <div id="p2s2ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Correcting Types with <code>dplyr</code></h4>
        <p>The <code>mutate()</code> function is the perfect tool for applying conversion functions to one or more columns.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(tibble)

# Create a tibble with incorrect data types
messy_data <- tibble(
  product_id = c(123, 456, 789),      # Should be character
  sale_date = c("2025-01-10", "2025-01-11", "2025-01-12"), # Should be date
  is_active = c(1, 0, 1)              # Should be logical
)
glimpse(messy_data) # See the incorrect types: <dbl>, <chr>, <dbl>

# Use mutate() to apply conversion functions to each column
clean_data <- messy_data %>%
  mutate(
    product_id = as.character(product_id),
    sale_date = as.Date(sale_date),
    is_active = as.logical(is_active)
  )

glimpse(clean_data) # See the corrected types: <chr>, <Date>, <lgl>
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>parse_*()</code> Functions</h3>
    <div class="scenario-content">
        <p>The <code>readr</code> package provides a family of <code>parse_*()</code> functions (e.g., <code>parse_number()</code>, <code>parse_datetime()</code>) that are often more robust and flexible than the base R <code>as.*()</code> functions. For example, <code>parse_number("$1,250.75")</code> will correctly extract the number <code>1250.75</code>, whereas <code>as.numeric()</code> would fail and return <code>NA</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create the following tibble: <code>practice_data <- tibble(a = c("TRUE", "FALSE", "TRUE"), b = c("1.5", "2.8", "3.1"))</code>.</li>
            <li>Use <code>glimpse()</code> to see that both columns are currently characters (<code>&lt;chr&gt;</code>).</li>
            <li>Use <code>mutate()</code> to create a new tibble where column <code>a</code> is converted to a logical type and column <code>b</code> is converted to a numeric type.</li>
            <li>Use <code>glimpse()</code> on your new tibble to confirm the types have been corrected.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram. The JS implementation will create it directly.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p2s2ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> Before performing sentiment analysis on customer reviews, an analyst must clean the raw text. The text contains unwanted punctuation, inconsistent capitalization, and common "stopwords" (like "the", "a", "is") that add noise and don't carry meaningful sentiment. This preprocessing is essential for any text analysis task.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s2ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p2s2ss5-usecase">Use Case in R</button>
    </div>

    <div id="p2s2ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Standardizing Text for Analysis</h4>
        <p>Text preprocessing is the task of cleaning and standardizing raw text to prepare it for modeling. The goal is to reduce the "surface variability" of the text so that the underlying meaning can be more easily analyzed.</p>
        <p><strong>Analogy:</strong> Text preprocessing is like **preparing raw vegetables for a salad**. You have to wash them (e.g., convert to lowercase), peel them (e.g., remove punctuation), and trim off the inedible or uninteresting parts (e.g., remove stopwords) before you can combine them into a useful dish (your analysis).</p>
        <p>The <code>stringr</code> package from the Tidyverse is the primary tool for these tasks in R.</p>
    </div>
    
    <div id="p2s2ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">A Common Cleaning Workflow</h4>
        <p>This example shows a typical sequence of cleaning steps applied to a vector of text using <code>dplyr</code> and <code>stringr</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(stringr)
library(tibble)

# A tibble with raw customer reviews
reviews <- tibble(
  text = c(
    "  The App is GREAT!! But the customer service is terrible (1/10). ",
    "My delivery was late, which was SO annoying. #fail"
  )
)

# A cleaning workflow using a dplyr pipe
clean_reviews <- reviews %>%
  mutate(clean_text = text, # Start with a copy
         
         # 1. Convert to lowercase
         clean_text = str_to_lower(clean_text),
         
         # 2. Remove all punctuation using a regular expression
         # The regex [[:punct:]] matches all punctuation characters
         clean_text = str_remove_all(clean_text, "[[:punct:]]"),
         
         # 3. Remove all numbers
         clean_text = str_remove_all(clean_text, "[[:digit:]]"),
         
         # 4. Remove leading/trailing whitespace
         clean_text = str_trim(clean_text)
  )

# View the original and cleaned text side-by-side
print(clean_reviews)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The final <code>clean_text</code> column contains a standardized version of the original reviews. Words like "GREAT" and "great" will now be treated as the same word, and punctuation and numbers that carry little meaning have been removed, making the text ready for further steps like stopword removal and tokenization.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Removing Stopwords with <code>tidytext</code></h3>
    <div class="scenario-content">
        <p>A common next step is to remove "stopwords" (common words like "the", "a", "is", "in"). The <code>tidytext</code> package is perfect for this. It uses a special function <code>unnest_tokens()</code> to split the text into one-word-per-row, and then you can use an <code>anti_join()</code> with its built-in <code>stop_words</code> dataset to filter them out easily.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a character vector: <code>sentences <- c("R is #1!", "I love the Tidyverse... so powerful.")</code></li>
            <li>Use <code>str_to_lower()</code> to convert it to lowercase.</li>
            <li>Use <code>str_remove_all()</code> to remove all the punctuation from the lowercase text.</li>
            <li>Use <code>str_detect()</code> to check which of the original sentences contains the word "Tidyverse". It should return a logical vector <code>c(FALSE, TRUE)</code>.</li>
        </ul>
    </div>
</div>
`,
          p2s2ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is working with a 10GB dataset containing 100 million rows of transaction data. Their standard <code>dplyr</code> code, while readable, is running out of memory and taking hours to perform a simple grouped aggregation. They need a more memory-efficient and significantly faster tool for the job.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s2ss6-concepts">The <code>data.table</code> Package</button>
        <button class="tab-button" data-tab="p2s2ss6-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s2ss6-usecase">Syntax Comparison</button>
    </div>

    <div id="p2s2ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Need for Speed</h4>
        <p>The <code>data.table</code> package is a community-contributed package that provides a high-performance version of R's data frame. It is renowned for its speed and memory efficiency.</p>
        <p><strong>Analogy:</strong> If <code>dplyr</code> is a comfortable and readable <strong>luxury sedan</strong>, great for most everyday trips, then <code>data.table</code> is a <strong>Formula 1 race car</strong>. It has a steeper learning curve, and the controls (syntax) are less intuitive at first, but for pure speed and performance on a demanding track (very large data), it is unmatched in the R ecosystem.</p>
        <p>Its power comes from its concise general form: <code>DT[i, j, by]</code></p>
        <ul class="prose-list">
            <li><code>i</code>: is for subsetting or filtering rows.</li>
            <li><code>j</code>: is for selecting or computing on columns.</li>
            <li><code>by</code>: is for grouping the data.</li>
        </ul>
    </div>

    <div id="p2s2ss6-viz" class="tab-pane">
        <h4 class="subsection-title">Performance Benchmark</h4>
        <p>This chart shows a conceptual benchmark for a complex grouped aggregation task on a large dataset. The performance difference between standard tools and <code>data.table</code> can be dramatic.</p>
        <div class="plot-container">
            <div id="p2s2ss6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The y-axis is on a logarithmic scale. The bar for <code>data.table</code> is orders of magnitude smaller than for <code>dplyr</code>, illustrating its significant speed advantage for large-scale data wrangling tasks. The annotation highlights the massive speedup factor.</p>
        </div>
    </div>
    
    <div id="p2s2ss6-usecase" class="tab-pane">
        <h4 class="subsection-title"><code>dplyr</code> vs. <code>data.table</code></h4>
        <p>Let's perform the same task in both syntaxes: "For the iris dataset, find the mean <code>Sepal.Length</code> for each <code>Species</code>, but only for rows where <code>Sepal.Width</code> is greater than 3."</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong><code>dplyr</code> (Verbose and Readable)</strong>
                <div class="code-container">
                    <pre><code class="language-r">
library(dplyr)
iris %>%
  filter(Sepal.Width > 3) %>%
  group_by(Species) %>%
  summarise(
    avg_sepal_length = mean(Sepal.Length)
  )
</code></pre>
                </div>
            </div>
            <div class="decision-branch">
                <strong><code>data.table</code> (Concise and Fast)</strong>
                 <div class="code-container">
                    <pre><code class="language-r">
# install.packages("data.table")
library(data.table)
iris_dt <- as.data.table(iris)

# DT[i, j, by]
iris_dt[Sepal.Width > 3, 
        .(avg_sepal_length = mean(Sepal.Length)), 
        by = Species]
</code></pre>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Modifying in Place with <code>:=</code></h3>
    <div class="scenario-content">
        <p>One of the key reasons <code>data.table</code> is so memory efficient is its ability to add or update columns *by reference* using the special <code>:=</code> operator. While <code>dplyr</code>'s <code>mutate()</code> always makes a copy of your data, <code>DT[, new_col := col_a * 2]</code> modifies the data table directly without making any copies, saving huge amounts of time and RAM on large datasets.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Load the <code>data.table</code> package and convert the <code>mtcars</code> dataset to a data.table: <code>mtcars_dt <- as.data.table(mtcars, keep.rownames = "car")</code>.</li>
            <li>Using <code>data.table</code> syntax, find the average horsepower (<code>hp</code>) for each cylinder group (<code>cyl</code>).</li>
            <li>Now, add a condition: only include cars that have more than 15 miles per gallon (<code>mpg > 15</code>).</li>
            <li>Finally, sort the results in descending order of average horsepower. (Hint: you can chain commands: <code>DT[...][order(-avg_hp)]</code>).</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(ggplot2)
library(plotly)

# Conceptual performance data 
perf_data <- data.frame(
  Method = c("dplyr", "data.table"),
  Time = c(120, 5) # Time in seconds for a large task
)

p <- ggplot(perf_data, aes(x = Method, y = Time, fill = Method, text = paste(round(Time,1), 'sec'))) +
  geom_bar(stat = "identity") +
  scale_y_log10() +
  labs(
    title = "Performance Benchmark (Log Scale)",
    x = "Package",
    y = "Execution Time (seconds)"
  ) +
  annotate("text", x = 1.5, y = 30, label = paste0(round(120/5), "x Faster"), 
           size = 6, color = "white", fontface = "bold") +
  scale_fill_manual(values = c("dplyr" = "#3b82f6", "data.table" = "#10b981"))+
  theme_minimal() +
  theme(legend.position = "none")

ggplotly(p, tooltip = "text")
</code></pre>
    </div>
</div>
`,
          p2s2ss7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has quarterly sales data in a "wide" format, with a separate column for each quarter (e.g., <code>q1_sales</code>, <code>q2_sales</code>). To create a time-series plot with <code>ggplot2</code> where "quarter" is mapped to the x-axis, they need the data in a "long" format, with one column for the quarter and another column for the sales value.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s2ss7-concepts">Wide vs. Long Data</button>
        <button class="tab-button" data-tab="p2s2ss7-viz">The Pivoting Process</button>
        <button class="tab-button" data-tab="p2s2ss7-usecase">Pivoting with <code>tidyr</code></button>
    </div>

    <div id="p2s2ss7-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Shape of Tidy Data</h4>
        <p>Data reshaping is the process of changing the layout of a dataset. Tidy data principles, which are central to the Tidyverse, advocate for a "long" data format where:</p>
        <ul class="prose-list">
            <li>Each variable forms a column.</li>
            <li>Each observation forms a row.</li>
            <li>Each type of observational unit forms a table.</li>
        </ul>
        <p><strong>Analogy:</strong> Reshaping data is like **reorganizing a bookshelf**. <code>pivot_longer()</code> is like taking books that are spread out horizontally across one long shelf (wide format) and stacking them neatly in a single, tall pile (long format), where each book has a label saying which shelf it came from. <code>pivot_wider()</code> does the reverse.</p>
    </div>

    <div id="p2s2ss7-viz" class="tab-pane">
        <h4 class="subsection-title">From Wide to Long</h4>
        <p>This diagram shows how <code>pivot_longer()</code> takes the column headers from a wide table and turns them into a new variable, "stacking" the corresponding values into another new variable.</p>
        <div class="plot-container">
            <div id="p2s2ss7-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The wide table on the left has separate columns for Q1 and Q2 sales. <code>pivot_longer()</code> gathers these columns. The old column names (<code>q1_sales</code>, <code>q2_sales</code>) become the values in the new <code>quarter</code> column, and their cell values populate the new <code>sales</code> column. The colors show how the data moves from the wide to the long format.</p>
        </div>
    </div>
    
    <div id="p2s2ss7-usecase" class="tab-pane">
        <h4 class="subsection-title">The <code>pivot_*()</code> Functions</h4>
        <p>The <code>tidyr</code> package provides the two main functions for reshaping data.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(tidyr)
library(dplyr)
library(tibble)

# Create a sample wide-format dataset
wide_data <- tibble(
  product_id = c("A", "B"),
  q1_sales = c(100, 150),
  q2_sales = c(110, 160)
)

# --- 1. Going from WIDE to LONG ---
long_data <- wide_data %>%
  pivot_longer(
    cols = starts_with("q"), # Use a tidyselect helper
    names_to = "quarter",    # Name of the new column for the old headers
    values_to = "sales"      # Name of the new column for the old values
  )
print(long_data)


# --- 2. Going back from LONG to WIDE ---
wide_data_again <- long_data %>%
  pivot_wider(
    names_from = quarter,   # The column to get the new header names from
    values_from = sales     # The column to get the cell values from
  )
print(wide_data_again)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Tidyselect Helpers</h3>
    <div class="scenario-content">
        <p>When you have many columns to pivot, you don't need to type them all out. The <code>cols</code> argument in <code>pivot_longer()</code> understands "tidyselect" helpers. You can write things like <code>cols = starts_with("q")</code> to pivot all columns that start with "q", <code>cols = where(is.numeric)</code> to pivot all numeric columns, or <code>cols = -product_id</code> to pivot all columns *except* <code>product_id</code>. This makes your code much more robust and readable.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>R has a built-in dataset called <code>relig_income</code> which is in a wide format.</p>
        <ul class="prose-list">
            <li>Take a look at <code>relig_income</code>.</li>
            <li>Use <code>pivot_longer()</code> to reshape it. You need to pivot all columns *except* the <code>religion</code> column. Use the tidyselect helper <code>-religion</code> for the <code>cols</code> argument.</li>
            <li>Name the new key column "income_bracket" and the new value column "count".</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram. The JS implementation will create it directly.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p2s2ss8: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst presents their final, clean dataset. A colleague asks how they handled missing values and outliers. The analyst can't remember the exact steps or justify their choices because their cleaning script was a messy, uncommented series of commands that overwrote the original data. This makes the result less trustworthy and impossible to reproduce.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s2ss8-concepts">The Core Principles</button>
        <button class="tab-button" data-tab="p2s2ss8-viz">The Cleaning Lifecycle</button>
        <button class="tab-button" data-tab="p2s2ss8-usecase">A Good Workflow Example</button>
    </div>

    <div id="p2s2ss8-concepts" class="tab-pane active">
        <h4 class="subsection-title">Ensuring Trust and Reproducibility</h4>
        <p>A good data cleaning process is not just about the final output; it's about creating a clear, documented, and reproducible workflow.</p>
        <p><strong>Analogy:</strong> A good data cleaning script is like a **scientist's lab notebook**. It doesn't just show the final, purified chemical. It documents every single step, measurement, and decision made during the experiment, so that any other scientist can follow the same steps to reproduce the result and trust its validity.</p>
        <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1rem;">The Data Cleaning Checklist:</h5>
        <ul class="prose-list">
            <li><strong><i class="fas fa-eye-slash"></i> Be Non-Destructive:</strong> Never overwrite your raw data. Read the raw data in, and save cleaned, processed versions as new files in a separate directory (e.g., <code>data/processed/</code>).</li>
            <li><strong><i class="fas fa-project-diagram"></i> Use Pipelines:</strong> Chain your cleaning steps together using the pipe operator (<code>%>%</code>). This creates a readable, linear workflow from start to finish.</li>
            <li><strong><i class="fas fa-comments"></i> Document Your Decisions:</strong> Use comments or a literate programming notebook (R Markdown/Quarto) to explain *why* you made certain cleaning choices (e.g., "Replacing NAs with the median because the data is highly skewed.").</li>
            <li><strong><i class="fas fa-check-double"></i> Validate at Each Step:</strong> After a major transformation, use functions like <code>summary()</code>, <code>glimpse()</code>, or a quick plot to check that the result is what you expected.</li>
            <li><strong><i class="fas fa-box"></i> Write Functions for Repeated Steps:</strong> If you perform the same set of cleaning operations on multiple datasets, encapsulate that logic in a reusable function.</li>
        </ul>
    </div>
    
    <div id="p2s2ss8-viz" class="tab-pane">
        <h4 class="subsection-title">The Cleaning Lifecycle</h4>
        <p>This diagram shows an ideal, iterative process for data cleaning. It's a cycle of loading, cleaning, validating, and documenting, which ensures the final output is robust and trustworthy.</p>
        <div class="plot-container">
            <div id="p2s2ss8-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The workflow is a cycle, not a straight line. You load raw data, apply cleaning steps, and then critically validate the output. The validation step might reveal new issues, leading you back to more cleaning. Every step of this process should be documented before the final clean data is saved.</p>
        </div>
    </div>

    <div id="p2s2ss8-usecase" class="tab-pane">
        <h4 class="subsection-title">A Clean and Documented Script</h4>
        <p>This example combines several best practices into a single, professional cleaning script.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# --- My Data Cleaning Script ---
# Author: A. Data Scientist
# Date: 2025-09-18
# Goal: Clean the raw patient data for analysis.

# 1. Load Libraries
library(dplyr)
library(janitor)
library(readr)

# 2. Load Raw Data (Non-destructive)
raw_patient_data <- read_csv("data/raw/patient_data.csv")

# 3. Cleaning Pipeline
clean_patient_data <- raw_patient_data %>%
  # Standardize column names automatically
  clean_names() %>%
  
  # Correct data types
  mutate(
    patient_id = as.character(patient_id),
    visit_date = as.Date(visit_date)
  ) %>%
  
  # Handle outliers in 'age' column (likely data entry error)
  # Documenting the decision to treat ages > 100 as NA
  mutate(
    age = if_else(age > 100, NA_integer_, age)
  ) %>%
  
  # Impute missing age values with the median age of the group
  # Documenting the choice of median due to potential skew
  mutate(
    age = if_else(is.na(age), median(age, na.rm = TRUE), age)
  )

# 4. Final Validation
# Check the summary of the clean data to ensure it looks reasonable
summary(clean_patient_data)

# 5. Save the Processed Data
write_csv(clean_patient_data, "data/processed/clean_patient_data.csv")
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Assert Your Assumptions</h3>
    <div class="scenario-content">
        <p>For critical, automated pipelines, you can use packages like <code>assertr</code> to programmatically validate your data at each step. You can add a step to your pipe like <code>verify(that(column_is_numeric))</code> or <code>verify(that(value > 0))</code>. If the data doesn't meet your assertion, the pipeline will stop with an informative error. This is a powerful way to catch data quality issues early.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Review the final R script in the "Use Case" tab.</p>
        <ul class="prose-list">
            <li>Identify at least three distinct best practices that are being followed in that script.</li>
            <li>What is one validation step you might add after the pipeline is finished? (e.g., check for remaining <code>NA</code>s, check the range of a specific variable).</li>
            <li>How would you modify the script if you decided to remove rows with invalid ages instead of imputing them?</li>
        </ul>
    </div>
</div>
`,
          p2s3ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A marketing analyst is trying to predict customer lifetime value. The raw dataset contains features like <code>first_purchase_date</code> and <code>most_recent_purchase_date</code>. While these contain information, they are not directly useful to most models. The analyst must use domain knowledge to engineer new, powerful features like <code>customer_tenure_days</code> and <code>days_since_last_purchase</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Art of Feature Creation</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-lightbulb"></i> Domain Knowledge</div>
            <div class="scenario">Feature creation is where data science meets art. It is the process of using your understanding of the problem and the data to create new variables that expose the underlying patterns to your model.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-calendar-alt"></i> From Dates</div>
                        <div class="scenario">Extracting components like day of the week, month, or calculating durations between events. Uses the <code>lubridate</code> package.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-font"></i> From Text</div>
                        <div class="scenario">Extracting information like the length of a review, the number of words, or the presence of a specific keyword. Uses the <code>stringr</code> package.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-calculator"></i> From Numbers</div>
                        <div class="scenario">Creating ratios, differences, or polynomial features to capture non-linear relationships. Uses <code>dplyr</code>.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">Step-by-Step Example in R</h3>
    <p>Let's use <code>dplyr::mutate()</code> to create a variety of new features based on our scenario.</p>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
library(dplyr)
library(lubridate)
library(tibble)

# Create some raw customer data
raw_data <- tibble(
  customer_id = 1:3,
  signup_date = ymd(c("2023-01-15", "2024-03-20", "2024-08-01")),
  first_purchase_date = ymd(c("2023-01-20", "2024-04-01", "2024-08-02")),
  total_spend = c(500, 150, 75),
  total_purchases = c(10, 3, 1)
)

# Engineer new features
feature_data <- raw_data %>%
  mutate(
    # --- Date-based features ---
    # Calculate the time between signup and first purchase
    days_to_first_purchase = as.numeric(first_purchase_date - signup_date, units = "days"),
    
    # Extract the day of the week of signup (1=Sun, 2=Mon...)
    signup_weekday = wday(signup_date),

    # --- Numeric features ---
    # Calculate the average purchase value
    avg_purchase_value = total_spend / total_purchases,

    # --- Indicator (flag) feature ---
    # Create a flag for high-value customers
    is_high_value = if_else(total_spend > 200, TRUE, FALSE)
  )

glimpse(feature_data)
        </code></pre>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Create Features Before Imputing</h3>
    <div class="scenario-content">
        <p>It's often a good idea to create new features *before* you handle missing values. For example, you might have a feature like <code>years_on_job</code>. If this value is missing, you might also want to create a binary flag feature called <code>is_years_on_job_missing</code>. This new flag can sometimes be a powerful predictor in itself—the fact that a value is missing can be informative.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the built-in <code>mtcars</code> dataset.</li>
            <li>Create a new feature called <code>weight_kg</code> by converting the weight in pounds (<code>wt</code>) to kilograms. The conversion is <code>wt * 1000 / 2.20462</code>.</li>
            <li>Create another new feature called <code>power_to_weight_ratio</code>, calculated as horsepower (<code>hp</code>) divided by your new <code>weight_kg</code> feature.</li>
            <li>Which car has the highest power-to-weight ratio? (Hint: use <code>arrange()</code>).</li>
        </ul>
    </div>
</div>
`,
          p2s3ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist has built a predictive model using 1,000 features. The model is slow, hard to interpret, and suffers from overfitting (it has learned noise, not just the signal). Many of these features are likely redundant or irrelevant. The analyst needs a systematic way to select a smaller, more powerful subset of features to build a better model.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s3ss2-concepts">Methods Overview</button>
        <button class="tab-button" data-tab="p2s3ss2-viz">Conceptual Workflows</button>
        <button class="tab-button" data-tab="p2s3ss2-usecase">Use Case in R</button>
    </div>

    <div id="p2s3ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Why Select Features? Less is More.</h4>
        <p>Feature selection reduces the number of input variables to reduce the computational cost of modeling and, in many cases, to improve the performance of the model.</p>
        <p><strong>Analogy:</strong> Feature selection is like **packing for a hike**. You can't bring everything in your house. You must select the most useful items (features) for your specific journey (the model's objective). Bringing irrelevant items (noisy features) just adds weight and slows you down.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Filter Methods</strong>
                <p><strong>How:</strong> Features are ranked based on their statistical relationship with the target variable, independent of any model.
                <br><strong>Examples:</strong> Correlation coefficient, Chi-squared test.
                <br><strong>Pros:</strong> Very fast, computationally cheap.
                <br><strong>Cons:</strong> Ignores relationships between features.
                </p>
            </div>
            <div class="decision-branch">
                <strong>Wrapper Methods</strong>
                <p><strong>How:</strong> Uses a "wrapper" model to evaluate different subsets of features. It searches for the combination that produces the best model performance.
                <br><strong>Examples:</strong> Recursive Feature Elimination (RFE).
                <br><strong>Pros:</strong> Often finds the best-performing feature set.
                <br><strong>Cons:</strong> Extremely slow and computationally expensive.
                </p>
            </div>
             <div class="decision-branch">
                <strong>Embedded Methods</strong>
                <p><strong>How:</strong> The feature selection is performed *during* the model training process itself.
                <br><strong>Examples:</strong> LASSO (L1) Regression, which penalizes complexity and can shrink irrelevant feature coefficients to exactly zero.
                <br><strong>Pros:</strong> A good balance of performance and efficiency.
                <br><strong>Cons:</strong> Tied to a specific model type.
                </p>
            </div>
        </div>
    </div>
    
    <div id="p2s3ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Approaches</h4>
        <p>This diagram shows the conceptual flow of the three main feature selection strategies.</p>
        <div class="plot-container">
            <div id="p2s3ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Filter** method is a simple pre-processing step. The **Wrapper** method is an iterative loop that involves repeatedly training and evaluating a model. The **Embedded** method integrates the selection process directly into the model training algorithm itself.</p>
        </div>
    </div>
    
    <div id="p2s3ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">LASSO Regression for Feature Selection</h4>
        <p>LASSO (L1) regression is a powerful embedded method. It's a form of linear regression that adds a penalty for large coefficients, forcing the coefficients of the least important features to become exactly zero, effectively removing them from the model.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("glmnet")
library(glmnet)

# Use the mtcars dataset. Target is 'mpg'.
x_vars <- model.matrix(mpg ~ ., data = mtcars)[, -1]
y_var <- mtcars$mpg

# Fit a LASSO model. alpha=1 specifies LASSO.
# glmnet fits the model for a whole range of penalty values (lambda).
lasso_model <- glmnet(x_vars, y_var, alpha = 1)

# We can find the optimal lambda value using cross-validation
cv_lasso <- cv.glmnet(x_vars, y_var, alpha = 1)
best_lambda <- cv_lasso$lambda.min

# Now, inspect the coefficients at the best lambda value
# Features with a non-zero coefficient are the ones selected by the model.
best_coeffs <- coef(lasso_model, s = best_lambda)
print(best_coeffs)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use RFE for Wrapper Selection</h3>
    <div class="scenario-content">
        <p>The <code>caret</code> package in R has a powerful and easy-to-use implementation of Recursive Feature Elimination (RFE) via the <code>rfe()</code> function. It allows you to specify any model (like a random forest) and it will iteratively train the model, discard the least important features, and repeat until it finds the optimal feature subset based on cross-validated performance.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's use a simple filter method on the <code>iris</code> dataset.</p>
        <ul class="prose-list">
            <li>The goal is to predict the <code>Sepal.Length</code>.</li>
            <li>Calculate the Pearson correlation between <code>Sepal.Length</code> and the other three numeric variables (Sepal.Width, Petal.Length, Petal.Width). (Hint: <code>cor(iris[,1:4])</code> will give you a correlation matrix).</li>
            <li>Based on the correlation values, which single feature would you select as the best predictor for <code>Sepal.Length</code>? Which one is the worst?</li>
        </ul>
    </div>
</div>
`,
          p2s3ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is building a linear regression model to predict sales based on advertising spend. A scatter plot reveals that the relationship is not linear; instead, sales seem to increase rapidly at first and then level off. A standard linear model will perform poorly. By applying a transformation (like a logarithm) to the advertising spend feature, the relationship can be made linear, dramatically improving the model's performance.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s3ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p2s3ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s3ss3-usecase">Use Case in R</button>
    </div>

    <div id="p2s3ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Why Transform Features?</h4>
        <p>Feature transformation is the process of applying a mathematical function to a feature to change its distribution or its relationship with other variables. The two primary reasons for doing this are:</p>
        <ul class="prose-list">
            <li><strong>To Linearize Relationships:</strong> Many models, like linear regression, assume a linear relationship between features and the target. If the true relationship is curved, transforming a feature can make it linear.</li>
            <li><strong>To Stabilize Variance:</strong> Some models work better when the features have a more symmetric, less skewed distribution (e.g., closer to a normal distribution).</li>
        </ul>
        <p><strong>Analogy:</strong> Transforming a feature is like putting on a pair of **corrective glasses**. Your raw data (your blurry vision) shows a distorted picture of the world. The transformation (the glasses) corrects this distortion, allowing you to see the true, underlying patterns more clearly.</p>
    </div>
    
    <div id="p2s3ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Linearizing a Relationship</h4>
        <p>This plot shows the "before and after" effect of a log transformation on our advertising scenario.</p>
        <div class="plot-container">
            <div id="p2s3ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The left panel shows the raw data. The relationship is clearly curved, and a straight linear regression line (in blue) is a poor fit. The right panel shows the data after applying a log transformation to the X-axis (Ad Spend). The relationship is now much more linear, and the regression line is a far better representation of the underlying trend.</p>
        </div>
    </div>

    <div id="p2s3ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Common Transformations</h4>
        <p>The choice of transformation depends on the nature of the data and the problem.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Log Transformation: log(x)</strong>
                <p>Very effective for right-skewed data and relationships that show diminishing returns. The variable must be positive.</p>
            </div>
            <div class="decision-branch">
                <strong>Square Root Transformation: sqrt(x)</strong>
                <p>A milder transformation than the log, also for right-skewed data. Useful for count data.</p>
            </div>
             <div class="decision-branch">
                <strong>Box-Cox / Yeo-Johnson</strong>
                <p>A family of power transformations that can find the optimal transformation automatically to make the data as "normal-like" as possible. The <code>recipes</code> package has steps for these.</p>
            </div>
        </div>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)

# Create skewed data
ad_spend <- c(100, 200, 500, 1000, 2000, 5000, 10000)
# Sales show diminishing returns
sales <- c(50, 75, 120, 160, 200, 240, 260)
df <- data.frame(ad_spend, sales)

# --- Model on raw data (poor fit) ---
model_raw <- lm(sales ~ ad_spend, data = df)
summary(model_raw)$r.squared # Low R-squared

# --- Model on transformed data (better fit) ---
df_transformed <- df %>%
  mutate(log_ad_spend = log(ad_spend))

model_transformed <- lm(sales ~ log_ad_spend, data = df_transformed)
summary(model_transformed)$r.squared # Much higher R-squared!
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Don't Forget the Inverse Transformation</h3>
    <div class="scenario-content">
        <p>If you build a model on a transformed feature (e.g., you predict <code>log(price)</code>), your model's predictions will also be on the log scale. To convert the prediction back to the original units (dollars), you must apply the inverse transformation. The inverse of <code>log(x)</code> is <code>exp(x)</code>. Forgetting to do this is a very common and critical mistake.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The <code>dist</code> column in the <code>cars</code> dataset is right-skewed.</li>
            <li>Create two histograms side-by-side: one of the original <code>cars$dist</code> and one of <code>log(cars$dist)</code>. (Hint: use <code>par(mfrow=c(1,2))</code> to set up the plotting window).</li>
            <li>Observe how the log transformation makes the distribution more symmetric.</li>
        </ul>
    </div>
</div>
`,
          p2s3ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An e-commerce company is modeling product sales. Their model includes features for <code>advertising_spend</code> and a flag for <code>is_holiday</code>. They suspect that the effect of advertising is much stronger during a holiday period than during a normal period. A simple model that just adds the effects of these two features will miss this crucial synergistic effect. An **interaction feature** is needed to capture it.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s3ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p2s3ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s3ss4-usecase">Use Case in R</button>
    </div>

    <div id="p2s3ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">When the Whole is Greater Than the Sum of its Parts</h4>
        <p>An <strong>interaction effect</strong> occurs when the effect of one feature on the outcome depends on the value of another feature.</p>
        <p>A model with only **main effects** assumes the features are independent. The effect of adding $1000 in ad spend is the same on a holiday as it is on a normal day.
        <br><code>Sales = intercept + B1*ad_spend + B2*is_holiday</code></p>
        <p>A model with an **interaction term** allows the effects to be dependent. The effect of ad spend can now be different for holidays vs. non-holidays.
        <br><code>Sales = intercept + B1*ad_spend + B2*is_holiday + B3*(ad_spend * is_holiday)</code></p>
        <p><strong>Analogy:</strong> Think about making a sandwich. The main effects of "bread" and "peanut butter" are both positive. But the interaction effect of putting them *together* is much greater than just having a piece of bread and a scoop of peanut butter separately.</p>
    </div>
    
    <div id="p2s3ss4-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Interaction</h4>
        <p>An interaction plot is the best way to see this effect. If the lines are parallel, there is no interaction. If the lines have different slopes (or cross), an interaction is present.</p>
        <div class="plot-container">
            <div id="p2s3ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The plot shows sales versus advertising spend. The orange line represents the holiday period, and the blue line represents a normal period. The slope of the orange line is much steeper than the blue line. This means that each dollar of ad spend generates significantly more sales during a holiday, visually confirming a strong positive interaction effect.</p>
        </div>
    </div>

    <div id="p2s3ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Creating and Modeling Interactions in R</h4>
        <p>In R's formula syntax, you can specify main effects with <code>+</code> and interaction effects with <code>*</code> or <code>:</code>.</p>
        <ul class="prose-list">
            <li><code>y ~ a + b</code>: Includes only the main effects for a and b.</li>
            <li><code>y ~ a * b</code>: A shortcut that includes the main effects for a, the main effect for b, AND their interaction (<code>a:b</code>).</li>
            <li><code>y ~ a + b + a:b</code>: The explicit, long-form version of <code>a * b</code>.</li>
        </ul>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
set.seed(42)
# Create data with an interaction effect
df <- data.frame(
  ad_spend = rep(1:10, 2),
  is_holiday = rep(c(0, 1), each = 10)
)
# The effect of ad_spend (slope=5) is boosted by 10 during holidays
df$sales <- 50 + 5*df$ad_spend + 200*df$is_holiday + 10*df$ad_spend*df$is_holiday + rnorm(20, 0, 10)
df$is_holiday <- as.factor(df$is_holiday)

# --- Model 1: Main effects only ---
model_main <- lm(sales ~ ad_spend + is_holiday, data = df)
summary(model_main) # Will not fit the data well

# --- Model 2: With interaction term ---
# The '*' automatically includes main effects and the interaction
model_interaction <- lm(sales ~ ad_spend * is_holiday, data = df)
summary(model_interaction) # Will fit much better and the interaction term will be significant
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Interactions Everywhere</h3>
    <div class="scenario-content">
        <p>Interactions are not limited to linear models. More complex models like decision trees and random forests are inherently capable of capturing complex interaction effects automatically without you needing to specify them. This is one of their major advantages over linear models.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the built-in <code>mtcars</code> dataset.</p>
        <ul class="prose-list">
            <li>Fit a linear model to predict miles per gallon (<code>mpg</code>) using only the main effects of car weight (<code>wt</code>) and transmission type (<code>am</code>, where 0=automatic, 1=manual). Formula: <code>mpg ~ wt + am</code>.</li>
            <li>Now, fit a second model that includes an interaction term between weight and transmission type. Formula: <code>mpg ~ wt * am</code>.</li>
            <li>Use the <code>summary()</code> function on both models. Is the interaction term in the second model statistically significant? What does this suggest about the relationship between weight, transmission, and fuel efficiency?</li>
        </ul>
    </div>
</div>
`,
          p2s3ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has a complex feature engineering workflow with many steps: median imputation for missing numerics, creating "other" categories for rare factors, converting categorical variables to dummy variables, and normalizing all numeric predictors. Applying these steps manually and consistently to the training, validation, and testing datasets is repetitive and dangerously prone to data leakage. The <code>recipes</code> package creates a reusable "recipe" to streamline and safeguard this entire process.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s3ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p2s3ss5-viz">The Recipe Workflow</button>
        <button class="tab-button" data-tab="p2s3ss5-usecase">Use Case in R</button>
    </div>

    <div id="p2s3ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">A Blueprint for Preprocessing</h4>
        <p>The <code>recipes</code> package, part of the <code>tidymodels</code> ecosystem, provides a framework for defining a sequence of data preprocessing and feature engineering steps. A **recipe** is an object that stores the steps of the transformation plan but doesn't execute them immediately.</p>
        <p><strong>Analogy:</strong> A recipe is like a **Mise en Place list for a chef**. It's a detailed, step-by-step plan for preparing all the ingredients (the data).
            <ul>
                <li><code>recipe()</code>: Writing the initial recipe card, specifying the ingredients (variables) and the goal (the formula).</li>
                <li><code>step_*()</code>: Adding specific instructions to the recipe, like "chop onions" (<code>step_dummy()</code>) or "preheat oven" (<code>step_normalize()</code>).</li>
                <li><code>prep()</code>: The chef reads the recipe and prepares their station, calculating any necessary values from the *training data only* (e.g., learning the mean for normalization).</li>
                <li><code>bake()</code>: The chef follows the prepared recipe to process a set of ingredients (e.g., the training or testing data).</li>
            </ul>
        </p>
    </div>
    
    <div id="p2s3ss5-viz" class="tab-pane">
        <h4 class="subsection-title">The <code>prep</code> and <code>bake</code> Workflow</h4>
        <p>This workflow is the key to preventing data leakage. The recipe is <code>prep</code>ared using only the training data, and then the same learned transformations are <code>bake</code>d (applied) to all datasets.</p>
        <div class="plot-container">
            <div id="p2s3ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The process starts with a single recipe blueprint. The <code>prep()</code> function learns all necessary parameters (like means, medians, factor levels) exclusively from the training data. This "trained" recipe is then applied via <code>bake()</code> to both the training and testing sets, ensuring consistent and leak-free preprocessing.</p>
        </div>
    </div>

    <div id="p2s3ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Building a Preprocessing Recipe</h4>
        <p>Let's create a recipe for the <code>iris</code> dataset to predict <code>Species</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("recipes")
library(recipes)
library(dplyr)

# 1. Define the recipe blueprint
iris_recipe <- recipe(Species ~ ., data = iris) %>%
  # Step 1: Center all numeric predictors (subtract the mean)
  step_center(all_numeric_predictors()) %>%
  # Step 2: Scale all numeric predictors (divide by standard deviation)
  step_scale(all_numeric_predictors()) %>%
  # Step 3: Combine any of the original 4 predictors that account for 
  # less than 5% of the total variance into a single "other" component.
  step_pca(all_numeric_predictors(), threshold = 0.95)

# 2. Prepare the recipe using the data
# This learns the means and standard deviations from the iris data
trained_recipe <- prep(iris_recipe, training = iris)

# 3. Bake the recipe to apply the transformations
processed_data <- bake(trained_recipe, new_data = iris)

# See the result: the 4 numeric columns have been replaced by PCs
glimpse(processed_data)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use Role Selectors</h3>
    <div class="scenario-content">
        <p><code>recipes</code> provides powerful selectors to apply steps to variables based on their role. The most common are <code>all_numeric_predictors()</code>, <code>all_nominal_predictors()</code> (for factors/characters), and <code>all_outcomes()</code>. This makes your recipes incredibly robust; if you add or remove a numeric column from your data, you don't need to change your recipe code at all.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>mtcars</code> dataset, let's create a recipe to predict <code>mpg</code>.</p>
        <ul class="prose-list">
            <li>Load the <code>recipes</code> package.</li>
            <li>Create a base recipe where <code>mpg</code> is the outcome and all other variables are predictors.</li>
            <li>The <code>cyl</code>, <code>vs</code>, <code>am</code>, <code>gear</code>, and <code>carb</code> columns are categorical but stored as numbers. Add a <code>step_num2factor()</code> to the recipe to convert them.</li>
            <li>Add a <code>step_normalize()</code> to center and scale all numeric predictors.</li>
            <li>Add a <code>step_dummy()</code> to convert all nominal predictors into dummy variables.</li>
            <li><code>prep</code> and <code>bake</code> your recipe and inspect the resulting data frame with <code>glimpse()</code>.</li>
        </ul>
    </div>
</div>
`,
          p2s3ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A geneticist has data with expression levels for thousands of genes for each patient. It's impossible to visualize or directly model this high-dimensional data. They need a way to reduce the number of features to a few key "meta-genes" (principal components) that capture the most important patterns of variation in the data, making it suitable for visualization and modeling.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s3ss6-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p2s3ss6-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s3ss6-usecase">Use Case in R</button>
    </div>

    <div id="p2s3ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">Summarizing High-Dimensional Data</h4>
        <p><strong>Dimensionality reduction</strong> is the process of reducing the number of features in a dataset while trying to preserve as much of the important information as possible.</p>
        <p><strong>Analogy:</strong> It's like summarizing a complex, multi-instrument song into just the main melody and harmony. You lose some of the fine detail (the triangle part in the background), but you retain the vast majority of the song's structure and feel.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>PCA (Principal Component Analysis)</strong>
                <p>Finds the linear combinations of the original features that capture the maximum variance. It's excellent for data compression and creating uncorrelated features for modeling.</p>
            </div>
            <div class="decision-branch">
                <strong>t-SNE & UMAP</strong>
                <p>These are non-linear techniques primarily used for *visualization*. They are incredibly good at taking high-dimensional data and creating a 2D or 3D "map" that preserves the local neighborhood structure, revealing complex clusters.</p>
            </div>
        </div>
    </div>
    
    <div id="p2s3ss6-viz" class="tab-pane">
        <h4 class="subsection-title">Comparing Reduction Techniques</h4>
        <p>This plot shows the 4-dimensional <code>iris</code> dataset reduced to 2 dimensions using three different techniques.</p>
        <div class="plot-container">
            <div id="p2s3ss6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> **PCA** finds the two components that explain the most overall variance, but the clusters are not perfectly separated. **t-SNE** and **UMAP** are non-linear and focus on preserving local neighborhoods. They do a much better job of creating tight, well-separated clusters in the 2D visualization, making them superior for exploratory data visualization.</p>
        </div>
    </div>

    <div id="p2s3ss6-usecase" class="tab-pane">
        <h4 class="subsection-title">PCA with <code>prcomp</code></h4>
        <p>R's built-in <code>prcomp()</code> function is the standard tool for performing PCA.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# Select only the numeric columns from iris
iris_numeric <- iris[, 1:4]

# Run prcomp(). The <code>scale. = TRUE</code> argument is crucial!
pca_result <- prcomp(iris_numeric, scale. = TRUE)

# The summary shows the proportion of variance explained by each component
summary(pca_result)

# The new, transformed data is in pca_result$x
# We can combine this with the original species for plotting
pca_data <- as.data.frame(pca_result$x)
pca_data$Species <- iris$Species

library(ggplot2)
ggplot(pca_data, aes(x = PC1, y = PC2, color = Species)) +
  geom_point() +
  labs(title = "Iris Data Projected onto First Two Principal Components")
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Don't Use t-SNE for Modeling</h3>
    <div class="scenario-content">
        <p>While t-SNE and UMAP create beautiful visualizations, they are generally not suitable for creating features for a machine learning model. The axes in a t-SNE plot don't have a concrete mathematical meaning in the same way that principal components do, and the algorithms don't produce a stable transformation that can be applied to new data. Use PCA for feature engineering and t-SNE/UMAP for exploratory visualization.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Perform PCA on the numeric columns of the <code>mtcars</code> dataset. Remember to set <code>scale. = TRUE</code>.</li>
            <li>Use the <code>summary()</code> function on the result. How many components are needed to explain at least 85% of the total variance?</li>
            <li>Create a scree plot by making a bar chart of the "Proportion of Variance" from the summary's importance table.</li>
        </ul>
    </div>
</div>
`,
          p2s3ss7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A junior data scientist engineers a new feature: the average sales for a product's category. They calculate this using the *entire* dataset and add it as a new feature. Their model's accuracy skyrockets on the test set. However, when deployed, the model fails miserably. They have committed the cardinal sin of feature engineering: **data leakage**.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Golden Rules of Feature Engineering</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-gavel"></i> Feature Engineering Best Practices</div>
            <div class="scenario">Creating good features is crucial, but creating them correctly and safely is even more important to build trustworthy models.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-secret"></i> 1. Avoid Data Leakage at All Costs</div>
                        <div class="scenario"><strong>The Rule:</strong> Any calculation for a feature that is learned from the data (e.g., means for scaling, frequencies for encoding, principal components) **must be learned from the training set only**. You then apply this learned transformation to the test set. The <code>recipes</code> package is specifically designed to enforce this workflow correctly.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-book-open"></i> 2. Document Everything</div>
                        <div class="scenario"><strong>The Rule:</strong> For every feature you create, document what it is, why you created it (the hypothesis), and how you created it. Your feature engineering script should be a readable, step-by-step narrative. This is crucial for collaboration and for debugging your model later.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chart-line"></i> 3. Validate Feature Impact</div>
                        <div class="scenario"><strong>The Rule:</strong> Don't just add features blindly. After adding a new feature or set of features, retrain your model and use a rigorous validation strategy (like cross-validation) to check if it actually improved performance. Sometimes, a new feature can add more noise than signal.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Think About the Point of Prediction</h3>
    <div class="scenario-content">
        <p>To avoid data leakage, always ask yourself: "At the moment I need to make a prediction in the real world, what information would I have access to?" For example, if you're predicting customer churn, you can't use a feature like <code>total_spend_in_final_month</code> because you wouldn't know that information until after they have (or have not) churned. Any feature that uses information from the future relative to the point of prediction is a source of leakage.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are building a model to predict if a loan application will be approved. Your dataset includes the application date and the final approval/denial date.</li>
            <li>You engineer a feature called <code>days_to_decision</code>, calculated as <code>approval_date - application_date</code>.</li>
            <li>Why is this feature a catastrophic example of data leakage?</li>
            <li>What is a valid, non-leaky feature you *could* create from the application date?</li>
        </ul>
    </div>
</div>
`,
          p2s4ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is building a k-Nearest Neighbors (k-NN) model to segment customers. The dataset has two features: <code>age</code> (ranging from 20-70) and <code>income</code> (ranging from 30,000-150,000). Because the scale of income is so much larger, the model will incorrectly think that a $1,000 income difference is more important than a 10-year age difference. The features must be standardized to be treated equally.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s4ss1-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p2s4ss1-math">The Formula</button>
        <button class="tab-button" data-tab="p2s4ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s4ss1-usecase">Use Case in R</button>
    </div>

    <div id="p2s4ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Creating a Universal Scale</h4>
        <p><strong>Standardization</strong> (or Z-score scaling) is a transformation that rescales features so they have the properties of a standard normal distribution, with a **mean of 0** and a **standard deviation of 1**.</p>
        <p>This is the most common scaling technique and is essential for algorithms that are sensitive to the scale of input features, especially those that calculate distance (k-NN, SVMs, Clustering) or rely on gradient descent.</p>
        <p><strong>Analogy:</strong> Standardization is like converting prices from different currencies (Japanese Yen, US Dollars, British Pounds) into a single, universal standard, like the **number of Big Macs** you could buy. It removes the arbitrary units and allows you to compare values on a common, meaningful scale.</p>
    </div>

    <div id="p2s4ss1-math" class="tab-pane">
        <h4 class="subsection-title">The Z-Score Formula</h4>
        <p>For each original value $x_i$ in a feature, its standardized value $z_i$ is calculated by subtracting the feature's mean ($\\bar{x}$) and dividing by its standard deviation ($s$).</p>
        <div class="math-foundation">$$ z_i = \\frac{x_i - \\bar{x}}{s} $$</div>
        <div class="plot-description">
            <p>The resulting Z-score represents how many standard deviations the original value was from the mean. A Z-score of 1.5 means the original value was 1.5 standard deviations above the average.</p>
        </div>
    </div>
    
    <div id="p2s4ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Effect of Standardization</h4>
        <p>This plot shows a distribution of data before and after standardization. Notice how the shape of the distribution remains identical, but the scale of the x-axis changes dramatically.</p>
        <div class="plot-container">
            <div id="p2s4ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The "Before" plot shows a distribution centered around a mean of 50 with a standard deviation of 10. The "After" plot shows the exact same shape, but the x-axis has been rescaled. The mean is now exactly 0, and the values are expressed in terms of standard deviations from that mean.</p>
        </div>
    </div>
    
    <div id="p2s4ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Standardizing with <code>recipes</code></h4>
        <p>The safest way to standardize data is to use the <code>recipes</code> package, which prevents data leakage.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(recipes)

# Use two variables with very different scales from mtcars
data_to_scale <- mtcars[, c("hp", "wt")]
summary(data_to_scale)

# 1. Define the recipe blueprint
scaling_recipe <- recipe(~ ., data = data_to_scale) %>%
  # Add the standardization step for all numeric variables
  step_center(all_numeric()) %>% # Subtracts the mean
  step_scale(all_numeric())    # Divides by the standard deviation
  # The step_normalize() function combines both of these!

# 2. Prepare the recipe (learns the means and SDs)
trained_recipe <- prep(scaling_recipe, training = data_to_scale)

# 3. Bake the recipe to apply the transformation
scaled_data <- bake(trained_recipe, new_data = data_to_scale)

# The new data has a mean of ~0 and SD of 1 for both columns
summary(scaled_data)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Base R <code>scale()</code> Function</h3>
    <div class="scenario-content">
        <p>For quick exploratory work where you aren't building a predictive model, base R provides a simple <code>scale()</code> function that performs standardization. Running <code>scaled_data <- scale(my_data)</code> will return a matrix with the standardized values. However, for modeling, the <code>recipes</code> approach is far safer and more robust.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Take the numeric columns of the <code>iris</code> dataset: <code>iris_numeric <- iris[, 1:4]</code>.</li>
            <li>Use the base R <code>scale()</code> function to standardize this data.</li>
            <li>Use the <code>colMeans()</code> and <code>apply(scaled_data, 2, sd)</code> functions on the result. Confirm that the means of the scaled columns are effectively zero and the standard deviations are 1.</li>
        </ul>
    </div>
</div>
`,
          p2s4ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist is preparing image data for a neural network. The pixel values for each color channel range from 0 to 255. The neural network's activation functions perform best when the input data is on a small, fixed scale. The analyst must normalize all pixel values to a range of 0 to 1.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s4ss2-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p2s4ss2-math">The Formula</button>
        <button class="tab-button" data-tab="p2s4ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s4ss2-usecase">Use Case in R</button>
    </div>

    <div id="p2s4ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Scaling to a Fixed Range</h4>
        <p><strong>Normalization</strong> (or Min-Max Scaling) is a transformation that rescales features to fit within a specific range, most commonly **0 to 1**.</p>
        <p>This is useful for algorithms that do not make assumptions about the distribution of the data, such as some neural networks and k-Nearest Neighbors. It ensures that all features have the same scale without altering the relative distances between data points within a feature.</p>
        <p><strong>Analogy:</strong> Normalization is like adjusting the volume on all your audio tracks so that the quietest part is at 0% and the loudest part is at 100%. It scales everything to fit neatly within a specific, predefined range, without changing the song itself.</p>
    </div>

    <div id="p2s4ss2-math" class="tab-pane">
        <h4 class="subsection-title">The Min-Max Formula</h4>
        <p>For each original value $x_i$ in a feature, its normalized value $x'_{i}$ is calculated by subtracting the feature's minimum value and dividing by the feature's range.</p>
        <div class="math-foundation">$$ x'_{i} = \\frac{x_i - \\min(x)}{\\max(x) - \\min(x)} $$</div>
        <div class="plot-description">
            <p>The smallest value in the original feature will become 0, the largest will become 1, and all other values will be a decimal in between.</p>
        </div>
    </div>
    
    <div id="p2s4ss2-viz" class="tab-pane">
        <h4 class="subsection-title">The Effect of Normalization</h4>
        <p>This plot shows a distribution of data before and after normalization. The shape of the distribution remains the same, but it is "squished" to fit exactly between 0 and 1.</p>
        <div class="plot-container">
            <div id="p2s4ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The "Before" plot shows a distribution ranging from roughly 20 to 80. The "After" plot shows the exact same shape, but the x-axis has been rescaled. The minimum value is now exactly 0 and the maximum value is now exactly 1.</p>
        </div>
    </div>
    
    <div id="p2s4ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Normalizing with <code>recipes</code></h4>
        <p>The <code>recipes</code> package provides <code>step_range()</code> for safe and effective normalization.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(recipes)

# Use two variables with very different scales from mtcars
data_to_scale <- mtcars[, c("hp", "wt")]
summary(data_to_scale)

# 1. Define the recipe blueprint
norm_recipe <- recipe(~ ., data = data_to_scale) %>%
  # Add the normalization step to scale data between 0 and 1
  step_range(all_numeric(), min = 0, max = 1)
  
# 2. Prepare the recipe (learns the min and max from the data)
trained_recipe <- prep(norm_recipe, training = data_to_scale)

# 3. Bake the recipe to apply the transformation
normalized_data <- bake(trained_recipe, new_data = data_to_scale)

# The new data has a min of 0 and max of 1 for both columns
summary(normalized_data)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Standardization vs. Normalization</h3>
    <div class="scenario-content">
        <p>When should you use one over the other? <strong>Standardization</strong> is generally the default choice because it does not bound the data and is less affected by outliers. <strong>Normalization</strong> is useful when your algorithm requires data to be on a fixed scale (like neural networks) or when you want features to have an equal weight. If your data has extreme outliers, normalization can be problematic as it will squish most of the data into a tiny range.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a simple numeric vector in R: <code>my_vec <- c(10, 20, 30, 40, 50)</code>.</li>
            <li>Write a custom function called <code>normalize_manual</code> that takes a vector <code>x</code> and implements the min-max formula to scale it to a 0-1 range.</li>
            <li>Apply your function to <code>my_vec</code>.</li>
            <li>Verify that the output is <code>0.00, 0.25, 0.50, 0.75, 1.00</code>.</li>
        </ul>
    </div>
</div>
`,
          p2s4ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is preparing a dataset of company revenues for a clustering algorithm. The data is highly skewed, with most companies having revenues under $1M, but a few tech giants having revenues in the hundreds of billions. Using standard scaling (Z-score) would be disastrous, as the mean and standard deviation would be massively distorted by these few outliers, rendering the scaled values for the vast majority of companies useless.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s4ss3-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p2s4ss3-math">The Formula</button>
        <button class="tab-button" data-tab="p2s4ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s4ss3-usecase">Use Case in R</button>
    </div>

    <div id="p2s4ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Scaling in the Presence of Outliers</h4>
        <p><strong>Robust Scaling</strong> is a transformation that scales features using statistics that are resistant to outliers. Instead of the mean and standard deviation, it uses the **median** and the **Interquartile Range (IQR)**.</p>
        <p>This method is the preferred choice when your data contains significant outliers that you do not want to remove but whose influence you want to reduce during scaling.</p>
        <p><strong>Analogy:</strong> Robust scaling is like judging a **diving competition**. To get a fair score, the judges ignore the highest and lowest scores from the panel (the outliers) and calculate the average from the more consistent middle scores. Robust scaling similarly focuses on the "middle bulk" of the data (the IQR) and is not thrown off by extreme values.</p>
    </div>

    <div id="p2s4ss3-math" class="tab-pane">
        <h4 class="subsection-title">The Median and IQR Formula</h4>
        <p>For each original value $x_i$ in a feature, its robustly scaled value $x'_{i}$ is calculated by subtracting the feature's median and dividing by its Interquartile Range.</p>
        <div class="math-foundation">$$ x'_{i} = \\frac{x_i - \\text{median}(x)}{Q_3(x) - Q_1(x)} = \\frac{x_i - \\text{median}(x)}{\\text{IQR}(x)} $$</div>
        <div class="plot-description">
            <p>This centers the data around a median of 0 and scales it based on the range of the middle 50% of the data.</p>
        </div>
    </div>
    
    <div id="p2s4ss3-viz" class="tab-pane">
        <h4 class="subsection-title">The Effect of Robust Scaling</h4>
        <p>This plot shows a skewed distribution with a large outlier. Notice how standard scaling fails, while robust scaling successfully centers and scales the main bulk of the data.</p>
        <div class="plot-container">
            <div id="p2s4ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The "Standard Scaled" plot shows that the outlier has pulled the mean so far to the right that all the other data points are squished into a narrow range below zero. The "Robust Scaled" plot, however, shows the bulk of the data nicely centered around zero, as it used the median and IQR, which were unaffected by the outlier.</p>
        </div>
    </div>
    
    <div id="p2s4ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Robust Scaling with <code>recipes</code></h4>
        <p>The <code>recipes</code> package does not have a single step for robust scaling, but you can easily create it by combining <code>step_median()</code> and a custom step using <code>step_mutate()</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(recipes)

# Create skewed data with an outlier
revenue <- c(rlnorm(100, meanlog = 10, sdlog = 1), 1e9)

# Define the recipe blueprint
robust_recipe <- recipe(~ ., data = data.frame(revenue)) %>%
  # Subtract the median instead of the mean
  step_center(all_numeric(), options = list(center = median)) %>%
  # Divide by the IQR instead of the SD
  step_mutate_at(all_numeric(), fn = ~ . / IQR(., na.rm = TRUE))
  
# Prepare and bake the recipe
trained_recipe <- prep(robust_recipe)
robust_scaled_data <- bake(trained_recipe, new_data = data.frame(revenue))

summary(robust_scaled_data)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: When to Use Each Scaler</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li><strong>Default Choice:</strong> Start with **Standardization** (<code>step_normalize</code> or <code>scale</code>). It's the most common and works well for roughly symmetric data.</li>
            <li><strong>If you have outliers:</strong> Use **Robust Scaling** (median and IQR).</li>
            <li><strong>If you need a fixed range:</strong> Use **Normalization** (<code>step_range</code>), but be cautious if outliers are present.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The <code>Ozone</code> column in the <code>airquality</code> dataset is right-skewed and has some high outliers.</li>
            <li>Create a new data frame containing only this column: <code>ozone_data <- data.frame(ozone = airquality$Ozone)</code>.</li>
            <li>Create two separate recipes: one that standardizes <code>ozone</code> and one that robustly scales it (using median and IQR as in the example).</li>
            <li>Prepare and bake both recipes.</li>
            <li>Use <code>summary()</code> on both of the resulting data frames. Compare the Min, Median, Mean, and Max values. How do they differ? Why?</li>
        </ul>
    </div>
</div>
`,
          p2s4ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst scales their entire dataset and then splits it into training and testing sets. Their model performs amazingly well on the test set, but fails miserably when deployed on new, real-world data. The reason: they inadvertently "leaked" information from the test set into the training process by scaling them together, violating a fundamental best practice of machine learning.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Golden Rule of Preprocessing</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-gavel"></i> The Golden Rule</div>
            <div class="scenario">Any data transformation that is "learned" from the data must be learned from the training set ONLY. This learned transformation is then applied to the validation and test sets.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-times-circle" style="color:#ef4444;"></i> Incorrect Workflow (Data Leakage)</div>
                        <div class="scenario">
                            <ul class="prose-list"></ul>
                                <li>Take the entire dataset.</li>
                                <li>Calculate mean and SD from ALL data.</li>
                                <li>Scale ALL data.</li>
                                <li>Split into training and test sets.</li>
                            </ul>
                            <strong>Result:</strong> The training process has "seen" information about the test set (its mean and SD), leading to an over-optimistic performance estimate.
                        </div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-check-circle" style="color:#10b981;"></i> Correct Workflow (Safe)</div>
                         <div class="scenario">
                            <ul class="prose-list"></ul>
                                <li>Split data into training and test sets first.</li>
                                <li>Calculate mean and SD from the <strong>training set only</strong>.</li>
                                <li>Scale the training set using these values.</li>
                                <li>Scale the <strong>test set</strong> using the *same values learned from the training set*.</li>
                            </ul>
                            <strong>Result:</strong> The test set remains a truly unseen simulation of new data.
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: recipes Enforces Best Practices</h3>
    <div class="scenario-content">
        <p>This workflow might seem complicated to implement manually, but this is exactly the problem the <code>recipes</code> package was built to solve. Its <code>prep()</code> and <code>bake()</code> workflow is the embodiment of this best practice. When you call <code>prep(recipe, training = training_data)</code>, the recipe learns all scaling parameters from the training data only. When you later call <code>bake(trained_recipe, new_data = test_data)</code>, it correctly applies those learned parameters to the test data. Using <code>recipes</code> is the easiest way to prevent data leakage.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are building a model and need to impute missing values by replacing them with the mean.</li>
            <li>Following the "Golden Rule," describe the correct, step-by-step process for imputing missing values when you have a training and a testing set.</li>
            <li>Which dataset should you calculate the mean from?</li>
            <li>What value should you use to fill missing values in the test set?</li>
        </ul>
    </div>
</div>
`,
          p2s5ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A dataset contains a <code>color</code> feature with three possible text values: "Red", "Green", and "Blue". A machine learning model cannot understand these text labels. We need to convert this single categorical column into a numerical format that the model can interpret, without accidentally implying a false order (e.g., that Blue is "greater" than Red).</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s5ss1-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p2s5ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s5ss1-usecase">Use Case in R</button>
    </div>

    <div id="p2s5ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Creating Binary Features</h4>
        <p><strong>One-Hot Encoding</strong> is the process of converting a single categorical column with N distinct categories into N new binary columns (containing only 0s and 1s). Each of these new columns, often called "dummy variables," indicates the presence (1) or absence (0) of a single category.</p>
        <p>This is the standard and safest way to encode **nominal** categorical data, where the categories have no intrinsic order.</p>
        <p><strong>Analogy:</strong> One-hot encoding is like converting a single multiple-choice question into a series of True/False questions. Instead of one column "What is your favorite color?", you get three new columns: "Is Red your favorite color? (Yes/No)", "Is Green your favorite color? (Yes/No)", and "Is Blue your favorite color? (Yes/No)".</p>
    </div>
    
    <div id="p2s5ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Transformation Process</h4>
        <p>This diagram shows how the original single <code>color</code> column is expanded into three new binary <code>color_*</code> columns.</p>
        <div class="plot-container">
            <div id="p2s5ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The original table has one column for color. The one-hot encoding process transforms it into a new table. The original <code>color</code> column is removed, and three new columns are added. For the first row where the color was "Red", the <code>color_Red</code> column is 1 and the others are 0.</p>
        </div>
    </div>

    <div id="p2s5ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">One-Hot Encoding with <code>recipes</code></h4>
        <p>The <code>recipes</code> package provides <code>step_dummy()</code> to perform one-hot encoding safely and correctly within a preprocessing pipeline.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(recipes)
library(tibble)

# Sample data
df <- tibble(
  id = 1:4,
  color = factor(c("Red", "Green", "Blue", "Green"))
)

# 1. Define the recipe
one_hot_recipe <- recipe(~ ., data = df) %>%
  # Convert the 'color' column into dummy variables
  step_dummy(color)
  # You can use all_nominal_predictors() to do this for all categorical columns

# 2. Prepare and bake the recipe
trained_recipe <- prep(one_hot_recipe)
encoded_data <- bake(trained_recipe, new_data = NULL)

print(encoded_data)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Dummy Variable Trap</h3>
    <div class="scenario-content">
        <p>If you one-hot encode a feature with N categories into N new columns, the N columns are perfectly multicollinear (e.g., if you know <code>color_Red</code> is 0 and <code>color_Green</code> is 0, you know for a fact that <code>color_Blue</code> must be 1). This can cause problems for some regression models. To avoid this "dummy variable trap," you can drop one of the new columns. Most modern modeling packages handle this automatically, but the <code>step_dummy()</code> function has an argument <code>one_hot = TRUE</code> which, when set to <code>FALSE</code>, will drop the first category level to prevent this issue.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The <code>Species</code> column in the <code>iris</code> dataset is a factor with three levels.</li>
            <li>Create a recipe that one-hot encodes the <code>Species</code> column.</li>
            <li>Prepare and bake the recipe.</li>
            <li>How many columns does the resulting data frame have? What are the names of the new columns created by <code>step_dummy()</code>?</li>
        </ul>
    </div>
</div>
`,
          p2s5ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A dataset contains a feature for customer satisfaction with ordered levels: "Poor", "Good", "Excellent". We need to convert this to a numerical format, but it's crucial that we preserve the inherent order of the categories (i.e., Excellent > Good > Poor). Simply assigning random numbers would lose this vital information.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s5ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p2s5ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s5ss2-usecase">Use Case in R</button>
    </div>

    <div id="p2s5ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Preserving Order in Categories</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Label Encoding</strong>
                <p>Assigns an arbitrary integer to each category (e.g., Red=1, Green=2, Blue=3).
                <br><strong>Warning:</strong> This is dangerous for nominal data, as it creates a false sense of order that can mislead models like linear regression. It's generally only suitable for tree-based models that don't interpret the numbers as having magnitude.
                </p>
            </div>
            <div class="decision-branch">
                <strong>Ordinal Encoding</strong>
                <p>Assigns integers to categories in a way that respects their natural order. This is the correct approach for variables like survey rankings, size categories, etc.</p>
                <p><strong>Analogy:</strong> Ordinal encoding is like assigning medals at the Olympics. Gold (e.g., 3) is meaningfully greater than Silver (2), which is greater than Bronze (1). The numbers have a meaningful rank.</p>
            </div>
        </div>
    </div>
    
    <div id="p2s5ss2-viz" class="tab-pane">
        <h4 class="subsection-title">The Transformation Process</h4>
        <p>This diagram shows how the text labels for satisfaction are converted into meaningful integers.</p>
        <div class="plot-container">
            <div id="p2s5ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The table on the left contains the raw text data. The ordinal encoding process correctly maps these levels to integers that preserve the original ranking, as shown in the table on the right. A simple alphabetical encoding would have incorrectly assigned a higher value to "Poor" than "Good".</p>
        </div>
    </div>

    <div id="p2s5ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Ordinal Encoding with Factors</h4>
        <p>The best way to handle ordinal data in R is to convert the column to an **ordered factor**. Many modeling functions will then handle the conversion to integers automatically under the hood.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(tibble)

df <- tibble(
  satisfaction = c("Good", "Poor", "Excellent", "Good")
)

# 1. Define the correct order of the levels
satisfaction_levels <- c("Poor", "Good", "Excellent")

# 2. Convert the column to an ordered factor
df_encoded <- df %>%
  mutate(
    satisfaction_ordered = factor(satisfaction, levels = satisfaction_levels, ordered = TRUE)
  )

# 3. View the result and its integer representation
glimpse(df_encoded)
as.numeric(df_encoded$satisfaction_ordered) # Result: 2 1 3 2
</code></pre>
        </div>
        <p>In a <code>recipes</code> workflow, you would use <code>step_ordinalscore()</code> or ensure the column is an ordered factor before using <code>step_dummy()</code>, which will then respect the order.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Don't Use Label Encoding for Regression</h3>
    <div class="scenario-content">
        <p>It's very risky to use simple label encoding (e.g., Red=1, Green=2, Blue=3) as a feature in a regression model. The model will interpret "Green" as being exactly halfway between "Red" and "Blue", and that a one-unit increase from Red to Green has the same effect as a one-unit increase from Green to Blue. This is almost never true. For nominal data, One-Hot Encoding is always safer.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a character vector of T-shirt sizes: <code>sizes <- c("M", "S", "L", "M", "S", "XL")</code>.</li>
            <li>Define a character vector with the correct order: <code>size_levels <- c("S", "M", "L", "XL")</code>.</li>
            <li>Convert the <code>sizes</code> vector into an ordered factor using the <code>levels</code> you defined.</li>
            <li>Convert your new ordered factor to a numeric vector using <code>as.numeric()</code> to see the integer representation.</li>
        </ul>
    </div>
</div>
`,
          p2s5ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist is building a churn prediction model. One feature is <code>zip_code</code>, which has thousands of unique values (high cardinality). One-hot encoding this feature would create thousands of new columns, making the model slow and inefficient. They need a more compact way to encode this feature that captures its relationship with the outcome (churn).</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s5ss3-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p2s5ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s5ss3-usecase">Use Case in R</button>
    </div>

    <div id="p2s5ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Encoding with the Target Variable</h4>
        <p><strong>Target Encoding</strong> (or Mean Encoding) is a technique for high-cardinality features where each category is replaced with a number. This number is derived from the target variable itself: it is the **average of the target variable for that category**.</p>
        <p>This creates a single, powerful new feature that directly summarizes the relationship between the categorical feature and the outcome.</p>
        <p><strong>Analogy:</strong> Target encoding is like giving each zip code a **"reputation score"** based on the outcome you're trying to predict. If you're predicting customer churn, a zip code where many people have previously churned gets a high "churn risk" score. A zip code with very few churners gets a low score. This single score is then used as a feature.</p>
    </div>
    
    <div id="p2s5ss3-viz" class="tab-pane">
        <h4 class="subsection-title">The Transformation Process</h4>
        <p>This diagram shows how the average of the binary target variable (Churn) is calculated for each category (City) and then used to replace the original text values.</p>
        <div class="plot-container">
            <div id="p2s5ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> On the left, we calculate the average churn rate for each city. For Boston, 2 out of 3 customers churned, so its mean is 0.67. For Denver, 0 out of 2 churned, so its mean is 0. On the right, these calculated means replace the city names to create the new numeric feature.</p>
        </div>
    </div>

    <div id="p2s5ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">A <code>dplyr</code> Implementation</h4>
        <p>Here is a simple way to perform target encoding using a grouped summary and a join.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(tibble)

# Sample data
df <- tibble(
  city = c("Boston", "Denver", "Boston", "Boston", "Denver"),
  churn = c(1, 0, 1, 0, 0) # 1 = Churned, 0 = Not Churned
)

# 1. Calculate the encoding map (the average target per category)
encoding_map <- df %>%
  group_by(city) %>%
  summarise(city_encoded = mean(churn))

print(encoding_map)

# 2. Join the map back to the original data to create the new feature
df_encoded <- df %>%
  left_join(encoding_map, by = "city")

print(df_encoded)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Danger of Data Leakage</h3>
    <div class="scenario-content">
        <p>The simple implementation above has a critical flaw: it causes **data leakage**. The encoding for a given row is calculated using the target value of that same row. To do this safely, you must calculate the encodings on your training data *only*, and then join those learned values to your test data. Better still, use a cross-validation approach within the training data to generate the encodings. The <code>recipes</code> package's <code>step_lencode_glm()</code> is a safe, advanced implementation.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, let's target-encode the <code>cyl</code> (number of cylinders) feature to predict <code>vs</code> (engine shape, 0 or 1).</li>
            <li>Use <code>group_by(cyl)</code> and <code>summarise()</code> to calculate the average value of <code>vs</code> for each cylinder group. This is your encoding map.</li>
            <li>Use <code>left_join()</code> to merge this map back to the original <code>mtcars</code> dataset.</li>
            <li>You should now have a new column that represents the "probability of having a V-shaped engine" for each cylinder type.</li>
        </ul>
    </div>
</div>
`,
          p2s5ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> Similar to the target encoding scenario, an analyst has a high-cardinality feature like <code>city</code>. They hypothesize that the *popularity* or *rarity* of a city might be a predictive signal in itself, regardless of its relationship with the target variable. They need a way to capture this information as a numeric feature.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s5ss4-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p2s5ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s5ss4-usecase">Use Case in R</button>
    </div>

    <div id="p2s5ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Encoding by Rarity</h4>
        <p><strong>Frequency Encoding</strong> (or Count Encoding) is a technique for high-cardinality features where each category is replaced by a number representing how often it appears in the dataset. This can be either the raw count or the percentage (frequency).</p>
        <p>This method is simple and doesn't use the target variable, so it avoids the data leakage risk inherent in target encoding. It's useful if the prevalence of a category is a genuinely useful feature.</p>
        <p><strong>Analogy:</strong> Frequency encoding is like assigning a **"popularity score"** to each city based on your customer data. A big city like New York, which appears many times, gets a high score. A small, rare town gets a low score. This popularity score can then be used as a feature in your model.</p>
    </div>
    
    <div id="p2s5ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Transformation Process</h4>
        <p>This diagram shows how the original text categories are replaced by their counts from the dataset.</p>
        <div class="plot-container">
            <div id="p2s5ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> On the left, we count the occurrences of each city in the original data: Boston appears 3 times and Denver appears 2 times. On the right, these counts replace the original city names to create the new numeric feature.</p>
        </div>
    </div>

    <div id="p2s5ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">A <code>dplyr</code> Implementation</h4>
        <p>This can be implemented easily using a grouped count and a join.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(tibble)

# Sample data
df <- tibble(
  city = c("Boston", "Denver", "Boston", "Boston", "Denver")
)

# 1. Calculate the encoding map (the count of each category)
encoding_map <- df %>%
  count(city, name = "city_freq_encoded")

print(encoding_map)

# 2. Join the map back to the original data
df_encoded <- df %>%
  left_join(encoding_map, by = "city")

print(df_encoded)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Handling New Categories</h3>
    <div class="scenario-content">
        <p>A major drawback of both target and frequency encoding is what to do when a new, unseen category appears in the test data or in production. Your encoding map won't have a value for it, resulting in an <code>NA</code>. A common strategy is to fill these <code>NA</code>s with a global value, such as the global mean for target encoding or a small number (like 1) for frequency encoding.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, let's frequency-encode the <code>cyl</code> (number of cylinders) feature.</li>
            <li>Use <code>dplyr</code>'s <code>count()</code> function to get the number of cars in each cylinder group. This is your encoding map.</li>
            <li>Use <code>left_join()</code> to merge this map back to the original <code>mtcars</code> dataset.</li>
            <li>You should now have a new column that shows how common each car's cylinder configuration is in the dataset.</li>
        </ul>
    </div>
</div>
`,
          p2s5ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst one-hot encodes a feature with three categories ("Red", "Green", "Blue") into three new columns. When they fit a linear regression model, the model fails due to perfect multicollinearity. They have fallen into the "dummy variable trap." This highlights the need to understand the best practices and potential pitfalls of categorical encoding.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Decision Framework for Encoding</h3>
    <p>Choosing the right encoding method is a critical step that depends on the nature of your variable and the type of model you are using.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-question-circle"></i> Is the variable ordinal?</div>
            <div class="scenario">(Does the order of categories matter, e.g., "low", "medium", "high"?)</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-check-circle" style="color:#10b981;"></i> YES</div>
                        <div class="scenario"><strong>Use Ordinal Encoding.</strong> Convert the categories to ordered integers (e.g., using an ordered factor in R). This preserves the crucial ranking information.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-times-circle" style="color:#f59e0b;"></i> NO (It's Nominal)</div>
                        <div class="scenario">Next question: Does it have low or high cardinality (number of unique categories)?</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="mindmap-container" style="margin-top: 2rem;">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-question-circle"></i> Does the nominal variable have low cardinality?</div>
            <div class="scenario">(e.g., < 15-20 unique categories)</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-check-circle" style="color:#10b981;"></i> YES</div>
                        <div class="scenario"><strong>Use One-Hot Encoding.</strong> This is the safest and most expressive method. Be mindful of the "dummy variable trap" for some models by dropping one of the new columns.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-times-circle" style="color:#ef4444;"></i> NO (It has high cardinality)</div>
                        <div class="scenario"><strong>Consider Target, Frequency, or Hashing Encoders.</strong> One-hot encoding would create too many new features. These methods create a more compact representation, but come with their own trade-offs (like data leakage risk for target encoding).</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Dummy Variable Trap Explained</h3>
    <div class="scenario-content">
        <p>If you one-hot encode a feature with N categories into N new columns, the N columns are perfectly multicollinear (e.g., if you know <code>color_Red</code> is 0 and <code>color_Green</code> is 0, you know for a fact that <code>color_Blue</code> must be 1). This can cause problems for some regression models. To avoid this "dummy variable trap," you can drop one of the new columns. Most modern modeling packages handle this automatically, but the <code>recipes::step_dummy()</code> function has an argument <code>one_hot = TRUE</code> which, when set to <code>FALSE</code>, will drop the first category level to prevent this issue.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Consider the following features for a churn prediction model. For each one, state which encoding method you would choose first and why.</p>
        <ul class="prose-list">
            <li><strong>Feature 1:</strong> <code>subscription_plan</code> with levels: "Basic", "Premium", "Enterprise".</li>
            <li><strong>Feature 2:</strong> <code>customer_satisfaction_rating</code> with levels: "Very Unsatisfied", "Unsatisfied", "Neutral", "Satisfied", "Very Satisfied".</li>
            <li><strong>Feature 3:</strong> <code>state</code> with 50 unique US state abbreviations.</li>
        </ul>
    </div>
</div>
`,
          p2s6ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A credit risk analyst is building a model to predict loan defaults. One of the features is the applicant's <code>age</code>, a continuous variable. The analyst believes the relationship between age and risk is not linear; for instance, very young and very old applicants might pose higher risks than middle-aged applicants. To capture this non-linear effect in a simpler model, they decide to group the continuous age feature into discrete bins or categories.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s6ss1-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p2s6ss1-methods">Methods & Trade-offs</button>
        <button class="tab-button" data-tab="p2s6ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s6ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p2s6ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Simplifying Continuous Variables</h4>
        <p><strong>Unsupervised Binning</strong> is the process of converting a continuous numerical variable into a discrete categorical variable without using any information from the target variable. The "bins" or intervals are created based solely on the properties of the feature's own distribution.</p>
        <p><strong>Why do this?</strong></p>
        <ul class="prose-list">
            <li><strong>Capture Non-linearity:</strong> Allows simpler models (like logistic regression) to capture complex, non-linear relationships.</li>
            <li><strong>Reduce Noise:</strong> Can reduce the impact of small fluctuations or measurement errors in the data.</li>
            <li><strong>Handle Outliers:</strong> Groups extreme values into a single bin, reducing their influence.</li>
        </ul>
        <p><strong>Analogy:</strong> Binning is like converting a high-resolution photograph (continuous data) into a stylized poster with a limited color palette (discrete bins). You lose some fine detail, but the main shapes and patterns become clearer and easier to interpret.</p>
    </div>

    <div id="p2s6ss1-methods" class="tab-pane">
        <h4 class="subsection-title">Equal Width vs. Equal Frequency</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Equal Width (Uniform) Binning</strong>
                <p>Divides the range of the variable into N intervals of the same size. For a variable from 0 to 100 with 4 bins, the intervals would be [0-25], (25-50], (50-75], (75-100].</p>
                <p><strong>Trade-off:</strong> This method is simple but can be heavily skewed by outliers. If one outlier exists, it can stretch the range, causing most of the data to fall into just one or two bins.</p>
            </div>
            <div class="decision-branch">
                <strong>Equal Frequency (Quantile) Binning</strong>
                <p>Divides the data into N bins such that each bin contains approximately the same number of observations. The intervals will likely have different widths.</p>
                <p><strong>Trade-off:</strong> This method handles skewed data and outliers very well, creating balanced bins. However, it can sometimes group very different values together into the same bin if they fall near a quantile boundary.</p>
            </div>
        </div>
    </div>

    <div id="p2s6ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Binning Methods</h4>
        <p>This plot shows how the two methods handle a skewed distribution. Equal width binning is distorted by the long tail, while equal frequency binning creates balanced groups.</p>
        <div class="plot-container">
            <div id="p2s6ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The top panel shows the original skewed distribution of income. The middle panel shows that **Equal Width** binning results in most data points being crammed into the first bin, because the outlier stretches the total range. The bottom panel shows that **Equal Frequency** binning creates bins with an equal number of data points, resulting in a balanced histogram, which is often more useful for models.</p>
        </div>
    </div>
    
    <div id="p2s6ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Implementing Binning in R</h4>
        <p>We can use <code>dplyr::cut()</code> for equal-width and <code>dplyr::ntile()</code> for equal-frequency binning.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(tibble)

# Create skewed income data
set.seed(42)
income_data <- tibble(
  income = round(c(rlnorm(100, meanlog = 10.5, sdlog = 0.5), 500000))
)

# --- Equal Width Binning ---
# Divide the data into 4 bins of the same width
binned_width <- income_data %>%
  mutate(
    income_bin_width = cut(income, breaks = 4, labels = c("Low", "Mid", "High", "Very High"))
  )
# Notice how most fall into the 'Low' bin due to the outlier
binned_width %>% count(income_bin_width)

# --- Equal Frequency (Quantile) Binning ---
# Divide the data into 4 bins with (roughly) equal numbers of observations
binned_freq <- income_data %>%
  mutate(
    income_bin_quantile = ntile(income, 4)
  ) %>% 
  # ntile creates numbers; we can convert to labels
  mutate(income_bin_quantile = factor(income_bin_quantile, labels = c("Q1", "Q2", "Q3", "Q4")))

# Notice the counts are much more balanced
binned_freq %>% count(income_bin_quantile)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: cut() is Highly Customizable</h3>
    <div class="scenario-content">
        <p>The <code>cut()</code> function is not just for equal-width binning. You can pass it a vector of manually chosen break points, e.g., <code>breaks = c(0, 30000, 70000, Inf)</code>, to perform custom binning, which we'll see in the next topic. This makes it a very flexible and powerful tool.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Take the <code>hp</code> (horsepower) column from the <code>mtcars</code> dataset.</li>
            <li>Use <code>summary()</code> and <code>hist()</code> to inspect its distribution. Is it skewed?</li>
            <li>Use <code>mutate()</code> and <code>ntile()</code> to create a new column that bins the horsepower into 5 equal-frequency groups (quintiles).</li>
            <li>Use <code>count()</code> or <code>tabyl()</code> to verify that the number of cars in each bin is approximately equal.</li>
        </ul>
    </div>
</div>
`,
          p2s6ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> The credit risk analyst wants to bin the <code>age</code> feature. Instead of creating arbitrary bins, they want to create bins that are most effective at separating the "good" customers (non-defaulters) from the "bad" ones (defaulters). This requires using the target variable (<code>default_status</code>) to determine the optimal bin boundaries, a technique known as **Supervised Binning**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s6ss2-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p2s6ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s6ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p2s6ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Using the Target to Create Bins</h4>
        <p><strong>Supervised Binning</strong> uses the target variable to find the best split-points in a continuous feature. The goal is to create bins that are as "pure" as possible, meaning each bin contains a high proportion of one class (e.g., a high percentage of defaulters or non-defaulters).</p>
        <p>The most common way to achieve this is by using a **Decision Tree** algorithm.</p>
        <p><strong>Analogy:</strong> Supervised binning is like a professional organizer sorting a mixed box of screws, nuts, and bolts. Instead of sorting by arbitrary size (unsupervised), they sort with a specific goal: to create separate, pure piles for each type of fastener. They look at the "target" (the type of fastener) to decide where to make a division in the pile.</p>
    </div>
    
    <div id="p2s6ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Finding the Optimal Splits</h4>
        <p>This plot shows a relationship between <code>age</code> and the probability of default. A decision tree would find the split points (vertical lines) that best separate the high-risk and low-risk groups, maximizing the "purity" of the resulting bins.</p>
        <div class="plot-container">
            <div id="p2s6ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The points represent individual customers (1=default, 0=no default). A decision tree algorithm has identified two optimal split points. The first split at age 28 separates a high-risk younger group. The second split at age 60 separates a high-risk older group. This results in three bins that are much more predictive than arbitrary ones.</p>
        </div>
    </div>

    <div id="p2s6ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Using a Decision Tree in R</h4>
        <p>We can use the <code>rpart</code> package to fit a shallow decision tree and extract its split points to use as our bin boundaries.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages(c("rpart", "rpart.plot"))
library(rpart)
library(dplyr)

# Create sample data
set.seed(42)
credit_data <- tibble(
  age = sample(20:70, 200, replace = TRUE),
  # Create a non-linear relationship with risk
  default = if_else(age < 28 | age > 60, 1, 0) 
)

# 1. Fit a shallow decision tree to find the best splits for 'age'
# We set maxdepth to 2 to get a few main splits.
tree_model <- rpart(default ~ age, data = credit_data, maxdepth = 2, cp = 0.01)

# 2. Extract the split points found by the tree
split_points <- tree_model$splits[,"index"]
boundaries <- c(min(credit_data$age), sort(unique(split_points)), max(credit_data$age))

print("Optimal Boundaries Found by Tree:")
print(boundaries)

# 3. Use these boundaries to create the new supervised bins
credit_data_binned <- credit_data %>%
  mutate(
    age_bin_supervised = cut(age, breaks = boundaries, include.lowest = TRUE)
  )

# Check the purity of the new bins
credit_data_binned %>%
  group_by(age_bin_supervised) %>%
  summarise(default_rate = mean(default))
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Danger of Overfitting</h3>
    <div class="scenario-content">
        <p>Supervised binning is extremely powerful, but it has a high risk of **overfitting**. Because you are using the target variable to create the feature, the feature might learn noise specific to your training data. It is absolutely critical that supervised binning is done within a proper cross-validation loop to ensure the bins you create are robust and generalize to new data.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>mtcars</code> dataset, let's find the optimal bins for <code>hp</code> (horsepower) to predict <code>vs</code> (engine shape).</p>
        <ul class="prose-list">
            <li>Fit a shallow decision tree using <code>rpart(vs ~ hp, data = mtcars, maxdepth = 1)</code>. A max depth of 1 will find the single best split point.</li>
            <li>Extract the split point from the <code>tree_model$splits</code> object.</li>
            <li>Use this single split point to create two "supervised" bins for horsepower using the <code>cut()</code> function.</li>
            <li>Calculate the average <code>vs</code> value for each of your two new bins. You should see a large difference, indicating the split was effective.</li>
        </ul>
    </div>
</div>
`,
          p2s6ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A marketing analyst needs to categorize customers by age for a new campaign. Based on prior experience and business knowledge, they know that standard demographic groups (e.g., 18-24, 25-34, 35-49, 50+) are the most meaningful for their company's marketing efforts. They don't want the algorithm to decide the bins; they need to impose these specific, pre-defined custom bins onto the data.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s6ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p2s6ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p2s6ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Leveraging Domain Expertise</h4>
        <p><strong>Custom Binning</strong> is the process of grouping a continuous variable into discrete bins based on pre-defined boundaries that come from business rules, expert knowledge, or regulatory requirements.</p>
        <p>This is a "top-down" approach where the analyst's expertise guides the feature engineering process, rather than a "bottom-up" approach where the algorithm discovers patterns from the data itself.</p>
        <p><strong>Analogy:</strong> Custom binning is like a chef using a **set of pre-made cookie cutters**. The chef isn't trying to discover the best shapes from the dough (the data); they already know they need specific, meaningful shapes (stars, hearts, trees) for the holiday cookies they are making. They use their domain knowledge (it's a holiday) to choose the right tools.</p>
    </div>

    <div id="p2s6ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Implementing Custom Bins in R</h4>
        <p>The <code>dplyr::cut()</code> function is perfect for this. We can provide it with a vector of our custom break points.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(tibble)

customer_data <- tibble(
  customer_id = 1:5,
  age = c(22, 31, 45, 68, 19)
)

# 1. Define the custom boundaries for our age groups
age_boundaries <- c(0, 24, 34, 49, Inf)

# 2. Define the labels for the new bins
age_labels <- c("18-24", "25-34", "35-49", "50+")

# 3. Use cut() to apply the custom bins
customer_data_binned <- customer_data %>%
  mutate(
    age_group = cut(
      age, 
      breaks = age_boundaries, 
      labels = age_labels,
      right = TRUE # Determines if the interval is closed on the right
    )
  )

print(customer_data_binned)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The code successfully converts the continuous <code>age</code> variable into a new factor column, <code>age_group</code>, where each customer is assigned to a meaningful, business-defined demographic segment. This new feature is often much more interpretable and useful for stakeholders than the original continuous variable.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use <code>case_when()</code> for Complex Logic</h3>
    <div class="scenario-content">
        <p>For more complex binning rules that might involve multiple columns, <code>dplyr::case_when()</code> is an excellent and highly readable alternative to nested <code>if_else()</code> statements. It allows you to define a series of conditions and outcomes, making it perfect for implementing complex business logic.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>mtcars</code> dataset, let's create custom bins for horsepower (<code>hp</code>).</p>
        <ul class="prose-list">
            <li>Define a vector of boundaries to create three groups: "Low Power" (up to 120hp), "Medium Power" (120-200hp), and "High Power" (above 200hp). Remember to include the minimum and maximum values (e.g., 0, 120, 200, Inf).</li>
            <li>Define a corresponding vector of labels for these groups.</li>
            <li>Use <code>mutate()</code> and <code>cut()</code> to create a new <code>horsepower_group</code> column in the <code>mtcars</code> dataset.</li>
            <li>Use <code>janitor::tabyl()</code> to see how many cars fall into each of your custom power categories.</li>
        </ul>
    </div>
</div>
`,
          p2s6ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst creates bins for a feature using a supervised method on their entire dataset. Their model shows incredible accuracy. However, when deployed, it performs poorly because the bins were overfit to the specific noise in their dataset, a classic case of data leakage. This highlights the need for a rigorous set of best practices when binning features.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Decision Framework for Binning</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-sitemap"></i> Choosing a Binning Strategy</div>
            <div class="scenario">Start with your continuous feature and a clear goal for why you are binning it.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-question-circle"></i> Do you have strong domain knowledge or business rules?</div>
                        <div class="scenario"><strong>YES:</strong> Use <strong>Custom Binning</strong>. Your expertise is more valuable than what the data might suggest on its own. Create meaningful, interpretable bins.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-question-circle"></i> NO, let the data decide. Is the target variable involved?</div>
                        <div class="scenario"><strong>YES:</strong> Use <strong>Supervised Binning</strong> (e.g., with a decision tree). <br><strong>CRITICAL:</strong> You MUST do this within a cross-validation loop to prevent overfitting and data leakage.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-question-circle"></i> NO, use the feature's own distribution. Is it skewed?</div>
                        <div class="scenario"><strong>YES:</strong> Prefer <strong>Equal Frequency (Quantile) Binning</strong>. It is robust to outliers and creates balanced groups. <br><strong>NO (Symmetric):</strong> <strong>Equal Width Binning</strong> is a simple and reasonable choice.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Binning as a Precursor to One-Hot Encoding</h3>
    <div class="scenario-content">
        <p>After you have created your new binned feature (which is a factor), you can then treat it like any other categorical variable. For linear models, it's often a good idea to one-hot encode the new binned feature. This allows the model to assign a separate coefficient to each bin, effectively letting it learn the specific risk or effect associated with each interval. The <code>recipes</code> package can chain these steps together beautifully with <code>step_cut()</code> followed by <code>step_dummy()</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise to test your understanding.</p>
        <ul class="prose-list">
            <li>You have a feature for <code>years_of_experience</code> and you want to bin it to predict <code>salary</code>. Why might a simple equal-width binning approach be a poor choice?</li>
            <li>You are working for a bank and need to bin customer <code>age</code> for a credit model. The bank has legally defined age groups for their products. Which binning strategy should you use and why?</li>
            <li>You use supervised binning on your training data and find optimal splits at ages 25.5, 38.5, and 55.5. How do you apply these bins to your unseen test data?</li>
        </ul>
    </div>
</div>
`,
          p2s7ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst receives a dataset with a <code>transaction_date</code> column stored as text. The dates are in an inconsistent, non-standard format (e.g., "09/18/2025", "15-Jan-2024"). Before they can perform any time-based analysis, like calculating the time between transactions or plotting sales by month, they must first parse these messy strings into a standardized R date object.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s7ss1-concepts">The <code>lubridate</code> Package</button>
        <button class="tab-button" data-tab="p2s7ss1-viz">The Parsing Process</button>
        <button class="tab-button" data-tab="p2s7ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p2s7ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Making Dates and Times Simple</h4>
        <p>Working with dates and times in base R can be notoriously difficult. The <strong><code>lubridate</code></strong> package, part of the Tidyverse, provides a set of intuitive functions that make parsing, manipulating, and performing arithmetic with date/time data incredibly simple.</p>
        <p><strong>Analogy:</strong> Think of <code>lubridate</code> as a **multilingual translator for dates**. Your raw data speaks many different "dialects" of dates (<code>"YYYY-MM-DD"</code>, <code>"Month D, YYYY"</code>, <code>"D/M/YY"</code>). <code>lubridate</code> understands all of them. You simply tell it the "order" of the components (e.g., year, then month, then day), and it instantly translates the string into R's universal, standard date format.</p>
    </div>

    <div id="p2s7ss1-viz" class="tab-pane">
        <h4 class="subsection-title">From Messy Strings to Standard Dates</h4>
        <p>This diagram shows how <code>lubridate</code>'s helper functions can take a variety of inconsistent string formats and parse them correctly into a single, standardized date object that R can understand and compute with.</p>
        <div class="plot-container">
            <div id="p2s7ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> Each messy date string on the left is fed into the <code>lubridate</code> function that matches its specific Year-Month-Day order. The output for all of them is the same: a standard, unambiguous date object that R recognizes as September 18th, 2025.</p>
        </div>
    </div>
    
    <div id="p2s7ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Parsing and Manipulating in R</h4>
        <p>The core of <code>lubridate</code> is a family of functions named after the order of the date components.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("lubridate")
library(lubridate)

# --- 1. Parsing Strings into Dates ---
date_string1 <- "2025-09-18"
date_string2 <- "09/18/25"
date_string3 <- "18-Sep-2025"

# Use the function that matches the order: ymd, mdy, dmy
date1 <- ymd(date_string1)
date2 <- mdy(date_string2)
date3 <- dmy(date_string3)

class(date1) # Result: "Date"
print(date1)   # Result: "2025-09-18"

# --- 2. Manipulating Date Objects ---
# You can easily extract components
year(date1)   # Result: 2025
month(date1, label = TRUE) # Result: Sep
wday(date1, label = TRUE)  # Result: Thu

# You can also do arithmetic
date1 + days(10) # Result: "2025-09-28"
date1 - date2  # Result: Time difference of 9131 days
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Handling Date-Times</h3>
    <div class="scenario-content">
        <p>For data that includes time information (e.g., "2025-09-18 14:30:00"), <code>lubridate</code> has a corresponding set of functions that include hours (h), minutes (m), and seconds (s). For example, you would use <code>ymd_hms()</code> to parse the example string. These functions create a POSIXct object, which is R's standard for date-time data.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create the following character vector of dates: <code>messy_dates <- c("04/10/2021", "10/05/2022")</code>. Note the ambiguity: is the first date April 10th or October 4th? Let's assume the format is Month/Day/Year.</li>
            <li>Use the appropriate <code>lubridate</code> function to parse this vector into date objects.</li>
            <li>Create a new vector containing the day of the week for each of your parsed dates. (Hint: use <code>wday(..., label = TRUE)</code>).</li>
        </ul>
    </div>
</div>
`,
          p2s7ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An e-commerce company wants to model daily sales. Simply using the date as a feature isn't very effective. A good data scientist knows that sales are heavily influenced by temporal patterns like the day of the week, the month, and whether it's a holiday. They need to extract these specific signals from the date column to create powerful new features for their model.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s7ss2-concepts">Types of Features</button>
        <button class="tab-button" data-tab="p2s7ss2-viz">Visualizing Cyclical Features</button>
        <button class="tab-button" data-tab="p2s7ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p2s7ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Extracting Signals from Time</h4>
        <p>Once you have a clean date or date-time object, you can extract a wealth of new features from it.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Time-Stamp Components</strong>
                <p>These are the most basic features you can extract. They are useful for capturing trends and seasonal effects.</p>
                <ul class="prose-list">
                    <li><code>year()</code>, <code>month()</code>, <code>day()</code></li>
                    <li><code>wday()</code> (day of week), <code>qday()</code> (day of quarter)</li>
                    <li><code>hour()</code>, <code>minute()</code>, <code>second()</code></li>
                </ul>
            </div>
            <div class="decision-branch">
                <strong>Time-Based Features</strong>
                <p>These features calculate durations or counts relative to other events. They are often extremely powerful predictors.</p>
                 <ul class="prose-list">
                    <li>Time since a key event (e.g., days since customer's first purchase).</li>
                    <li>Time until the next key event (e.g., days until subscription renewal).</li>
                    <li>Rolling aggregates (e.g., sales in the last 7 days).</li>
                </ul>
            </div>
        </div>
    </div>
    
    <div id="p2s7ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Handling Cyclical Features</h4>
        <p>Features like "month" or "hour of day" are cyclical. For example, December (12) is as "close" to January (1) as February (2) is. A model won't understand this if you just use the numbers 1-12. A common technique is to encode these variables using sine and cosine transformations, which places them on a circle.</p>
        <div class="plot-container">
            <div id="p2s7ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This plot shows how the 12 months of the year can be represented by two new features: a sine component and a cosine component. This 2D representation correctly places December next to January, allowing a model to understand the cyclical nature of the year.</p>
        </div>
    </div>

    <div id="p2s7ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Creating a Rich Feature Set</h4>
        <p>Let's take a vector of dates and extract a variety of useful features.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(lubridate)
library(dplyr)

# Create a sequence of daily dates
dates <- tibble(
  date = seq(ymd("2025-01-01"), ymd("2025-12-31"), by = "day")
)

# Extract a rich set of features
date_features <- dates %>%
  mutate(
    # Basic components
    month = month(date, label = TRUE),
    day_of_week = wday(date, label = TRUE, week_start = 1),
    day_of_year = yday(date),
    
    # Cyclical features for the month
    month_sin = sin(2 * pi * month(date) / 12),
    month_cos = cos(2 * pi * month(date) / 12),

    # Indicator feature
    is_weekend = if_else(day_of_week %in% c("Sat", "Sun"), TRUE, FALSE)
  )

glimpse(date_features)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use Holiday Packages</h3>
    <div class="scenario-content">
        <p>Creating holiday features manually is tedious. Packages like <code>timeDate</code> or country-specific calendar packages can be used to easily generate a list of holidays for a given year. You can then check if your date falls on a holiday to create a powerful binary feature (<code>is_holiday</code>) that often has a huge impact on sales and user behavior models.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a date object for your own birthday this year using <code>ymd()</code>.</li>
            <li>What day of the week will it fall on? Use <code>wday()</code>.</li>
            <li>How many days will there be from today until your birthday? (Hint: Subtract <code>today()</code> from your birthday date object).</li>
            <li>What quarter of the year is your birthday in? Use <code>quarter()</code>.</li>
        </ul>
    </div>
</div>
`,
          p2s7ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst at a global company combines transaction logs from servers in New York ("2025-09-18 10:00"), London ("2025-09-18 15:00"), and Tokyo ("2025-09-18 22:00"). A naive analysis would treat these as different times. However, these times might all represent the *exact same instant* in universal time. To analyze sequences of events correctly, all timestamps must be converted to a single, standard time zone like UTC.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p2s7ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p2s7ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p2s7ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p2s7ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Challenge of Time Zones</h4>
        <p>A "local time" is only meaningful when you also know the location (the time zone). Failing to handle time zones correctly can lead to incorrect ordering of events, wrong calculations of durations, and flawed analysis. The best practice is to **convert all timestamps to Coordinated Universal Time (UTC)** as early as possible in your analysis.</p>
        <p><strong>Analogy:</strong> Time zones are like **different languages**. If you have reports written in English, Spanish, and Japanese, you can't compare them directly. You must first translate all of them into a single, common language (like English, or in this case, UTC) before you can understand the complete story.</p>
    </div>

    <div id="p2s7ss3-viz" class="tab-pane">
        <h4 class="subsection-title">One Instant, Many Clocks</h4>
        <p>This diagram shows how a single moment in time (represented by the UTC timestamp) corresponds to different local clock times around the world.</p>
        <div class="plot-container">
            <div id="p2s7ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The central point represents a single, absolute moment. The lines radiating outwards show the local time representation of that same moment in different time zones. The goal of time zone management is to take all these different local representations and convert them back to the single, unambiguous UTC standard.</p>
        </div>
    </div>
    
    <div id="p2s7ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Managing Zones with <code>lubridate</code></h4>
        <p><code>lubridate</code> provides two key functions for managing time zones:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong><code>force_tz()</code></strong>
                <p>Use this when your data is "naive" (has no time zone information) but you know what time zone it was recorded in. It **changes the time zone attribute without changing the clock time**.</p>
            </div>
            <div class="decision-branch">
                <strong><code>with_tz()</code></strong>
                <p>Use this to convert a date-time from one time zone to another. It **changes the clock time to represent the same instant in a new zone**.</p>
            </div>
        </div>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(lubridate)

# A naive timestamp recorded in New York
naive_time_str <- "2025-09-18 10:00:00"
time_obj <- ymd_hms(naive_time_str)

# 1. Force the correct time zone onto the naive object
ny_time <- force_tz(time_obj, tzone = "America/New_York")
print(ny_time)

# 2. Convert this instant to other time zones
# The clock time will change, but the instant is the same.
london_time <- with_tz(ny_time, tzone = "Europe/London")
tokyo_time <- with_tz(ny_time, tzone = "Asia/Tokyo")

print(london_time)
print(tokyo_time)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Always Store in UTC</h3>
    <div class="scenario-content">
        <p>The universal best practice for handling time series data is to convert all timestamps to UTC as the very first step after parsing. Do all your calculations and modeling in UTC. Only convert back to a local time zone at the very end of your analysis, for final reporting or visualization purposes. This eliminates a huge category of potential bugs and errors.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a date-time object for noon today in your current time zone: <code>my_time <- ymd_hms(paste(today(), "12:00:00"), tz = Sys.timezone())</code>.</li>
            <li>What time is it at this same instant in New York? Use <code>with_tz()</code> to convert your time object to the "America/New_York" time zone.</li>
            <li>What time is it at this same instant in UTC? Use <code>with_tz()</code> again.</li>
        </ul>
    </div>
</div>
`,
          p2s7ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist is building a model to predict hourly user activity. They create a feature for <code>avg_activity_last_3_hours</code>. When building the model, they calculate this feature using the entire dataset, then split into train and test. Their model performs amazingly well because the training data has "leaked" information from the future. Adhering to best practices would have prevented this critical error.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Best Practice Workflow for Time Series</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-gavel"></i> Best Practices Workflow</div>
            <div class="scenario">A robust workflow for handling date/time data prevents common errors and ensures reproducibility.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-sign-in-alt"></i> 1. Parse Explicitly</div>
                        <div class="scenario">Don't rely on guesswork. Use <code>lubridate</code> functions like <code>ymd_hms()</code> to convert raw text strings into proper date-time objects as the first step.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-globe"></i> 2. Convert to UTC</div>
                        <div class="scenario">Immediately convert all date-time objects to a single, unambiguous standard: UTC. Perform all calculations and modeling in UTC.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-secret"></i> 3. Avoid Future Leakage</div>
                        <div class="scenario">When creating features like rolling averages or "time since last event," ensure the calculation for a given timestamp only uses information from *before* that timestamp.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-sign-out-alt"></i> 4. Localize for Reporting</div>
                        <div class="scenario">Only convert timestamps back to a local time zone at the very final stage, for creating plots and reports for human consumption.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>slider</code> Package for Safe Rolling Windows</h3>
    <div class="scenario-content">
        <p>Calculating rolling or sliding window features (e.g., "average sales over the last 7 days") can be tricky to implement correctly without data leakage. The <code>slider</code> package provides a powerful and safe way to do this. Functions like <code>slide_dbl()</code> allow you to apply a function over a "sliding" window of your data, with precise control over the window's alignment to prevent it from seeing future data.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You have a daily dataset of stock prices and you want to create a feature <code>5_day_moving_average</code>.</li>
            <li>To predict the price for January 10th, what range of dates should you use to calculate the moving average feature? Why is it wrong to use the prices from Jan 8th to Jan 12th?</li>
            <li>You receive a new dataset with a column labeled <code>timestamp</code>. It contains values like <code>1663477200</code>. What is the first question you should ask about this data before doing any analysis? (Hint: It relates to time zones).</li>
        </ul>
    </div>
</div>
`,
          p3s1ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A senior manager asks an analyst for a "one-page summary" of a large, new sales dataset. They don't have time to look at the raw data or individual plots; they need a quick, high-level statistical overview of every variable to understand its basic properties and identify any potential red flags. This requires powerful data summarization tools.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s1ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p3s1ss1-viz">Visualization of Outputs</button>
        <button class="tab-button" data-tab="p3s1ss1-usecase">Key Functions in R</button>
    </div>

    <div id="p3s1ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Distilling Data to its Essence</h4>
        <p><strong>Data Summarization</strong> is the process of computing a compact set of descriptive statistics for each variable in a dataset. It's a foundational step in EDA that provides a bird's-eye view of your data's characteristics.</p>
        <p>A good summary should quickly answer questions like:</p>
        <ul class="prose-list">
            <li>What is the central tendency (e.g., mean, median) of this numeric variable?</li>
            <li>How spread out is this variable (e.g., standard deviation)?</li>
            <li>Are there missing values? How many?</li>
            <li>What are the unique categories for this character variable?</li>
        </ul>
        <p><strong>Analogy:</strong> Data summarization is like an **executive summary** at the beginning of a long business report. The full report contains all the details (the raw data), but the executive summary distills the most critical information—key findings, major trends, and important numbers—into a condensed format that allows a decision-maker to grasp the main points quickly.</p>
    </div>
    
    <div id="p3s1ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Comparing Summary Outputs</h4>
        <p>While base R's <code>summary()</code> is useful, the <code>skimr</code> package provides a much richer and more readable console-based summary, especially for EDA.</p>
        <div class="plot-container">
            <div id="p3s1ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This diagram compares the outputs for the same data. The base R <code>summary()</code> (left) provides a simple six-number summary. The <code>skimr::skim()</code> output (right) is far more detailed, providing missing value counts, standard deviation, a wider range of percentiles, and even a small inline histogram (a "spark graph") to give you an immediate sense of the data's distribution shape.</p>
        </div>
    </div>

    <div id="p3s1ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Step-by-Step Summarization</h4>
        <p>Let's use both base R and <code>skimr</code> to summarize the <code>airquality</code> dataset.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# --- Method 1: Base R summary() ---
# Provides a simple six-number summary for numeric columns
# and counts for factors.
summary(airquality)

# --- Method 2: skimr::skim() (Recommended for EDA) ---
# install.packages("skimr")
library(skimr)

# skim() gives a much richer, more organized summary
skim(airquality)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The <code>skim()</code> output immediately tells us not just the mean and median, but also the number of missing values (<code>n_missing</code>), the completion rate, the standard deviation (<code>sd</code>), a full range of percentiles (p0, p25, p50, p75, p100), and that handy histogram. This is a far more effective starting point for an analysis than the basic summary.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Grouped Summaries</h3>
    <div class="scenario-content">
        <p>The real power of these tools is unleashed when combined with <code>dplyr::group_by()</code>. Running <code>airquality %>% group_by(Month) %>% skim()</code> will produce a separate, detailed summary for each month, allowing you to quickly compare the statistical properties of your data across different categories. This is an incredibly powerful and fast way to generate initial hypotheses.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Load the <code>ggplot2</code> library to access the <code>msleep</code> dataset.</li>
            <li>Run the base R <code>summary()</code> function on <code>msleep</code>.</li>
            <li>Now, run <code>skimr::skim()</code> on the <code>msleep</code> dataset.</li>
            <li>Compare the outputs. What piece of information does <code>skim()</code> give you for character variables (like <code>vore</code>) that <code>summary()</code> does not?</li>
            <li>Using the <code>skim()</code> output, what is the average sleep_total? Which variable has the most missing values?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p3s1ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is handed a new dataset in an <code>.Rds</code> file with no documentation. Before any analysis can begin, they must perform a quick "reconnaissance" to answer fundamental questions: What is the shape of the data (rows and columns)? What are the column names? What type of data does each column hold (numbers, text, dates)?</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s1ss2-concepts">The Tools of Inspection</button>
        <button class="tab-button" data-tab="p3s1ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p3s1ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">A First Look at Your Data</h4>
        <p>Structure inspection is the essential first step of any analysis. It provides a high-level overview of your dataset's contents and layout.</p>
        <p><strong>Analogy:</strong> This process is like looking at the **table of contents and the first page of a new book**. It doesn't tell you the whole story, but it gives you a crucial overview of the book's structure, chapter names, and writing style before you commit to reading the whole thing.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong><code>glimpse()</code> (Recommended)</strong>
                <p>From the <code>dplyr</code> package, this is the modern, readable choice. It provides a compact, transposed view of the data that is easy to scan.</p>
            </div>
            <div class="decision-branch">
                <strong><code>str()</code> (The Classic)</strong>
                <p>A base R function that provides a more detailed, but often more cluttered, view of the object's <strong>str</strong>ucture. It is very powerful for complex objects like lists or models.</p>
            </div>
            <div class="decision-branch">
                <strong><code>head()</code> & <code>tail()</code></strong>
                <p>Base R functions that let you "peek" at the first or last 6 rows of your data frame, respectively. Useful for seeing example values.</p>
            </div>
        </div>
    </div>
    
    <div id="p3s1ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Inspecting the <code>iris</code> Dataset</h4>
        <p>Let's use <code>glimpse()</code> to get a quick, comprehensive look at the familiar <code>iris</code> dataset.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
glimpse(iris)
            </code></pre>
        </div>
        <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">How to Read the Output:</h5>
        <div class="code-container">
            <pre><code class="language-text">
Rows: 150         # The number of observations in the dataset.
Columns: 5          # The number of variables (features).
$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, ... # Column name, type, and first few values.
$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, ...
$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, ...
$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, ...
$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, set... # Note: <fct> means Factor (categorical).
            </code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> In seconds, we've learned the dataset has 150 rows and 5 columns. The first four columns are numeric (<code><dbl></code> for double-precision numbers), and the last column, <code>Species</code>, is a factor with the first level being "setosa". This initial check is fundamental before proceeding to quality assessment.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Chaining Inspection with the Pipe</h3>
    <div class="scenario-content">
        <p>Because <code>glimpse()</code> is part of the Tidyverse, it works beautifully with the pipe. It's common to see it at the end of a long data wrangling pipeline (<code>... %>% glimpse()</code>) as a final check to ensure all the previous steps produced the expected output structure and data types.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The <code>msleep</code> dataset (from <code>ggplot2</code>) contains data about mammal sleep habits. Load the <code>ggplot2</code> library to make it available.</li>
            <li>Use <code>glimpse()</code> on the <code>msleep</code> dataset.</li>
            <li>How many rows and columns does it have?</li>
            <li>How many character columns (<code><chr></code>) are there? What are their names?</li>
            <li>Use <code>head()</code> to see the first few rows of the actual data.</li>
        </ul>
    </div>
</div>
`,
          p3s1ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has inspected the structure of a new dataset. Now, they must perform a more detailed check for common problems that could invalidate their analysis. Are there missing values that need to be handled? Are there duplicate records from a faulty data export? Are there any obvious inconsistencies that need to be addressed before modeling?</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s1ss3-concepts">Core Tasks</button>
        <button class="tab-button" data-tab="p3s1ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p3s1ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p3s1ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Looking for Trouble</h4>
        <p>Data quality assessment is the process of profiling your data to uncover issues that need to be addressed during the cleaning phase.</p>
        <p><strong>Analogy:</strong> This process is like being a <strong>building inspector</strong>. The initial structure inspection showed you the blueprint, but now you need to go inside and check for specific flaws: leaky pipes (missing values), faulty wiring (inconsistencies), and identical, redundant rooms (duplicate rows).</p>
        <p>The two most fundamental checks are:</p>
        <ul class="prose-list">
            <li><strong>Missing Values (<code>NA</code>s):</strong> Identifying the number and pattern of missing values is critical, as they can cause errors in many functions and may require imputation.</li>
            <li><strong>Duplicate Rows:</strong> Duplicate records can artificially inflate counts and skew the results of your analysis. They must be identified and typically removed.</li>
        </ul>
    </div>
    
    <div id="p3s1ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Missingness per Column</h4>
        <p>A simple bar chart is often the most effective way to quickly see which columns have the most severe missing data problems.</p>
        <div class="plot-container">
            <div id="p3s1ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This plot, generated by <code>naniar::gg_miss_var()</code>, shows the count of missing values for each column in the <code>airquality</code> dataset. It immediately draws your attention to the <code>Ozone</code> column as having a significant missing data problem that must be addressed, while <code>Solar.R</code> has a smaller issue.</p>
        </div>
    </div>

    <div id="p3s1ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">A Quality Assessment Workflow</h4>
        <p>Let's perform both checks on the <code>airquality</code> dataset after adding a duplicate row.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(janitor)

# Use airquality data and add a duplicate row for demonstration
aq_dupes <- rbind(airquality, airquality[1, ])

# --- 1. Check for Missing Values ---
# Get a count of NAs for each column
colSums(is.na(aq_dupes))

# For a visual, the naniar package is excellent:
# install.packages("naniar")
# naniar::gg_miss_var(aq_dupes)

# --- 2. Check for Duplicate Rows ---
# Use janitor's get_dupes() to find and inspect duplicate records
# It shows both the original and the duplicate row.
aq_dupes %>%
  get_dupes()
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The output from <code>colSums(is.na(...))</code> would show 37 missing <code>Ozone</code> values and 7 missing <code>Solar.R</code> values. The output from <code>get_dupes()</code> would show rows 1 and 154, confirming the presence of the duplicate we added. This assessment gives us a clear to-do list for the data cleaning phase.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Check for Inconsistencies with <code>count()</code></h3>
    <div class="scenario-content">
        <p>A common data quality issue is inconsistent factor levels (e.g., "USA", "U.S.A.", "United States"). A quick way to find these is to use <code>dplyr::count()</code> on your categorical columns. For example, <code>my_data %>% count(country, sort = TRUE)</code> will give you a frequency table that makes it easy to spot slightly different versions of the same category.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The <code>airquality</code> dataset has 153 rows.</li>
            <li>Create a new data frame called <code>aq_no_na</code> by removing all rows with missing values using <code>tidyr::drop_na()</code>. How many rows are in this new data frame? How many were dropped?</li>
            <li>Use the <code>count()</code> function on the original <code>airquality</code> dataset to see the frequency of each <code>Month</code>. Are the observations balanced across months?</li>
        </ul>
    </div>
</div>
`,
          p3s1ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> Before building a model to predict a car's fuel efficiency (<code>mpg</code>), an analyst needs to understand how the other variables relate to it and to each other. Which features are strongly positively or negatively correlated with <code>mpg</code>? Are any of the predictor variables highly correlated with each other (a condition called multicollinearity)? A correlation matrix can provide these insights at a glance.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s1ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p3s1ss4-viz">Visualization with a Heatmap</button>
        <button class="tab-button" data-tab="p3s1ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p3s1ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Summarizing Pairwise Relationships</h4>
        <p>A **correlation matrix** is a table that shows the correlation coefficients between many variables at once. Each cell in the table shows the correlation between two variables. The diagonal of the matrix is always 1, as a variable is perfectly correlated with itself.</p>
        <p>It is the single most powerful tool for getting a rapid overview of the linear relationships within your data.</p>
        <p><strong>Analogy:</strong> A correlation matrix is like a **social relationship map for your variables**. It quickly tells you who the "best friends" are (strong positive correlation, close to +1), who the "enemies" are (strong negative correlation, close to -1), and who doesn't know each other (no linear correlation, close to 0).</p>
    </div>
    
    <div id="p3s1ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Correlation Heatmap</h4>
        <p>The best way to visualize a correlation matrix is with a heatmap, where the color of each cell represents the strength and direction of the correlation.</p>
        <div class="plot-container">
            <div id="p3s1ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This heatmap shows the correlations for the <code>mtcars</code> dataset. Dark red cells indicate strong positive correlations (e.g., <code>cyl</code> and <code>disp</code>). Dark blue cells indicate strong negative correlations (e.g., <code>mpg</code> and <code>wt</code>). Lighter colors are closer to zero. By hovering over a cell, you can see the exact correlation coefficient. This instantly reveals the strongest relationships in the data.</p>
        </div>
    </div>

    <div id="p3s1ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Creating a Correlation Matrix in R</h4>
        <p>The base R <code>cor()</code> function calculates the matrix, and we can use <code>plotly</code> to create the heatmap.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(dplyr)
library(plotly)

# Select only the numeric columns from mtcars
mtcars_numeric <- mtcars %>% select_if(is.numeric)

# 1. Calculate the correlation matrix
cor_matrix <- cor(mtcars_numeric)
print(round(cor_matrix, 2)) # Print the rounded matrix to the console

# 2. Visualize with a heatmap
plot_ly(
  x = colnames(cor_matrix), 
  y = colnames(cor_matrix),
  z = cor_matrix, 
  type = "heatmap",
  colorscale = "RdBu", # Red-Blue colorscale is great for correlations
  zmin = -1, zmax = 1  # Fix the color scale from -1 to 1
) %>%
  layout(title = "Correlation Heatmap of mtcars Dataset")
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> From the matrix and heatmap, the analyst can quickly see that <code>mpg</code> is strongly negatively correlated with <code>wt</code> (weight) and <code>disp</code> (displacement), which is expected. They can also see that <code>wt</code> and <code>disp</code> are very highly correlated with each other, which is a warning sign for multicollinearity if both were included in a linear model.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: ggcorrplot for ggplot2 Fans</h3>
    <div class="scenario-content">
        <p>If you prefer to work within the <code>ggplot2</code> ecosystem, the <code>ggcorrplot</code> package is an excellent tool. The function <code>ggcorrplot(cor_matrix)</code> creates a beautiful and highly customizable heatmap using <code>ggplot2</code> grammar. You can easily add the correlation values as text, reorder the matrix, and remove the upper triangle to make it even more readable.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Select only the numeric columns of the <code>iris</code> dataset.</li>
            <li>Calculate the correlation matrix for these four variables using <code>cor()</code>.</li>
            <li>Which two variables have the strongest positive correlation?</li>
            <li>Which two variables have the weakest (closest to 0) correlation?</li>
            <li>Try to create a heatmap of this matrix using the <code>plotly</code> code from the example.</li>
        </ul>
    </div>
</div>
`,
          p3s1ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is about to perform a t-test, which assumes that the underlying data is approximately normally distributed. Before running the test, they must perform a distribution assessment on their sample data. Is the data roughly symmetric and bell-shaped, or is it skewed to one side? The choice of statistical test may depend on this assessment.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s1ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p3s1ss5-viz">Visualizing Distributions</button>
        <button class="tab-button" data-tab="p3s1ss5-usecase">Step-by-Step Example</button>
    </div>

    <div id="p3s1ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Understanding the Shape of Your Data</h4>
        <p>A distribution describes the frequency of different values for a variable. Assessing the distribution allows you to understand its **central tendency**, **dispersion**, and **shape** (e.g., symmetry, skewness).</p>
        <p>This is a critical step in EDA because many statistical models and tests make assumptions about the distribution of the data (most commonly, the assumption of normality).</p>
        <p><strong>Analogy:</strong> Assessing a variable's distribution is like understanding the **demographics of a city**. A histogram is like an age-distribution chart. Does the city have a lot of young people (left-skewed), a lot of old people (right-skewed), or an even spread (symmetric)? Knowing the shape of the population is vital for city planning (or in our case, model planning).</p>
    </div>
    
    <div id="p3s1ss5-viz" class="tab-pane">
        <h4 class="subsection-title">A Trio of Visual Tools</h4>
        <p>No single plot tells the whole story. Using a histogram, density plot, and boxplot together provides a comprehensive view of a variable's distribution.</p>
        <div class="plot-container">
            <div id="p3s1ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This shows the same right-skewed data visualized in three ways. The **Histogram** shows the frequency counts in discrete bins. The **Density Plot** provides a smoothed estimate of the distribution's shape. The **Boxplot** clearly summarizes the quartiles and flags the high-end outliers that are causing the skew.</p>
        </div>
    </div>

    <div id="p3s1ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Assessing <code>Ozone</code> in <code>airquality</code></h4>
        <p>The <code>Ozone</code> column in the <code>airquality</code> dataset is a classic example of skewed data.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(ggplot2)
library(patchwork) # For combining plots
library(dplyr)

# Remove NA values for plotting
ozone_data <- airquality %>% filter(!is.na(Ozone))

# 1. Histogram
p1 <- ggplot(ozone_data, aes(x = Ozone)) +
  geom_histogram(bins = 15, fill = "#3b82f6", alpha = 0.7) +
  theme_minimal() +
  ggtitle("Histogram")

# 2. Density Plot
p2 <- ggplot(ozone_data, aes(x = Ozone)) +
  geom_density(fill = "#10b981", alpha = 0.7) +
  theme_minimal() +
  ggtitle("Density Plot")

# 3. Boxplot
p3 <- ggplot(ozone_data, aes(y = Ozone)) +
  geom_boxplot() +
  theme_minimal() +
  ggtitle("Boxplot") +
  theme(axis.text.x = element_blank()) # Hide x-axis text

# Display plots together (in RStudio)
(p1 + p2) / p3
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> All three plots confirm that the Ozone data is positively (right) skewed. The histogram and density plots show a long tail to the right. The boxplot shows the median line is close to the bottom of the box, and there are several high-end outliers. A statistical test assuming normality would be inappropriate for this raw data.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Q-Q Plot for a Formal Normality Check</h3>
    <div class="scenario-content">
        <p>For a more rigorous check of normality, use a Quantile-Quantile (Q-Q) plot. In R, you can create one with <code>ggplot(my_data, aes(sample = my_variable)) + stat_qq() + stat_qq_line()</code>. If the data is normally distributed, the points will fall closely along the straight diagonal line. Deviations from the line indicate deviations from normality (like skewness or heavy tails).</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The <code>Sepal.Length</code> column in the <code>iris</code> dataset is known to be fairly symmetric and normally distributed.</li>
            <li>Create a histogram, a density plot, and a boxplot for <code>iris$Sepal.Length</code>.</li>
            <li>Compare these plots to the ones for the skewed <code>Ozone</code> data. How do they differ? What features of the plots indicate that the <code>Sepal.Length</code> distribution is more symmetric?</li>
        </ul>
    </div>
</div>
`,
          p3s2ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst needs to get a quick but comprehensive overview of a brand new, wide dataset with over 100 columns. Creating summary tables and plots for each variable individually would be extremely time-consuming. They need a tool that can automatically generate a full Exploratory Data Analysis (EDA) report with a single command.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s2ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p3s2ss1-viz">Anatomy of a Report</button>
        <button class="tab-button" data-tab="p3s2ss1-usecase">Use Case in R</button>
    </div>

    <div id="p3s2ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">EDA on Autopilot</h4>
        <p><strong>Automated Exploratory Data Analysis (AutoEDA)</strong> tools are packages designed to scan a dataset and automatically generate a detailed report containing the most common EDA summaries and visualizations.</p>
        <p>This is invaluable at the start of a project for rapidly understanding the data's structure, quality, and basic relationships. The leading package for this in R is <code>DataExplorer</code>.</p>
        <p><strong>Analogy:</strong> AutoEDA tools are like hiring a **team of junior analysts**. You give them the dataset, and they come back in minutes with a fully prepared binder containing every basic chart (histograms, bar charts), summary table (missing values, quartiles), and analysis (correlations) you could ask for. This frees you up to focus on the more complex, high-level insights.</p>
    </div>
    
    <div id="p3s2ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The <code>DataExplorer</code> Report</h4>
        <p>The HTML report generated by <code>DataExplorer</code> is a comprehensive, interactive dashboard with a sidebar for navigating the different sections of the analysis.</p>
        <div class="plot-container">
            <div id="p3s2ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This diagram simulates the structure of the output report. It has a sidebar for easy navigation between key sections like "Basic Statistics," distributions for discrete and continuous variables ("Univariate Analysis"), "Correlation Analysis," and more. The actual report contains dozens of detailed plots and tables.</p>
        </div>
    </div>

    <div id="p3s2ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">One Line of Code, One Full Report</h4>
        <p>The power of <code>DataExplorer</code> lies in its simplicity. The <code>create_report()</code> function does all the work.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("DataExplorer")
library(DataExplorer)

# Use the full diamonds dataset from ggplot2
library(ggplot2)
data(diamonds)

# --- Generate the entire interactive HTML report with one command ---
# This will create a file named "report.html" in your working directory.
create_report(
  data = diamonds,
  output_file = "diamonds_eda_report.html",
  output_dir = getwd(), # Save in current project folder
  y = "price" # Optionally specify a target variable for more detailed analysis
)
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> After running this single command, you will have a complete, multi-page HTML file. Opening this file in a web browser will give you an interactive dashboard to explore every variable, its distribution, its missing values, and its relationship with other variables and the specified target, <code>price</code>.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Configure and Customize</h3>
    <div class="scenario-content">
        <p>The <code>create_report()</code> function is highly configurable. You can pass a global configuration list to change plot types, colors, and which analyses to include or exclude. For example, <code>config = configure_report(add_plot_qq = FALSE)</code> would generate a report without the Q-Q plots. This allows you to tailor the report to your specific needs.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Load the <code>ggplot2</code> library and the <code>msleep</code> dataset with <code>data(msleep)</code>.</li>
            <li>Run <code>create_report(msleep)</code>.</li>
            <li>Open the generated <code>report.html</code> file.</li>
            <li>Navigate to the "Correlation Analysis" section. Which two variables have the strongest positive correlation in the dataset?</li>
        </ul>
    </div>
</div>
`,
          p3s2ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst doesn't need a full, multi-page HTML report of their data. Instead, they want a very quick yet comprehensive summary of the dataset's health printed directly in their R console or R Markdown document. They need a tool for rapid, in-line data profiling.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s2ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p3s2ss2-usecase">Use Case in R</button>
    </div>

    <div id="p3s2ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">A Quick Look Under the Hood</h4>
        <p><strong>Data Profiling</strong> is the process of generating concise, high-level summary statistics about a dataset. It's designed to give you a quick "feel" for the data's characteristics and quality without the overhead of a full graphical report.</p>
        <p>The <code>skimr</code> package provides the quintessential function for this task: <code>skim()</code>. It's praised for its intelligent and beautifully formatted console output.</p>
        <p><strong>Analogy:</strong> If <code>DataExplorer</code> is a full medical check-up that results in a detailed, multi-page report, then <code>skimr::skim()</code> is like the nurse taking your **vital signs**. It gives you a quick, essential summary of the patient's (your data's) health—like completion rate, mean, min/max, and a tiny histogram—right on the clipboard at the front of the bed (your console).</p>
    </div>
    
    <div id="p3s2ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Reading the <code>skim()</code> Output</h4>
        <p>The output of <code>skim()</code> is organized by data type and is designed to be easy to read.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("skimr")
library(skimr)

# Run skim() on a dataset with missing values and different data types
skim(airquality)
</code></pre>
        </div>
        <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Example Console Output:</h5>
        <div class="code-container">
            <pre><code class="language-text">
-- Data Summary ------------------------
                           Values
Name                       airquality
Number of rows             153     
Number of columns          6       
_______________________
Column type frequency:
  numeric                  6       
_______________________
Group variables            None    

-- Variable type: numeric ---------------------------------------------------------------------------------------
  skim_variable n_missing complete_rate   mean     sd    p0    p25    p50    p75   p100 hist 
1 Ozone                37         0.758  42.1   33.0     1    18     31.5   63.2    168 ▇▃   
2 Solar.R               7         0.954 186.    90.1     7   116.   205    259.     334 ▇▇▇▇▇
3 Wind                  0         1      9.96   3.52   1.7   7.4     9.7   11.5    20.7 ▆▇▇▂ 
4 Temp                  0         1     77.9    9.47  56    72      79     85      97   ▃▇▇▅
5 Month                 0         1      6.99   1.42   5     6       7      8       9  ▇▇▅▇▇
6 Day                   0         1     15.8    8.86   1     8      16     23      31  ▇▇▇▇▇
            </code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> In one glance, you get the number of missing values and completion rate for each variable, standard summary statistics (mean, sd, percentiles), and a small inline histogram (a "spark graph") that gives you a rough idea of the distribution shape.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: skim() inside a dplyr Pipe</h3>
    <div class="scenario-content">
        <p><code>skim()</code> works beautifully within a <code>dplyr</code> pipeline. A common workflow is to perform some initial cleaning or grouping, and then pass the result to <code>skim()</code> to quickly check the result. For example: <code>airquality %>% group_by(Month) %>% skim()</code> will produce a separate <code>skim</code> summary for each month!</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Load the <code>dplyr</code> package.</li>
            <li>Run <code>skim()</code> on the <code>msleep</code> dataset.</li>
            <li>Which numeric variable has the most missing values?</li>
            <li>What is the mean, 50th percentile (median), and 100th percentile (max) for the <code>sleep_total</code> variable?</li>
            <li>Look at the tiny histogram for <code>sleep_total</code>. Does the distribution appear to be skewed or symmetric?</li>
        </ul>
    </div>
</div>
`,
          p3s2ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A junior analyst runs an AutoEDA report, sees a high correlation between two variables, and immediately tells the CEO that "X causes Y." A senior analyst gently intervenes, explaining that these automated tools are a fantastic starting point for generating questions and hypotheses, but they are not a replacement for deep, critical thinking and rigorous statistical testing.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Human in the Loop</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-brain"></i> The Role of the Analyst</div>
            <div class="scenario">AutoEDA tools are powerful assistants, not replacements. Their purpose is to accelerate the tedious parts of EDA so the analyst can focus on what humans do best: asking intelligent questions and applying domain knowledge.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-cogs"></i> AutoEDA's Job: Hypothesis Generation</div>
                        <div class="scenario">The tool scans the data and presents hundreds of patterns, correlations, and potential issues. Its output is a set of "interesting leads" or questions.<br><em>e.g., "It looks like Ozone and Temp are highly correlated."</em></div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-tie"></i> The Analyst's Job: Hypothesis Testing</div>
                        <div class="scenario">The analyst takes these leads, applies their domain expertise, and designs a rigorous approach to investigate them. They move from observation to conclusion.<br><em>e.g., "This correlation makes physical sense. Let's build a model to quantify the relationship and check for confounding variables."</em></div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Look for the Unexpected</h3>
    <div class="scenario-content">
        <p>The greatest value of AutoEDA tools is not in confirming what you already expect (e.g., "sales are higher in December"), but in uncovering patterns you *didn't* expect. When you review a report, actively look for the surprising correlations, the strange distributions, or the unusual patterns in missingness. These are often the starting points for the most valuable insights.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You run an AutoEDA report on a customer dataset and it shows a strong negative correlation between <code>customer_satisfaction_score</code> and <code>time_spent_on_hold_with_support</code>.</li>
            <li>What is the initial hypothesis generated by the tool?</li>
            <li>What is a follow-up step *you*, the human analyst, would take to investigate this relationship more deeply? What kind of plot would you make?</li>
            <li>Why would it be wrong to immediately conclude that hold time *causes* low satisfaction based only on this report? (Hint: Think about confounding variables).</li>
        </ul>
    </div>
</div>
`,
          p3s3ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A marketing manager at an e-commerce company in Karachi believes a new ad campaign on a local website increased the average number of daily signups from the historical baseline of 150. After running the campaign for a month, the new average is 162. They need to formulate a precise, testable statistical hypothesis to determine if this increase is a real effect or just random daily fluctuation. 🇵🇰</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s3ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p3s3ss1-analogy">The Courtroom Analogy</button>
        <button class="tab-button" data-tab="p3s3ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p3s3ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p3s3ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Stating the Claim and the Status Quo</h4>
        <p>Hypothesis formulation is the process of translating a research question into two precise, opposing statements. This is the first and most critical step of any statistical test.</p>
        <ul class="prose-list">
            <li><strong>Null Hypothesis ($H_0$):</strong> This is the default assumption, the "status quo," or the "no effect" position. It's the statement we seek to find evidence against.</li>
            <li><strong>Alternative Hypothesis ($H_A$ or $H_1$):</strong> This is the new claim or the effect we are trying to prove. It's what we will conclude if we find the null hypothesis to be unlikely.</li>
        </ul>
        <p>💡 **Crucially, we never "accept" the null hypothesis.** We only "fail to reject" it, which means we did not find sufficient evidence to support the alternative claim. The burden of proof is always on the alternative hypothesis.</p>
    </div>
    
    <div id="p3s3ss1-analogy" class="tab-pane">
        <h4 class="subsection-title">Innocent Until Proven Guilty</h4>
        <p>The best analogy for hypothesis testing is a courtroom trial:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Null Hypothesis ($H_0$)</strong>
                <p>The defendant is presumed innocent. This is the default state.</p>
            </div>
            <div class="decision-branch">
                <strong>Alternative Hypothesis ($H_A$)</strong>
                <p>The prosecutor's claim: the defendant is guilty.</p>
            </div>
        </div>
       <div class="plot-description" style="margin-top: 1rem;">
            <p>The prosecutor must present strong evidence (the data) to convince the jury to reject the assumption of innocence. If the evidence isn't strong enough (a high p-value), the jury delivers a "not guilty" verdict. This doesn't mean they proved the defendant is innocent, only that there wasn't enough evidence to convict.</p>
        </div>
    </div>
    
    <div id="p3s3ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Hypothesis Flow</h4>
        <p>This diagram visualizes the process of breaking down a general research question into the two opposing, testable hypotheses that form the foundation of a statistical test.</p>
        <div class="plot-container">
            <div id="p3s3ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The process starts at the top with a high-level research question. This is then split into two mutually exclusive and exhaustive statements: the Null Hypothesis ($H_0$), which represents the default state of no effect, and the Alternative Hypothesis ($H_A$), which represents the new claim or effect the researcher wants to prove.</p>
        </div>
    </div>

    <div id="p3s3ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Formulating the Signup Hypothesis</h4>
        <p>Let's translate our Karachi marketing scenario into formal hypotheses. The key parameter we are interested in is the true mean number of daily signups ($\\mu$).</p>
        <div class="mindmap-container">
            <div class="mindmap-root">
                <div class="title"><i class="fas fa-question"></i> Research Question</div>
                <div class="scenario">Did the new ad campaign increase the average daily signups above 150?</div>
            </div>
            <div class="mindmap-branches-container">
                <div class="mindmap-branches">
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-balance-scale"></i> Null Hypothesis ($H_0$)</div>
                            <div class="scenario">The campaign had no effect. The true mean of daily signups is still the same as the historical baseline.
                            <div class="math-foundation" style="margin-top:0.5rem;">$$ H_0: \\mu \\le 150 $$</div>
                            </div>
                        </div>
                    </div>
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-lightbulb"></i> Alternative Hypothesis ($H_A$)</div>
                            <div class="scenario">The campaign worked. The true mean of daily signups is now greater than the baseline.
                            <div class="math-foundation" style="margin-top:0.5rem;">$$ H_A: \\mu > 150 $$</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: One-Tailed vs. Two-Tailed Tests</h3>
    <div class="scenario-content">
        <p>Our scenario uses a **one-tailed** (or one-sided) alternative hypothesis (<code>> 150</code>) because we only care if the campaign *increased* signups. A **two-tailed** test would be used if we wanted to know if the campaign caused any change at all (increase *or* decrease). The alternative hypothesis would be $H_A: \\mu \\ne 150$. You should choose your test based on your research question before looking at the data.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>A car manufacturer claims their new model gets "at least 25 miles per gallon." You suspect it's less.</p>
        <ul class="prose-list">
            <li>What is the parameter of interest ($\\mu$)?</li>
            <li>What is the Null Hypothesis ($H_0$), representing the manufacturer's claim?</li>
            <li>What is your Alternative Hypothesis ($H_A$), representing your suspicion?</li>
            <li>Is this a one-tailed or two-tailed test?</li>
        </ul>
    </div>
</div>
`,
          p3s3ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has several questions about a dataset: 1) Is the average weight of male penguins different from female penguins? 2) Is there a difference in average bill length across three different penguin species? 3) Is there a relationship between a penguin's species and the island it lives on? Each of these questions requires a different statistical tool.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">Choosing the Right Tool for the Job</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-wrench"></i> What is your research question?</div>
            <div class="scenario">The type of data you have and the question you ask determine which statistical test you should use.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title">Comparing Means?</div>
                        <div class="scenario">
                            <ul class="phase-topics" style="margin-top:0;">
                                <li><strong>2 Groups:</strong> T-Test (e.g., male vs. female weight)</li>
                                <li><strong>3+ Groups:</strong> ANOVA (e.g., bill length across 3 species)</li>
                            </ul>
                        </div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title">Testing a Relationship?</div>
                        <div class="scenario">
                            <ul class="phase-topics" style="margin-top:0;">
                                <li><strong>2 Continuous Variables:</strong> Correlation Test (e.g., bill length vs. flipper length)</li>
                                <li><strong>2 Categorical Variables:</strong> Chi-Squared Test (e.g., species vs. island)</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s3ss2-ttest">T-Tests</button>
        <button class="tab-button" data-tab="p3s3ss2-anova">ANOVA</button>
        <button class="tab-button" data-tab="p3s3ss2-chisq">Chi-Squared Test</button>
    </div>

    <div id="p3s3ss2-ttest" class="tab-pane active">
        <h4 class="subsection-title">Comparing the Means of Two Groups</h4>
        <p>The t-test is used to determine if there is a significant difference between the means of two groups. The base R function is <code>t.test()</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# H0: There is no difference in extra sleep between the two drug groups.
# HA: There is a difference.
t.test(extra ~ group, data = sleep)
            </code></pre>
        </div>
    </div>
    <div id="p3s3ss2-anova" class="tab-pane">
        <h4 class="subsection-title">Comparing the Means of Three or More Groups</h4>
        <p>ANOVA (Analysis of Variance) is used to determine if there are any statistically significant differences between the means of three or more independent groups. The base R function is <code>aov()</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# H0: There is no difference in mean sepal length among the three species.
# HA: At least one species has a different mean sepal length.
anova_result <- aov(Sepal.Length ~ Species, data = iris)
summary(anova_result)
            </code></pre>
        </div>
    </div>
    <div id="p3s3ss2-chisq" class="tab-pane">
        <h4 class="subsection-title">Testing for Independence Between Two Categorical Variables</h4>
        <p>The Chi-Squared test is used to determine if there is a significant association between two categorical variables. It compares the observed frequencies in a contingency table to the frequencies that would be expected if the variables were independent.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# H0: The number of cylinders is independent of the transmission type.
# HA: There is an association between cylinders and transmission type.
contingency_table <- table(mtcars$cyl, mtcars$am)
chisq.test(contingency_table)
            </code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Non-Parametric Alternatives</h3>
    <div class="scenario-content">
        <p>Many common tests (like the t-test and ANOVA) assume that your data is approximately normally distributed. If this assumption is violated, you should use a **non-parametric** alternative. For the t-test, the alternative is the **Wilcoxon rank-sum test** (<code>wilcox.test()</code>). For ANOVA, it's the **Kruskal-Wallis test** (<code>kruskal.test()</code>). These tests work on the ranks of the data rather than the actual values, making them robust to non-normality and outliers.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>mtcars</code> dataset:</p>
        <ul class="prose-list">
            <li>What test would you use to see if there's a significant difference in horsepower (<code>hp</code>) between cars with V-shaped engines (<code>vs = 0</code>) and straight engines (<code>vs = 1</code>)? Run this test in R.</li>
            <li>What test would you use to see if there's a significant difference in miles per gallon (<code>mpg</code>) across the different cylinder groups (4, 6, and 8)? Run this test in R.</li>
        </ul>
    </div>
</div>
`,
          p3s3ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst runs a t-test on their marketing campaign data and gets the following output from R: <code>p-value = 0.021</code>, <code>95 percent confidence interval: [2.5, 21.5]</code>. They now need to translate these numbers into a clear, actionable conclusion for their manager, explaining not just *if* there's an effect, but also how big it might be.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s3ss3-concepts">The Key Outputs</button>
        <button class="tab-button" data-tab="p3s3ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p3s3ss3-usecase">Putting It All Together</button>
    </div>

    <div id="p3s3ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">From Numbers to Narrative</h4>
        <p>The output of a statistical test provides several key pieces of information that must be interpreted together to form a complete conclusion.</p>
        <ul class="prose-list">
            <li><strong>p-value:</strong> Answers the question, "Is there a statistically significant effect?" If the p-value is less than your chosen significance level (e.g., 0.05), you conclude the effect is unlikely to be due to random chance.</li>
            <li><strong>Confidence Interval:</strong> Answers the question, "How large is the effect?" It provides a plausible range for the true effect size in the population. A wide interval suggests a lot of uncertainty, while a narrow one suggests a more precise estimate.</li>
            <li><strong>Effect Size:</strong> A standardized measure of the magnitude of the effect, independent of sample size (e.g., Cohen's d). It answers, "Is the effect practically meaningful?" A tiny p-value can be achieved with a massive sample size even for a trivial, unimportant effect.</li>
        </ul>
    </div>
    
    <div id="p3s3ss3-viz" class="tab-pane">
        <h4 class="subsection-title">A Complete Picture of Inference</h4>
        <p>This plot shows how all the key concepts relate. The blue curve is the distribution of outcomes if the null hypothesis (no effect) is true. The orange curve shows the distribution if the alternative hypothesis is true. </p>
        <div class="plot-container">
            <div id="p3s3ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **p-value** (red area) is the probability of getting our observed result (or more extreme) if the blue curve were true. The **Confidence Interval** is a range of plausible values for the true mean of the orange curve. **Effect Size** is the distance between the means of the two curves. **Power** is the probability that we would correctly reject the null hypothesis (the area of the orange curve that is to the right of the critical value line).</p>
        </div>
    </div>

    <div id="p3s3ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Crafting a Conclusion</h4>
        <p>Let's interpret the results from our scenario: <code>p-value = 0.021</code>, <code>95% CI: [2.5, 21.5]</code>. Assume the values are the difference in average daily signups.</p>
        <div class="decision-point">
            <h4>Analyst's Report to the Manager</h4>
            <div class="decision-branches">
                <div class="decision-branch" style="text-align: left;">
                    <p><strong>1. State the significance:</strong> "The analysis shows a statistically significant increase in daily signups (p = 0.021), meaning the improvement we observed is unlikely to be due to random chance."</p>
                    <p><strong>2. Quantify the effect with the confidence interval:</strong> "We are 95% confident that the true average increase in daily signups caused by the campaign is somewhere between 2.5 and 21.5 additional signups per day."</p>
                    <p><strong>3. Provide a practical recommendation:</strong> "Because the entire plausible range is positive and the effect is statistically significant, I recommend we continue with the new ad campaign. The next step should be to monitor the cost to determine the return on investment based on this lift."</p>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Report CIs, Not Just P-Values</h3>
    <div class="scenario-content">
        <p>A p-value only tells you whether an effect is "not zero." It tells you nothing about its magnitude or precision. A confidence interval is much more informative. Always report the confidence interval alongside the p-value. A result like <code>p < 0.05, 95% CI: [0.01, 1000.0]</code> is statistically significant, but the incredibly wide interval tells you that you have very little precision and the true effect could be tiny or huge.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Run the following t-test in R: <code>t.test(x = rnorm(30, mean = 10.5), mu = 10)</code>. This tests if a sample with a mean of 10.5 is significantly different from a population mean of 10.</li>
            <li>Look at the p-value. At a significance level of 0.05, would you reject or fail to reject the null hypothesis?</li>
            <li>Look at the 95% confidence interval. Does it contain the null hypothesis value of 10?</li>
            <li>How would you summarize these results in a single sentence?</li>
        </ul>
    </div>
</div>
`,
          p3s3ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst runs an ANOVA test to compare the means of three groups and gets a significant p-value. However, they didn't check the assumptions first. A senior colleague points out that one group has a much larger variance than the others, violating the "homogeneity of variance" assumption. This violation could mean the small p-value is a false positive, and the entire result is invalid.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s3ss4-concepts">Why Assumptions Matter</button>
        <button class="tab-button" data-tab="p3s3ss4-normality">Normality Assumption</button>
        <button class="tab-button" data-tab="p3s3ss4-homogeneity">Homogeneity of Variance</button>
    </div>

    <div id="p3s3ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">The "Fine Print" of Statistical Tests</h4>
        <p>Most classical statistical tests (known as parametric tests) are derived from a set of underlying assumptions about the data. If your data violates these assumptions, the mathematical guarantees of the test no longer hold, and the results (like the p-value) can be unreliable or completely wrong.</p>
        <p><strong>Analogy:</strong> A statistical test is like a **high-performance recipe**. The recipe (the test) promises a delicious outcome (a valid p-value), but it comes with a list of required ingredients and conditions (the assumptions), such as "use fresh eggs" or "preheat oven to 375°F". If you use spoiled eggs (non-normal data) or the wrong temperature (unequal variances), you can't trust that the final dish will be any good.</p>
    </div>
    
    <div id="p3s3ss4-normality" class="tab-pane">
        <h4 class="subsection-title">Is the Data Approximately Bell-Shaped?</h4>
        <p>Many tests (like t-tests and ANOVA) assume that the data (or the model's residuals) are drawn from a normal distribution.</p>
        <p><strong>How to Check:</strong></p>
        <ul class="prose-list">
            <li><strong>Visually:</strong> Use a Q-Q (Quantile-Quantile) plot. If the data is normal, the points will fall closely along the diagonal line.</li>
            <li><strong>Formally:</strong> Use a statistical test like the Shapiro-Wilk test (<code>shapiro.test()</code>). A low p-value suggests the data is *not* normal.</li>
        </ul>
        <div class="plot-container">
            <div id="p3s3ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The left panel shows normally distributed data; the points hug the theoretical diagonal line. The right panel shows skewed data; the points systematically deviate from the line in an "S" shape, clearly violating the normality assumption.</p>
        </div>
    </div>

    <div id="p3s3ss4-homogeneity" class="tab-pane">
        <h4 class="subsection-title">Do the Groups Have Equal Spread?</h4>
        <p>Tests that compare groups (like t-tests and ANOVA) often assume **homogeneity of variance** (or homoscedasticity), meaning the variance/spread of the data should be roughly equal across all groups.</p>
        <p><strong>How to Check:</strong></p>
        <ul class="prose-list">
            <li><strong>Visually:</strong> Create side-by-side boxplots for each group. Check if the heights of the boxes (the IQRs) are approximately the same.</li>
            <li><strong>Formally:</strong> Use a statistical test like Levene's Test (<code>car::leveneTest()</code>). A low p-value suggests the variances are *not* equal.</li>
        </ul>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("car")
library(car)
# H0: The variances are equal across the groups.
# HA: At least one group has a different variance.
leveneTest(Sepal.Length ~ Species, data = iris)
            </code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: What to Do When Assumptions are Violated</h3>
    <div class="scenario-content">
        <p>Don't panic! You have options. For a normality violation, you can try transforming your data (e.g., with a log transformation) or use a non-parametric alternative like the Wilcoxon test. For a homogeneity of variance violation in a t-test, you can simply use the Welch's t-test (the default in R's <code>t.test()</code>), which does not assume equal variances. For ANOVA, you can use a Welch's ANOVA (<code>oneway.test()</code>).</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>iris</code> dataset, create a Q-Q plot for <code>Sepal.Length</code>. Does it look normally distributed?</li>
            <li>Run the Shapiro-Wilk test on it: <code>shapiro.test(iris$Sepal.Length)</code>. Based on the p-value, do you have evidence to say it's not normal?</li>
            <li>Now run Levene's Test to check for homogeneity of variance for <code>Sepal.Width</code> across the three <code>Species</code>. Based on the p-value, can you assume the variances are equal?</li>
        </ul>
    </div>
</div>
`,
          p3s3ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A geneticist tests 10,000 genes to see if any are associated with a disease, using a significance level of $\alpha = 0.05$. By pure random chance, they would expect to find $10,000 \times 0.05 = 500$ "significant" genes, even if none of them truly have an effect. This is the **multiple comparisons problem**. They cannot simply report all 500 genes; they must apply a correction to control for this inflated false positive rate.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s3ss5-concepts">The Core Problem</button>
        <button class="tab-button" data-tab="p3s3ss5-viz">Methods Compared</button>
        <button class="tab-button" data-tab="p3s3ss5-usecase">Step-by-Step Example</button>
    </div>

    <div id="p3s3ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">The More You Look, the More You Find</h4>
        <p>The multiple comparisons problem arises when a single analysis involves multiple simultaneous statistical tests. The more tests you run, the higher the probability that you will find at least one "significant" result purely by chance (a Type I error).</p>
        <p><strong>Analogy:</strong> It's like a **lottery**. If you buy one ticket, your chance of winning is tiny. If you buy 10,000 tickets, your chance of finding at least one winner is much higher, even though the odds for each individual ticket remain the same. Each hypothesis test is like a lottery ticket for a false positive finding.</p>
        <p>To combat this, we must **adjust the p-values** from the individual tests to control the overall error rate.</p>
    </div>
    
    <div id="p3s3ss5-viz" class="tab-pane">
        <h4 class="subsection-title">Bonferroni vs. FDR</h4>
        <p>This plot compares the two most common correction methods on a set of original p-values. </p>
        <div class="plot-container">
            <div id="p3s3ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The x-axis shows the original, uncorrected p-values, sorted. The y-axis shows the new, adjusted p-values. The **Bonferroni** correction is very strict, pushing all p-values up significantly. The **False Discovery Rate (FDR)** correction is more adaptive and powerful; it is less stringent on the p-values that are already low, giving you a better chance to find true effects while still controlling the error rate.</p>
        </div>
    </div>

    <div id="p3s3ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Adjusting p-values in R</h4>
        <p>The base R function <code>p.adjust()</code> is the perfect tool for this. Let's simulate a set of p-values and apply corrections.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Bonferroni Correction</strong>
                <p>Very simple: it multiplies each p-value by the total number of tests. This controls the "Family-Wise Error Rate" (the chance of even one false positive).</p>
                <p><strong>Trade-off:</strong> It is extremely conservative and can lead to many false negatives (missing real effects). It's often too strict.</p>
            </div>
            <div class="decision-branch">
                <strong>FDR (Benjamini-Hochberg)</strong>
                <p>A more modern and powerful method. It controls the "False Discovery Rate"—the expected proportion of rejected null hypotheses that are actually false positives.</p>
                <p><strong>Trade-off:</strong> It is more powerful than Bonferroni but allows for a small proportion of false positives among the significant results. This is usually an acceptable trade-off.</p>
            </div>
        </div>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
set.seed(42)
# Simulate 100 p-values, where a few are truly significant
original_p_values <- c(runif(90, 0, 1), runif(10, 0, 0.01))

# --- Apply Corrections ---
p_bonferroni <- p.adjust(original_p_values, method = "bonferroni")
p_fdr <- p.adjust(original_p_values, method = "BH") # "BH" for Benjamini-Hochberg

# --- Compare the number of significant results ---
# Use an adjusted significance level of 0.05
sum(original_p_values < 0.05) # How many we'd find without correction
sum(p_bonferroni < 0.05)      # How many are left after Bonferroni
sum(p_fdr < 0.05)             # How many are left after FDR
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: FDR is Often the Better Choice</h3>
    <div class="scenario-content">
        <p>In exploratory research fields like genomics, neuroscience, or large-scale A/B testing, the **False Discovery Rate (FDR)** is almost always the preferred correction method. The Bonferroni correction is often so strict that it prevents you from discovering any effects at all (low statistical power). FDR provides a much better balance between making discoveries and controlling for errors.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a vector of 10 p-values: <code>my_pvals <- c(0.001, 0.004, 0.009, 0.02, 0.04, 0.1, 0.3, 0.5, 0.7, 0.9)</code>.</li>
            <li>How many of these are significant at an $\alpha$ of 0.05 without any correction?</li>
            <li>Use the <code>p.adjust()</code> function to apply a Bonferroni correction. Now how many are significant?</li>
            <li>Use the <code>p.adjust()</code> function to apply an FDR ("BH") correction. How many are significant now? Compare the results.</li>
        </ul>
    </div>
</div>
`,
          p3s3ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst in Karachi, excited by a "significant" p-value from an A/B test run on September 18th, 2025, rushes to their manager claiming a new website design is better. 📈 The manager asks, "What was the effect size? Is a 0.1% increase in clicks worth the development cost? And is this the only test you ran, or one of twenty?" The analyst is unprepared. This highlights the need to go beyond a single p-value and follow best practices for responsible statistical inference.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s3ss6-concepts">Beyond the p-value</button>
        <button class="tab-button" data-tab="p3s3ss6-viz">A Responsible Workflow</button>
        <button class="tab-button" data-tab="p3s3ss6-usecase">Avoiding P-Hacking</button>
    </div>

    <div id="p3s3ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">Significance vs. Importance</h4>
        <p>A statistically significant result is not always a practically important one. A tiny p-value can be achieved with a massive sample size, even if the actual effect is trivial. A responsible analysis always considers three key components together.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong><i class="fas fa-search"></i> Statistical Significance</strong>
                <p><strong>The Question:</strong> "Is the effect real?"<br><strong>The Tool:</strong> The p-value. It tells you the probability of observing your data if there were truly no effect. A low p-value suggests the effect is unlikely to be due to random chance.</p>
            </div>
            <div class="decision-branch">
                <strong><i class="fas fa-ruler-combined"></i> Practical Importance</strong>
                <p><strong>The Question:</strong> "How big is the effect?"<br><strong>The Tool:</strong> Effect size (e.g., Cohen's d, or the simple difference in means). This quantifies the magnitude of the finding, helping you judge if it's meaningful in a real-world context.</p>
            </div>
             <div class="decision-branch">
                <strong><i class="fas fa-arrows-alt-h"></i> Precision</strong>
                <p><strong>The Question:</strong> "How uncertain is our estimate of the effect?"<br><strong>The Tool:</strong> The Confidence Interval. A narrow interval suggests a precise estimate, while a very wide interval suggests the true effect size could be very different from your sample estimate.</p>
            </div>
        </div>
    </div>
    
    <div id="p3s3ss6-viz" class="tab-pane">
        <h4 class="subsection-title">A Checklist for Responsible Inference</h4>
        <p>This workflow outlines the key steps to ensure your statistical conclusions are robust, ethical, and well-supported.</p>
        <div class="plot-container">
            <div id="p3s3ss6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This mindmap shows a structured approach. The process doesn't end when you get a p-value. A responsible analysis involves crucial steps *before* the test (stating the hypothesis) and *after* (reporting the effect size and confidence interval) to provide a complete and honest picture of the findings.</p>
        </div>
    </div>

    <div id="p3s3ss6-usecase" class="tab-pane">
        <h4 class="subsection-title">The Danger of p-Hacking</h4>
        <p><strong>p-hacking</strong> is the practice of re-analyzing data in many different ways and only reporting the combinations that produce a significant p-value. This is a form of scientific dishonesty that leads to false discoveries.</p>
        <p><strong>Analogy:</strong> p-hacking is like a **sharpshooter firing a hundred shots at a barn door and then drawing a target around the one bullet hole that happened to be close to the center**. It creates the illusion of skill or significance where there is none.</p>
        <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">How to Avoid It:</h5>
        <ul class="prose-list">
            <li><strong>Define your hypothesis and analysis plan *before* you start.</strong> Don't go on a "fishing expedition" for significant results.</li>
            <li>If you do conduct multiple tests, you **must** use a multiple testing correction method (like Bonferroni or FDR) to adjust your p-values, as discussed in the previous topic.</li>
            <li>Be transparent. Report all the tests you conducted, not just the ones that "worked."</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Pre-Registration is the Gold Standard</h3>
    <div class="scenario-content">
        <p>In academic research, the gold standard for preventing p-hacking is **pre-registration**, where researchers publicly post their hypothesis and analysis plan before collecting any data. While less common in industry, the principle is invaluable. Writing down your hypothesis and analysis plan in a document *before* you start the main analysis enforces intellectual honesty and makes your results far more credible.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Critique the following analytical conclusion based on the best practices you've just learned:</p>
        <p><i>"We ran 15 different t-tests comparing user engagement metrics between our new and old website designs. We found that the average time on page was significantly higher for the new design (p = 0.04). Therefore, the new design is a success and we should launch it."</i></p>
        <ul class="prose-list">
            <li>What is the most significant flaw in this conclusion?</li>
            <li>What two crucial pieces of information are missing from their report that you would need to make a real business decision?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p3s4ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst creates a 3D exploding pie chart with 15 slices and a rainbow color scheme to show market share. While colorful, it's impossible for the audience to accurately compare the slices or grasp the key insights. A senior analyst explains that effective visualization isn't about being flashy; it's about communicating information with clarity and integrity.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s4ss1-concepts">The Core Goal</button>
        <button class="tab-button" data-tab="p3s4ss1-viz">Good vs. Bad Charts</button>
        <button class="tab-button" data-tab="p3s4ss1-rules">Key Principles</button>
    </div>

    <div id="p3s4ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Clarity and Integrity in Communication</h4>
        <p>The fundamental goal of data visualization is to provide the viewer with the greatest number of ideas in the shortest time with the least ink in the smallest space. It's about clear, efficient, and honest communication.</p>
        <p><strong>Analogy:</strong> Effective visualization is like **good writing**. A great essay is clear, concise, and gets straight to the point without distracting fluff or jargon. A bad essay is verbose, confusing, and buries the main idea. Similarly, a good plot is clean and communicates an idea instantly, while a bad plot (full of "chart junk") obscures it.</p>
        <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1rem;">Exploration vs. Explanation</h5>
        <p>It's critical to distinguish between the plots you make for yourself (exploration) and the plots you make for others (explanation). Exploratory plots can be complex and messy as you search for patterns. Explanatory plots must be simple, focused, and tell a single, clear story.</p>
    </div>
    
    <div id="p3s4ss1-viz" class="tab-pane">
        <h4 class="subsection-title">A Tale of Two Charts</h4>
        <p>This plot shows the exact same data visualized in two different ways. The 3D pie chart is a classic example of a terrible, misleading visualization. The simple bar chart is clear, accurate, and effective.</p>
        <div class="plot-container">
            <div id="p3s4ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> In the 3D pie chart, the perspective distortion makes it impossible to accurately compare the slices. It's difficult to tell if Category B is larger than Category C. In the bar chart, the comparison is immediate and unambiguous. The human eye is excellent at comparing lengths along a common baseline, which is why bar charts are far superior to pie charts for showing comparisons.</p>
        </div>
    </div>

    <div id="p3s4ss1-rules" class="tab-pane">
        <h4 class="subsection-title">Rules for Effective Graphics</h4>
        <ul class="prose-list">
            <li><strong>Maximize the Data-Ink Ratio:</strong> Coined by Edward Tufte, this principle states that a large share of the "ink" on a graphic should present data-information. Erase non-data ink (like redundant labels, heavy grid lines, unnecessary decorations) wherever possible.</li>
            <li><strong>Choose the Right Plot for the Job:</strong> Don't use a line chart for categorical comparisons. Use the appropriate foundational plot type (bar, line, scatter, histogram) for your specific question.</li>
            <li><strong>Use Color Purposefully:</strong> Don't use a rainbow of colors just to be decorative. Use color to highlight specific data points, to group categories, or to represent a continuous variable. Be mindful of color-blindness.</li>
            <li><strong>Avoid Deception:</strong> Never truncate the y-axis of a bar chart to exaggerate differences. Ensure your scales are honest and clearly labeled.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: theme_minimal() and theme_void()</h3>
    <div class="scenario-content">
        <p>A simple way to increase your data-ink ratio in <code>ggplot2</code> is to move away from the default grey background theme. Adding <code>+ theme_minimal()</code> or <code>+ theme_bw()</code> to your plot instantly removes a lot of non-data ink and makes your plot cleaner and more professional.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>The built-in R dataset <code>VADeaths</code> contains death rates in Virginia in the 1940s.</p>
        <ul class="prose-list">
            <li>Run the command <code>pie(VADeaths[, "Rural Male"])</code> to create a default pie chart. Notice how difficult it is to compare the age group slices.</li>
            <li>Now, create a data frame from this data: <code>df <- data.frame(age_group = rownames(VADeaths), rate = VADeaths[, "Rural Male"])</code>.</li>
            <li>Use <code>ggplot2</code> to create a <code>geom_bar(stat="identity")</code> plot from this data frame.</li>
            <li>Which plot makes it easier to see which age group had the highest death rate? This demonstrates the superiority of bar charts for comparison.</li>
        </ul>
    </div>
</div>
`,
          p3s4ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst needs to answer four basic business questions from a new dataset: 1) Which product category has the highest total sales? (Comparison) 2) What is the spread of our customer ratings? (Distribution) 3) Is there a link between advertising spend and sales? (Relationship) 4) How have our sales grown over the past year? (Trend over Time). Each of these questions maps directly to a foundational plot type.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s4ss2-concepts">The Four Pillars of Plotting</button>
        <button class="tab-button" data-tab="p3s4ss2-viz">The Plots in Action</button>
        <button class="tab-button" data-tab="p3s4ss2-usecase">Implementation in <code>ggplot2</code></button>
    </div>

    <div id="p3s4ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Matching the Plot to the Question</h4>
        <p>Nearly all basic data questions can be answered with one of four plot types. Choosing the correct one is the most important step in visualization.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong><i class="fas fa-chart-bar"></i> Bar Chart</strong>
                <p><strong>Purpose:</strong> Comparing a numeric value across different categories.
                <br><strong>Use for:</strong> Ranking, showing parts of a whole (as a stacked bar chart).</p>
            </div>
            <div class="decision-branch">
                <strong><i class="fas fa-align-left"></i> Histogram</strong>
                <p><strong>Purpose:</strong> Showing the distribution of a single continuous variable.
                <br><strong>Use for:</strong> Understanding the frequency, center, spread, and shape (skewness) of your data.</p>
            </div>
        </div>
        <div class="decision-branches" style="align-items: flex-start; margin-top:1rem;">
            <div class="decision-branch">
                <strong><i class="fas fa-braille"></i> Scatter Plot</strong>
                <p><strong>Purpose:</strong> Displaying the relationship between two continuous variables.
                <br><strong>Use for:</strong> Identifying correlation, linearity, and outliers in the relationship.</p>
            </div>
            <div class="decision-branch">
                <strong><i class="fas fa-chart-line"></i> Line Plot</strong>
                <p><strong>Purpose:</strong> Displaying the trend of a continuous variable over a continuous interval, usually time.
                <br><strong>Use for:</strong> Showing growth, seasonality, and volatility over time.</p>
            </div>
        </div>
    </div>
    
    <div id="p3s4ss2-viz" class="tab-pane">
        <h4 class="subsection-title">A Visual Glossary</h4>
        <p>This grid shows an example of each of the four foundational plot types, each designed to answer a different kind of question.</p>
        <div class="plot-container">
            <div id="p3s4ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Bar Chart** makes it easy to see that <code>setosa</code> has the highest sepal width. The **Histogram** shows that petal lengths are not unimodally distributed; there are two distinct groups. The **Scatter Plot** reveals a strong positive correlation between petal length and width. The **Line Plot** shows a clear upward trend in stock price over time.</p>
        </div>
    </div>

    <div id="p3s4ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Creating the Plots in R</h4>
        <p>The <code>ggplot2</code> package uses a "grammar of graphics," where you add layers (<code>geoms</code>) to create plots. The geom you choose determines the plot type.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(ggplot2)
library(dplyr)

# 1. Bar Chart: Comparing mean Sepal.Width by Species
iris %>%
  group_by(Species) %>%
  summarise(avg_sepal_width = mean(Sepal.Width)) %>%
  ggplot(aes(x = Species, y = avg_sepal_width)) +
  geom_bar(stat = "identity") # or geom_col()

# 2. Histogram: Distribution of Petal.Length
ggplot(iris, aes(x = Petal.Length)) +
  geom_histogram(bins = 20)

# 3. Scatter Plot: Relationship between Petal.Length and Petal.Width
ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point()

# 4. Line Plot: Trend of a stock over time (using built-in economics dataset)
ggplot(economics, aes(x = date, y = psavert)) +
  geom_line()
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: geom_col() vs. geom_bar()</h3>
    <div class="scenario-content">
        <p>In ggplot2, the choice between <code>geom_bar()</code> and <code>geom_col()</code> can be confusing. It's simple: If your data is already summarized and you want the height of the bar to be the value in a column, use <strong><code>geom_col()</code></strong>. If you have the raw data and you want ggplot to count the number of rows for each category, use <strong><code>geom_bar()</code></strong> (with its default <code>stat="count"</code>).</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>mtcars</code> dataset:</p>
        <ul class="prose-list">
            <li>What plot type would you use to see the relationship between horsepower (<code>hp</code>) and weight (<code>wt</code>)? Create this plot.</li>
            <li>What plot type would you use to see the distribution of miles per gallon (<code>mpg</code>)? Create this plot.</li>
            <li>What plot type would you use to compare the average horsepower for each cylinder group (<code>cyl</code>)? Create this plot.</li>
        </ul>
    </div>
</div>
`,
          p3s4ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A simple histogram shows an analyst that their data is skewed. Now they need a more nuanced view. They want to compare the distributions of a variable across several different categories and formally check if the data conforms to a normal distribution. This requires moving beyond a basic histogram to more specialized distributional plots.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s4ss3-concepts">An Enhanced Toolkit</button>
        <button class="tab-button" data-tab="p3s4ss3-viz">Visualizing the Toolkit</button>
        <button class="tab-button" data-tab="p3s4ss3-usecase">Implementation in R</button>
    </div>

    <div id="p3s4ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">A Deeper Look at Distributions</h4>
        <p>While a histogram is a great start, a more advanced toolkit gives you greater insight.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Boxplots & Violin Plots</strong>
                <p><strong>Use for:</strong> Comparing distributions across multiple categories.
                <br>A **Boxplot** concisely shows the median, quartiles, and outliers. A **Violin Plot** is like a density plot mirrored on both sides, showing the full shape and probability density of the data.</p>
            </div>
            <div class="decision-branch">
                <strong>Heatmaps</strong>
                <p><strong>Use for:</strong> Visualizing the magnitude of a phenomenon in two dimensions. It's excellent for correlation matrices or showing user activity over time (e.g., day of week vs. hour of day).</p>
            </div>
            <div class="decision-branch">
                <strong>Q-Q Plots</strong>
                <p><strong>Use for:</strong> Visually checking if a variable follows a specific theoretical distribution (most often, the normal distribution). If the points fall on the diagonal line, the assumption of normality is met.</p>
            </div>
        </div>
    </div>
    
    <div id="p3s4ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Complementary Views of the Data</h4>
        <p>This plot demonstrates how a boxplot and a violin plot of the same data provide complementary information, and how a Q-Q plot reveals deviations from normality.</p>
        <div class="plot-container">
            <div id="p3s4ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Violin Plot** on the left shows the full, bimodal (two-humped) shape of the distribution for the 'versicolor' species. The **Boxplot** in the middle concisely summarizes the same data, showing the median and quartiles, but it hides the bimodal nature. The **Q-Q Plot** on the right shows points deviating from the line in an "S" shape, which is characteristic of a distribution with "lighter tails" than a normal distribution.</p>
        </div>
    </div>

    <div id="p3s4ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Creating Advanced Plots in R</h4>
        <p>These plots are all straightforward to create with <code>ggplot2</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(ggplot2)

# 1. Violin plot to compare distributions across categories
ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +
  geom_violin()

# 2. Add a boxplot inside the violin plot
ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white")

# 3. Q-Q plot to check for normality of a single variable
ggplot(iris, aes(sample = Sepal.Length)) +
  stat_qq() +
  stat_qq_line() # Adds the theoretical line
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: patchwork for Combining Plots</h3>
    <div class="scenario-content">
        <p>To create complex, multi-panel figures like the one in the visualization tab, the <code>patchwork</code> package is a lifesaver. After creating your individual ggplots (<code>p1</code>, <code>p2</code>, etc.), you can combine them with intuitive arithmetic operators: <code>p1 + p2</code> places them side-by-side, and <code>p1 / p2</code> stacks them vertically. It makes creating publication-quality figures incredibly easy.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>msleep</code> dataset from <code>ggplot2</code>.</p>
        <ul class="prose-list">
            <li>Create side-by-side violin plots to compare the distribution of <code>sleep_total</code> for different <code>vore</code> categories (carnivore, herbivore, etc.).</li>
            <li>Create a Q-Q plot for the <code>sleep_total</code> variable. Does total sleep time appear to be normally distributed?</li>
        </ul>
    </div>
</div>
`,
          p3s4ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst needs to show a company's monthly profit change, breaking down the positive contributions (new sales) and negative contributions (costs, taxes) that lead from the starting profit to the ending profit. A simple bar chart can't show this cumulative story. A **Waterfall Chart** is the perfect custom plot for this task.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s4ss4-concepts">Beyond the Basics</button>
        <button class="tab-button" data-tab="p3s4ss4-viz">Waterfall Chart Example</button>
        <button class="tab-button" data-tab="p3s4ss4-usecase">Implementation in R</button>
    </div>

    <div id="p3s4ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Specialized Tools for Specialized Questions</h4>
        <p>While the foundational plots cover 80% of cases, sometimes you need a more specialized tool to tell a specific story.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Waterfall Charts</strong>
                <p>Used to show the cumulative effect of sequentially introduced positive and negative values. Ideal for financial statements or showing how a starting value changes over time.</p>
            </div>
            <div class="decision-branch">
                <strong>Bubble Charts</strong>
                <p>A scatter plot that uses a third dimension: the *size* of the bubbles. It's a great way to show the relationship between three numeric variables at once.</p>
            </div>
            <div class="decision-branch">
                <strong>Network Plots</strong>
                <p>Used to visualize relationships between entities (nodes) connected by lines (edges). Perfect for social network analysis, system diagrams, or dependency graphs.</p>
            </div>
        </div>
    </div>
    
    <div id="p3s4ss4-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Financial Flow</h4>
        <p>This waterfall chart clearly shows how an initial revenue is reduced by costs and increased by other income to arrive at a final profit figure.</p>
        <div class="plot-container">
            <div id="p3s4ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> You read it from left to right. It starts with the total revenue. Green bars represent positive contributions (Other Income) that increase the total. Red bars are negative contributions (Costs, Tax) that decrease the total. Each bar begins where the previous bar ended, showing the cumulative story. The final blue bar represents the calculated net profit.</p>
        </div>
    </div>

    <div id="p3s4ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Creating a Waterfall Chart with <code>plotly</code></h4>
        <p>While <code>ggplot2</code> requires some workarounds, <code>plotly</code> has a built-in <code>waterfall</code> trace that makes this easy.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(plotly)

fig <- plot_ly(
  type = "waterfall",
  measure = c("relative", "relative", "total", "relative", "total"),
  x = c("Sales", "Other Income", "Total Revenue", "Costs", "Net Profit"),
  y = c(100, 20, 0, -40, 0), # Use 0 for calculated totals
  text = c("+100", "+20", "120", "-40", "80"),
  textposition = "outside",
  connector = list(line = list(color = "rgb(63, 63, 63)"))
) %>%
  layout(
    title = "Monthly Profit Breakdown",
    xaxis = list(title = "Category"),
    yaxis = list(title = "Amount ($)")
  )

fig
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Find Your Package on CRAN Task Views</h3>
    <div class="scenario-content">
        <p>If you need to make a highly specialized plot, chances are someone has already written an R package for it. The best place to start your search is the <a href="https://cran.r-project.org/web/views/" target="_blank">CRAN Task Views</a> page. Look at the "Graphics" or domain-specific views (like "Finance" or "Genetics") to find curated lists of the best packages for your specific visualization need.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>mtcars</code> dataset:</p>
        <ul class="prose-list">
            <li>Create a bubble chart using <code>ggplot2</code>.</li>
            <li>Map horsepower (<code>hp</code>) to the x-axis.</li>
            <li>Map miles per gallon (<code>mpg</code>) to the y-axis.</li>
            <li>Map the car's weight (<code>wt</code>) to the <strong>size</strong> of the points.</li>
            <li>What kind of cars (heavy or light) tend to have high horsepower and low MPG? The bubble sizes should make this relationship obvious.</li>
        </ul>
    </div>
</div>
`,
          p3s4ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A real estate analyst in Karachi wants to plot housing prices on a map to see if there are geographic clusters of high and low value. Separately, a marketing analyst wants to create a scatter plot of ad spend vs. sales, but also show density contours to identify the most common combinations. These tasks require moving beyond single-geom plots to more complex, multi-layered visualizations.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s4ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p3s4ss5-viz">Multi-Layered Plot</button>
        <button class="tab-button" data-tab="p3s4ss5-usecase">Implementation in R</button>
    </div>

    <div id="p3s4ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Layering Information</h4>
        <p>Complex visualizations are about conveying multiple layers of information in a single, coherent graphic. The Grammar of Graphics, which underpins ggplot2, is built on this idea of layering.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Geospatial Maps</strong>
                <p>This involves plotting data onto a geographical map. It requires specialized data formats (like simple features, <code>sf</code>) and packages (<code>sf</code>, <code>leaflet</code>, <code>ggmap</code>). You typically start with a base map layer and then add your data as another layer of points, polygons, or lines.</p>
            </div>
            <div class="decision-branch">
                <strong>Multi-Layered Plots</strong>
                <p>In ggplot2, you can add multiple <code>geom_*</code> layers to a single plot. Each layer can even use a different dataset. This allows you to combine different visual elements, like points, lines, and density areas, to tell a richer story.</p>
            </div>
        </div>
    </div>
    
    <div id="p3s4ss5-viz" class="tab-pane">
        <h4 class="subsection-title">Combining Geoms for Deeper Insight</h4>
        <p>This single plot combines three layers to show the relationship between two variables: the individual data points (<code>geom_point</code>), the overall linear trend (<code>geom_smooth</code>), and the areas of highest data concentration (<code>geom_density_2d</code>).</p>
        <div class="plot-container">
            <div id="p3s4ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The scatter plot shows the general relationship. The blue trend line confirms the positive linear association. The density contours act like a topographical map, showing the "peaks" where the data points are most densely clustered, providing more insight than the scatter points alone.</p>
        </div>
    </div>

    <div id="p3s4ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Building a Multi-Layered <code>ggplot</code></h4>
        <p>You can add as many geom layers as you need to a <code>ggplot</code> object using the <code>+</code> operator.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(ggplot2)

# Using the faithful dataset (eruption duration and waiting time)
# 1. Start with the base plot object and the point layer
p <- ggplot(faithful, aes(x = waiting, y = eruptions)) +
  geom_point(alpha = 0.5)

# 2. Add a density contour layer
p <- p + geom_density_2d()

# 3. Add a linear regression trend line
p <- p + geom_smooth(method = "lm", se = FALSE, color = "red")

# 4. Add titles and improve the theme
p <- p + 
  labs(
    title = "Eruption Duration vs. Waiting Time",
    subtitle = "with Density Contours and Trend Line",
    x = "Waiting Time (mins)",
    y = "Eruption Duration (mins)"
  ) +
  theme_bw()

# Display the final, multi-layered plot
print(p)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: leaflet for Interactive Maps</h3>
    <div class="scenario-content">
        <p>For creating interactive, zoomable web maps, the <code>leaflet</code> package is the industry standard in R. It has an intuitive, pipe-based syntax. You can start with a base map (<code>leaflet() %>% addTiles()</code>) and then add layers of markers (<code>addMarkers()</code>), circles, or polygons, complete with pop-up information. It's incredibly powerful for geospatial data exploration and communication.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>iris</code> dataset, let's build a multi-layered plot.</p>
        <ul class="prose-list">
            <li>Create a scatter plot of <code>Sepal.Length</code> vs. <code>Sepal.Width</code>, with points colored by <code>Species</code>.</li>
            <li>Add a linear regression trend line for *each species* separately. (Hint: geom_smooth(method="lm", se=FALSE) will automatically do this when you have a color aesthetic).</li>
            <li>Give the plot a professional title and axis labels.</li>
        </ul>
    </div>
</div>
`,
          p3s4ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst creates a technically correct plot, but it has no title, the axis labels are just the raw variable names from the dataset (e.g., <code>cust_satisf_score</code>), and the legend is unclear. They show it to a manager, who has no idea what they are looking at. The plot fails as a communication tool because it lacks essential explanatory elements.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">Anatomy of a Great Plot</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> A Checklist for Publication-Quality Plots</div>
            <div class="scenario">A great plot is more than just data and geoms; it's a complete piece of communication that anticipates and answers the viewer's questions.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-heading"></i> 1. Informative Title & Subtitle</div>
                        <div class="scenario">The title should state the main finding or takeaway of the plot. The subtitle can provide additional context, such as the timeframe or data source.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-text-width"></i> 2. Clear Axis & Legend Labels</div>
                        <div class="scenario">Never use raw variable names. Convert <code>avg_spend_usd</code> to "Average Spend (USD)". Ensure all labels are readable and include units. The legend title should be clear.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-highlighter"></i> 3. Purposeful Use of Color & Annotations</div>
                        <div class="scenario">Use color to highlight key findings, not just for decoration. Add text annotations to point out specific data points or trends that are important to your story.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-universal-access"></i> 4. Accessibility</div>
                        <div class="scenario">Use color-blind friendly palettes (e.g., <code>scale_color_viridis_d()</code>). Ensure text is large and has sufficient contrast. Avoid conveying information using color alone.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Add a Caption for Your Source</h3>
    <div class="scenario-content">
        <p>A key part of a trustworthy plot is citing your data source. You can add this information neatly in <code>ggplot2</code> by using the <code>caption</code> argument inside the <code>labs()</code> function. For example: <code>labs(caption = "Source: Corporate Data Warehouse, September 18th, 2025")</code>. This adds a small, right-aligned caption at the bottom of your plot, which is a standard practice in professional reports.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's turn a basic <code>ggplot</code> into a professional one.</p>
        <ul class="prose-list">
            <li>Start with this basic plot: <code>ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point()</code>.</li>
            <li>Add a <code>labs()</code> layer to give it an informative title ("Heavier Cars Tend to Have Lower Fuel Efficiency"), a clear x-axis label ("Weight (1000 lbs)"), and a clear y-axis label ("Miles per Gallon").</li>
            <li>Add a trend line with <code>geom_smooth(method = "lm")</code> to make the relationship clearer.</li>
            <li>Change the theme to <code>theme_minimal()</code> to improve the data-ink ratio.</li>
        </ul>
    </div>
</div>
`,
          p3s5ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst presents a series of disconnected charts and statistical outputs to their team. While the analysis is technically correct, the audience is left confused, asking "So what? What's the main takeaway?" The analyst needs to learn how to move from simply presenting data to telling a compelling story with it.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Data Analysis Story Arc</h3>
    <p>A data narrative follows the same structure as any good story: a beginning, a middle, and an end. Framing your analysis in this structure turns a dry report into a persuasive and memorable argument.</p>
    <p><strong>Analogy:</strong> A data storyteller is a **tour guide**. They don't just dump a pile of maps and photos (your charts and tables) on you and walk away. They lead you on a planned journey. They start by setting the scene (the context), point out the important landmarks along the way (the key findings), and bring you to a final, memorable destination (the conclusion and recommendation).</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-book-open"></i> The Data Narrative</div>
            <div class="scenario">A structured analysis that guides an audience from a question to a conclusion.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-flag"></i> The Beginning: Context & Question</div>
                        <div class="scenario"><strong>Goal:</strong> Hook your audience. Explain the business context and the specific question your analysis will answer. Why does this matter? What is the core problem we are trying to solve?</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chart-line"></i> The Middle: The Analysis</div>
                        <div class="scenario"><strong>Goal:</strong> Build your case. Present your findings in a logical sequence. Each piece of evidence (a plot, a table, a test result) should build on the last, leading the audience step-by-step through your thought process.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-flag-checkered"></i> The End: Conclusion & Recommendation</div>
                        <div class="scenario"><strong>Goal:</strong> Deliver the "So what?". State your main conclusion clearly and directly. Answer the original question. Provide an actionable recommendation or suggest next steps based on your findings.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Know Your Audience</h3>
    <div class="scenario-content">
        <p>The most important part of storytelling is tailoring the story to your audience. A presentation for a technical audience can be filled with detailed charts and statistical outputs. A presentation for a senior executive should be high-level, focusing on 1-2 key visuals and the final business recommendation. Before you build your narrative, always ask: "Who am I talking to, and what do they care about?"</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Imagine you are an analyst for a subscription service. Your task is to investigate customer churn.</p>
        <ul class="prose-list">
            <li><strong>The Beginning:</strong> How would you frame the business context and the core question for your analysis?</li>
            <li><strong>The Middle:</strong> What are 2-3 key pieces of evidence (e.g., plots or summaries) you would present to build your case?</li>
            <li><strong>The End:</strong> Based on your hypothetical findings, what would be a sample conclusion and an actionable recommendation?</li>
        </ul>
    </div>
</div>
`,
          p3s5ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has a clear narrative in their head, but their visuals don't support it. They show a complex, exploratory plot with all data points when a simple, explanatory bar chart highlighting the key takeaway is needed. They need to learn how to design and sequence their plots specifically to advance their story and guide the audience's attention.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p3s5ss2-concepts">Key Concepts</button>
        <button class="tab-button" data-tab="p3s5ss2-viz">From Exploration to Explanation</button>
        <button class="tab-button" data-tab="p3s5ss2-usecase">Implementation in R</button>
    </div>

    <div id="p3s5ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Designing for an Audience</h4>
        <ul class="prose-list">
            <li><strong>Exploration vs. Explanation:</strong> Exploratory plots are for you, the analyst. They can be dense and complex as you search for patterns. Explanatory plots are for your audience. They must be simple, clean, and focus on communicating a single, clear message.</li>
            <li><strong>The Power of Annotation:</strong> Don't make your audience work to find the insight. Use titles, subtitles, text labels, and arrows to explicitly point out the most important parts of your visualization. Guide their eye to the story.</li>
            <li><strong>Logical Sequencing:</strong> Don't just show plots in a random order. Arrange them in a sequence that builds your narrative. Start with a high-level overview, then drill down into the specific findings that support your main point.</li>
        </ul>
    </div>
    
    <div id="p3s5ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Transforming a Plot for Storytelling</h4>
        <p>This shows how a standard, exploratory scatter plot can be transformed into a powerful, explanatory graphic that tells a clear story.</p>
        <div class="plot-container">
            <div id="p3s5ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Exploratory Plot** on the left is a standard <code>ggplot</code> output. It's useful for the analyst but doesn't tell a story. The **Explanatory Plot** on the right is designed for an audience. It has a clear title that states the main finding. It uses color and annotations to highlight the specific group of interest (the high-MPG, 4-cylinder cars) and fades the other data into the background, focusing the viewer's attention on the key insight.</p>
        </div>
    </div>

    <div id="p3s5ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Adding a Narrative Layer with <code>ggplot2</code></h4>
        <p>The <code>labs()</code> and <code>annotate()</code> functions are your primary storytelling tools in <code>ggplot2</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(ggplot2)
library(dplyr)

# Create a 'highlight' column for our story
mtcars_highlight <- mtcars %>%
  mutate(highlight = if_else(cyl == 4 & mpg > 30, "highlight", "default"))

# Create the explanatory plot
ggplot(mtcars_highlight, aes(x = wt, y = mpg)) +
  # Draw the faded points first
  geom_point(data = filter(mtcars_highlight, highlight == "default"), color = "grey70") +
  # Draw the highlighted points on top
  geom_point(data = filter(mtcars_highlight, highlight == "highlight"), color = "#ef4444", size = 3) +
  # Add the annotation
  annotate(
    "text", x = 2.4, y = 33, 
    label = "Lightweight 4-cylinder models\nachieve the highest efficiency.",
    hjust = 0, color = "#ef4444", fontface = "bold"
  ) +
  # Add storytelling labs
  labs(
    title = "Fuel Efficiency is Driven by Lightweight 4-Cylinder Cars",
    x = "Weight (1000 lbs)",
    y = "Miles per Gallon (MPG)",
    caption = "Source: 1974 Motor Trend US magazine"
  ) +
  theme_minimal()
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>ggrepel</code> Package</h3>
    <div class="scenario-content">
        <p>When you have many points to label on a scatter plot, using <code>geom_text()</code> can result in a cluttered mess of overlapping labels. The <code>ggrepel</code> package provides an amazing drop-in replacement, <code>geom_text_repel()</code>, that automatically moves labels around to prevent them from overlapping, resulting in a much cleaner and more readable plot.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the <code>iris</code> dataset:</p>
        <ul class="prose-list">
            <li>Create a scatter plot of <code>Petal.Length</code> vs. <code>Petal.Width</code>, colored by <code>Species</code>.</li>
            <li>Add a title that tells a story, such as "Petal Dimensions Clearly Separate Iris Species."</li>
            <li>Use <code>annotate()</code> to draw a rectangle around the <code>setosa</code> cluster, which is clearly distinct from the other two. (Hint: <code>annotate("rect", xmin=..., xmax=..., ymin=..., ymax=..., fill="blue", alpha=0.1)</code>).</li>
        </ul>
    </div>
</div>
`,
          p3s5ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has crafted a compelling data story. Now they must choose the right medium to deliver it. Should they create a static, reproducible report for a technical appendix? An interactive dashboard for an executive who wants to explore the data? Or a slide deck for a live presentation to the department?</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">Choosing Your Medium</h3>
    <p>The final step of communication is packaging your narrative into the right format for your audience and purpose. R provides a world-class ecosystem for creating a variety of professional outputs from a single analytical script.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-bullseye"></i> What is the goal of your communication?</div>
            <div class="scenario">The answer determines the best tool for the job.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-file-alt"></i> Goal: A Reproducible Report</div>
                        <div class="scenario"><strong>Audience:</strong> Technical peers, future self, journal reviewers.<br><strong>Tool:</strong> <strong>R Markdown</strong> or <strong>Quarto</strong> to produce static HTML, PDF, or Word documents. This is the gold standard for reproducible research.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-desktop"></i> Goal: An Interactive Dashboard</div>
                        <div class="scenario"><strong>Audience:</strong> Executives, business stakeholders, non-technical users.<br><strong>Tool:</strong> <strong>Shiny</strong> or <strong>flexdashboard</strong> to create web-based applications that allow users to explore the data and change parameters in real-time.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chalkboard-teacher"></i> Goal: A Live Presentation</div>
                        <div class="scenario"><strong>Audience:</strong> Department meeting, conference, stakeholders.<br><strong>Tool:</strong> <strong>Quarto</strong> can produce beautiful slide decks (similar to PowerPoint) directly from your R code, ensuring your presentation is in sync with your analysis.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Quarto for Everything</h3>
    <div class="scenario-content">
        <p>If you're starting a new project today, consider using <strong>Quarto</strong> (<code>.qmd</code> files) as your default. It is the next generation of R Markdown and is designed from the ground up to be a single, consistent framework for creating all of these outputs—reports, dashboards, presentations, websites, and books—from a single source, and it works with multiple languages (R, Python, etc.).</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's create a simple set of slides with Quarto.</p>
        <ul class="prose-list">
            <li>In RStudio (you may need to update to a recent version), go to <code>File > New File > Quarto Presentation</code>.</li>
            <li>Give it a title and author. Choose "Beamer" for PDF output or "Revealjs" for HTML slides.</li>
            <li>Click the "Render" button. RStudio will process the template file and produce a professional-looking slide deck.</li>
            <li>Try editing the Markdown. Use <code>##</code> to create a new slide, and include a code chunk to put a plot on one of your slides.</li>
        </ul>
    </div>
</div>
`,
          p4s1ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A real estate analyst in Karachi wants to build a simple model to predict house prices. 🇵🇰 They have a dataset of recently sold houses and believe the most important factor is the size (square footage). They need to quantify this relationship: for every extra square meter, how much does the price typically increase? This is a classic problem for <strong>Simple Linear Regression</strong>. Later, they will add more features like number of bedrooms and age of the house, turning it into a <strong>Multiple Linear Regression</strong> problem.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s1ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s1ss1-math">Mathematical Foundation</button>
        <button class="tab-button" data-tab="p4s1ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s1ss1-usecase">Implementation & Interpretation</button>
    </div>

    <div id="p4s1ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Finding the Line of Best Fit</h4>
        <p>Linear regression is a statistical method used to model the relationship between a continuous dependent variable (the outcome or target) and one or more independent variables (the predictors or features).</p>
        <ul class="prose-list">
            <li><strong>Simple Linear Regression:</strong> Models the relationship with a single predictor variable.</li>
            <li><strong>Multiple Linear Regression:</strong> Extends this to model the relationship with two or more predictor variables.</li>
        </ul>
        <p><strong>Analogy:</strong> Linear regression is like finding the single **best-fitting ruler** to draw a straight line through a cloud of data points on a scatter plot. The goal is to find the ruler's angle (slope) and starting point (intercept) that minimizes the overall distance from the line to all the data points.</p>
    </div>

    <div id="p4s1ss1-math" class="tab-pane">
        <h4 class="subsection-title">The Equation of the Line</h4>
        <p>The relationship is described by a simple equation. For a single predictor, it is:</p>
        <div class="math-foundation">$$ Y = \\beta_0 + \\beta_1X + \\epsilon $$</div>
        <div class="plot-description">
            <ul>
                <li>$Y$ is the dependent variable (e.g., house price).</li>
                <li>$X$ is the independent variable (e.g., square meters).</li>
                <li>$\\beta_0$ (Beta-naught) is the <strong>intercept</strong>: the predicted value of Y when X is 0.</li>
                <li>$\\beta_1$ (Beta-one) is the <strong>coefficient</strong> or <strong>slope</strong>: how much Y is predicted to change for a one-unit increase in X.</li>
                <li>$\\epsilon$ (epsilon) is the <strong>error term</strong>, representing random noise and unmeasured factors.</li>
            </ul>
        </div>
        <p>The model finds the "best" $\\beta_0$ and $\\beta_1$ values by minimizing the sum of the squared errors (the vertical distances from each point to the line), a method called <strong>Ordinary Least Squares (OLS)</strong>.</p>
    </div>
    
    <div id="p4s1ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Model</h4>
        <p>A scatter plot is the perfect way to visualize a simple linear regression model. It shows the data, the fitted line, and helps interpret the coefficients.</p>
        <div class="plot-container">
            <div id="p4s1ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The blue line is the "line of best fit" found by the model. The <strong>Intercept</strong> ($\\beta_0$) is the value where the line crosses the y-axis (when square meters = 0). The <strong>Slope</strong> ($\\beta_1$) is the "rise over run" of the line; it represents the increase in price for each one-unit increase in area. The grey vertical lines are the **residuals** (errors)—the distances the model is trying to minimize.</p>
        </div>
    </div>
    
    <div id="p4s1ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> A manager at a car dealership wants to know: "How strong is the relationship between a car's speed and the distance it takes to stop? Can we predict the stopping distance for a new car model?"</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Fit the Model.</strong> We use the built-in <code>cars</code> dataset and the <code>lm()</code> function to model stopping distance (<code>dist</code>) as a function of speed.
                <div class="code-container">
                    <pre><code class="language-r">car_model <- lm(dist ~ speed, data = cars)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Inspect the Model.</strong> We use the <code>summary()</code> function to get the detailed statistical output.
                <div class="code-container">
                    <pre><code class="language-r">summary(car_model)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Interpret the Key Outputs.</strong> We focus on the "Coefficients" table and the R-squared value.
                <div class="plot-description">
                    <p>From the summary, we find the coefficient for <code>speed</code> is 3.932. The p-value is extremely small, so the relationship is statistically significant. The Adjusted R-squared is 0.6433.</p>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong> We translate the statistical output into a clear business insight.
                <div class="plot-description">
                    <p><strong>Conclusion for the manager:</strong> "There is a strong, significant linear relationship between a car's speed and its stopping distance. Our model shows that, on average, for every 1 mph increase in speed, the required stopping distance increases by approximately 3.93 feet. The car's speed explains about 64% of the variation we see in stopping distances."</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>broom</code> Package</h3>
    <div class="scenario-content">
        <p>The output of <code>summary()</code> is text, which is hard to work with programmatically. The <code>broom</code> package provides a fantastic function, <code>tidy()</code>, that takes a model object and converts the coefficients table into a clean, tidy tibble. This is incredibly useful for comparing models or plotting coefficients.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, fit a **multiple** linear regression model to predict miles per gallon (<code>mpg</code>) from both weight (<code>wt</code>) and horsepower (<code>hp</code>). The formula will be <code>mpg ~ wt + hp</code>.</li>
            <li>Use <code>summary()</code> to inspect the model.</li>
            <li>How do you interpret the coefficient for <code>wt</code>? For a one-unit increase in weight, what is the predicted change in MPG, holding horsepower constant?</li>
            <li>Are both predictors statistically significant at an $\\alpha$ of 0.05?</li>
        </ul>
    </div>
</div>
`,
          p4s1ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst wants to predict a binary outcome: will a customer click on an ad (Yes/No)? A standard linear regression is a poor choice because it can predict probabilities less than 0 or greater than 1. Separately, a city planner wants to model the number of bicycle accidents at an intersection per month (count data). Linear regression is also a bad fit here, as it can predict a negative number of accidents. These problems require a more flexible tool: <strong>Generalized Linear Models (GLMs)</strong>.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s1ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s1ss2-types">Key GLM Types</button>
        <button class="tab-button" data-tab="p4s1ss2-viz">The Link Function</button>
        <button class="tab-button" data-tab="p4s1ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s1ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Extending Linear Regression</h4>
        <p>A GLM is a flexible generalization of ordinary linear regression. It allows the model to handle response variables that are not normally distributed (e.g., binary or count data).</p>
        <p><strong>Analogy:</strong> If a standard linear model is a rigid, straight pipe, a GLM is a **flexible hose**. It still contains a straight, linear component inside, but it uses a flexible "link function" at the end to bend the final output into the correct shape for your specific problem (e.g., constraining it between 0 and 1).</p>
        <p>Every GLM has three components:</p>
        <ul class="prose-list">
            <li><strong>Random Component:</strong> An assumed probability distribution for the response variable (e.g., Bernoulli/Binomial for binary, Poisson for counts).</li>
            <li><strong>Systematic Component:</strong> A linear combination of the predictors: $\\beta_0 + \\beta_1X_1 + \\dots$</li>
            <li><strong>Link Function:</strong> A function that "links" the systematic component to the random component, ensuring the model's predictions are on the correct scale.</li>
        </ul>
    </div>

    <div id="p4s1ss2-types" class="tab-pane">
        <h4 class="subsection-title">A GLM for Every Occasion</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Logistic Regression</strong>
                <p><strong>Use Case:</strong> Binary (0/1) outcomes.
                <br><strong>Distribution:</strong> Binomial/Bernoulli.
                <br><strong>Link Function:</strong> Logit. Transforms the linear predictor into log-odds, which can then be converted to a probability between 0 and 1.
                <br><strong>Example:</strong> Predicting customer churn, disease presence, ad clicks.
                </p>
            </div>
            <div class="decision-branch">
                <strong>Poisson Regression</strong>
                <p><strong>Use Case:</strong> Count data (non-negative integers).
                <br><strong>Distribution:</strong> Poisson.
                <br><strong>Link Function:</strong> Log. Ensures that the predicted counts are always non-negative.
                <br><strong>Example:</strong> Predicting number of calls to a call center, number of defects on a product.
                </p>
            </div>
        </div>
    </div>
    
    <div id="p4s1ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Logit Link Function</h4>
        <p>This plot shows how the logit link function takes the linear predictor (which can range from $-\\infty$ to $+\\infty$) and transforms it into a probability that is always bounded between 0 and 1, creating the characteristic "S"-shaped (sigmoid) curve of logistic regression.</p>
        <div class="plot-container">
            <div id="p4s1ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The x-axis represents the output of the linear part of the model. The y-axis is the final predicted probability. No matter how large or small the linear predictor gets, the link function ensures the final output is a valid probability, gracefully approaching but never exceeding 0 or 1.</p>
        </div>
    </div>
    
    <div id="p4s1ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> A manager wants to understand what car features predict whether a car has a manual or automatic transmission. Specifically, how does horsepower affect the probability of a car being manual?</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Fit the Model.</strong> Because the outcome (<code>am</code>) is binary (0=auto, 1=manual), we use a logistic regression. The <code>family = binomial</code> argument tells <code>glm()</code> to do this.
                <div class="code-container">
                    <pre><code class="language-r">logistic_model <- glm(am ~ hp, data = mtcars, family = binomial(link = "logit"))</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Inspect the Coefficients.</strong> The raw coefficients are in log-odds, which are difficult to interpret directly.
                <div class="code-container">
                    <pre><code class="language-r">summary(logistic_model)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Convert to Odds Ratios.</strong> We exponentiate the coefficients to convert them into the much more intuitive odds ratios.
                <div class="code-container">
                    <pre><code class="language-r">odds_ratios <- exp(coef(logistic_model))
print(odds_ratios)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong> We translate the odds ratio for <code>hp</code> into a business insight.
                <div class="plot-description">
                    <p><strong>Conclusion for the manager:</strong> "We found a significant relationship between horsepower and transmission type. The odds ratio for horsepower is 1.03. This means that for every one-unit increase in a car's horsepower, the odds of it having a manual transmission increase by approximately 3%. In other words, more powerful cars in this dataset are more likely to be manual."</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Interpreting Log-Odds</h3>
    <div class="scenario-content">
        <p>The coefficients from a logistic regression are on the log-odds scale, which is not intuitive. After you run <code>exp(coef(model))</code>, you get an **odds ratio**. An odds ratio of 1.05 for a predictor means that a one-unit increase in that predictor multiplies the odds of the outcome being "1" by 1.05 (a 5% increase). An odds ratio of 0.90 means it multiplies the odds by 0.90 (a 10% decrease).</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Fit a logistic regression model using the <code>mtcars</code> dataset to predict the engine shape (<code>vs</code>) based on the car's weight (<code>wt</code>).</li>
            <li>Use <code>summary()</code> to check if <code>wt</code> is a significant predictor.</li>
            <li>Use <code>exp(coef(model))</code> to get the odds ratio for <code>wt</code>.</li>
            <li>Write a sentence interpreting this odds ratio. How do the odds of having a V-shaped engine (<code>vs=0</code>) change for each 1000-lb increase in weight?</li>
        </ul>
    </div>
</div>
`,
          p4s1ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An agricultural scientist is modeling crop yield based on the amount of fertilizer applied. A scatter plot shows a clear non-linear, parabolic relationship: yield increases with fertilizer up to an optimal point, after which too much fertilizer becomes toxic and the yield starts to decrease. A simple straight line is a very poor fit for this data. This requires <strong>Polynomial Regression</strong>.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s1ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s1ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s1ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s1ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Fitting Curves with a Linear Model</h4>
        <p>Polynomial regression is a form of linear regression in which the relationship between the independent variable $X$ and the dependent variable $Y$ is modeled as an $n^{th}$-degree polynomial in $X$.</p>
        <p>Even though it fits a curve, it is still considered a **linear model** because the equation is linear in its *coefficients* ($\\beta$). We are just creating new predictor variables by taking our original predictor to a higher power (e.g., $X^2, X^3$).</p>
        <p><strong>Analogy:</strong> Polynomial regression is like giving a simple linear model a **flexible, French curve ruler** instead of a rigid, straight one. It's still using the same basic principles of fitting a line, but the "line" is now allowed to bend to better capture the patterns in the data.</p>
        <div class="math-foundation">$$ Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\dots + \\beta_nX^n + \\epsilon $$</div>
    </div>
    
    <div id="p4s1ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Capturing the Curve</h4>
        <p>This plot shows our fertilizer scenario. The simple linear model (blue line) completely misses the true pattern. The second-degree polynomial model (red curve) captures the parabolic relationship almost perfectly.</p>
        <div class="plot-container">
            <div id="p4s1ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The data points clearly follow an inverted "U" shape. The straight blue line is a terrible fit, systematically under-predicting at the ends and over-predicting in the middle. The curved red line, which is a quadratic (2nd-degree polynomial) fit, follows the data's trend much more closely, resulting in a significantly better model.</p>
        </div>
    </div>

    <div id="p4s1ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> An automotive engineer knows that the relationship between a car's horsepower and its fuel efficiency isn't linear. They want to fit a model that captures the curve in this relationship to better predict MPG.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Fit a Simple Linear Model.</strong> First, we fit a standard linear model to establish a baseline.
                <div class="code-container">
                    <pre><code class="language-r">model_linear <- lm(mpg ~ hp, data = mtcars)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit a Polynomial Model.</strong> We add a quadratic term using the <code>poly()</code> function, which is the most stable way to do this in R.
                <div class="code-container">
                    <pre><code class="language-r">model_poly <- lm(mpg ~ poly(hp, 2), data = mtcars)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Compare the Models.</strong> We can use the <code>anova()</code> function to formally test if the additional complexity of the polynomial model provides a statistically significant improvement in fit.
                <div class="code-container">
                    <pre><code class="language-r">anova(model_linear, model_poly)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong> We interpret the results of the model comparison.
                <div class="plot-description">
                    <p><strong>Conclusion for the engineer:</strong> The ANOVA test will yield a very small p-value (e.g., < 0.001). This indicates that the polynomial model is a significantly better fit to the data than the simple linear model. The summary of <code>model_poly</code> will show that the quadratic term is significant, confirming that the relationship between horsepower and MPG is curved. This more complex model will provide more accurate predictions.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Danger of Overfitting</h3>
    <div class="scenario-content">
        <p>Be very careful with high-degree polynomials! A 10th-degree polynomial can be made to pass through any 11 points, but it will be an incredibly "wiggly" and overfit line that is just modeling noise. This is called **Runge's phenomenon**. In practice, it is very rare to need a polynomial of a degree higher than 3. Always start simple and only add complexity if it is statistically justified and visually makes sense.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>cars</code> dataset, fit a simple linear model: <code>lm(dist ~ speed, data = cars)</code>.</li>
            <li>Now fit a quadratic model: <code>lm(dist ~ poly(speed, 2), data = cars)</code>.</li>
            <li>Use the <code>summary()</code> function on both models and compare their "Adjusted R-squared" values. Which model explains more of the variance in the data?</li>
            <li>Is the quadratic term in the second model statistically significant? What does this tell you about the relationship?</li>
        </ul>
    </div>
</div>
`,
          p4s1ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist is building a model to predict house prices using 100 features, including many that are highly correlated (e.g., <code>sqft_living_room</code>, <code>sqft_bedrooms</code>, <code>sqft_total</code>). A standard linear regression produces a model that is overfit—it has learned the noise in the training data too well—and its coefficients are unstable and make no real-world sense. They need a method to create a simpler, more robust model by shrinking the coefficients and performing automatic feature selection.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s1ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s1ss4-methods">The Methods</button>
        <button class="tab-button" data-tab="p4s1ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s1ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s1ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Penalizing Complexity</h4>
        <p><strong>Regularization</strong> is a technique used to combat overfitting in machine learning models. It works by adding a **penalty term** to the model's loss function. This penalty discourages the model from learning overly complex patterns or assigning too much importance to any single feature.</p>
        <p>The goal is to find a balance between fitting the training data well and keeping the model simple enough to generalize to new, unseen data.</p>
        <p><strong>Analogy:</strong> Regularization is like a **parent telling a child to clean their room**. The child's goal is to put their toys away (fit the data). The parent adds a penalty: "You have to do it without making a huge mess in the hallway" (keep the coefficients small). This constraint forces the child to find a simpler, more efficient solution.</p>
    </div>

    <div id="p4s1ss4-methods" class="tab-pane">
        <h4 class="subsection-title">Ridge vs. LASSO vs. Elastic Net</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Ridge (L2 Penalty)</strong>
                <p>Adds a penalty proportional to the *square* of the coefficients' magnitude. It shrinks large coefficients towards zero but **never sets them exactly to zero**. It is excellent at handling multicollinearity.</p>
            </div>
            <div class="decision-branch">
                <strong>LASSO (L1 Penalty)</strong>
                <p>Adds a penalty proportional to the *absolute value* of the coefficients' magnitude. This has the powerful property of being able to shrink irrelevant feature coefficients to **exactly zero**, effectively performing automatic feature selection.</p>
            </div>
            <div class="decision-branch">
                <strong>Elastic Net</strong>
                <p>A combination of both Ridge and LASSO penalties. It can shrink coefficients to zero like LASSO, but is also more stable in the presence of highly correlated predictors, like Ridge. It is often the best choice in practice.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s1ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Coefficient Path</h4>
        <p>The best way to visualize regularization is with a coefficient path plot. It shows how the value of each feature's coefficient changes as the strength of the penalty (lambda, $\\lambda$) increases.</p>
        <div class="plot-container">
            <div id="p4s1ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This shows a LASSO path. On the left (low penalty), all features have non-zero coefficients. As we move to the right and the penalty increases, the coefficients are "squeezed" towards zero. The least important features (like <code>drat</code> and <code>gear</code>) hit zero first and are eliminated from the model. The most important features (like <code>wt</code>) survive the longest.</p>
        </div>
    </div>

    <div id="p4s1ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have a dataset with many predictors and want to build a simple, robust linear model. We will use LASSO regression to both fit the model and select the most important features simultaneously.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Prepare the Data.</strong> The <code>glmnet</code> package requires a numeric matrix of predictors and a vector for the response. We also must standardize our predictors.
                <div class="code-container">
                    <pre><code class="language-r">x_vars <- model.matrix(mpg ~ ., data = mtcars)[, -1]
y_var <- mtcars$mpg
# IMPORTANT: glmnet has a built-in argument to standardize for you
# We don't need to do it manually.</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Find the Optimal Penalty (Lambda).</strong> We use k-fold cross-validation to test a range of lambda values and find the one that gives the best performance on unseen data.
                <div class="code-container">
                    <pre><code class="language-r">library(glmnet)
set.seed(42)
cv_lasso <- cv.glmnet(x_vars, y_var, alpha = 1, standardize = TRUE)
best_lambda <- cv_lasso$lambda.min
plot(cv_lasso) # Visualize the cross-validation error</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Final Coefficients.</strong> We extract the coefficients from our model using the best lambda value found in the previous step.
                <div class="code-container">
                    <pre><code class="language-r">best_coeffs <- coef(cv_lasso, s = "lambda.min")
print(best_coeffs)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong> We interpret the result.
                <div class="plot-description">
                    <p><strong>Conclusion:</strong> The LASSO model has automatically performed feature selection for us. The output shows that many of the original predictors (like drat, vs, gear) have coefficients of exactly zero (represented by a "."). The model has determined that the most important predictors for MPG are wt, cyl, and hp, resulting in a more parsimonious and robust model.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Always Standardize Your Data!</h3>
    <div class="scenario-content">
        <p>Regularization penalizes the magnitude of coefficients. If your features are on different scales (e.g., <code>age</code> from 20-70 and <code>income</code> from 30k-150k), the penalty will be applied unfairly. The coefficient for income will be naturally smaller, and the penalty will affect it less. It is **absolutely essential** to standardize your predictors before fitting a regularized regression model. The <code>glmnet</code> package conveniently has a <code>standardize = TRUE</code> argument to handle this for you.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>glmnet</code> code from the example.</li>
            <li>Run a Ridge regression instead of LASSO by changing the <code>alpha</code> argument to 0: <code>cv.glmnet(..., alpha = 0)</code>.</li>
            <li>Inspect the coefficients for the best Ridge model. How do they compare to the LASSO coefficients? Do any of them become exactly zero?</li>
            <li>Now try an Elastic Net by setting <code>alpha = 0.5</code>.</li>
        </ul>
    </div>
</div>
`,
          p4s1ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A dataset of company revenues has significant, legitimate outliers (e.g., a few massive tech companies). These outliers are not errors and cannot be removed, but they will heavily distort a standard linear regression model. The analyst needs a model that is resistant to the influence of these extreme values. Separately, an economist wants to model the factors affecting the 10th percentile of wages (low-income earners), not just the average wage.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s1ss5-concepts">Two Approaches to Robustness</button>
        <button class="tab-button" data-tab="p4s1ss5-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s1ss5-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s1ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Modeling Beyond the Mean</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Robust Regression</strong>
                <p>A family of regression methods designed to be less sensitive to outliers than Ordinary Least Squares (OLS). While OLS minimizes the *sum of squared errors* (which heavily penalizes large errors from outliers), robust methods minimize a function that gives less weight to these extreme points.</p>
                <p><strong>Analogy:</strong> OLS is like a **judge who gives an equally loud megaphone to every witness**. The one shouting witness (the outlier) can drown out everyone else. Robust regression is a judge who **gives a quieter microphone to the shouting witness**, listening more closely to the consensus of the calmer majority.</p>
            </div>
            <div class="decision-branch">
                <strong>Quantile Regression</strong>
                <p>A method that models the relationship between predictors and a specific **quantile** (or percentile) of the response variable. By modeling the **median** (the 0.5 quantile), we get a regression line that is naturally robust to outliers.</p>
                <p><strong>Analogy:</strong> OLS models the **center of gravity** of the data cloud, which can be pulled by heavy outliers. Quantile regression at the median finds the line that **cuts the data cloud into two equal halves**, which is much less affected by where the extreme points lie.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s1ss5-viz" class="tab-pane">
        <h4 class="subsection-title">Seeing the Impact of an Outlier</h4>
        <p>This plot shows data with a clear outlier. The OLS line is pulled significantly towards the outlier, misrepresenting the trend in the bulk of the data. The Robust and Median Quantile Regression lines ignore the outlier and provide a much better fit.</p>
        <div class="plot-container">
            <div id="p4s1ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The single red outlier in the top-right has a massive influence on the standard OLS model (blue line), tilting it upwards. The Robust (green) and Median (orange) regression lines are nearly identical and successfully capture the true trend of the main cluster of data points, demonstrating their resistance to the outlier.</p>
        </div>
    </div>

    <div id="p4s1ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> The built-in <code>stackloss</code> dataset on the operation of a plant for converting ammonia to nitric acid is famous for containing several outliers. We want to fit a regression model that is not overly influenced by these points.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Fit a Standard OLS Model.</strong> We fit a normal linear model as a baseline to see what it looks like.
                <div class="code-container">
                    <pre><code class="language-r">model_ols <- lm(stack.loss ~ Air.Flow + Water.Temp, data = stackloss)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit a Robust Model.</strong> We use the <code>rlm()</code> function (robust linear model) from the <code>MASS</code> package. It automatically down-weights the influence of outliers.
                <div class="code-container">
                    <pre><code class="language-r">library(MASS)
model_robust <- rlm(stack.loss ~ Air.Flow + Water.Temp, data = stackloss)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Fit a Median Regression Model.</strong> We use the <code>rq()</code> function from the <code>quantreg</code> package, specifying <code>tau = 0.5</code> to model the median.
                <div class="code-container">
                    <pre><code class="language-r">library(quantreg)
model_median <- rq(stack.loss ~ Air.Flow + Water.Temp, data = stackloss, tau = 0.5)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Compare Coefficients.</strong> We compare the estimated coefficients from the three models.
                <div class="plot-description">
                    <p><strong>Conclusion:</strong> You will notice that the coefficients from the <code>rlm</code> and <code>rq</code> models are different from the standard <code>lm</code> model. This is because they have ignored or down-weighted the outlier points, providing an estimate of the relationship that is more representative of the bulk of the data.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Modeling Prediction Intervals</h3>
    <div class="scenario-content">
        <p>Quantile regression is incredibly powerful for more than just handling outliers. By modeling multiple quantiles, you can understand how predictors affect the entire distribution of the outcome. For example, by fitting models for the 10th quantile (<code>tau = 0.1</code>) and the 90th quantile (<code>tau = 0.9</code>), you can create a prediction interval and see if the effect of a predictor is different for low-value vs. high-value outcomes.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a copy of the <code>cars</code> dataset: <code>cars_mod <- cars</code>.</li>
            <li>Add a significant outlier to it: <code>cars_mod[51,] <- c(10, 200)</code>.</li>
            <li>Fit three models to predict <code>dist</code> from <code>speed</code> on this modified data: a standard lm(), a robust rlm(), and a median rq().</li>
            <li>Create a scatter plot of the data. Use <code>abline()</code> to add the regression lines from all three models to the plot, using different colors. Which lines are most affected by the outlier?</li>
        </ul>
    </div>
</div>
`,
          p4s1ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has a dataset with 20 potential predictors for customer churn. Should they include all of them in their logistic regression model? Some might be irrelevant noise, and others might be highly correlated. Including them all could lead to an overfit model that performs poorly on new customers. The analyst needs a principled strategy for <strong>Model Selection</strong>.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s1ss6-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s1ss6-viz">The Bias-Variance Tradeoff</button>
        <button class="tab-button" data-tab="p4s1ss6-usecase">Stepwise Selection in R</button>
    </div>

    <div id="p4s1ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Principle of Parsimony</h4>
        <p>Model selection is the task of selecting a final model from a set of candidate models. The goal is to find the most **parsimonious** model—the simplest model that provides the best possible fit to the data. This involves navigating the **bias-variance tradeoff**.</p>
        <p><strong>Analogy:</strong> Model selection is like **packing for a hike**. A simple model is a **light daypack**: it's fast and easy to carry (low variance) but might not have everything you need (high bias). A complex model is a **giant expedition backpack**: it has gear for every possible situation (low bias) but is heavy, slow, and full of unnecessary items (high variance, overfitting). The goal is to find the perfectly packed bag that has everything you need and nothing you don't.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Information Criteria (AIC/BIC)</strong>
                <p>Calculates a score for a model that balances its goodness-of-fit with its complexity (number of parameters). The goal is to find the model with the lowest AIC or BIC score.</p>
            </div>
            <div class="decision-branch">
                <strong>Stepwise Selection</strong>
                <p>An automated algorithm that iteratively adds or removes predictors from a model based on an information criterion like AIC. It can be forward, backward, or both.</p>
            </div>
            <div class="decision-branch">
                <strong>Cross-Validation</strong>
                <p>The gold standard. It directly estimates a model's predictive performance on unseen data. You can compare several candidate models and choose the one with the best cross-validated performance.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s1ss6-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Tradeoff</h4>
        <p>This classic plot shows how error changes as model complexity increases.</p>
        <div class="plot-container">
            <div id="p4s1ss6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> As you add more predictors (moving right), the **Training Error** (blue line) always goes down. The model gets better and better at fitting the data it was trained on. However, the **Test Error** (orange line) on unseen data follows a U-shape. It decreases at first, hits an optimal point, and then starts to increase as the model becomes overfit. The goal of model selection is to find the "sweet spot" at the bottom of the U.</p>
        </div>
    </div>

    <div id="p4s1ss6-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to find the most parsimonious model to predict a car's MPG from all the other variables in the <code>mtcars</code> dataset, without manually testing every combination.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Define the Fullest and Simplest Models.</strong> We define the most complex model we are willing to consider (<code>full_model</code>) and the simplest possible model (<code>null_model</code>, which only includes the intercept).
                <div class="code-container">
                    <pre><code class="language-r">full_model <- lm(mpg ~ ., data = mtcars)
null_model <- lm(mpg ~ 1, data = mtcars)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Perform Stepwise Selection.</strong> We use the <code>step()</code> function to search for the model with the lowest AIC score, starting from the null model and exploring models up to the complexity of the full model.
                <div class="code-container">
                    <pre><code class="language-r">stepwise_model <- step(null_model, 
                               scope = list(lower = null_model, upper = full_model),
                               direction = "both", trace = 0)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Analyze the Result.</strong> We use <code>summary()</code> on the final selected model to inspect its properties.
                <div class="code-container">
                    <pre><code class="language-r">summary(stepwise_model)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong>
                <div class="plot-description">
                    <p><strong>Conclusion:</strong> The stepwise algorithm selected a model that includes wt, qsec, and am as the most important predictors. By comparing the Adjusted R-squared of this model to that of the full_model, we can see that our simpler, more parsimonious model explains nearly the same amount of variance with far fewer variables, making it less prone to overfitting and easier to interpret.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Stepwise is a Tool, Not a Truth Machine</h3>
    <div class="scenario-content">
        <p>Automated stepwise methods are useful for exploration but should be used with caution. They can be unstable (small changes in the data can lead to a very different final model) and they can't use domain knowledge. They are best used as a tool to generate a few good candidate models, which you should then validate rigorously using cross-validation and compare against models built using your own expert knowledge.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Take the final model object from the <code>step()</code> example (<code>stepwise_model</code>).</li>
            <li>What variables did the algorithm select as the best predictors for <code>mpg</code>?</li>
            <li>What is the Adjusted R-squared of this selected model? How does it compare to the Adjusted R-squared of the <code>full_model</code>? This demonstrates the principle of parsimony—we often get a similar or better predictive power with fewer variables.</li>
        </ul>
    </div>
</div>
`,
          p4s1ss7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst fits a linear regression model and the <code>summary()</code> output looks great: a high R-squared and significant p-values. They declare the model a success. However, they have failed to perform diagnostic checks. A residual plot would have revealed a strong non-linear pattern in the data, violating a key model assumption and making the standard p-values and confidence intervals invalid. The model is actually a poor fit.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s1ss7-concepts">The OLS Assumptions</button>
        <button class="tab-button" data-tab="p4s1ss7-viz">The Four Diagnostic Plots</button>
        <button class="tab-button" data-tab="p4s1ss7-usecase">Interpreting the Plots</button>
    </div>

    <div id="p4s1ss7-concepts" class="tab-pane active">
        <h4 class="subsection-title">Is Your Model Trustworthy?</h4>
        <p>For the results of an Ordinary Least Squares (OLS) regression to be valid and unbiased, the data must meet several key assumptions. Model diagnostics is the process of checking if these assumptions are met.</p>
        <p><strong>Analogy:</strong> Model diagnostics is like a **post-flight inspection for an airplane**. The flight (the model fitting) may have seemed smooth, and you arrived at your destination (a result). But a responsible pilot will always walk around the plane and check the engines, tires, and control surfaces (the assumptions) to ensure the journey was truly safe and that the plane is reliable for the next flight.</p>
        <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">The "LINE" Assumptions:</h5>
        <ul class="prose-list">
            <li><strong>L - Linearity:</strong> The true relationship between the predictors and the outcome is linear.</li>
            <li><strong>I - Independence:</strong> The errors (residuals) are independent of each other.</li>
            <li><strong>N - Normality:</strong> The errors are normally distributed.</li>
            <li><strong>E - Equal Variance (Homoscedasticity):</strong> The errors have a constant variance across all levels of the predictors.</li>
        </ul>
    </div>
    
    <div id="p4s1ss7-viz" class="tab-pane">
        <h4 class="subsection-title">Your Diagnostic Dashboard</h4>
        <p>In R, simply calling <code>plot()</code> on a fitted <code>lm</code> object will produce four essential diagnostic plots.</p>
        <div class="plot-container">
            <div id="p4s1ss7-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe these plots:</strong> Each plot checks a different assumption. For a good model, the **Residuals vs. Fitted** plot should look like a random cloud of points with no pattern. The **Normal Q-Q** plot should have points that fall along the diagonal line. The **Scale-Location** plot should also show a random cloud of points. The **Residuals vs. Leverage** plot helps identify influential outliers.</p>
        </div>
    </div>

    <div id="p4s1ss7-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have fit a linear model. Now we must act as a detective and use the diagnostic plots to check if the model's assumptions have been met.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Fit a Model and Generate Plots.</strong> We fit a simple model and use <code>par(mfrow = c(2, 2))</code> to arrange the four plots in a grid.
                <div class="code-container">
                    <pre><code class="language-r">car_model <- lm(mpg ~ wt, data = mtcars)
par(mfrow = c(2, 2)); plot(car_model); par(mfrow = c(1, 1))</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Check for Linearity & Equal Variance.</strong> We examine the "Residuals vs. Fitted" plot.
                <div class="plot-description">
                    <p>The red line is roughly flat and the points are randomly scattered around it. This is good! It suggests no major violation of the linearity or equal variance assumptions.</p>
                </div>
            </li>
            <li><strong>Step 3: Check for Normality of Residuals.</strong> We examine the "Normal Q-Q" plot.
                <div class="plot-description">
                    <p>The points fall very closely along the dashed diagonal line, with only minor deviations at the tails. This indicates that the residuals are very close to being normally distributed, so this assumption is met.</p>
                </div>
            </li>
            <li><strong>Step 4: Check for Influential Outliers.</strong> We examine the "Residuals vs. Leverage" plot.
                <div class="plot-description">
                    <p>No points fall outside the dashed red "Cook's distance" lines. This means that while some points have higher leverage, none are so influential that they are single-handedly controlling the slope of the regression line.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>performance</code> Package</h3>
    <div class="scenario-content">
        <p>The <code>performance</code> package provides a fantastic function, <code>check_model()</code>, that automatically generates a comprehensive and beautifully annotated set of diagnostic plots using <code>ggplot2</code>. It's a modern, one-line alternative to the base R <code>plot()</code> method and provides clear, plain-English interpretations of the results, making it excellent for beginners and experts alike.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Fit a linear model to the <code>cars</code> dataset: <code>model <- lm(dist ~ speed, data = cars)</code>.</li>
            <li>Generate the four diagnostic plots for this model using <code>plot(model)</code>.</li>
            <li>Look at the "Residuals vs. Fitted" plot. Do you see a slight curve in the points? What might this suggest about the relationship between speed and distance?</li>
            <li>Look at the "Normal Q-Q" plot. Do the residuals appear to be approximately normally distributed?</li>
        </ul>
    </div>
</div>
`,
          p4s2ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A bank in Karachi wants to predict whether a new loan applicant will default (a Yes/No outcome). They have data like the applicant's income and credit score. A standard linear regression is a poor choice because it could predict a "probability of default" of -20% or 150%, which is nonsensical. 🇵🇰 They need a model that always outputs a valid probability between 0 and 1. This is the primary use case for <strong>Logistic Regression</strong>.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s2ss1-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p4s2ss1-math">The Sigmoid & Log-Odds</button>
        <button class="tab-button" data-tab="p4s2ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s2ss1-usecase">Implementation & Interpretation</button>
    </div>

    <div id="p4s2ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Predicting Probabilities, Not Values</h4>
        <p>Logistic Regression is a foundational classification algorithm used to predict a binary (Yes/No, 0/1) outcome. Despite its name, it's a model for classification, not regression.</p>
        <p>It works by taking a linear combination of the input features (just like linear regression) but then passing that result through a special non-linear function to produce the final output.</p>
        <p><strong>Analogy:</strong> Logistic Regression is like a **dimmer switch for probability**. A simple light switch is either ON or OFF (a hard classification). A dimmer switch takes an input (how far you turn the knob) and smoothly maps it to a value between 0% (off) and 100% (fully on). The logistic function is this dimmer, ensuring the model's output is always a valid probability.</p>
    </div>
    
    <div id="p4s2ss1-math" class="tab-pane">
        <h4 class="subsection-title">From Linear to Logistic</h4>
        <p>The model achieves its goal in two steps:</p>
        <ul class="prose-list"></ul>
            <li><strong>The Linear Predictor:</strong> It first calculates the <strong>log-odds</strong> (also called the logit) of the event, which *is* linear.
            <div class="math-foundation">$$ \\text{log}(\\frac{p}{1-p}) = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_nX_n $$</div>
            </li>
            <li><strong>The Sigmoid Function:</strong> It then uses the inverse of the logit function, called the logistic or **sigmoid function**, to map the log-odds back to a probability $p$.
            <div class="math-foundation">$$ p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\dots)}} $$</div>
            </li>
        </ul>
        <p>This two-step process is the core of how a GLM works, using a link function to connect a linear model to a non-linear outcome.</p>
    </div>

    <div id="p4s2ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Sigmoid Curve</h4>
        <p>This plot shows the S-shaped sigmoid curve. It takes the output of the linear model (the log-odds, which can be any number) and squishes it onto a scale from 0 to 1.</p>
        <div class="plot-container">
            <div id="p4s2ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The x-axis represents the raw output of the linear equation. The y-axis is the final predicted probability. Notice how a large positive input results in a probability close to 1, a large negative input results in a probability close to 0, and an input of 0 results in a probability of exactly 0.5. This mathematical property is what makes logistic regression so suitable for binary classification.</p>
        </div>
    </div>
    
    <div id="p4s2ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> A manager wants to understand what car features predict whether a car has a manual or automatic transmission. Specifically, how does horsepower affect the probability of a car being manual?</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Fit the Model.</strong> Because the outcome (<code>am</code>) is binary (0=auto, 1=manual), we use a logistic regression. The <code>family = binomial</code> argument tells <code>glm()</code> to do this.
                <div class="code-container">
                    <pre><code class="language-r">logistic_model <- glm(am ~ hp, data = mtcars, family = binomial(link = "logit"))</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Inspect the Coefficients.</strong> The raw coefficients are in log-odds, which are difficult to interpret directly.
                <div class="code-container">
                    <pre><code class="language-r">summary(logistic_model)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Convert to Odds Ratios.</strong> We exponentiate the coefficients to convert them into the much more intuitive odds ratios.
                <div class="code-container">
                    <pre><code class="language-r">odds_ratios <- exp(coef(logistic_model))
print(odds_ratios)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong> We translate the odds ratio for <code>hp</code> into a business insight.
                <div class="plot-description">
                    <p><strong>Conclusion for the manager:</strong> "We found a significant relationship between horsepower and transmission type. The odds ratio for horsepower is 1.03. This means that for every one-unit increase in a car's horsepower, the **odds** of it having a manual transmission increase by approximately 3%. In other words, more powerful cars in this dataset are more likely to be manual."</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Predicting Probabilities</h3>
    <div class="scenario-content">
        <p>After fitting your model, you can use the <code>predict()</code> function to get predictions on new data. Crucially, you must set the argument <code>type = "response"</code>. If you forget this, <code>predict()</code> will return the log-odds, not the final probabilities!</p>
        <div class="code-container">
             <pre><code class="language-r">
# Correct way to get predicted probabilities
predicted_probs <- predict(logistic_model, newdata = new_cars, type = "response")
             </code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, fit a logistic regression model to predict the engine shape (<code>vs</code>) based on the car's weight (<code>wt</code>).</li>
            <li>Use <code>summary()</code> to check if <code>wt</code> is a significant predictor.</li>
            <li>Use <code>exp(coef(model))</code> to get the odds ratio for <code>wt</code>.</li>
            <li>Write a sentence interpreting this odds ratio. How do the odds of having a straight engine (<code>vs=1</code>) change for each 1000-lb increase in weight?</li>
        </ul>
    </div>
</div>
`,
          p4s2ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A marketing team wants to understand what defines their most valuable customers. The rules are likely complex and non-linear (e.g., "customers with high income AND who are young, OR customers with medium income AND many past purchases"). A single **Decision Tree** can discover these rules automatically. However, to build a more robust and accurate model, they will combine many trees into a **Random Forest**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s2ss2-concepts">Decision Trees</button>
        <button class="tab-button" data-tab="p4s2ss2-viz">Visualizing a Tree</button>
        <button class="tab-button" data-tab="p4s2ss2-ensembles">From Trees to Forests</button>
        <button class="tab-button" data-tab="p4s2ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s2ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Classification by Asking Questions</h4>
        <p>A <strong>Decision Tree</strong> is a non-parametric model that works by recursively partitioning the data into smaller and smaller subsets based on simple rules. It creates a tree-like model of decisions.</p>
        <p><strong>Analogy:</strong> A decision tree is like a game of **"20 Questions."** The algorithm asks a series of simple, yes/no questions about the data's features (e.g., "Is <code>Petal.Length</code> < 2.45?"). Each question splits the data into two branches. It continues asking questions until it reaches a "leaf" node that is pure (contains mostly one class).</p>
        <p>The splits are chosen to maximize **information gain** (or reduce impurity), often measured by metrics like Gini Impurity or Entropy.</p>
    </div>

    <div id="p4s2ss2-viz" class="tab-pane">
        <h4 class="subsection-title">A Tree for Classifying Irises</h4>
        <p>This diagram shows a simple decision tree trained on the <code>iris</code> dataset. It's highly interpretable and clearly shows the rules learned from the data.</p>
        <div class="plot-container">
            <div id="p4s2ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> You read the tree from the root node at the top. The first question is "Is Petal.Length < 2.45?". If yes, you follow the left branch and immediately classify the flower as <code>setosa</code>. If no, you follow the right branch and ask a second question, "Is Petal.Width < 1.75?". This process continues until you reach a terminal leaf node, which gives you the final classification.</p>
        </div>
    </div>
    
    <div id="p4s2ss2-ensembles" class="tab-pane">
        <h4 class="subsection-title">The Wisdom of the Crowd</h4>
        <p>A single decision tree is prone to overfitting. **Ensemble methods** combine many trees to create a more powerful and robust model.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Random Forest (Bagging)</strong>
                <p>Trains hundreds of deep decision trees independently on different bootstrap samples of the data. It also only considers a random subset of features at each split. To make a prediction, it takes a majority vote from all the trees.</p>
                <p><strong>Analogy:</strong> It's like asking a **diverse committee of experts**. Each expert (tree) has slightly different experience (data sample) and focuses on different evidence (feature sample). The final decision is based on their collective vote, which is much more reliable than any single expert.</p>
            </div>
            <div class="decision-branch">
                <strong>Gradient Boosting (Boosting)</strong>
                <p>Trains trees sequentially. The second tree is trained to correct the errors of the first tree, the third corrects the errors of the second, and so on. Each new tree focuses on the most difficult cases.</p>
                 <p><strong>Analogy:</strong> It's like a **team of students working on homework together**. The first student does their best. The second student looks at the first's mistakes and focuses specifically on fixing them. The third student corrects the remaining mistakes. The final answer is the cumulative work of the entire team.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s2ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to build a highly accurate model to classify penguin species from the <code>palmerpenguins</code> dataset. We will use a random forest because it's robust and handles complex interactions automatically.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load and Prepare Data.</strong> We load the necessary packages and remove any rows with missing data for this example.
                <div class="code-container">
                    <pre><code class="language-r">library(randomForest)
library(palmerpenguins)
library(tidyr)
penguins_clean <- penguins %>% drop_na()</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the Random Forest Model.</strong> We specify the formula and the data. <code>ntree</code> determines how many trees to grow in our "forest".
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42)
penguin_forest <- randomForest(species ~ ., data = penguins_clean, ntree = 500)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Model Output.</strong> The printed model object contains a wealth of information, including the out-of-bag (OOB) error estimate.
                <div class="code-container">
                    <pre><code class="language-r">print(penguin_forest)</code></pre>
                </div>
                 <div class="plot-description">
                    <p>The OOB estimate of error rate is a reliable measure of the model's accuracy on unseen data, calculated automatically during training. In this case, it's very low, indicating a highly accurate model.</p>
                </div>
            </li>
            <li><strong>Step 4: Check Feature Importance.</strong> A random forest can tell us which variables were most important for making accurate predictions.
                <div class="code-container">
                    <pre><code class="language-r">importance(penguin_forest)
varImpPlot(penguin_forest) # A useful plot of the importance scores</code></pre>
                </div>
                <div class="plot-description">
                    <p><strong>Conclusion:</strong> The random forest is a highly accurate classifier for this task. The feature importance plot reveals that <code>flipper_length_mm</code> and <code>bill_length_mm</code> are the two most powerful predictors for determining a penguin's species.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Pruning a Decision Tree</h3>
    <div class="scenario-content">
        <p>A single decision tree that is grown very deep will almost always overfit the training data. To prevent this, you can "prune" it. This involves growing a full, complex tree and then trimming back the branches that add the least predictive power. The <code>rpart</code> package has built-in cross-validation functions to help you find the optimal level of pruning.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, convert the <code>am</code> column to a factor: <code>mtcars$am <- as.factor(mtcars$am)</code>.</li>
            <li>Fit a single decision tree with <code>rpart()</code> to predict <code>am</code> using <code>mpg</code> and <code>hp</code>.</li>
            <li>Use <code>rpart.plot()</code> to visualize your tree. What is the single most important rule the tree discovered?</li>
            <li>Now, fit a random forest model to predict <code>am</code> using all other variables. Inspect the model output. How accurate is the forest?</li>
        </ul>
    </div>
</div>
`,
          p4s2ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A botanist wants to classify iris flowers into one of three species based on their sepal and petal measurements. They have good reason to believe that the features for each species follow a roughly multivariate normal distribution (a bell curve in multiple dimensions). This is the ideal scenario for using <strong>Discriminant Analysis</strong>, a classic and powerful generative classification model.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s2ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s2ss3-methods">LDA vs. QDA</button>
        <button class="tab-button" data-tab="p4s2ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s2ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s2ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Classification via Bayes' Theorem</h4>
        <p>Unlike logistic regression which models the probability of the outcome directly, Discriminant Analysis is a **generative model**. It takes a different approach:</p>
        <ul class="prose-list"></ul>
            <li>It first models the distribution of the predictor variables ($X$) separately for each class ($Y$). It learns the "profile" of each class.</li>
            <li>It then uses Bayes' theorem to flip this around and calculate the probability of a class given a new set of predictors: $P(Y|X)$.</li>
        </ul>
        <p><strong>Analogy:</strong> Discriminant Analysis is like a **profiler**. A profiler first learns the typical "profile" (the data distribution) of different known groups (e.g., artists, engineers, doctors). When a new, unknown individual comes along, the profiler checks how well their characteristics match each known profile to make the most likely classification.</p>
    </div>

    <div id="p4s2ss3-methods" class="tab-pane">
        <h4 class="subsection-title">Linear vs. Quadratic Boundaries</h4>
        <p>The two main types of discriminant analysis differ in one key assumption, which determines the shape of their decision boundaries.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Linear Discriminant Analysis (LDA)</strong>
                <p>Assumes that all classes share the same covariance matrix. This simplifying assumption means the resulting decision boundary between any two classes will always be a **straight line**.</p>
                <p><strong>Pro/Con:</strong> Less flexible, but also less prone to overfitting. A good choice with limited data.</p>
            </div>
            <div class="decision-branch">
                <strong>Quadratic Discriminant Analysis (QDA)</strong>
                <p>Allows each class to have its own unique covariance matrix. This means the resulting decision boundary can be **curved (a quadratic shape)**.</p>
                <p><strong>Pro/Con:</strong> More flexible and can fit more complex relationships, but requires more data to estimate the covariance matrices and can be more prone to overfitting.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s2ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Decision Boundaries</h4>
        <p>This plot shows the same data classified by both LDA and QDA. Notice how LDA's boundaries are straight lines, while QDA's are curved to better fit the shape of the classes.</p>
        <div class="plot-container">
            <div id="p4s2ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The points represent three distinct classes. The solid black lines are the **linear decision boundaries** found by LDA. The dashed red curves are the more flexible **quadratic decision boundaries** found by QDA. In this case, QDA's flexibility allows it to capture the shape of the classes more accurately.</p>
        </div>
    </div>
    
    <div id="p4s2ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> A botanist wants to classify iris species. They have checked the data and confirmed that the predictors are roughly normally distributed within each species and the variances are similar, making LDA an excellent choice.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Data and Package.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(MASS)
data(iris)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Split Data into Training and Testing Sets.</strong> We must evaluate the model on unseen data.
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42)
train_idx <- sample(1:nrow(iris), 100)
train_data <- iris[train_idx, ]
test_data <- iris[-train_idx, ]</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Fit the LDA Model.</strong> We fit the model using only the training data.
                <div class="code-container">
                    <pre><code class="language-r">lda_model <- lda(Species ~ ., data = train_data)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Make Predictions and Evaluate.</strong> We use the trained model to make predictions on the unseen test data and create a confusion matrix.
                <div class="code-container">
                    <pre><code class="language-r">predictions <- predict(lda_model, newdata = test_data)
# The output contains several components, the class is what we want
predicted_class <- predictions$class
# Create a confusion matrix
conf_matrix <- table(predicted_class, test_data$Species)
print(conf_matrix)
# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Accuracy:", accuracy))</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The confusion matrix shows that the LDA model is highly accurate (often 98% or higher on this dataset), correctly classifying the unseen flowers with very few errors. This confirms it's a good model for this problem.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: LDA for Dimensionality Reduction</h3>
    <div class="scenario-content">
        <p>Beyond just classification, LDA is also a powerful **supervised** dimensionality reduction technique. It finds the linear combinations of predictors (called "linear discriminants") that best separate the classes. You can use an LDA model to project your high-dimensional data down to a few dimensions and create visualizations that show maximum class separation, which is often more effective than using PCA for this purpose.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, convert <code>am</code> to a factor.</li>
            <li>Fit an LDA model to predict <code>am</code> using <code>mpg</code> and <code>hp</code>.</li>
            <li>Fit a QDA model using the same formula.</li>
            <li>Use the <code>predict()</code> function on both models for the entire <code>mtcars</code> dataset. Create a confusion matrix for each (e.g., <code>table(predicted = ..., actual = mtcars$am)</code>). Which model appears to be more accurate on the training data?</li>
        </ul>
    </div>
</div>
`,
          p4s2ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is trying to separate two classes of data points that are not perfectly linearly separable. They don't just want a line that separates *most* of them; they want the line that creates the **widest possible buffer** or "street" between the two classes, for maximum robustness against new data. This is the core principle of a <strong>Support Vector Machine (SVM)</strong>.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s2ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s2ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s2ss4-kernel">The Kernel Trick</button>
        <button class="tab-button" data-tab="p4s2ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s2ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Maximum Margin Classifier</h4>
        <p>A Support Vector Machine is a powerful classification algorithm that works by finding the optimal **hyperplane** (a line in 2D, a plane in 3D, etc.) that best separates the classes in the feature space.</p>
        <p>"Optimal" is defined as the hyperplane that has the largest **margin**—the distance between the hyperplane and the closest data point from either class. This makes the model robust.</p>
        <p><strong>Analogy:</strong> An SVM is like finding the **widest possible road** you can build between two neighborhoods (the classes) without touching any of the houses (the data points). The houses on the very edge of the road that dictate its position are called the **support vectors**.</p>
    </div>
    
    <div id="p4s2ss4-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Margin</h4>
        <p>This diagram shows the optimal separating hyperplane found by an SVM. The dashed lines represent the margins, and the circled points are the support vectors that define the margin's width.</p>
        <div class="plot-container">
            <div id="p4s2ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The solid black line is the decision boundary. The SVM works to maximize the width of the grey shaded area (the margin). The circled points are the support vectors; they are the only points that matter for defining the boundary. If any of the other points were moved, the line would not change.</p>
        </div>
    </div>
    
    <div id="p4s2ss4-kernel" class="tab-pane">
        <h4 class="subsection-title">Handling Non-Linear Data</h4>
        <p>What if the data can't be separated by a straight line? The **Kernel Trick** is a powerful mathematical technique that allows SVMs to create complex, non-linear decision boundaries.</p>
        <p>It works by implicitly projecting the data into a much higher dimension where it *is* linearly separable, without ever having to actually compute the coordinates in that new space. The most popular kernel is the **Radial Basis Function (RBF) kernel**.</p>
        <p><strong>Analogy:</strong> Imagine a set of red and blue marbles on a table that you can't separate with a ruler. The Kernel Trick is like **hitting the table from underneath**. The marbles fly into the air (a higher dimension). For a brief moment, you can easily slide a piece of paper (a linear hyperplane) between the red and blue marbles. When they fall back down, the piece of paper's shadow on the table creates a complex, curved boundary.</p>
    </div>
    
    <div id="p4s2ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to build a powerful classifier for the <code>iris</code> dataset. We suspect the boundary between <code>versicolor</code> and <code>virginica</code> might be slightly curved, so we will use a non-linear SVM with a radial kernel.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Split Data.</strong> We load the <code>e1071</code> package and split our data for proper validation.
                <div class="code-container">
                    <pre><code class="language-r">library(e1071)
set.seed(42)
train_idx <- sample(1:nrow(iris), 100)
train_data <- iris[train_idx, ]
test_data <- iris[-train_idx, ]</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Tune Hyperparameters.</strong> The <code>cost</code> and <code>gamma</code> parameters are critical. We use the <code>tune()</code> function to perform a grid search using cross-validation to find the best combination.
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42)
tune_result <- tune(svm, Species ~ ., data = train_data,
                    kernel = "radial",
                    ranges = list(cost = c(0.1, 1, 10), gamma = c(0.1, 0.5, 1)))
# Inspect the best parameters found
print(tune_result$best.parameters)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Fit the Final Model.</strong> We use the best parameters found during tuning to train the final model on the full training set.
                <div class="code-container">
                    <pre><code class="language-r">final_svm_model <- svm(Species ~ ., data = train_data, kernel = "radial",
                           cost = tune_result$best.parameters$cost,
                           gamma = tune_result$best.parameters$gamma)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Evaluate on Test Set.</strong>
                <div class="code-container">
                    <pre><code class="language-r">predictions <- predict(final_svm_model, newdata = test_data)
table(predicted = predictions, actual = test_data$Species)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> After tuning, the SVM provides extremely high accuracy on the unseen test data. The process of tuning the hyperparameters is essential for achieving this robust performance.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Tune the <code>cost</code> and <code>gamma</code> Hyperparameters</h3>
    <div class="scenario-content">
        <p>The performance of an SVM, especially with a radial kernel, is highly sensitive to its hyperparameters. The <code>cost</code> parameter controls the tradeoff between a wide margin and misclassifying training points. A high cost will try to classify every point correctly, leading to a complex boundary and potential overfitting. The <code>gamma</code> parameter controls the influence of a single training example. Using cross-validation to tune these parameters (e.g., with <code>tune.svm()</code> in <code>e1071</code>) is essential for building a good SVM model.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, with <code>am</code> as a factor.</li>
            <li>Fit a **linear** SVM to predict <code>am</code> from <code>mpg</code> and <code>wt</code>. Formula: <code>am ~ mpg + wt</code>, <code>kernel = "linear"</code>.</li>
            <li>Now fit a **non-linear** SVM using the same formula but with <code>kernel = "radial"</code>.</li>
            <li>For both models, generate predictions and create a confusion matrix. Which model performs better on the training data?</li>
        </ul>
    </div>
</div>
`,
          p4s2ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A developer needs to build a fast and effective email spam filter. The filter must classify thousands of emails per second based on the presence of certain words (e.g., "deal", "free", "viagra"). The model needs to be simple, incredibly fast to train, and quick at making predictions, even if it makes a strong, simplifying assumption about the data.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s2ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s2ss5-math">The "Naive" Assumption</button>
        <button class="tab-button" data-tab="p4s2ss5-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s2ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Fast, Probabilistic Classification</h4>
        <p>The <strong>Naive Bayes</strong> classifier is a probabilistic algorithm based on Bayes' Theorem. It's particularly popular for text classification tasks like spam filtering and document categorization.</p>
        <p>It works by calculating the probability of a document belonging to each class, based on the words it contains, and then choosing the class with the highest probability.</p>
        <p><strong>Analogy:</strong> A Naive Bayes classifier is like a **detective who is a bit naive**. They see a suspect with a ripped shirt and muddy boots. Instead of considering that the mud and the rip might be related (e.g., from a single struggle), they treat each piece of evidence as completely independent when calculating the overall probability of guilt. It's a simplification, but it's fast and often gets the job done surprisingly well.</p>
    </div>
    
    <div id="p4s2ss5-math" class="tab-pane">
        <h4 class="subsection-title">Conditional Independence</h4>
        <p>The "naive" part of the name comes from its core assumption: all predictor variables are **conditionally independent** of each other, given the class.</p>
        <p>For a spam filter, this means the model assumes that the probability of the word "deal" appearing in an email is completely independent of the probability of the word "free" appearing, given that we know the email is spam. This is almost never true in reality (these words often appear together), but the assumption makes the calculations extremely fast and efficient.</p>
        <div class="math-foundation">$$ P(\\text{class} | \\text{features}) \\propto P(\\text{class}) \\times \\prod_{i=1}^{n} P(\\text{feature}_i | \\text{class}) $$</div>
        <div class="plot-description">
            <p>The formula shows that the probability of a class given the features is proportional to the prior probability of the class, multiplied by the product of the individual likelihoods of each feature given that class. The product ($\\prod$) is only mathematically correct because of the independence assumption.</p>
        </div>
    </div>
    
    <div id="p4s2ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We need a simple baseline model to classify the species of Iris flowers. Since the features are numeric and roughly bell-shaped, the default Gaussian Naive Bayes is a reasonable choice.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Split Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(e1071)
set.seed(42)
train_idx <- sample(1:nrow(iris), 100)
train_data <- iris[train_idx, ]
test_data <- iris[-train_idx, ]</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the Naive Bayes Model.</strong> The syntax is straightforward. The model learns the distribution of each feature for each class from the training data.
                <div class="code-container">
                    <pre><code class="language-r">nb_model <- naiveBayes(Species ~ ., data = train_data)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Learned Parameters.</strong> Printing the model shows the learned class proportions (A-priori) and the conditional feature distributions (mean and sd for each predictor within each species).
                <div class="code-container">
                    <pre><code class="language-r">print(nb_model)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Evaluate on the Test Set.</strong>
                <div class="code-container">
                    <pre><code class="language-r">predictions <- predict(nb_model, newdata = test_data)
table(predicted = predictions, actual = test_data$Species)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The confusion matrix shows that even with its strong "naive" assumption, the model is highly accurate on this dataset, demonstrating its effectiveness as a quick and simple baseline classifier.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Laplace Smoothing</h3>
    <div class="scenario-content">
        <p>What happens if your model is trying to classify a new email that contains a word it has never seen before in spam emails? The probability of that word given "spam" would be zero, which would make the entire final probability for "spam" zero, regardless of the other words. To prevent this, Naive Bayes classifiers use **Laplace smoothing**. It adds a small pseudo-count (usually 1) to every feature's count, ensuring that no probability is ever exactly zero. The <code>naiveBayes()</code> function in R does this by default.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The <code>HouseVotes84</code> dataset in the <code>mlbench</code> package contains the voting records of US congresspeople. It's a great dataset for Naive Bayes because all features are categorical.</li>
            <li>First, install and load <code>mlbench</code> and get the data: <code>library(mlbench); data(HouseVotes84)</code>. The <code>Class</code> column is the political party.</li>
            <li>Fit a Naive Bayes model to predict <code>Class</code> from all other variables.</li>
            <li>Use the model to predict the classes for the entire dataset and create a confusion matrix. How accurate is the model?</li>
        </ul>
    </div>
</div>
`,
          p4s2ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A streaming service wants to recommend a movie to a new user. The service finds the 5 users (<code>k=5</code>) with the most similar viewing histories to the new user. It then looks at the movies those 5 "neighbors" liked and recommends the one that was most popular within that small, similar group. This is the core logic of the <strong>k-Nearest Neighbors (k-NN)</strong> algorithm.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s2ss6-concepts">Core Idea</button>
        <button class="tab-button" data-tab="p4s2ss6-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s2ss6-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s2ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">Classification by Proximity</h4>
        <p>k-Nearest Neighbors is a simple, non-parametric, and "lazy" learning algorithm.
            <ul>
                <li><strong>Non-parametric:</strong> It makes no assumptions about the underlying distribution of the data.</li>
                <li><strong>Lazy Learning:</strong> It doesn't build a "model" during the training phase. It simply stores the entire training dataset. The real work happens during the prediction phase.</li>
            </ul>
        </p>
        <p>To classify a new data point, k-NN finds the <code>k</code> points in the training data that are closest to it (its "nearest neighbors") and takes a majority vote among those neighbors to determine the class.</p>
        <p><strong>Analogy:</strong> k-NN is the **"birds of a feather flock together"** algorithm. To classify a new, unknown bird, you simply look at the <code>k</code> birds it's closest to in the sky. If most of them are blue jays, you classify the new bird as a blue jay.</p>
    </div>
    
    <div id="p4s2ss6-viz" class="tab-pane">
        <h4 class="subsection-title">The Neighborhood Vote</h4>
        <p>This plot shows how a new, unknown point (the star) is classified. The algorithm finds its k-nearest neighbors and uses their classes to make a prediction.</p>
        <div class="plot-container">
            <div id="p4s2ss6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The star is the new point we want to classify. If we choose <code>k=3</code> (the inner circle), its three nearest neighbors are two red triangles and one blue square. By majority vote (2 to 1), the star is classified as "Red Triangle." If we had chosen <code>k=5</code> (the outer circle), the neighbors would be three blue squares and two red triangles, and the star would be classified as "Blue Square."</p>
        </div>
    </div>
    
    <div id="p4s2ss6-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to classify Iris species using a k-NN model. Because k-NN is a distance-based algorithm, scaling our features is a critical first step.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Prepare the Data.</strong> We scale all numeric predictors to have a mean of 0 and SD of 1.
                <div class="code-container">
                    <pre><code class="language-r">iris_scaled <- as.data.frame(scale(iris[, 1:4]))
iris_scaled$Species <- iris$Species</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Split Data into Training and Testing Sets.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(class)
set.seed(42)
train_indices <- sample(1:nrow(iris_scaled), size = 100)
train_data <- iris_scaled[train_indices, ]
test_data <- iris_scaled[-train_indices, ]</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Run the k-NN Algorithm.</strong> The <code>knn()</code> function requires separate objects for the training predictors, test predictors, and the training labels. We choose a value for <code>k</code>.
                <div class="code-container">
                    <pre><code class="language-r">predictions <- knn(
  train = train_data[, 1:4],
  test = test_data[, 1:4],
  cl = train_data$Species,
  k = 5
)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Evaluate the Model.</strong> We create a confusion matrix to assess the performance on the unseen test data.
                <div class="code-container">
                    <pre><code class="language-r">table(predicted = predictions, actual = test_data$Species)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The confusion matrix will show high accuracy, indicating that even this simple, instance-based model is very effective for this particular classification task once the data has been properly scaled.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Choosing the Right <code>k</code></h3>
    <div class="scenario-content">
        <p>The choice of <code>k</code> is critical. A small <code>k</code> (like 1) can be very sensitive to noise and outliers, leading to high variance. A very large <code>k</code> can be too smooth and miss local patterns, leading to high bias. The optimal <code>k</code> is typically found by testing many different values using cross-validation and choosing the one that gives the best performance on unseen data. A common starting point is to test odd numbers between 3 and 15.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, prepare the data for k-NN:
                <ul class="prose-list"></ul>
                    <li>Select only the numeric predictors (e.g., all columns except <code>vs</code> and <code>am</code>).</li>
                    <li>Scale the selected predictors using the <code>scale()</code> function.</li>
                    <li>Create a vector of the class labels you want to predict: <code>labels <- as.factor(mtcars$am)</code>.</li>
                </ul>
            </li>
            <li>For this exercise, we will test on the training data itself. Run the <code>knn()</code> function with the scaled data as both the <code>train</code> and <code>test</code> arguments, <code>cl = labels</code>, and <code>k = 3</code>.</li>
            <li>Create a confusion matrix to see how well the model classified the cars.</li>
        </ul>
    </div>
</div>
`,
          p4s2ss7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst builds a model to detect a rare disease. The model achieves 99% accuracy. This sounds fantastic, but the disease is only present in 1% of the population. A useless model that simply *always* predicts "no disease" would also have 99% accuracy. This illustrates why accuracy alone is a terrible metric for imbalanced classes and why a more nuanced set of evaluation tools is required.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s2ss7-matrix">The Confusion Matrix</button>
        <button class="tab-button" data-tab="p4s2ss7-metrics">Key Metrics</button>
        <button class="tab-button" data-tab="p4s2ss7-viz">The ROC Curve</button>
        <button class="tab-button" data-tab="p4s2ss7-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s2ss7-matrix" class="tab-pane active">
        <h4 class="subsection-title">The Foundation of Evaluation</h4>
        <p>The **Confusion Matrix** is a table that breaks down the performance of a classification model. It compares the model's predictions to the actual, true classes.</p>
        <div class="plot-container">
            <div id="p4s2ss7-plot1" class="plotly-chart"></div>
        </div>
        <ul class="prose-list" style="margin-top:1rem;">
            <li><strong>True Positives (TP):</strong> The model correctly predicted the positive class. (Predicted disease, patient has disease).</li>
            <li><strong>True Negatives (TN):</strong> The model correctly predicted the negative class. (Predicted no disease, patient has no disease).</li>
            <li><strong>False Positives (FP):</strong> The model incorrectly predicted the positive class. (Predicted disease, patient has no disease). Also called a **Type I Error**.</li>
            <li><strong>False Negatives (FN):</strong> The model incorrectly predicted the negative class. (Predicted no disease, patient has disease). Also called a **Type II Error**.</li>
        </ul>
    </div>
    
    <div id="p4s2ss7-metrics" class="tab-pane">
        <h4 class="subsection-title">Beyond Accuracy</h4>
        <p>All the key classification metrics are derived from the four quadrants of the confusion matrix.</p>
        <ul class="prose-list">
            <li><strong>Accuracy:</strong> $(TP + TN) / \\text{Total}$. The overall percentage of correct predictions. Can be very misleading on imbalanced datasets.</li>
            <li><strong>Precision (Positive Predictive Value):</strong> $TP / (TP + FP)$. Of all the times the model predicted positive, what proportion was correct? It measures the cost of false positives.</li>
            <li><strong>Recall (Sensitivity or True Positive Rate):</strong> $TP / (TP + FN)$. Of all the actual positive cases, what proportion did the model correctly identify? It measures the cost of false negatives.</li>
            <li><strong>F1-Score:</strong> The harmonic mean of Precision and Recall. It provides a single score that balances both metrics.</li>
        </ul>
    </div>
    
    <div id="p4s2ss7-viz" class="tab-pane">
        <h4 class="subsection-title">The ROC Curve</h4>
        <p>An **ROC (Receiver Operating Characteristic) Curve** is a plot that shows the performance of a classification model at all possible classification thresholds. It plots the True Positive Rate (Recall) against the False Positive Rate.</p>
        <p>The **AUC (Area Under the Curve)** is the single number that summarizes the ROC curve. An AUC of 1.0 represents a perfect model. An AUC of 0.5 represents a useless model that is no better than random guessing.</p>
        <div class="plot-container">
            <div id="p4s2ss7-plot2" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The dashed diagonal line represents a random-chance model (AUC = 0.5). A good model (the blue curve) has an ROC curve that bows up towards the top-left corner, indicating a high True Positive Rate and a low False Positive Rate. The area under this blue curve (AUC) will be close to 1.0, indicating excellent discriminatory power.</p>
        </div>
    </div>

    <div id="p4s2ss7-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have a set of model predictions and the true values. We need to generate a comprehensive report of the model's performance.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Prepare the Data.</strong> We create factor variables for our actual and predicted values. It is critical that they have the same levels.
                <div class="code-container">
                    <pre><code class="language-r">actual <- factor(c("Yes", "No", "Yes", "Yes", "No", "No", "Yes"), levels = c("Yes", "No"))
predicted <- factor(c("Yes", "Yes", "Yes", "No", "No", "No", "Yes"), levels = c("Yes", "No"))</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Generate the Confusion Matrix.</strong> We use the <code>caret::confusionMatrix()</code> function, making sure to specify which class is "positive".
                <div class="code-container">
                    <pre><code class="language-r">library(caret)
conf_matrix <- confusionMatrix(data = predicted, reference = actual, positive = "Yes")</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Interpret the Output.</strong> We print the full results object.
                <div class="code-container">
                    <pre><code class="language-r">print(conf_matrix)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong>
                <div class="plot-description">
                    <p><strong>Conclusion:</strong> "The model achieved an overall accuracy of 71.4%. For the 'Yes' class, the model had a Precision of 0.75 (meaning 75% of the time it predicted 'Yes', it was correct) and a Recall of 0.75 (meaning it successfully found 75% of all the actual 'Yes' cases). The F1-score provides a balanced summary of this performance."</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Precision-Recall Tradeoff</h3>
    <div class="scenario-content">
        <p>Precision and Recall are often in tension. If you want to increase Recall (find every possible case of the disease), you might have to lower your prediction threshold, which will likely lead to more False Positives and lower Precision. If you want to increase Precision (be very sure every positive prediction is correct), you might raise your threshold, which will lead to more False Negatives and lower Recall. The choice of which metric to optimize for is a business decision based on the costs of the two types of errors.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Consider a spam filter with the following results on 100 emails:</p>
        <ul class="prose-list">
            <li>True Positives (Spam correctly flagged): 8</li>
            <li>False Positives (Not-Spam flagged as Spam): 2</li>
            <li>True Negatives (Not-Spam correctly ignored): 88</li>
            <li>False Negatives (Spam missed): 2</li>
        </ul>
        <li>Calculate the Accuracy, Precision, and Recall for this model.</li>
        <li>Is accuracy a good metric here? Why or why not?</li>
        <li>Which error (False Positive or False Negative) do you think is more costly for a spam filter user? Which metric (Precision or Recall) does that correspond to?</li>
    </div>
</div>
`,
          p4s2ss8: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is building a model to predict credit card fraud, a problem with severe **class imbalance** (only 0.1% of transactions are fraudulent). They train a standard model and find it achieves 99.9% accuracy by simply predicting "not fraud" for every transaction. They need to apply best practices specifically designed for imbalanced classification problems to build a useful model.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Robust Classification</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Classification Best Practices</div>
            <div class="scenario">Moving from a basic model to a production-ready, trustworthy classification system.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-balance-scale-left"></i> 1. Address Class Imbalance</div>
                        <div class="scenario"><strong>Problem:</strong> Standard models are biased towards the majority class. <br><strong>Solution:</strong> Use sampling techniques. **Over-sampling** (e.g., SMOTE) creates synthetic minority class examples. **Under-sampling** randomly removes majority class examples.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-tachometer-alt"></i> 2. Choose the Right Metric</div>
                        <div class="scenario"><strong>Problem:</strong> Accuracy is misleading for imbalanced problems. <br><strong>Solution:</strong> Focus on metrics that are robust to imbalance, such as the **F1-Score**, **Precision-Recall AUC (PR-AUC)**, or Cohen's Kappa.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-sliders-h"></i> 3. Calibrate Probabilities</div>
                        <div class="scenario"><strong>Problem:</strong> Many models (like SVMs or Naive Bayes) produce predicted "probabilities" that aren't well-calibrated. A prediction of 0.8 might not actually correspond to an 80% chance. <br><strong>Solution:</strong> Use techniques like Platt Scaling or Isotonic Regression after training to calibrate the probabilities to better reflect the true likelihoods.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use the <code>themis</code> Package for Sampling</h3>
    <div class="scenario-content">
        <p>Handling class imbalance is a key part of the <code>tidymodels</code> ecosystem. The <code>themis</code> package provides a series of <code>step_*()</code> functions that can be added directly to a <code>recipes</code> workflow to perform sampling. For example, <code>step_smote(outcome_variable)</code> will perform SMOTE over-sampling as part of your reproducible preprocessing pipeline. This is the modern, preferred way to handle imbalance in R.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are building a model to predict which customers will respond to a very expensive, exclusive marketing offer. You expect only 2% of customers to respond. This is an imbalanced problem.</li>
            <li>Which is the more costly error: a False Positive (contacting a non-responder) or a False Negative (missing a potential responder)?</li>
            <li>Based on your answer, should you optimize your model for higher Precision or higher Recall? Why?</li>
            <li>Which evaluation metric (ROC AUC or PR AUC) would be more appropriate for this problem?</li>
        </ul>
    </div>
</div>
`,
          p4s3ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A marketing manager in Karachi wants to segment their customer base to create targeted campaigns. They have data on customer spending habits and loyalty, but no pre-existing labels like "high-value" or "at-risk." 🇵🇰 They need an algorithm that can automatically discover these natural groupings in the data. This is a classic problem for <strong>k-Means Clustering</strong>.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s3ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s3ss1-algorithm">The Algorithm Step-by-Step</button>
        <button class="tab-button" data-tab="p4s3ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s3ss1-usecase">Data Scientist Example</button>
    </div>

    <div id="p4s3ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Finding Centers of Gravity in Data</h4>
        <p><strong>k-Means Clustering</strong> is a partitioning algorithm that groups a dataset into a pre-specified number (<code>k</code>) of distinct, non-overlapping clusters. It aims to create clusters where the data points within a cluster are as similar as possible, and data points in different clusters are as dissimilar as possible.</p>
        <p><strong>Analogy:</strong> Imagine you have to place <code>k</code> new service centers in a city to best serve all the households. The k-Means algorithm is an iterative process to find the optimal locations (the **centroids**) for these service centers. It repeats two steps until the locations are stable: 1) Assign each household to its closest service center. 2) Move each service center to the average location of all the households it currently serves.</p>
    </div>

    <div id="p4s3ss1-algorithm" class="tab-pane">
        <h4 class="subsection-title">How k-Means Works</h4>
        <ol class="prose-list">
            <li><strong>Initialization:</strong> Randomly select <code>k</code> data points from your dataset to be the initial cluster centroids.</li>
            <li><strong>Assignment Step:</strong> For each data point, calculate its Euclidean distance to every centroid. Assign the data point to the cluster of the nearest centroid.</li>
            <li><strong>Update Step:</strong> For each cluster, recalculate its centroid by taking the mean of all the data points assigned to it.</li>
            <li><strong>Repeat:</strong> Repeat the Assignment and Update steps until the cluster assignments no longer change.</li>
        </ol>
    </div>
    
    <div id="p4s3ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Clusters</h4>
        <p>A scatter plot is the perfect way to visualize the output of a k-Means algorithm in two dimensions. It shows the final cluster assignments and the calculated centroids.</p>
        <div class="plot-container">
            <div id="p4s3ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The data points are colored according to their final cluster assignment. The large black diamonds represent the final calculated **centroids**, or the "center of gravity," for each cluster. The plot clearly shows three distinct groupings based on the two features.</p>
        </div>
    </div>
    
    <div id="p4s3ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> A botanist wants to see if the iris flowers naturally cluster into groups based on their petal dimensions, without using the known species labels.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Prepare the Data.</strong> k-Means is a distance-based algorithm, so we must select only the numeric columns and scale them.
                <div class="code-container">
                    <pre><code class="language-r">iris_numeric <- iris[, c("Petal.Length", "Petal.Width")]
iris_scaled <- as.data.frame(scale(iris_numeric))</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the k-Means Model.</strong> We specify the data and the desired number of clusters (<code>centers = 3</code>). We also set <code>nstart</code> to run the algorithm multiple times from different random starting points and choose the best result.
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42) # For reproducibility
kmeans_result <- kmeans(iris_scaled, centers = 3, nstart = 25)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Results.</strong> The model object contains the cluster assignments for each data point and the final centroid locations.
                <div class="code-container">
                    <pre><code class="language-r"># Add the cluster assignments back to the original data for analysis
iris_scaled$cluster <- as.factor(kmeans_result$cluster)
print(kmeans_result$centers) # View centroid coordinates</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Compare Clusters to True Labels.</strong> We can create a table to see how well the algorithm's discovered clusters match the actual flower species.
                <div class="code-container">
                    <pre><code class="language-r">table(Predicted_Cluster = iris_scaled$cluster, Actual_Species = iris$Species)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The confusion matrix shows that the k-Means algorithm did an excellent job of discovering the underlying species structure in the data, with very few misclassifications. This demonstrates its power as an unsupervised learning tool.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Choosing the Right <code>k</code></h3>
    <div class="scenario-content">
        <p>The biggest challenge with k-Means is choosing the optimal number of clusters, <code>k</code>. The most common method is the **Elbow Method**. You run the algorithm for many different values of <code>k</code> and plot the "within-cluster sum of squares" for each. The plot will look like an arm. The "elbow" of the arm—the point where the rate of decrease sharply shifts—is often a good indicator of the optimal <code>k</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, select only the <code>mpg</code> and <code>wt</code> columns and scale them.</li>
            <li>Run the k-Means algorithm on this scaled data, setting <code>centers = 2</code>.</li>
            <li>Add the cluster assignments as a new column to your scaled data frame.</li>
            <li>Create a scatter plot of <code>wt</code> vs. <code>mpg</code> and color the points by your new <code>cluster</code> column. Do the two clusters seem to represent a meaningful grouping of cars?</li>
        </ul>
    </div>
</div>
`,
          p4s3ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A biologist has genetic data for several animal species. They don't know in advance how many "families" or clusters exist. They want to create a visualization that shows the full hierarchy of relationships, from individual species up to broader animal kingdoms. This requires a clustering method that doesn't just partition the data, but builds a nested hierarchy: **Hierarchical Clustering**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s3ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s3ss2-methods">Linkage Methods</button>
        <button class="tab-button" data-tab="p4s3ss2-viz">The Dendrogram</button>
        <button class="tab-button" data-tab="p4s3ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s3ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Building a Family Tree for Data</h4>
        <p><strong>Hierarchical Clustering</strong> is a method of cluster analysis that seeks to build a hierarchy of clusters. There are two main types:</p>
        <ul class="prose-list">
            <li><strong>Agglomerative (Bottom-Up):</strong> Starts with each data point as its own cluster and iteratively merges the closest pairs of clusters until only one cluster (containing all data) remains. This is the most common type.</li>
            <li><strong>Divisive (Top-Down):</strong> Starts with all data points in one cluster and recursively splits them into smaller clusters.</li>
        </ul>
        <p><strong>Analogy:</strong> Agglomerative hierarchical clustering is like building a **family tree in reverse**. You start with all the individuals (the data points). You find the two most similar individuals (siblings) and join them. Then you find the next most similar pair (who might be cousins to the first pair) and join them. You continue this process, joining families into larger clans, until you have traced back to a single common ancestor (the root of the tree).</p>
    </div>

    <div id="p4s3ss2-methods" class="tab-pane">
        <h4 class="subsection-title">How Do We Measure Cluster Distance?</h4>
        <p>The key to hierarchical clustering is deciding how to measure the distance between two *clusters* (not just two points). This is determined by the **linkage method**.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Complete Linkage</strong>
                <p>Defines the distance between two clusters as the distance between the two **farthest** points in the clusters. It tends to produce compact, spherical clusters.</p>
            </div>
            <div class="decision-branch">
                <strong>Single Linkage</strong>
                <p>Defines the distance as the distance between the two **closest** points in the clusters. It can produce long, stringy clusters and is sensitive to noise.</p>
            </div>
             <div class="decision-branch">
                <strong>Average Linkage</strong>
                <p>Defines the distance as the average distance between every pair of points in the two clusters. A good compromise between complete and single linkage.</p>
            </div>
            <div class="decision-branch">
                <strong>Ward's Method</strong>
                <p>A popular method that merges the pair of clusters that results in the minimum increase in the total within-cluster variance. It aims to produce compact, well-separated clusters.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s3ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Hierarchy</h4>
        <p>The result of a hierarchical clustering is always visualized as a **dendrogram**, which is a tree diagram that shows the arrangement of the clusters produced.</p>
        <div class="plot-container">
            <div id="p4s3ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> You read a dendrogram from the bottom up. The individual data points are at the bottom. The vertical lines show the distance at which clusters were merged. The longer the vertical line, the more dissimilar the clusters were when they were merged. You can "cut" the tree at a certain height to get a specific number of clusters. For example, cutting the tree at a height of 4 would result in three distinct clusters.</p>
        </div>
    </div>
    
    <div id="p4s3ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to explore the relationships between different US states based on their crime rates, using the built-in <code>USArrests</code> dataset.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Prepare and Scale the Data.</strong> As this is a distance-based method, scaling is essential.
                <div class="code-container">
                    <pre><code class="language-r">arrests_scaled <- scale(USArrests)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Calculate the Distance Matrix.</strong> First, we must calculate the pairwise distances between all data points. The <code>dist()</code> function does this.
                <div class="code-container">
                    <pre><code class="language-r">dist_matrix <- dist(arrests_scaled, method = "euclidean")</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Perform Hierarchical Clustering.</strong> We use the <code>hclust()</code> function, providing the distance matrix and a chosen linkage method.
                <div class="code-container">
                    <pre><code class="language-r">hc_result <- hclust(dist_matrix, method = "complete")</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Plot the Dendrogram and Interpret.</strong>
                <div class="code-container">
                    <pre><code class="language-r">plot(hc_result, cex = 0.6, hang = -1)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The dendrogram visually groups the states. You can see clusters of states with similar crime profiles. By "cutting" the tree, we can extract a specific number of clusters for further analysis. For example, a horizontal cut at a height of 4 on the y-axis would give us three distinct clusters of states.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Cutting the Tree with <code>cutree()</code></h3>
    <div class="scenario-content">
        <p>After you've created the dendrogram, you often want to get the actual cluster assignments for each data point. The <code>cutree()</code> function does exactly this. You provide it with your hierarchical clustering result object (from <code>hclust()</code>) and either the desired number of clusters (<code>k</code>) or the height at which to cut the tree (<code>h</code>). It will return a vector of cluster labels.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, select only the numeric columns and scale them.</li>
            <li>Calculate the distance matrix using <code>dist()</code>.</li>
            <li>Perform hierarchical clustering using <code>hclust()</code> with Ward's linkage method (<code>method = "ward.D2"</code>).</li>
            <li>Plot the resulting dendrogram. Based on the visual, how many natural clusters of cars do you think there are?</li>
            <li>Use <code>cutree()</code> with your chosen number of clusters to get the cluster labels for each car.</li>
        </ul>
    </div>
</div>
`,
          p4s3ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An astronomer is analyzing a map of the night sky, with data points representing galaxies. The galaxies are not in simple, spherical blobs; they form long, stringy filaments and irregularly shaped superclusters. k-Means would fail to identify these structures. They need a clustering algorithm that can find arbitrarily shaped clusters and also identify isolated stars as noise. This is the perfect use case for **DBSCAN**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s3ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s3ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s3ss3-usecase">Implementation in R</button>
    </div>

    <div id="p4s3ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Clustering by Neighborhood Density</h4>
        <p><strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong> is a clustering algorithm that defines clusters as continuous regions of high density, separated by regions of low density.</p>
        <p><strong>Analogy:</strong> DBSCAN is like **finding islands in an archipelago**. It doesn't assume the islands are round. It starts at a random beach (a data point) and explores. If there are enough other beaches within a short swimming distance, it considers them all part of the same island. It keeps expanding until it reaches a wide expanse of open ocean (a low-density region). Any small, isolated rocks in the middle of the ocean are classified as noise.</p>
        <p>It works based on two key parameters:</p>
        <ul class="prose-list">
            <li><strong><code>eps</code> (epsilon):</strong> The maximum distance between two points for them to be considered neighbors (the "swimming distance").</li>
            <li><strong><code>minPts</code>:</strong> The minimum number of points required to form a dense region (a "core point").</li>
        </ul>
    </div>
    
    <div id="p4s3ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Finding Arbitrarily Shaped Clusters</h4>
        <p>This plot shows a dataset that would be impossible for k-Means to cluster correctly. DBSCAN, however, can easily identify the two "moon" shapes and classify the central points as noise.</p>
        <div class="plot-container">
            <div id="p4s3ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The algorithm successfully identifies the two non-convex clusters (blue and red). The grey points in the middle are classified as **noise** because they do not have enough neighbors within the <code>eps</code> radius to be considered part of a dense region. This ability to find arbitrary shapes and handle noise is DBSCAN's primary advantage.</p>
        </div>
    </div>

    <div id="p4s3ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to find clusters in the <code>faithful</code> dataset (geyser eruptions), which has two natural, non-spherical clusters. We need to find the right <code>eps</code> value to correctly separate them.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Scale Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("dbscan")
library(dbscan)
faithful_scaled <- as.data.frame(scale(faithful))</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Find a good <code>eps</code> value.</strong> A common heuristic is to use a "k-distance plot". We calculate the distance of every point to its k-th nearest neighbor (where k = <code>minPts</code>) and plot these distances in sorted order. The "elbow" of this plot is a good estimate for <code>eps</code>.
                <div class="code-container">
                    <pre><code class="language-r"># Let's use minPts = 5
kNNdistplot(faithful_scaled, k = 5)
abline(h = 0.5, col = "red", lty = 2) # The "elbow" looks to be around 0.5</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Run DBSCAN.</strong> We use the <code>eps</code> value we found.
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42)
db_result <- dbscan(faithful_scaled, eps = 0.5, minPts = 5)
print(db_result)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Visualize the Results.</strong> We plot the data and color the points by the cluster assignments found by DBSCAN.
                <div class="code-container">
                    <pre><code class="language-r"># Cluster 0 represents noise points
faithful_scaled$cluster <- as.factor(db_result$cluster)
library(ggplot2)
ggplot(faithful_scaled, aes(x = eruptions, y = waiting, color = cluster)) +
  geom_point()</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The final plot shows that DBSCAN successfully identified the two main elliptical clusters in the <code>faithful</code> dataset and correctly classified a few points between them as noise, a task that k-Means might struggle with.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>optics()</code> function</h3>
    <div class="scenario-content">
        <p>Choosing the right <code>eps</code> value can be tricky. The <code>dbscan</code> package provides an alternative algorithm called OPTICS, which is a generalization of DBSCAN that doesn't require a fixed <code>eps</code> value. It produces a "reachability plot" that can be used to extract clusters corresponding to different density levels, making it more robust to clusters of varying densities.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the scaled <code>mpg</code> and <code>wt</code> columns from the <code>mtcars</code> dataset.</li>
            <li>Generate a k-distance plot using <code>kNNdistplot()</code> with <code>k=3</code> to estimate a good <code>eps</code> value.</li>
            <li>Run <code>dbscan()</code> with your chosen <code>eps</code> and <code>minPts=3</code>.</li>
            <li>Add the cluster assignments to your data frame. How many clusters did the algorithm find? How many noise points?</li>
            <li>Create a scatter plot of <code>wt</code> vs. <code>mpg</code> and color the points by the DBSCAN cluster labels.</li>
        </ul>
    </div>
</div>
`,
          p4s3ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst suspects their customer base isn't just one homogenous group, but a mixture of several distinct personas (e.g., "Thrifty Shoppers," "Brand Loyalists," "Impulse Buyers"). They believe each of these personas can be modeled by a separate Gaussian (bell curve) distribution. They need a model that can automatically find these underlying distributions and calculate the probability that any given customer belongs to each one.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s3ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s3ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s3ss4-usecase">Implementation in R</button>
    </div>

    <div id="p4s3ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Clustering by Probability</h4>
        <p><strong>Model-Based Clustering</strong> is an approach that assumes the data is a mixture of two or more underlying probability distributions. The goal of the algorithm is to find the parameters of these distributions that best fit the data.</p>
        <p>The most common form is a **Gaussian Mixture Model (GMM)**, which assumes the data is a mixture of several Gaussian (Normal) distributions, each with its own mean and covariance matrix.</p>
        <p><strong>Analogy:</strong> A GMM is like a **sound engineer trying to isolate individual instruments from a recording of a full orchestra**. The overall sound (the data) is a complex mixture. The GMM algorithm tries to identify the distinct sound profiles of the violins, the cellos, and the trumpets (the individual Gaussian components) and determine how much each instrument contributed to the overall sound at any given moment.</p>
    </div>
    
    <div id="p4s3ss4-viz" class="tab-pane">
        <h4 class="subsection-title">Finding the Underlying Distributions</h4>
        <p>This plot shows a dataset that is a mixture of three different Gaussian distributions. The GMM algorithm successfully finds the parameters (mean, variance, and orientation) of each underlying cluster.</p>
        <div class="plot-container">
            <div id="p4s3ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The points are colored by their most likely cluster assignment. The ellipses represent the Gaussian distributions learned by the GMM. Notice how the ellipses have different sizes, shapes, and orientations, allowing the model to fit clusters that are not necessarily spherical, a key advantage over standard k-Means.</p>
        </div>
    </div>

    <div id="p4s3ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to use a GMM to cluster the <code>faithful</code> geyser dataset, which we know contains two elliptical clusters. We also want to get the "soft" probabilistic assignments for each data point.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Scale Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("mclust")
library(mclust)
faithful_scaled <- as.data.frame(scale(faithful))</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the GMM.</strong> The <code>Mclust()</code> function is the main tool. By default, it will test different numbers of clusters and different covariance structures and choose the best one based on the Bayesian Information Criterion (BIC).
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42)
gmm_result <- Mclust(faithful_scaled)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Best Model.</strong> The summary tells us the best model it found.
                <div class="code-container">
                    <pre><code class="language-r">summary(gmm_result)
# The output will indicate the best model (e.g., VEV, 2) which means
# ellipsoidal clusters with varying volume, shape, and orientation, and 2 components.</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Analyze the Probabilities.</strong> Unlike k-Means, a GMM gives us the probability of each point belonging to each cluster. This is stored in the <code>z</code> component.
                <div class="code-container">
                    <pre><code class="language-r"># Probabilities for the first 5 data points
head(gmm_result$z)
# The 'classification' component gives the hard assignment (the most likely cluster)
head(gmm_result$classification)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Soft vs. Hard Clustering</h3>
    <div class="scenario-content">
        <p>k-Means performs **hard clustering**: every data point belongs to exactly one cluster. GMMs perform **soft clustering**: they provide a probability distribution over all clusters for each data point. This can be much more informative. A point that has a 51% probability of being in Cluster A and a 49% probability of being in Cluster B is very different from a point that has a 99% probability of being in Cluster A.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the numeric, scaled columns of the <code>iris</code> dataset.</li>
            <li>Fit a GMM using <code>Mclust()</code>. You can specify the number of clusters you're looking for with the <code>G</code> argument: <code>Mclust(iris_scaled, G = 3)</code>.</li>
            <li>Extract the cluster assignments: <code>gmm_result$classification</code>.</li>
            <li>Create a confusion matrix comparing the GMM's clusters to the true <code>iris$Species</code>. How well did the model do?</li>
            <li>Plot the results using <code>plot(gmm_result, what = "classification")</code>.</li>
        </ul>
    </div>
</div>
`,
          p4s3ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has run a k-Means algorithm on their customer data with k=3, k=4, and k=5. How do they objectively determine which number of clusters is the "best"? They need a quantitative method to evaluate the quality of the clusters produced by each run, a process known as **Cluster Validation**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s3ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s3ss5-viz">The Elbow Method</button>
        <button class="tab-button" data-tab="p4s3ss5-usecase">The Silhouette Method</button>
    </div>

    <div id="p4s3ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">How Good Are My Clusters?</h4>
        <p>Cluster validation is the process of evaluating the quality of the clusters produced by an algorithm. The goal is to find clusters that have high **intra-cluster similarity** (points within a cluster are close together) and low **inter-cluster similarity** (different clusters are far apart).</p>
        <p><strong>Analogy:</strong> Cluster validation is like grading a **student's attempt to sort a mixed box of fruit**. A good sorting (good clustering) results in tight, compact piles where all the apples are together and far away from the pile of oranges. A bad sorting would have piles with a mix of different fruits, or where the apple pile and orange pile are right on top of each other.</p>
    </div>
    
    <div id="p4s3ss5-viz" class="tab-pane">
        <h4 class="subsection-title">Finding the Optimal <code>k</code> with the Elbow Method</h4>
        <p>The **Elbow Method** is a popular heuristic for finding the optimal number of clusters in k-Means. You plot the within-cluster sum of squares (WSS) as a function of <code>k</code>. The "elbow" of the resulting curve represents the point of diminishing returns, where adding another cluster doesn't significantly reduce the WSS.</p>
        <div class="plot-container">
            <div id="p4s3ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The plot shows a clear "elbow" at <code>k=3</code>. The drop in WSS from k=1 to k=2, and from k=2 to k=3, is very large. However, the drop from k=3 to k=4 is much smaller. This suggests that adding a fourth cluster doesn't provide much new information, and that 3 is likely the optimal number of clusters for this dataset.</p>
        </div>
    </div>

    <div id="p4s3ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">The Silhouette Method</h4>
        <p>The **Silhouette Score** is another excellent metric. It measures how similar a data point is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("factoextra")
library(factoextra)
library(ggplot2)

# Use the scaled numeric iris data
iris_scaled <- scale(iris[, 1:4])

# --- 1. The Elbow Method ---
# fviz_nbclust automatically creates the plot for us!
fviz_nbclust(iris_scaled, kmeans, method = "wss") +
  geom_vline(xintercept = 3, linetype = 2) # Add a line at the elbow

# --- 2. The Silhouette Method ---
fviz_nbclust(iris_scaled, kmeans, method = "silhouette")

# --- 3. The Gap Statistic Method (often the best) ---
# This method compares the WSS of our data to a null reference distribution
set.seed(42)
fviz_nbclust(iris_scaled, kmeans, method = "gap_stat", nboot = 50)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: fviz_cluster for Visualization</h3>
    <div class="scenario-content">
        <p>The <code>factoextra</code> package not only helps you find the optimal k, but it also has a fantastic function for visualizing your final clusters: <code>fviz_cluster()</code>. You give it your k-Means result object and your data, and it automatically performs PCA to reduce the data to two dimensions and creates a beautiful, publication-quality plot of the clusters.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the scaled <code>mpg</code> and <code>wt</code> columns from the <code>mtcars</code> dataset.</li>
            <li>Use the <code>fviz_nbclust()</code> function with <code>method = "wss"</code> to generate an elbow plot. Based on the plot, what appears to be the optimal number of clusters (2, 3, or 4)?</li>
            <li>Now use <code>fviz_nbclust()</code> with <code>method = "silhouette"</code> to generate a silhouette score plot. Does it suggest the same number of clusters?</li>
        </ul>
    </div>
</div>
`,
          p4s3ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst runs a k-Means algorithm on an unscaled customer dataset containing <code>age</code> and <code>income</code>. The algorithm produces bizarre clusters that are just horizontal bands based on income, completely ignoring the age variable. This is because the massive scale of the income variable made the age variable mathematically irrelevant. This highlights the most critical best practice for clustering.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Robust Clustering</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Clustering Best Practices</div>
            <div class="scenario">Following these steps ensures that your clustering results are meaningful, valid, and interpretable.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-balance-scale"></i> 1. Scale Your Features</div>
                        <div class="scenario"><strong>The Rule:</strong> For distance-based algorithms like k-Means and Hierarchical Clustering, you **must** scale your numeric features first (e.g., using standardization). This ensures all variables contribute equally to the distance calculations.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-bullseye"></i> 2. Determine Optimal <code>k</code></div>
                        <div class="scenario"><strong>The Rule:</strong> Don't just guess the number of clusters. Use quantitative methods like the Elbow, Silhouette, or Gap Statistic methods to find a statistically justified value for <code>k</code>.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-tie"></i> 3. Interpret the Clusters</div>
                        <div class="scenario"><strong>The Rule:</strong> A clustering result is useless without interpretation. After finding your clusters, you must analyze them to understand what they represent. Calculate summary statistics for each cluster and give them meaningful "persona" names (e.g., "High-Spending, Low-Frequency Shoppers").</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-random"></i> 4. Check for Stability</div>
                        <div class="scenario"><strong>The Rule:</strong> A good clustering result should be stable. Run your algorithm multiple times with different random seeds (<code>nstart</code> in <code>kmeans()</code>). If you get drastically different clusters each time, your groupings may not be very meaningful.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Interpreting Clusters with <code>group_by()</code></h3>
    <div class="scenario-content">
        <p>The best way to interpret your clusters is to add the cluster assignments back to your *unscaled*, original data frame. Then, you can use <code>dplyr::group_by(cluster)</code> and <code>summarise()</code> to calculate the mean (or median) of each original feature for each cluster. This will give you a clear profile of the "average" member of each discovered group, making it easy to create personas.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's practice the interpretation step.</p>
        <ul class="prose-list">
            <li>Run k-Means on the scaled, numeric columns of the <code>iris</code> dataset with <code>k=3</code>.</li>
            <li>Add the resulting cluster assignments back to the original, *unscaled* <code>iris</code> dataset as a new column.</li>
            <li>Use <code>group_by()</code> and <code>summarise()</code> to calculate the mean of all four numeric variables for each of the three clusters.</li>
            <li>Based on the summary statistics, create a "persona" for each cluster. For example, which cluster represents flowers with small petals? Which one represents flowers with large sepals?</li>
        </ul>
    </div>
</div>
`,
          p4s4ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A grocery store manager in Karachi wants to find out which items are frequently bought together to optimize store layout and promotions. 🇵🇰 They have a massive dataset of customer transactions. Manually searching for patterns is impossible. They need an efficient algorithm to mine this data for "if-then" rules, like "If a customer buys bread, they are also likely to buy butter."</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s4ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s4ss1-viz">The Apriori Principle</button>
        <button class="tab-button" data-tab="p4s4ss1-usecase">Implementation in R</button>
    </div>

    <div id="p4s4ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Discovering Frequent Itemsets</h4>
        <p><strong>Association Rule Mining</strong> is a technique to discover interesting relationships hidden in large datasets. The uncovered relationships are expressed as **association rules**.</p>
        <p>The goal of algorithms like <strong>Apriori</strong> and <strong>Eclat</strong> is to efficiently find the **frequent itemsets**—the groups of items that appear together in transactions more often than a given threshold.</p>
        <p><strong>Analogy:</strong> This is the classic **"market basket analysis"** problem. You are analyzing thousands of shopping carts to find patterns. The algorithms are like a super-fast assistant who can scan every single cart and tell you, "The combination of {Diapers, Beer} appeared in 8% of all shopping carts this month."</p>
    </div>
    
    <div id="p4s4ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Pruning the Search Space</h4>
        <p>Searching for every possible combination of items would be computationally impossible. The Apriori algorithm uses a clever trick called the **Apriori Principle** to make this feasible.</p>
        <div class="mindmap-container">
            <div class="mindmap-root">
                <div class="title"><i class="fas fa-cut"></i> The Apriori Principle</div>
                <div class="scenario">If an itemset is frequent, then all of its subsets must also be frequent. Conversely, if a subset is infrequent, then all of its supersets must also be infrequent.</div>
            </div>
            <div class="mindmap-branches-container">
                <div class="mindmap-branches">
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-search"></i> Step 1</div>
                            <div class="scenario">First, find all the individual items that are frequent on their own (e.g., Bread, Milk). Discard the infrequent ones (e.g., Caviar).</div>
                        </div>
                    </div>
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-cogs"></i> Step 2</div>
                            <div class="scenario">Now, only create pairs from the items you found in Step 1. Because of the principle, you know that any pair containing "Caviar" would be infrequent, so you don't even need to waste time counting them.</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="plot-description">
            <p>This "pruning" of the search space is what allows the algorithm to run efficiently on massive datasets.</p>
        </div>
    </div>
    
    <div id="p4s4ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Step-by-Step with <code>arules</code></h4>
        <p>The <code>arules</code> package is the standard for association rule mining in R.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("arules")
library(arules)

# The package comes with a built-in Groceries dataset.
# It's in a special "transactions" format.
data("Groceries")
summary(Groceries)

# 1. Mine for frequent itemsets using the Apriori algorithm
# We set a minimum support threshold (e.g., must appear in at least 0.75% of transactions)
# and a minimum confidence threshold for the rules.
rules <- apriori(Groceries, 
                 parameter = list(supp = 0.0075, conf = 0.25, minlen = 2))

# 2. Inspect the top rules
# We sort the rules by "lift" (a measure of how interesting they are)
inspect(sort(rules, by = "lift")[1:10])
</code></pre>
        </div>
        <div class="plot-description">
            <p><strong>Interpretation:</strong> The output of <code>inspect()</code> will show a list of rules. A rule like <code>{herbs} => {root vegetables}</code> with a high lift value suggests that customers who buy herbs are significantly more likely to also buy root vegetables than the average customer.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Eclat for Speed</h3>
    <div class="scenario-content">
        <p>The <strong>Eclat</strong> algorithm is a more modern and often faster alternative to Apriori for the first step of the process: finding frequent itemsets. It uses a different counting method that can be more efficient. In the <code>arules</code> package, you can simply change the algorithm name in the parameters: <code>apriori(data, parameter = list(target = "frequent itemsets"), control = list(alg = "eclat"))</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>Groceries</code> dataset and the <code>apriori()</code> function from the example.</li>
            <li>Change the parameters to find rules with a higher minimum support (<code>supp = 0.02</code>). How does this affect the number of rules found?</li>
            <li>Use the <code>subset()</code> function to find only the rules that have "whole milk" on the right-hand side (the consequence). (Hint: <code>subset(rules, rhs %in% "whole milk")</code>). Inspect these rules. What items are most frequently bought along with whole milk?</li>
        </ul>
    </div>
</div>
`,
          p4s4ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An Apriori algorithm generates two rules: 1) If a customer buys a hot dog, they will also buy a hot dog bun. 2) If a customer buys milk, they will also buy bread. The first rule has extremely high confidence, but it's obvious and not very interesting. The second rule has lower confidence but might be more commercially valuable. The analyst needs a set of metrics to distinguish between obvious rules and genuinely insightful ones.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s4ss2-concepts">The Three Core Metrics</button>
        <button class="tab-button" data-tab="p4s4ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s4ss2-usecase">Interpreting in R</button>
    </div>

    <div id="p4s4ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Measuring Rule Strength and Importance</h4>
        <p>For a rule of the form $\{A\} \rightarrow \{B\}$:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Support</strong>
                <p><strong>The Question:</strong> "How popular is this itemset?"
                <br>It's the proportion of all transactions that contain both A and B. A low support means the rule is based on a rare combination, which might not be commercially significant.
                <div class="math-foundation" style="margin-top:0.5rem;">$$ \\text{Support}(A \\rightarrow B) = \\frac{\\text{Transactions containing both A and B}}{\\text{Total Transactions}} $$</div>
                </p>
            </div>
            <div class="decision-branch">
                <strong>Confidence</strong>
                <p><strong>The Question:</strong> "When A is bought, how often is B also bought?"
                <br>It's the probability of seeing B in a transaction, given that it also contains A. High confidence suggests a strong co-occurrence.
                <div class="math-foundation" style="margin-top:0.5rem;">$$ \\text{Confidence}(A \\rightarrow B) = \\frac{\\text{Support}(A \\rightarrow B)}{\\text{Support}(A)} $$</div>
                </p>
            </div>
            <div class="decision-branch">
                <strong>Lift</strong>
                <p><strong>The Question:</strong> "How much more likely are people to buy B if they've bought A?"
                <br>It compares the confidence of the rule to the base popularity of B. It's the most important metric for finding *interesting* rules.
                <div class="math-foundation" style="margin-top:0.5rem;">$$ \\text{Lift}(A \\rightarrow B) = \\frac{\\text{Confidence}(A \\rightarrow B)}{\\text{Support}(B)} $$</div>
                </p>
            </div>
        </div>
    </div>
    
    <div id="p4s4ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Rules with a Scatter Plot</h4>
        <p>A common way to visualize association rules is to plot them on a scatter plot with Support on the x-axis, Confidence on the y-axis, and Lift represented by the color of the points.</p>
        <div class="plot-container">
            <div id="p4s4ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> Each point represents a single rule. The most interesting rules are typically found in the **top-right quadrant**: they have high support (they happen often), high confidence (they are reliable), and high lift (the relationship is strong and not just due to the items being popular on their own). The interactive tooltips allow you to hover over a point to see the specific rule it represents.</p>
        </div>
    </div>

    <div id="p4s4ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Interpretation</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have a rule <code>{yogurt} => {whole milk}</code> and we need to interpret its metrics to decide if it's a valuable insight for the store manager.</p>
        </div>
        <div class="code-container">
            <pre><code class="language-text">
# A sample output from the inspect() function in arules
#      lhs          rhs            support    confidence lift     count
# [1] {yogurt}  => {whole milk}   0.011      0.401      1.57     110
            </code></pre>
        </div>
        <ol class="prose-list">
            <li><strong>Interpret Support:</strong> "The combination of yogurt and whole milk appears together in 1.1% of all transactions. This is a reasonably frequent combination."</li>
            <li><strong>Interpret Confidence:</strong> "Of all the times a customer bought yogurt, they also bought whole milk 40.1% of the time. This is a fairly reliable rule."</li>
            <li><strong>Interpret Lift:</strong> A lift of 1.57 means that a customer who buys yogurt is **1.57 times (or 57%) more likely** to buy whole milk than a random customer off the street. A lift > 1 indicates a positive association.</li>
            <li><strong>Formulate a Conclusion:</strong>
                <div class="plot-description">
                    <p><strong>Conclusion for the manager:</strong> "We've found a strong and interesting association between yogurt and whole milk. Customers who purchase yogurt are 57% more likely to also purchase whole milk. We could consider placing these items closer together in the store or running a joint promotion to further boost sales."</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Lift can be Misleading for Rare Items</h3>
    <div class="scenario-content">
        <p>Be cautious when interpreting rules with very high lift but very low support. For example, the rule <code>{caviar} => {champagne}</code> might have an incredibly high lift because the few people who buy caviar are very likely to also buy champagne. However, because this combination is so rare (very low support), it's probably not a commercially significant rule that warrants a major store reorganization.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Interpret the following rule found in an online streaming service's data:</p>
        <div class="code-container">
            <pre><code class="language-text">
#      lhs                    rhs                  support    confidence lift
# [1] {The Matrix, Inception} => {The Dark Knight}    0.05       0.85       3.4
            </code></pre>
        </div>
        <ul class="prose-list">
            <li>Write a sentence explaining the **Support** of this rule.</li>
            <li>Write a sentence explaining the **Confidence** of this rule.</li>
            <li>Write a sentence explaining the **Lift** of this rule.</li>
            <li>Based on these metrics, would you recommend this as a good rule for a recommendation engine? Why?</li>
        </ul>
    </div>
</div>
`,
          p4s4ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> The manager of a large e-commerce website in Pakistan wants to increase sales by improving product recommendations. 🇵🇰 An analyst has run an association rule mining algorithm on millions of transactions and found strong rules. Now, they must translate these abstract rules into concrete business applications that will drive value.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">From Rules to Revenue</h3>
    <p>Association rule mining is not just an academic exercise; it has a wide range of powerful, real-world applications for businesses.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-store"></i> Business Applications</div>
            <div class="scenario">The core goal is to use the discovered "if-then" rules to make smarter business decisions.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-shopping-basket"></i> Market Basket Analysis</div>
                        <div class="scenario">This is the classic application. Retailers use rules to optimize store layout (placing milk and bread far apart to increase store travel time), design promotions (buy one, get one on related items), and plan catalog designs.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-thumbs-up"></i> Recommendation Engines</div>
                        <div class="scenario">E-commerce sites like Amazon use association rules for their "Frequently Bought Together" and "Customers who bought this also bought..." features. This is a direct application of rules with high lift and confidence.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-mouse-pointer"></i> Web Usage Mining</div>
                        <div class="scenario">By analyzing user clickstreams, companies can discover patterns like "Visitors who read about Product A and then visit the FAQ page are highly likely to then visit the pricing page." This can be used to optimize website design and user flow.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-notes-medical"></i> Medical Diagnosis</div>
                        <div class="scenario">In healthcare, association rules can be used to find relationships between symptoms, patient characteristics, and diseases, helping to identify potential risk factors or predict likely diagnoses.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Visualizing Rules as a Graph</h3>
    <div class="scenario-content">
        <p>When you have many rules, interpreting them from a table can be difficult. The <code>arulesViz</code> package provides a powerful way to visualize your rules as a graph network. In this graph, items are nodes, and a rule is a directed arrow from the antecedent (LHS) to the consequent (RHS). The size and color of the nodes and arrows can be mapped to the support, confidence, or lift, providing a rich, interactive way to explore your rule set.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You are a data scientist for a music streaming service. You've mined user listening data and found the following strong rule:</p>
        <p><code>{Listen to 'Artist A'} => {Listen to 'Artist B'}</code></p>
        <ul class="prose-list">
            <li>What is one concrete, actionable business recommendation you could make based on this rule to improve user engagement?</li>
            <li>What is a second recommendation you could make for the marketing team?</li>
        </ul>
    </div>
</div>
`,
          p4s5ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst trains a single decision tree to predict customer churn. The tree is very deep and complex, achieving 99% accuracy on the training data. However, when tested on new customers, its accuracy plummets to 60%. The model has high variance and has overfit the training data. To create a more robust and stable model, the analyst decides to use **Bagging**, a technique that builds and averages hundreds of trees.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s5ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s5ss1-viz">The Workflow</button>
        <button class="tab-button" data-tab="p4s5ss1-rf">Random Forest: Bagging++</button>
        <button class="tab-button" data-tab="p4s5ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s5ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Wisdom of the Crowd</h4>
        <p><strong>Bagging</strong>, which stands for **B**ootstrap **Aggregat**ing, is an ensemble method designed to reduce the variance of a machine learning model. It works by training many instances of the same base model (like a decision tree) on different random subsets of the training data and then averaging their predictions.</p>
        <p><strong>Analogy:</strong> Bagging is like asking **hundreds of different people to measure the height of a tree and then taking their average measurement**. Any single person might make a small error (high variance), but the average of all their measurements will be extremely close to the true height. Each person gets a slightly different view of the tree (a bootstrap sample of the data), making their individual errors uncorrelated.</p>
    </div>

    <div id="p4s5ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Bagging Process</h4>
        <p>This diagram illustrates the parallel nature of bagging, where multiple models are trained independently before their results are combined.</p>
        <div class="plot-container">
            <div id="p4s5ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The original training data is sampled *with replacement* to create multiple bootstrap samples. A separate, deep decision tree is trained on each sample in parallel. To make a final prediction for a new data point, the point is run through every tree, and the final class is determined by a majority vote of all the individual tree predictions.</p>
        </div>
    </div>
    
    <div id="p4s5ss1-rf" class="tab-pane">
        <h4 class="subsection-title">Random Forest: A Smarter Kind of Bagging</h4>
        <p>The <strong>Random Forest</strong> algorithm is a specific and highly effective implementation of bagging. It adds one extra trick to the process to further improve performance by "decorrelating" the trees.</p>
        <p>In addition to sampling the data for each tree, a random forest also samples the **features**. At each split in a decision tree, the algorithm is only allowed to consider a small, random subset of the total available predictors. This prevents any single, very strong predictor from dominating all the trees in the forest, leading to a more diverse and robust ensemble.</p>
    </div>

    <div id="p4s5ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to build a highly accurate model to classify penguin species from the <code>palmerpenguins</code> dataset. A single decision tree might overfit, so we will use a random forest.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load and Prepare Data.</strong> We load the necessary packages and remove any rows with missing data for this example.
                <div class="code-container">
                    <pre><code class="language-r">library(randomForest)
library(palmerpenguins)
library(tidyr)
penguins_clean <- penguins %>% drop_na()</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the Random Forest Model.</strong> We specify the formula and the data. <code>ntree</code> determines how many trees to grow in our "forest".
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42)
penguin_forest <- randomForest(species ~ ., data = penguins_clean, ntree = 500)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Model Output.</strong> The printed model object contains the out-of-bag (OOB) error estimate, a reliable measure of the model's accuracy on unseen data.
                <div class="code-container">
                    <pre><code class="language-r">print(penguin_forest)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Check Feature Importance.</strong> A random forest can tell us which variables were most important for making accurate predictions.
                <div class="code-container">
                    <pre><code class="language-r">importance(penguin_forest)
varImpPlot(penguin_forest) # A useful plot of the importance scores</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The random forest is a highly accurate classifier, achieving a very low OOB error rate. The feature importance plot reveals that <code>flipper_length_mm</code> and <code>bill_length_mm</code> are the two most powerful predictors for determining a penguin's species. This robust performance is a direct result of bagging many decorrelated trees.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Out-of-Bag (OOB) Error</h3>
    <div class="scenario-content">
        <p>A brilliant feature of bagging and random forests is that they provide a free, built-in estimate of test error without needing a separate validation set. Because each tree is trained on a bootstrap sample, roughly one-third of the original data points are "out-of-bag" (OOB) for that tree. To get the OOB error, you take each data point, find all the trees that did *not* see it during training, and get their majority vote. This provides an unbiased estimate of the model's performance on unseen data.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, convert the <code>am</code> column to a factor.</li>
            <li>Fit a random forest model to predict <code>am</code> using all other variables. Set <code>ntree = 1000</code>.</li>
            <li>Print the model object. What is the Out-of-Bag estimate of the error rate?</li>
            <li>Use the <code>varImpPlot()</code> function to visualize the feature importances. Which two variables are the most important predictors for whether a car is automatic or manual?</li>
        </ul>
    </div>
</div>
`,
          p4s5ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is competing in a Kaggle competition where maximum predictive accuracy is the only goal. They have a good baseline model, but they need to squeeze out every last bit of performance. They decide to use a **Boosting** algorithm, which builds models sequentially, with each new model focusing on correcting the mistakes of the previous one. This is often the winning strategy in machine learning competitions.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s5ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s5ss2-viz">The Workflow</button>
        <button class="tab-button" data-tab="p4s5ss2-methods">Key Algorithms</button>
        <button class="tab-button" data-tab="p4s5ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s5ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Learning from Mistakes</h4>
        <p><strong>Boosting</strong> is an ensemble method that combines a set of weak learners (typically shallow decision trees) into a single strong learner in a sequential, iterative way. Unlike bagging, where trees are built in parallel, boosting builds trees one after another, and each new tree is trained to fix the errors made by the previous ones.</p>
        <p><strong>Analogy:</strong> Boosting is like a **team of students taking an exam one by one**. The first student takes the exam and gets some questions wrong. The teacher gives the second student a copy of the exam but tells them to "pay extra attention" to the questions the first student missed. The third student focuses on the questions the second one missed, and so on. The final team score is a weighted average of all their answers, giving more weight to the students who performed better.</p>
    </div>

    <div id="p4s5ss2-viz" class="tab-pane">
        <h4 class="subsection-title">The Sequential Process of Boosting</h4>
        <p>This diagram illustrates how boosting works. Each sequential model focuses on the data points that the previous model misclassified, giving them higher weight.</p>
        <div class="plot-container">
            <div id="p4s5ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> In **Iteration 1**, the first simple model misclassifies three points (circled). In **Iteration 2**, the algorithm gives these three points a much higher weight (represented by their larger size), forcing the second model to focus on getting them right. In **Iteration 3**, the focus shifts to the new set of misclassified points. The final prediction is a weighted combination of all three models.</p>
        </div>
    </div>
    
    <div id="p4s5ss2-methods" class="tab-pane">
        <h4 class="subsection-title">A Family of Powerful Algorithms</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Gradient Boosting Machine (GBM)</strong>
                <p>The foundational algorithm. It builds new trees that predict the *residuals* (the errors) of the previous ensemble of trees.</p>
            </div>
            <div class="decision-branch">
                <strong>XGBoost (Extreme Gradient Boosting)</strong>
                <p>A highly optimized and regularized implementation of gradient boosting. It is famous for its speed and performance and is often the winning algorithm in competitions.</p>
            </div>
            <div class="decision-branch">
                <strong>LightGBM</strong>
                <p>Another modern implementation of gradient boosting that is even faster than XGBoost, especially on very large datasets. It grows trees "leaf-wise" instead of "depth-wise," which can be more efficient.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s5ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to build a high-performance regression model to predict a car's MPG. We will use the powerful <code>xgboost</code> package.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Prepare Data.</strong> XGBoost requires all data to be in a numeric matrix format. We use <code>model.matrix</code> to automatically handle factor conversions.
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("xgboost")
library(xgboost)
x_vars <- model.matrix(mpg ~ ., data = mtcars)[, -1] # Predictor matrix
y_var <- mtcars$mpg                             # Response vector</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Perform Cross-Validation to Find the Best Number of Trees.</strong> Boosting models can easily overfit. We must use cross-validation to find the optimal number of sequential trees (<code>nrounds</code>).
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42)
cv_model <- xgb.cv(data = x_vars, label = y_var, nrounds = 100,
                   nfold = 5, # 5-fold cross-validation
                   objective = "reg:squarederror", verbose = 0)
best_iteration <- cv_model$best_iteration</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Fit the Final Model.</strong> We train the final model using the optimal number of rounds found by CV.
                <div class="code-container">
                    <pre><code class="language-r">final_xgb_model <- xgboost(data = x_vars, label = y_var, nrounds = best_iteration,
                               objective = "reg:squarederror", verbose = 0)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Check Feature Importance.</strong>
                <div class="code-container">
                    <pre><code class="language-r">xgb.importance(model = final_xgb_model)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> By using cross-validation to prevent overfitting, we have trained a powerful XGBoost model. The feature importance table shows which variables the model relied on most heavily to make its predictions, providing valuable insight into the drivers of fuel efficiency.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>tidymodels</code> Framework</h3>
    <div class="scenario-content">
        <p>While using packages like <code>randomForest</code> and <code>xgboost</code> directly is powerful, the modern <code>tidymodels</code> framework provides a unified, consistent interface for fitting, tuning, and evaluating hundreds of different models, including these ensemble methods. Learning <code>tidymodels</code> is a fantastic investment for any serious R modeler.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>iris</code> dataset, prepare your data for XGBoost. Create a numeric <code>y_var</code> for the species (e.g., as.numeric(iris$Species) - 1, since XGBoost expects classes to start at 0). Create the <code>x_vars</code> matrix as in the example.</li>
            <li>Run <code>xgb.cv</code> to find the best number of rounds for a classification task. You will need to change the <code>objective</code> to <code>"multi:softmax"</code> and also set the <code>num_class</code> parameter to 3.</li>
            <li>Fit the final model using the best number of rounds you found.</li>
            <li>Use the model to make predictions on the data with <code>predict(final_model, x_vars)</code>. How accurate is it?</li>
        </ul>
    </div>
</div>
`,
          p4s5ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has built several different models to predict customer churn: a logistic regression, a random forest, and a gradient boosting machine. Each model has different strengths and makes different types of errors. To achieve the absolute best performance, they decide to combine the predictions of all three models into a single, powerful "super learner" using **Stacking**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s5ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s5ss3-viz">The Stacking Architecture</button>
        <button class="tab-button" data-tab="p4s5ss3-usecase">Step-by-Step with <code>tidymodels</code></button>
    </div>

    <div id="p4s5ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Learning How to Combine Models</h4>
        <p><strong>Stacking</strong> (or Stacked Generalization) is an ensemble method where the predictions from multiple different base models are used as input features for a final "meta-model."</p>
        <p>Instead of just taking a simple average or vote, stacking lets a new model *learn* the best way to combine the predictions from the base learners.</p>
        <p><strong>Analogy:</strong> Stacking is like a **senior manager making a final decision after listening to a committee of diverse experts**. The committee consists of a financial expert (e.g., a linear model), a marketing expert (e.g., a decision tree), and an engineering expert (e.g., an SVM). Each expert provides their prediction. The senior manager (the meta-model) learns over time which expert to trust more under different conditions and combines their advice to make the final, most informed decision.</p>
    </div>
    
    <div id="p4s5ss3-viz" class="tab-pane">
        <h4 class="subsection-title">A Multi-Layered Learning Process</h4>
        <p>This diagram shows the two-level architecture of a stacking ensemble.</p>
        <div class="plot-container">
            <div id="p4s5ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The process starts with the training data. In **Level 0**, several different base models are trained on this data. Their predictions on a hold-out set are then collected. In **Level 1**, these predictions become the *new features* used to train the final meta-model, which produces the final output. It is crucial that the base models' predictions are generated on data they weren't trained on to prevent data leakage.</p>
        </div>
    </div>
    
    <div id="p4s5ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to build a high-performance stacked ensemble for the <code>iris</code> dataset. The <code>stacks</code> package, part of the <code>tidymodels</code> ecosystem, is the modern way to do this in R.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Define Base Models and Preprocessing.</strong> We define several different model specifications (e.g., random forest, k-NN) and a resampling strategy (cross-validation).
                <div class="code-container">
                    <pre><code class="language-r"># This is a conceptual overview of a tidymodels workflow
                    # A full example is more verbose but follows these steps.
                    # See the tidymodels documentation for a complete guide.
                    library(tidymodels)
                    library(stacks)

                    # Create resampling folds
                    iris_folds <- vfold_cv(iris, v = 10)

                    # Define multiple model specifications
                    rf_spec <- rand_forest(trees = 100) %>% set_engine("randomForest") %>% set_mode("classification")
                    knn_spec <- nearest_neighbor(neighbors = 5) %>% set_engine("kknn") %>% set_mode("classification")
                    </code></pre>
                </div>
            </li>
            <li><strong>Step 2: Initialize the Stack and Add Candidates.</strong> We create a data stack and add the results of fitting our base models using our cross-validation folds. The <code>stacks</code> package handles the creation of the out-of-sample predictions needed for the meta-model.
                <div class="code-container">
                    <pre><code class="language-r"># This step would involve fitting each model to the resamples
                    # For brevity, we'll assume this has been done and we have results.
                    # data_stack <- stacks() %>% add_candidates(rf_results) %>% add_candidates(knn_results)
                    </code></pre>
                </div>
            </li>
            <li><strong>Step 3: Fit the Meta-Model.</strong> We use <code>blend_predictions()</code> to train a meta-model (typically a regularized regression) on the predictions from the base models.
                <div class="code-container">
                    <pre><code class="language-r"># stacked_model <- data_stack %>% blend_predictions()</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Fit the Final Ensemble and Predict.</strong> The final step is to fit the full ensemble to the entire training set.
                <div class="code-container">
                    <pre><code class="language-r"># final_ensemble <- stacked_model %>% fit_members()
                    # predictions <- predict(final_ensemble, new_data = test_data)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Blending vs. Stacking</h3>
    <div class="scenario-content">
        <p><strong>Blending</strong> is a simpler, faster version of stacking. Instead of using complex k-fold cross-validation to generate the base model predictions, you simply hold out a small validation set. The base models are trained on the rest of the training data, and their predictions on the validation set are used to train the meta-model. It's faster but may not be as robust as a full cross-validated stacking approach.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are building a stacked ensemble. Your base models are a Logistic Regression and a Naive Bayes classifier. What kind of model would be a good, simple choice for the meta-learner that combines their predictions? Why?</li>
            <li>You have three base models. Model A is very good at identifying one class but poor at others. Model B is the opposite. Model C is okay at all classes. Why might a stacked ensemble of these three models perform better than any of them individually?</li>
        </ul>
    </div>
</div>
`,
          p4s5ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has three different classification models. Instead of the complexity of stacking, they want the simplest possible way to combine them. They decide to use a **Voting Classifier**, which makes a final prediction based on a simple majority vote from the individual models.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s5ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s5ss4-viz">Hard vs. Soft Voting</button>
        <button class="tab-button" data-tab="p4s5ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s5ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Democratic Decision-Making</h4>
        <p>A **Voting Classifier** is an ensemble method that combines the predictions from multiple different models and makes a final prediction based on a "voting" system.</p>
        <p><strong>Analogy:</strong> A voting classifier is like a **panel of judges at a competition**. Each judge (model) gives their own score or verdict. The final winner is decided based on the collective decision of the panel.</p>
    </div>
    
    <div id="p4s5ss4-viz" class="tab-pane">
        <h4 class="subsection-title">Two Ways to Vote</h4>
        <p>The two main types of voting are Hard Voting and Soft Voting.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Hard Voting (Majority Rule)</strong>
                <p>Each model gets one vote for its predicted class. The final prediction is the class that receives the most votes. It's simple and democratic.</p>
                <p><strong>Example:</strong>
                <br>• Model A predicts "Spam"
                <br>• Model B predicts "Not Spam"
                <br>• Model C predicts "Spam"
                <br><strong>Final Prediction:</strong> "Spam" (2 votes to 1)
                </p>
            </div>
            <div class="decision-branch">
                <strong>Soft Voting (Weighted Average)</strong>
                <p>This method is used for models that can predict probabilities. It averages the predicted probabilities from all models for each class and then picks the class with the highest average probability. This is usually preferred as it uses more information.</p>
                <p><strong>Example:</strong>
                <br>• Model A: P(Spam) = 0.9
                <br>• Model B: P(Spam) = 0.2
                <br>• Model C: P(Spam) = 0.6
                <br><strong>Average P(Spam):</strong> (0.9 + 0.2 + 0.6) / 3 = 0.57
                <br><strong>Final Prediction:</strong> "Spam" (since 0.57 > 0.5)
                </p>
            </div>
        </div>
    </div>

    <div id="p4s5ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have predictions from three different models for a binary classification task. We need to combine them using both hard and soft voting to get a final ensemble prediction.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Get Predictions from Base Models.</strong> We assume we have already trained three models and have their predictions for a new data point.
                <div class="code-container">
                    <pre><code class="language-r"># Predicted probabilities from three different models
model_a_prob <- 0.70 # Predicts "Yes" ( > 0.5)
model_b_prob <- 0.40 # Predicts "No"  ( < 0.5)
model_c_prob <- 0.60 # Predicts "Yes" ( > 0.5)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Implement Hard Voting.</strong> We convert the probabilities to class predictions and take a majority vote.
                <div class="code-container">
                    <pre><code class="language-r"># Get class predictions
pred_a <- "Yes"
pred_b <- "No"
pred_c <- "Yes"
votes <- c(pred_a, pred_b, pred_c)

# Find the majority vote
hard_vote_result <- names(which.max(table(votes)))
print(paste("Hard Voting Result:", hard_vote_result)) # Result: "Yes"</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Implement Soft Voting.</strong> We average the probabilities and then make a final decision based on the average.
                <div class="code-container">
                    <pre><code class="language-r"># Average the probabilities
avg_prob <- mean(c(model_a_prob, model_b_prob, model_c_prob))
print(paste("Average Probability:", avg_prob)) # Result: 0.567

# Make a decision based on the average
soft_vote_result <- ifelse(avg_prob > 0.5, "Yes", "No")
print(paste("Soft Voting Result:", soft_vote_result)) # Result: "Yes"</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Diversity is Key</h3>
    <div class="scenario-content">
        <p>Ensemble methods, including voting, work best when the base models are **diverse**. This means the models should be different types (e.g., a linear model, a tree-based model, an instance-based model) and make different kinds of errors. If all your models are very similar, they will all make the same mistakes, and combining them won't provide much benefit.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>You have five models making a prediction for a new customer. Their class predictions are:</p>
        <p><code>"Churn", "No Churn", "Churn", "No Churn", "No Churn"</code></p>
        <ul class="prose-list">
            <li>What would be the final prediction using a Hard Voting classifier?</li>
            <li>Now imagine the models produced the following probabilities for "Churn":<br><code>0.6, 0.2, 0.7, 0.3, 0.4</code></li>
            <li>Calculate the average probability of "Churn". Based on this, what would be the final prediction using a Soft Voting classifier (with a 0.5 threshold)?</li>
        </ul>
    </div>
</div>
`,
          p4s5ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst trains a gradient boosting model with default parameters and achieves incredible accuracy on their training data. However, when they submit their predictions to the Kaggle competition, their score is very poor. Their model was massively overfit because they failed to properly tune the ensemble's hyperparameters. This highlights the critical need for best practices when working with these powerful but complex models.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Robust Ensemble Modeling</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Ensemble Method Best Practices</div>
            <div class="scenario">Moving from a simple ensemble to a high-performing, well-tuned model.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-random"></i> 1. Promote Diversity in Models</div>
                        <div class="scenario"><strong>The Rule:</strong> Ensembles are most effective when their base models are diverse and make uncorrelated errors. When stacking or voting, combine different *types* of models (e.g., a linear model, a tree model, a k-NN model) to capture different patterns in the data.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-sliders-h"></i> 2. Tune Hyperparameters Rigorously</div>
                        <div class="scenario"><strong>The Rule:</strong> Ensemble models have critical hyperparameters (e.g., <code>ntree</code> in a random forest, <code>nrounds</code> and <code>eta</code> in XGBoost). These **must** be tuned using a proper cross-validation strategy (like grid search or random search) to find the optimal values and prevent overfitting.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chart-line"></i> 3. Use Early Stopping for Boosting</div>
                        <div class="scenario"><strong>The Rule:</strong> Boosting models will eventually overfit if you add too many trees. Use cross-validation to find the optimal number of trees (<code>nrounds</code>) by monitoring the performance on a validation set and stopping the training when performance no longer improves. The <code>xgb.cv()</code> function is designed for this.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>tidymodels</code> Framework for Tuning</h3>
    <div class="scenario-content">
        <p>Manually setting up cross-validation and grid searches can be complex. The <code>tidymodels</code> ecosystem is the modern standard for this in R. You define your model specification, define a tuning grid of hyperparameters, and use functions from the <code>tune</code> package (like <code>tune_grid()</code>) to automatically perform the entire cross-validated tuning process and find the best combination of parameters for you.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are building a random forest model. You notice that as you increase the number of trees (<code>ntree</code>) from 500 to 1000, your OOB error rate doesn't improve. What does this suggest about adding more trees? Is it worth the extra computational cost?</li>
            <li>You are tuning an XGBoost model. You run a cross-validation and it tells you the optimal number of rounds is 85. When you build your final model, should you train it for 85 rounds, or for 500 rounds? Why?</li>
            <li>Why is it a bad idea to create a stacked ensemble using three different random forest models? What principle does this violate?</li>
        </ul>
    </div>
</div>
`,
          p4s6ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst wants to build a model that can "see" and classify images of handwritten digits. A traditional machine learning model would require the analyst to manually engineer features like "number of horizontal lines" or "number of loops." A **Neural Network** can *learn* these features automatically from the raw pixel data, making it a far more powerful tool for complex pattern recognition tasks.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s6ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s6ss1-viz">Visualization of a Neuron</button>
        <button class="tab-button" data-tab="p4s6ss1-learning">How Networks Learn</button>
    </div>

    <div id="p4s6ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Learning from Data Hierarchically</h4>
        <p><strong>Artificial Neural Networks (ANNs)</strong> are a class of machine learning models inspired by the structure of the human brain. They are composed of interconnected nodes, or **neurons**, organized in **layers**.</p>
        <p><strong>Analogy:</strong> A neural network is like a **team of specialists in an assembly line**. The raw materials (your input data) come in one end. The first group of workers (the first layer) performs a simple task, like identifying basic shapes. They pass their results to the next group (the second layer), which combines these basic shapes to recognize more complex parts, like an eye or a nose. This continues until the final worker (the output layer) makes the final decision ("It's a cat!").</p>
    </div>
    
    <div id="p4s6ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Artificial Neuron</h4>
        <p>The neuron is the fundamental processing unit of the network.</p>
        <div class="plot-container">
            <div id="p4s6ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> Each neuron receives one or more **inputs**. Each input is multiplied by a **weight** (which represents the connection's strength). All the weighted inputs are summed together, and a **bias** term is added. This result is then passed through an **activation function**, which introduces non-linearity and produces the neuron's final output. This output is then passed on to neurons in the next layer.</p>
        </div>
    </div>
    
    <div id="p4s6ss1-learning" class="tab-pane">
        <h4 class="subsection-title">Forward and Backward Propagation</h4>
        <p>A neural network learns by adjusting its weights and biases to minimize a loss function. This is a two-step process:</p>
        <ol class="prose-list">
            <li><strong>Forward Propagation:</strong> Input data is "fed forward" through the network, from the input layer to the output layer, to produce a prediction. The difference between the prediction and the true value is calculated as the error (loss).</li>
            <li><strong>Backpropagation:</strong> The error is then propagated *backward* through the network. The algorithm uses calculus (specifically, the chain rule) to calculate the gradient of the loss function with respect to each weight and bias in the network.</li>
        </ol>
        <p>An optimization algorithm (like Adam) then uses these gradients to update all the weights and biases slightly in the direction that will reduce the error. This forward/backward cycle is repeated thousands of times until the model converges.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Role of Activation Functions</h3>
    <div class="scenario-content">
        <p>The activation function is a critical component. Without a non-linear activation function (like ReLU or sigmoid), no matter how many layers you stack, a neural network would just be a complicated way of doing linear regression. The non-linearity is what allows the network to learn incredibly complex, curved decision boundaries and hierarchical features.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>Imagine a single neuron with two inputs, x1 and x2.</li>
            <li>The weight for x1 is w1 = 2, and the weight for x2 is w2 = -3. The neuron's bias is b = 1.</li>
            <li>If the input data is x1 = 5 and x2 = 2, what is the value of the neuron *before* the activation function? (The calculation is (x1*w1) + (x2*w2) + b).</li>
        </ul>
    </div>
</div>
`,
          p4s6ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst at a bank in Karachi needs to build a model to predict a customer's credit score (a continuous number) based on tabular data like their age, income, and number of credit lines. The relationships are likely to be complex and non-linear. A **Feedforward Neural Network (FNN)**, also known as a Multi-Layer Perceptron (MLP), is an excellent general-purpose tool for this kind of regression or classification task on structured, tabular data. 🇵🇰</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s6ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s6ss2-viz">Architecture</button>
        <button class="tab-button" data-tab="p4s6ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s6ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">The "Standard" Neural Network</h4>
        <p>An FNN is the most basic type of artificial neural network. It is characterized by the fact that information flows in only one direction—from the input layer, through any "hidden" layers, to the output layer. There are no loops or cycles in the network.</p>
        <p><strong>Analogy:</strong> An FNN is like a **bureaucratic process**. A form (your data) is submitted at the first desk (the input layer). The clerks at this desk process it and pass it on to the next department (the first hidden layer). This continues through one or more departments until it reaches the final desk (the output layer), where the final decision is stamped on the form. There is no going back to a previous desk.</p>
    </div>
    
    <div id="p4s6ss2-viz" class="tab-pane">
        <h4 class="subsection-title">A Simple FNN Architecture</h4>
        <p>This diagram shows a simple FNN for a classification task.</p>
        <div class="plot-container">
            <div id="p4s6ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The network has an **Input Layer** with four neurons, one for each feature in the data. It has two **Hidden Layers** where the network learns progressively more complex combinations of the input features. Finally, it has an **Output Layer** with three neurons, one for each possible class. The neuron with the highest output value determines the final classification.</p>
        </div>
    </div>
    
    <div id="p4s6ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Building an FNN with <code>keras</code> in R</h4>
        <p>The <code>keras</code> package provides a user-friendly, high-level interface for building neural networks in R.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# install.packages("keras")
library(keras)

# --- 1. Define the Model Architecture ---
# We build the model layer by layer using the pipe
model <- keras_model_sequential() %>%
  # Input layer and first hidden layer.
  # 'units' is the number of neurons. 'activation' is the activation function.
  # 'input_shape' is only needed for the very first layer.
  layer_dense(units = 64, activation = "relu", input_shape = c(10)) %>%
  
  # A second hidden layer
  layer_dense(units = 32, activation = "relu") %>%

  # The output layer for a regression task. It has one neuron and a linear activation.
  layer_dense(units = 1) 

# --- 2. Compile the Model ---
# This step configures the model for training by specifying the optimizer,
# the loss function to minimize, and any metrics to track.
model %>% compile(
  optimizer = "rmsprop",
  loss = "mse", # Mean Squared Error for regression
  metrics = c("mae")  # Mean Absolute Error
)

# --- 3. View the Model Summary ---
summary(model)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Choosing the Output Layer</h3>
    <div class="scenario-content">
        <p>The number of neurons and the activation function of your **output layer** are determined by your task, and you must get them right:</p>
        <ul class="prose-list">
            <li><strong>Binary Classification:</strong> 1 neuron, <code>activation = "sigmoid"</code>.</li>
            <li><strong>Multiclass Classification:</strong> N neurons (where N is the number of classes), <code>activation = "softmax"</code>.</li>
            <li><strong>Regression:</strong> 1 neuron, <code>activation = "linear"</code> (the default).</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's design an FNN architecture for the <code>iris</code> dataset.</p>
        <ul class="prose-list">
            <li>How many neurons should the <code>input_shape</code> be? (Hint: How many predictor variables are there?)</li>
            <li>Let's design a network with one hidden layer containing 16 neurons. What activation function would be a good choice for this hidden layer?</li>
            <li>The <code>Species</code> variable has 3 classes. How many neurons should the output layer have, and what should its activation function be?</li>
            <li>When you compile the model for this multiclass classification task, what would be an appropriate <code>loss</code> function? (Hint: Look up common loss functions for multiclass classification).</li>
        </ul>
    </div>
</div>
`,
          p4s6ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A radiologist needs an automated system to detect whether a medical scan image contains a tumor. The key patterns (like texture and shape) that indicate a tumor can appear anywhere in the image. A standard FNN would fail because it isn't "translation invariant." A **Convolutional Neural Network (CNN)** is the specialized architecture designed to solve this exact problem by learning and detecting spatial features.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s6ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s6ss3-viz">The Key Layers</button>
        <button class="tab-button" data-tab="p4s6ss3-usecase">Implementation in R</button>
    </div>

    <div id="p4s6ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Learning Spatial Hierarchies</h4>
        <p>A **Convolutional Neural Network (CNN)** is a specialized type of neural network designed for processing data that has a known, grid-like topology, such as an image. Images are 2D grids of pixels, and videos are 3D grids.</p>
        <p>CNNs are incredibly effective because they learn a hierarchy of features. Early layers learn to detect simple features like edges and corners. Deeper layers combine these simple features to detect more complex patterns like eyes, noses, or, in our scenario, the texture of a tumor.</p>
        <p><strong>Analogy:</strong> A CNN works like the **visual cortex in your brain**. The first neurons in your eye detect very simple patterns—light spots, dark spots, and edges. These signals are passed deeper into your brain, where other neurons combine them to recognize shapes, then objects, and finally, complex scenes. The CNN learns a similar, hierarchical set of "filters" to do the same thing.</p>
    </div>
    
    <div id="p4s6ss3-viz" class="tab-pane">
        <h4 class="subsection-title">The Anatomy of a CNN</h4>
        <p>A typical CNN architecture consists of two main parts: a feature extraction part (convolutional and pooling layers) and a classification part (fully connected layers).</p>
        <div class="plot-container">
            <div id="p4s6ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Convolutional Layer** acts as a feature detector, scanning the input image with a set of filters to find patterns like edges or textures, producing "feature maps." The **Pooling Layer** then downsamples these feature maps, making the representation smaller and more robust to the exact location of the feature. After several rounds of convolution and pooling, the final feature maps are **flattened** into a single vector and fed into a standard **Fully Connected Network** (like an FNN) for the final classification.</p>
        </div>
    </div>

    <div id="p4s6ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Building a Simple CNN with <code>keras</code></h4>
        <p>Let's build a simple CNN for classifying the famous MNIST handwritten digits dataset.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(keras)

# The MNIST dataset has 28x28 pixel grayscale images
input_shape <- c(28, 28, 1)

model <- keras_model_sequential() %>%
  # --- Feature Extraction Part ---
  # 1. First Convolutional Layer with 32 filters
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu', input_shape = input_shape) %>%
  # 2. Pooling Layer to downsample
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # --- Classification Part ---
  # 3. Flatten the 2D feature maps into a 1D vector
  layer_flatten() %>%
  # 4. A standard dense layer for classification
  layer_dense(units = 128, activation = 'relu') %>%
  # 5. The final output layer (10 digits from 0-9)
  layer_dense(units = 10, activation = 'softmax')

# Compile the model
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

summary(model)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Transfer Learning</h3>
    <div class="scenario-content">
        <p>Training a state-of-the-art CNN from scratch requires massive amounts of data and computational power. A powerful shortcut is **Transfer Learning**. You can take a very large, pre-trained model (like VGG16 or ResNet, which have been trained on millions of images from ImageNet) and use its learned feature extraction layers. You simply chop off the original output layer and add your own new classification layer on top, then train only this new layer on your specific, smaller dataset. This allows you to achieve very high performance with much less data.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise in designing a CNN architecture.</p>
        <ul class="prose-list">
            <li>You are building a CNN to classify color images of cats and dogs. The images are 64x64 pixels.</li>
            <li>What should the <code>input_shape</code> be for your first convolutional layer? (Hint: Color images have 3 channels: Red, Green, and Blue).</li>
            <li>This is a binary classification problem (cat or dog). How many neurons should your final output layer have, and what should its activation function be?</li>
        </ul>
    </div>
</div>
`,
          p4s6ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is building a model to predict the next word in a sentence. An FNN would fail because it treats every word independently and has no concept of order or memory of the previous words. To understand language, the model needs to process information sequentially and maintain an internal "memory" of what it has seen so far. This requires a **Recurrent Neural Network (RNN)**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s6ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s6ss4-viz">The Loop Architecture</button>
        <button class="tab-button" data-tab="p4s6ss4-methods">LSTM & GRU</button>
        <button class="tab-button" data-tab="p4s6ss4-usecase">Implementation in R</button>
    </div>

    <div id="p4s6ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Networks with Memory</h4>
        <p>A **Recurrent Neural Network (RNN)** is a type of neural network specialized for processing sequential data, where the order of information is critical. This includes time series data, text, and audio.</p>
        <p>The defining feature of an RNN is its **hidden state**, or "memory." The network has a loop that allows information to persist from one step of the sequence to the next.</p>
        <p><strong>Analogy:</strong> An RNN reads a sentence like a **human does**. When you read the word "not" in the sentence "The movie was not good," you remember it. Your understanding of the final word, "good," is completely changed by your memory of the word "not" that came before it. An FNN would forget "not" as soon as it moved to the next word.</p>
    </div>
    
    <div id="p4s6ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The "Unrolled" RNN</h4>
        <p>This diagram shows how a recurrent neuron can be "unrolled" over time to visualize the flow of information through a sequence.</p>
        <div class="plot-container">
            <div id="p4s6ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The network processes the sequence one element at a time (from $X_0$ to $X_t$). At each step, the neuron takes in the current input ($X_t$) AND the hidden state from the previous step ($h_{t-1}$). It then produces an output for that step ($Y_t$) and updates its hidden state to pass along to the next step ($h_t$). It's the same set of weights being used at every step, just with a different input and hidden state.</p>
        </div>
    </div>
    
    <div id="p4s6ss4-methods" class="tab-pane">
        <h4 class="subsection-title">Solving the Vanishing Gradient Problem</h4>
        <p>Simple RNNs struggle to learn long-range dependencies due to the **vanishing gradient problem**. Two more advanced architectures were created to solve this:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>LSTM (Long Short-Term Memory)</strong>
                <p>An advanced RNN cell that uses a system of "gates" (an input gate, forget gate, and output gate) to regulate the flow of information. It can learn to selectively remember important information for a long time and forget irrelevant information.</p>
            </div>
            <div class="decision-branch">
                <strong>GRU (Gated Recurrent Unit)</strong>
                <p>A simpler and more computationally efficient variation of the LSTM that combines the forget and input gates into a single "update gate." It often performs just as well as an LSTM.</p>
            </div>
        </div>
    </div>

    <div id="p4s6ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Building an LSTM with <code>keras</code></h4>
        <p>Let's build a simple LSTM model for a time series forecasting problem.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(keras)

# Assume we have time series data with a shape of (samples, timesteps, features)
# e.g., 1000 samples, each looking back 10 days, with 1 feature per day.
input_shape <- c(10, 1) 

model <- keras_model_sequential() %>%
  # Add an LSTM layer with 50 units (neurons)
  # It will process the sequence of 10 timesteps.
  layer_lstm(units = 50, input_shape = input_shape) %>%
  
  # A standard dense layer
  layer_dense(units = 25, activation = 'relu') %>%

  # The final output layer for our regression forecast
  layer_dense(units = 1)

# Compile the model
model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

summary(model)
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Rise of Transformers</h3>
    <div class="scenario-content">
        <p>While LSTMs and GRUs were the state-of-the-art for many years, the field of NLP has been revolutionized by a new architecture called the **Transformer**. Models like BERT and GPT are based on transformers, which use a mechanism called "self-attention" to process all words in a sequence simultaneously, rather than sequentially. This allows them to capture much more complex and long-range dependencies in text, leading to massive performance gains.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise in designing an RNN architecture.</p>
        <ul class="prose-list">
            <li>You are building an RNN to predict the stock price for tomorrow based on the prices from the last 30 days.</li>
            <li>What would be the number of "timesteps" in your input data?</li>
            <li>What kind of output layer would you use for this regression task (number of neurons and activation function)?</li>
            <li>You want your model to capture long-term trends in the stock price. Which type of recurrent layer would be a better choice: a <code>layer_simple_rnn</code> or a <code>layer_lstm</code>? Why?</li>
        </ul>
    </div>
</div>
`,
          p4s6ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst in Karachi has defined a complex CNN architecture. Now they must make the critical decisions that will govern how it learns: which optimizer should they use to navigate the complex loss surface? Which loss function accurately reflects their business goal? And most importantly, how can they prevent their powerful model from overfitting the training data?</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s6ss5-loss">Loss Functions</button>
        <button class="tab-button" data-tab="p4s6ss5-optimizers">Optimizers</button>
        <button class="tab-button" data-tab="p4s6ss5-regularization">Regularization</button>
    </div>

    <div id="p4s6ss5-loss" class="tab-pane active">
        <h4 class="subsection-title">Measuring the Model's Error</h4>
        <p>The **loss function** (or objective function) quantifies how "wrong" the model's prediction is compared to the true label. The entire goal of training is to adjust the model's weights to minimize this value.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>For Regression</strong>
                <p><strong>Mean Squared Error (<code>mse</code>):</strong> The default choice. It heavily penalizes large errors.</p>
                <p><strong>Mean Absolute Error (<code>mae</code>):</strong> Less sensitive to outliers than MSE.</p>
            </div>
            <div class="decision-branch">
                <strong>For Classification</strong>
                <p><strong>Binary Cross-Entropy (<code>binary_crossentropy</code>):</strong> For two-class problems.</p>
                <p><strong>Categorical Cross-Entropy (<code>categorical_crossentropy</code>):</strong> For multi-class problems where the labels are one-hot encoded.</p>
            </div>
        </div>
    </div>

    <div id="p4s6ss5-optimizers" class="tab-pane">
        <h4 class="subsection-title">Navigating the Loss Surface</h4>
        <p>The **optimizer** is the algorithm that uses the gradients from backpropagation to update the network's weights. The choice of optimizer can have a huge impact on training speed and performance.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>SGD with Momentum</strong>
                <p>A basic optimizer that often works well but may require careful tuning of the learning rate.</p>
            </div>
            <div class="decision-branch">
                <strong>RMSprop</strong>
                <p>An adaptive learning rate optimizer that works well on a variety of problems.</p>
            </div>
            <div class="decision-branch">
                <strong>Adam (Adaptive Moment Estimation)</strong>
                <p>The most popular and generally recommended default optimizer. It combines the ideas of Momentum and RMSprop and is effective across a wide range of tasks.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s6ss5-regularization" class="tab-pane">
        <h4 class="subsection-title">Preventing Overfitting</h4>
        <p>Deep neural networks have millions of parameters and can easily overfit by memorizing the training data. Regularization techniques are essential to prevent this.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>L1 & L2 Regularization</strong>
                <p>Just like in regularized regression, you can add a penalty to the loss function based on the magnitude of the network's weights.</p>
            </div>
            <div class="decision-branch">
                <strong>Dropout</strong>
                <p>A simple but powerful technique. During each training step, it randomly "drops out" (sets to zero) a fraction of the neurons in a layer. This prevents neurons from co-adapting too much and forces the network to learn more robust, redundant features.</p>
                <p><strong>Analogy:</strong> Dropout is like training a team where, at every practice, some random players are told they can't play. This forces the other players to learn the skills themselves and not rely too heavily on any single star player.</p>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Learning Rate Schedules</h3>
    <div class="scenario-content">
        <p>The learning rate is the most important hyperparameter to tune. A common and effective technique is to use a **learning rate schedule**. You start with a relatively high learning rate to converge quickly at the beginning of training, and then you gradually decrease it over time. This allows the model to make smaller, finer adjustments as it gets closer to the minimum of the loss function. Frameworks like Keras have built-in schedulers to automate this process.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's add a Dropout layer to the FNN we designed earlier.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(keras)
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = c(10)) %>%
  # Add a dropout layer after the first dense layer
  # It will randomly drop 50% of the neurons during training
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1)
</code></pre>
        </div>
        <ul class="prose-list">
            <li>Why is the dropout layer typically placed between hidden layers and not usually after the output layer?</li>
            <li>What would happen to the training process if you set the dropout rate to 0? What if you set it to 1?</li>
        </ul>
    </div>
</div>
`,
          p4s6ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is training a CNN to classify images of cats, dogs, and birds, but they only have 1,000 images per class. This is not enough data to train a deep network from scratch, and the model is overfitting badly. They need to use best practices specifically designed for training on limited data, such as **Data Augmentation** and **Early Stopping**.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Effective Training</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Deep Learning Best Practices</div>
            <div class="scenario">Moving from a basic network that doesn't converge to a high-performing, robust model.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-images"></i> 1. Use Data Augmentation</div>
                        <div class="scenario"><strong>The Rule:</strong> For image data, artificially increase the size of your training set by applying random but realistic transformations to your existing images (e.g., random rotations, shifts, flips, and zooms). This teaches the model to be robust to these variations and is a powerful form of regularization.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-stop-circle"></i> 2. Implement Early Stopping</div>
                        <div class="scenario"><strong>The Rule:</strong> Monitor the model's performance on a separate validation set during training. If the validation loss stops improving (or starts to get worse) for a certain number of epochs (the "patience"), stop the training process automatically. This prevents the model from overfitting.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-random"></i> 3. Initialize Weights Properly</div>
                        <div class="scenario"><strong>The Rule:</strong> Don't start all weights at zero. Use a smart initialization strategy (like "Glorot uniform" or "He normal," which are the defaults in Keras) that helps prevent gradients from vanishing or exploding at the start of training.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-layer-group"></i> 4. Use Batch Normalization</div>
                        <div class="scenario"><strong>The Rule:</strong> Add <code>layer_batch_normalization()</code> between your layers. This layer normalizes the activations from the previous layer, which can significantly speed up training, stabilize the process, and provide a slight regularization effect.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Start with a Pre-Trained Model (Transfer Learning)</h3>
    <div class="scenario-content">
        <p>The single most effective best practice when working with image data is to **not train a model from scratch**. Instead, use **Transfer Learning**. Take a very large, state-of-the-art model that has already been trained on a massive dataset like ImageNet, freeze its convolutional base (the feature extraction part), and just train a new, small classifier head on top for your specific task. This leverages the powerful, general-purpose features the model has already learned and allows you to achieve incredible performance with very little data.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's add some of these best practices to our Keras model definition from the previous topic.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(keras)
model <- keras_model_sequential() %>%
  layer_dense(units = 64, kernel_initializer = 'he_normal', input_shape = c(10)) %>%
  layer_batch_normalization() %>%
  layer_activation_relu() %>% # Apply activation after batch norm
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 32, kernel_initializer = 'he_normal') %>%
  layer_batch_normalization() %>%
  layer_activation_relu() %>%
  layer_dense(units = 1)
</code></pre>
        </div>
        <ul class="prose-list">
            <li>What does the <code>kernel_initializer = 'he_normal'</code> argument do?</li>
            <li>Why is the activation function now a separate layer? (Hint: Think about what batch normalization does).</li>
            <li>To implement early stopping, you would add a "callback" to your <code>model %>% fit(...)</code> call. What do you think the argument <code>patience = 10</code> would mean in an early stopping callback?</li>
        </ul>
    </div>
</div>
`,
          p4s7ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst at a Karachi-based retail chain is looking at a chart of the last three years of daily sales data. 🇵🇰 They can see some patterns, but it's a messy combination of overall growth, predictable seasonal spikes (like during Ramadan and Eid), and random daily noise. To build an accurate forecast, they must first systematically break down the time series into these distinct, underlying components.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s7ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s7ss1-methods">Additive vs. Multiplicative</button>
        <button class="tab-button" data-tab="p4s7ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s7ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s7ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Unpacking the Patterns of Time</h4>
        <p><strong>Time Series Decomposition</strong> is a statistical method that deconstructs a time series into several constituent components. It separates the signal from the noise.</p>
        <p><strong>Analogy:</strong> A time series is like a **musical song**. Decomposition is the process of using an audio mixer to isolate the individual tracks: one for the bassline (the **Trend**), one for the repeating drum beat (the **Seasonality**), and one for the random static and feedback (the **Residual**).</p>
        <ul class="prose-list">
            <li><strong>Trend ($T_t$):</strong> The long-term, underlying direction of the series (e.g., increasing, decreasing, or flat).</li>
            <li><strong>Seasonality ($S_t$):</strong> A repeating, cyclical pattern of a fixed and known frequency (e.g., daily, weekly, yearly).</li>
            <li><strong>Residual ($R_t$):</strong> The random, irregular, "noise" component that is left over after accounting for the trend and seasonality.</li>
        </ul>
    </div>

    <div id="p4s7ss1-methods" class="tab-pane">
        <h4 class="subsection-title">Two Ways to Combine Components</h4>
        <p>The components can be combined in two primary ways:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Additive Model</strong>
                <p>Assumes the components add together: $Y_t = T_t + S_t + R_t$. This is appropriate when the magnitude of the seasonal fluctuations or the variance of the error does **not** change over time.</p>
            </div>
            <div class="decision-branch">
                <strong>Multiplicative Model</strong>
                <p>Assumes the components multiply: $Y_t = T_t \times S_t \times R_t$. This is appropriate when the seasonal variation or the error increases or decreases in magnitude as the trend changes (e.g., holiday sales are proportionally larger in high-growth years).</p>
            </div>
        </div>
    </div>
    
    <div id="p4s7ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Decomposed Series</h4>
        <p>A decomposition plot shows the original time series and each of its isolated components in separate panels, making it easy to see the underlying patterns.</p>
        <div class="plot-container">
            <div id="p4s7ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This shows the decomposition of the classic <code>AirPassengers</code> dataset. The top panel is the raw data, showing both an upward trend and a clear yearly seasonality. The second panel isolates the long-term **Trend**. The third panel shows the repeating **Seasonal** pattern. The bottom panel shows the random **Residuals**, which should ideally look like white noise with no obvious patterns left.</p>
        </div>
    </div>
    
    <div id="p4s7ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We need to decompose the <code>AirPassengers</code> dataset to understand its trend and seasonal components.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Inspect the Data.</strong> We first plot the raw data to visually assess the model type. The seasonal fluctuations appear to grow as the trend increases, suggesting a multiplicative model is appropriate.
                <div class="code-container">
                    <pre><code class="language-r">plot(AirPassengers)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Perform Decomposition.</strong> We use the <code>decompose()</code> function, specifying the model type.
                <div class="code-container">
                    <pre><code class="language-r">decomposed_result <- decompose(AirPassengers, type = "multiplicative")</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Plot and Analyze the Components.</strong> The <code>plot()</code> function for a decomposed object automatically creates the multi-panel plot.
                <div class="code-container">
                    <pre><code class="language-r">plot(decomposed_result)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong>
                <div class="plot-description">
                    <p><strong>Conclusion:</strong> "The decomposition clearly shows a strong, near-linear upward trend in air travel from 1949 to 1960. It also isolates a consistent yearly seasonal pattern, with travel peaking in the summer months (July/August) and hitting a trough in the winter (November/February). The residual component is small and appears random, indicating that our model has successfully captured the main structural patterns in the data."</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use STL for More Robust Decomposition</h3>
    <div class="scenario-content">
        <p>While <code>decompose()</code> is a good starting point, a more modern and robust method is **STL (Seasonal and Trend decomposition using Loess)**. It is less sensitive to outliers and allows the seasonal component to change over time. You can easily use it with the <code>stl()</code> function in R.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The built-in <code>co2</code> dataset contains monthly CO2 concentrations.</li>
            <li>Plot the <code>co2</code> time series. Does it look additive or multiplicative?</li>
            <li>Use the <code>decompose()</code> function to decompose the series using the model type you chose.</li>
            <li>Plot the decomposed result. What can you say about the trend and seasonal components of CO2 concentration over time?</li>
        </ul>
    </div>
</div>
`,
          p4s7ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst wants to build an ARIMA model to forecast stock prices. A key assumption of these models is that the time series is **stationary**. The raw stock price data is clearly non-stationary (it has a strong upward trend). The analyst must first apply a transformation, such as **differencing**, to make the series stationary before they can proceed with modeling.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s7ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s7ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s7ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s7ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Data Without a Memory of the Past</h4>
        <p>A time series is said to be **stationary** if its statistical properties—specifically its mean, variance, and autocorrelation—are constant over time.</p>
        <p><strong>Analogy:</strong> A stationary time series is like a **person walking on a treadmill**. Their position fluctuates randomly around the center of the treadmill, but their average position (the center) and their range of motion (the variance) remain the same over time. A non-stationary series is like a person walking up a hill; their average position is constantly increasing.</p>
        <p>Most time series in the real world are non-stationary. We must transform them to meet the assumptions of many forecasting models.</p>
    </div>
    
    <div id="p4s7ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Making a Series Stationary</h4>
        <p>This plot shows a classic non-stationary stock price series and its stationary counterpart, the daily returns.</p>
        <div class="plot-container">
            <div id="p4s7ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Non-Stationary** plot on the left shows Google's stock price, which has a clear upward trend (its mean is not constant). The **Stationary** plot on the right shows the daily returns, calculated by taking the first difference of the log-prices. This new series has a constant mean (centered around zero) and a roughly constant variance, making it suitable for modeling.</p>
        </div>
    </div>

    <div id="p4s7ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We need to check if the <code>AirPassengers</code> dataset is stationary and, if not, make it stationary.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Visual Inspection.</strong> We plot the data and see a clear trend and increasing variance, so it's not stationary.
                <div class="code-container">
                    <pre><code class="language-r">plot(AirPassengers)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Formal Test for Stationarity.</strong> We use the Augmented Dickey-Fuller (ADF) test. The null hypothesis for this test is that the series is non-stationary.
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("tseries")
library(tseries)
adf.test(AirPassengers) # High p-value -> Fail to reject H0 -> Non-stationary</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Apply Transformations.</strong> To stabilize the variance, we first take the logarithm. To remove the trend, we take the first difference.
                <div class="code-container">
                    <pre><code class="language-r"># diff() calculates the difference between consecutive values
stationary_series <- diff(log(AirPassengers))
plot(stationary_series)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Re-run the Formal Test.</strong> We run the ADF test again on our transformed series.
                <div class="code-container">
                    <pre><code class="language-r">adf.test(stationary_series) # Low p-value -> Reject H0 -> Stationary!</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> Through a combination of logging and differencing, we have successfully transformed the non-stationary <code>AirPassengers</code> series into a stationary one, which is now ready for ARIMA modeling.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Seasonal Differencing</h3>
    <div class="scenario-content">
        <p>If your data has a strong seasonal component, you may also need to apply **seasonal differencing** in addition to regular differencing. This involves taking the difference between an observation and the corresponding observation from the previous season (e.g., for monthly data, the difference between this month and the same month last year). The <code>diff()</code> function can do this with the <code>lag</code> argument: <code>diff(my_data, lag = 12)</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The built-in <code>Nile</code> dataset contains measurements of the flow of the river Nile.</li>
            <li>Plot the <code>Nile</code> series. Does it appear to have a trend? Does it look stationary?</li>
            <li>Run the <code>adf.test()</code> on the raw <code>Nile</code> data. What do you conclude?</li>
            <li>Create a new series by taking the first difference of <code>Nile</code> using <code>diff()</code>. Plot this new series.</li>
            <li>Run <code>adf.test()</code> on your new differenced series. Is it stationary now?</li>
        </ul>
    </div>
</div>
`,
          p4s7ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has made their sales data stationary. Now they need to build a model that can capture the underlying temporal patterns to make a forecast. They will use the **ARIMA** framework, which combines three components—Autoregression (AR), Integration (I), and Moving Average (MA)—to model the dependencies in the time series.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s7ss3-concepts">The Three Components</button>
        <button class="tab-button" data-tab="p4s7ss3-viz">ACF & PACF Plots</button>
        <button class="tab-button" data-tab="p4s7ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s7ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Modeling with Past Values and Past Errors</h4>
        <p>An ARIMA(p, d, q) model has three parts:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>AR(p): Autoregression</strong>
                <p>The "AR" part models the relationship between an observation and a number of lagged observations (its own past values). The parameter <code>p</code> is the number of past values to include in the model.</p>
            </div>
            <div class="decision-branch">
                <strong>I(d): Integration</strong>
                <p>The "I" part refers to the use of differencing to make the series stationary. The parameter <code>d</code> is the number of times the data had to be differenced.</p>
            </div>
            <div class="decision-branch">
                <strong>MA(q): Moving Average</strong>
                <p>The "MA" part models the relationship between an observation and the residual errors from a moving average model applied to lagged observations. The parameter <code>q</code> is the order of the moving average.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s7ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Choosing <code>p</code> and <code>q</code></h4>
        <p>The key to building a manual ARIMA model is choosing the right orders for <code>p</code> and <code>q</code>. This is traditionally done by inspecting the **Autocorrelation Function (ACF)** and **Partial Autocorrelation Function (PACF)** plots.</p>
        <div class="plot-container">
            <div id="p4s7ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe these plots:</strong> The ACF plot shows the correlation of the series with its own lags. The PACF plot shows the partial correlation, controlling for the effect of shorter lags. For a pure AR(p) process, the PACF plot will cut off after lag <code>p</code>. For a pure MA(q) process, the ACF plot will cut off after lag <code>q</code>. The blue lines represent the significance threshold.</p>
        </div>
    </div>

    <div id="p4s7ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to find the best ARIMA model for the <code>Nile</code> river flow dataset and use it to forecast the next 10 years.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("forecast")
library(forecast)
data(Nile)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Let the Algorithm Choose the Best Model.</strong> Manually interpreting ACF/PACF plots can be tricky. The <code>auto.arima()</code> function automatically tests a wide range of models and selects the best one based on information criteria (like AIC).
                <div class="code-container">
                    <pre><code class="language-r"># This powerful function does all the hard work for us
fit <- auto.arima(Nile)
summary(fit)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Check the Residuals.</strong> A good model should leave residuals that look like random noise. We can use <code>checkresiduals()</code> to get diagnostic plots. The residuals should be uncorrelated (ACF plot) and normally distributed (histogram).
                <div class="code-container">
                    <pre><code class="language-r">checkresiduals(fit)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Generate and Plot the Forecast.</strong> We use the <code>forecast()</code> function to predict future values.
                <div class="code-container">
                    <pre><code class="language-r"># Forecast the next 10 years
fc <- forecast(fit, h = 10)
plot(fc)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: SARIMA for Seasonal Data</h3>
    <div class="scenario-content">
        <p>If your data has a seasonal component (like the <code>AirPassengers</code> data), you need a **Seasonal ARIMA (SARIMA)** model. A SARIMA model has additional parameters (P, D, Q)m that model the seasonal part of the series, where 'm' is the frequency of the seasonality (e.g., m=12 for monthly data). The <code>auto.arima()</code> function will automatically detect seasonality and fit a SARIMA model if appropriate.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The built-in <code>LakeHuron</code> dataset contains measurements of the level of Lake Huron.</li>
            <li>Plot the data. Does it look stationary?</li>
            <li>Use <code>auto.arima()</code> to find the best ARIMA model for this data. What are the chosen orders (p, d, q)?</li>
            <li>Use the fitted model to forecast the lake level for the next 5 years and plot the forecast.</li>
        </ul>
    </div>
</div>
`,
          p4s7ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is forecasting weekly sales for a store in Karachi. The data has a clear upward trend (growth) and strong yearly seasonality (spikes during holidays). Instead of ARIMA, they choose to use **Exponential Smoothing**, a family of forecasting methods that are excellent at modeling trend and seasonal components directly.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s7ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s7ss4-methods">The Methods</button>
        <button class="tab-button" data-tab="p4s7ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s7ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s7ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Forecasting with Weighted Averages</h4>
        <p><strong>Exponential Smoothing</strong> methods produce forecasts based on weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, more recent observations are given more weight in the forecast.</p>
        <p><strong>Analogy:</strong> Exponential smoothing is like a **student studying for an exam**. They give the most weight to the material they learned yesterday, a little less weight to what they learned the day before, and very little weight to what they learned a month ago. The forecast is based on this exponentially weighted "memory" of the past.</p>
    </div>

    <div id="p4s7ss4-methods" class="tab-pane">
        <h4 class="subsection-title">A Method for Every Pattern</h4>
        <p>The family of methods builds in complexity to handle different patterns.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Simple Exponential Smoothing</strong>
                <p>The simplest form, used for data with **no trend or seasonality**. It's just a weighted average of past values.</p>
            </div>
            <div class="decision-branch">
                <strong>Holt's Linear Trend Method</strong>
                <p>An extension that adds a parameter to explicitly model the **trend** in the data. It can be additive or multiplicative.</p>
            </div>
            <div class="decision-branch">
                <strong>Holt-Winters' Seasonal Method</strong>
                <p>The most advanced form, which adds a third parameter to explicitly model **seasonality**. This is the most powerful and widely used method in the family.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s7ss4-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Forecast</h4>
        <p>A forecast plot shows the original data, the fitted values from the model, and the extrapolated forecast into the future, complete with prediction intervals.</p>
        <div class="plot-container">
            <div id="p4s7ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The black line is the historical data. The blue line shows the fitted values from the Holt-Winters model. The model has clearly captured both the upward trend and the yearly seasonal pattern. The orange line is the forecast for the next two years, and the shaded areas represent the 80% and 95% prediction intervals, showing the uncertainty of the forecast.</p>
        </div>
    </div>
    
    <div id="p4s7ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to forecast the <code>AirPassengers</code> dataset for the next 3 years using an appropriate exponential smoothing model.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(forecast)
data(AirPassengers)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the Model.</strong> Since the data has both trend and seasonality, we use the <code>HoltWinters()</code> function.
                <div class="code-container">
                    <pre><code class="language-r"># The ets() function is a more modern alternative that can choose the model automatically
# But HoltWinters() is great for learning the classic method.
hw_model <- HoltWinters(AirPassengers)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Generate the Forecast.</strong> We use the <code>forecast()</code> function, specifying the number of future periods (<code>h</code>). Since this is monthly data, h=36 is 3 years.
                <div class="code-container">
                    <pre><code class="language-r">hw_forecast <- forecast(hw_model, h = 36)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Plot and Interpret.</strong>
                <div class="code-container">
                    <pre><code class="language-r">plot(hw_forecast)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The plot shows that the Holt-Winters model has effectively captured the historical patterns and produced a forecast that continues both the upward trend and the strong seasonal cycle. The widening prediction intervals correctly reflect that uncertainty increases the further out we forecast.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: ets() for Automatic Model Selection</h3>
    <div class="scenario-content">
        <p>The <code>forecast</code> package includes a powerful function called <code>ets()</code> (Error, Trend, Seasonality). It automatically tests a variety of exponential smoothing models (including additive, multiplicative, and damped trend versions) and selects the one that best fits the data based on information criteria like AIC. For most applications, <code>ets()</code> is preferred over manually choosing the model.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>The built-in <code>co2</code> dataset is a time series with trend and seasonality.</li>
            <li>Use the <code>ets()</code> function from the <code>forecast</code> package to automatically find the best exponential smoothing model for this data.</li>
            <li>Use the <code>forecast()</code> function to generate a forecast for the next 24 months (2 years).</li>
            <li>Plot your forecast object. Does the forecast look reasonable? Does it capture the patterns you saw in the original data?</li>
        </ul>
    </div>
</div>
`,
          p4s7ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An operations team at a manufacturing plant in Karachi monitors a sensor that measures machine vibrations. A sudden, sharp spike in the data could indicate an imminent mechanical failure. 🇵🇰 They need an automated system that can analyze the time series data in real-time and flag these **anomalies** so that preventative maintenance can be performed.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s7ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s7ss5-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s7ss5-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s7ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Finding the "Needle in the Haystack"</h4>
        <p><strong>Anomaly Detection</strong> (or Outlier Detection) in time series is the task of identifying data points or patterns that deviate significantly from the expected behavior of the series. These anomalies often correspond to important, real-world events.</p>
        <p><strong>Analogy:</strong> Anomaly detection is like being a **security guard watching a live video feed**. Most of the time, the scene is normal and predictable. The guard's job is to spot the one moment that breaks the pattern—a person jumping a fence, a car driving the wrong way—which represents a critical event that needs attention.</p>
    </div>
    
    <div id="p4s7ss5-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Anomalies</h4>
        <p>A time series plot is the best way to visualize anomalies. The algorithm first models the "normal" behavior of the series and then flags any points that fall too far outside the expected range.</p>
        <div class="plot-container">
            <div id="p4s7ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The black line is the actual time series data. The blue shaded area represents the "normal" range as learned by the model from the historical data. The red points are the observations that fall outside this expected range and are therefore flagged as **anomalies**. This provides a clear and intuitive way to identify and investigate unusual events.</p>
        </div>
    </div>
    
    <div id="p4s7ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to find anomalous daily Wikipedia article views using the <code>tidyverse_cran_downloads</code> dataset. The <code>anomalize</code> package from the <code>tidymodels</code> ecosystem is perfect for this.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Prepare Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("anomalize")
library(anomalize)
library(tidyverse)
# Use the sample dataset from the package
data(tidyverse_cran_downloads)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Apply the <code>anomalize</code> workflow.</strong> The workflow consists of three main piped steps.
                <div class="code-container">
                    <pre><code class="language-r">anomalized_data <- tidyverse_cran_downloads %>%
  # 1. Decompose the time series into trend, seasonal, and remainder
  time_decompose(count, method = "stl") %>%
  # 2. Analyze the remainder component to find the anomalies
  anomalize(remainder) %>%
  # 3. Recompose the series and create the "recomposed_l1/l2" columns which
  #    represent the upper and lower bounds of the "normal" range.
  time_recompose()</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Visualize the Anomalies.</strong> The package provides a convenient plotting function.
                <div class="code-container">
                    <pre><code class="language-r"># This creates the plot seen in the Visualization tab
plot_anomalies(anomalized_data, time_recomposed = TRUE)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The <code>anomalize</code> package provides a powerful and tidy workflow for decomposing a time series, identifying anomalies in the remainder, and visualizing them in context. The resulting plot clearly highlights the specific days where download counts were unexpectedly high or low.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Adjusting Anomaly Sensitivity</h3>
    <div class="scenario-content">
        <p>The <code>anomalize()</code> function has a key argument, <code>alpha</code>, which controls the sensitivity of the anomaly detection (default is <code>alpha = 0.05</code>). A smaller <code>alpha</code> (e.g., 0.01) will make the "normal" range wider and will only flag more extreme events as anomalies. A larger <code>alpha</code> (e.g., 0.10) will make the range narrower and will be more sensitive. You should tune this parameter based on how many false positives or false negatives are acceptable for your specific business problem.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Take the built-in <code>co2</code> dataset.</li>
            <li>The <code>anomalize</code> workflow requires a two-column tibble with a date column and a value column. Convert the <code>co2</code> object to this format. (Hint: <code>co2_tbl <- as_tibble(co2) %>% mutate(date = as.Date(time(co2)))</code>).</li>
            <li>Run the full <code>time_decompose()</code> -> <code>anomalize()</code> -> <code>time_recompose()</code> pipeline on this new tibble.</li>
            <li>Use <code>plot_anomalies()</code> to visualize the results. Did the algorithm find any anomalous months in the CO2 data?</li>
        </ul>
    </div>
</div>
`,
          p4s7ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A junior analyst builds a forecast model. To test it, they train the model on 90% of their historical data and test it on the last 10%. This seems reasonable, but it's a single, arbitrary split. Their good performance could just be luck. A senior analyst advises them to use a more robust validation technique that respects the temporal nature of the data, like **backtesting**.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Robust Forecasting</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Time Series Best Practices</div>
            <div class="scenario">Moving from a simple forecast to a robust, trustworthy, and reproducible forecasting system.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chart-area"></i> 1. Always Visualize First</div>
                        <div class="scenario"><strong>The Rule:</strong> Never start modeling without plotting your time series first. A simple plot will reveal the most important patterns: trend, seasonality, outliers, and structural breaks. This visual inspection guides your entire modeling strategy.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-random"></i> 2. Respect the Arrow of Time</div>
                        <div class="scenario"><strong>The Rule:</strong> Never use a random k-fold cross-validation for time series data. This would involve training on future data to predict the past, which is impossible in the real world and leads to data leakage.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-history"></i> 3. Use Backtesting for Validation</div>
                        <div class="scenario"><strong>The Rule:</strong> Use a validation method that simulates how the model would have performed in the real world. **Backtesting** (or walk-forward validation) involves training your model on an initial period, forecasting the next period, then sliding the window forward and repeating the process.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-diagnostics"></i> 4. Check Your Residuals</div>
                        <div class="scenario"><strong>The Rule:</strong> The errors (residuals) of a good forecasting model should be uncorrelated and normally distributed (i.e., they should look like white noise). Use diagnostic plots (like an ACF plot of the residuals) to ensure there are no patterns left that your model has failed to capture.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>timetk</code> Package</h3>
    <div class="scenario-content">
        <p>The <code>timetk</code> package (part of the <code>tidymodels</code> ecosystem) provides a comprehensive and tidy framework for time series analysis. It has powerful functions for visualization (<code>plot_time_series()</code>), feature engineering (<code>tk_augment_*()</code>), and, most importantly, for performing robust backtesting with sliding or expanding windows. It's the modern, preferred tool for serious time series validation in R.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You have 5 years of monthly sales data (60 data points). You want to perform a simple backtest to evaluate a model that forecasts 3 months ahead.</li>
            <li>Describe the first training/testing split. How many data points would be in your initial training set? How many in the first test set?</li>
            <li>After evaluating on the first test set, how would you create the *second* training and testing sets for the next step of the backtest?</li>
        </ul>
    </div>
</div>
`,
          p4s8ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has a dataset of customer reviews. Before they can analyze the sentiment or topics, the raw text is a mess of inconsistent capitalization, punctuation, and common "stop words" (like "the", "is", "a") that add noise. They must apply a systematic preprocessing pipeline to clean and standardize the text, making it suitable for feature extraction.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Text Cleaning and Standardization Pipeline</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-stream"></i> The Preprocessing Pipeline</div>
            <div class="scenario">Transforming raw, messy text into a clean, analyzable format involves a sequence of standard steps.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-exchange-alt"></i> 1. Uniform Case</div>
                        <div class="scenario">Convert all text to a single case (usually lowercase) to ensure that words like "Great" and "great" are treated as the same word.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-puzzle-piece"></i> 2. Tokenization</div>
                        <div class="scenario">Break the text down from full sentences into its individual components, or "tokens." Most commonly, this means splitting the text into a list of individual words.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-trash"></i> 3. Stop Word Removal</div>
                        <div class="scenario">Remove extremely common words that carry little semantic meaning (e.g., "a", "the", "in", "is"). This helps the analysis focus on the more important words.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-cut"></i> 4. Stemming / Lemmatization</div>
                        <div class="scenario">Reduce words to their root form. **Stemming** is a crude heuristic that chops off word endings (e.g., "running" -> "run"). **Lemmatization** is a more advanced process that uses a dictionary to convert words to their true root form (e.g., "better" -> "good").</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">Data Scientist Step-by-Step Example</h3>
    <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
        <p><strong>Problem:</strong> We have raw review text and need to process it into a clean, "tidy" format of one word per row, with stopwords removed. The <code>tidytext</code> package is the ideal tool for this.</p>
    </div>
    <ol class="prose-list">
        <li><strong>Step 1: Load Packages and Data.</strong>
            <div class="code-container">
                <pre><code class="language-r"># install.packages("tidytext")
library(tidytext)
library(dplyr)
reviews <- tibble(
  id = 1:2,
  text = c("This is a FANTASTIC product! I love it.", 
           "The delivery was late, and it was broken.")
)</code></pre>
            </div>
        </li>
        <li><strong>Step 2: Tokenize the Text.</strong> The <code>unnest_tokens()</code> function is the workhorse of <code>tidytext</code>. It takes a column of text and transforms the data frame into one-token-per-row format.
            <div class="code-container">
                <pre><code class="language-r">tokenized_reviews <- reviews %>%
  unnest_tokens(output = word, input = text)</code></pre>
            </div>
        </li>
        <li><strong>Step 3: Remove Stop Words.</strong> We use an <code>anti_join()</code> with the built-in <code>stop_words</code> dataset from <code>tidytext</code>.
            <div class="code-container">
                <pre><code class="language-r">data(stop_words)
clean_reviews <- tokenized_reviews %>%
  anti_join(stop_words, by = "word")</code></pre>
            </div>
             <div class="plot-description">
                <p><strong>Conclusion:</strong> The final <code>clean_reviews</code> tibble is in a tidy format, with common noise words removed. This format is the perfect input for feature extraction techniques like TF-IDF or for performing sentiment analysis.</p>
            </div>
        </li>
    </ol>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Stemming with <code>SnowballC</code></h3>
    <div class="scenario-content">
        <p>The <code>tidytext</code> workflow can be easily extended to include stemming. You can install the <code>SnowballC</code> package and then pipe your anti-join result into a mutate step: <code>mutate(stem = wordStem(word))</code>. This will add a new column with the "stemmed" version of each word, further consolidating your text data.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a simple tibble with a single sentence: <code>my_text <- tibble(text = "R is a powerful language for statistical analysis and visualization.")</code></li>
            <li>Use <code>unnest_tokens()</code> to convert this sentence into a one-word-per-row tibble.</li>
            <li>Use <code>anti_join()</code> with the <code>stop_words</code> dataset to remove the common stop words.</li>
            <li>Which words are left after cleaning?</li>
        </ul>
    </div>
</div>
`,
          p4s8ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has cleaned and tokenized a large collection of documents. Now they face a critical problem: machine learning models don't understand words; they understand numbers. The analyst needs to convert the text into a meaningful numerical representation (a process called **vectorization**) that captures the importance of each word in each document.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s8ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s8ss2-methods">Bag-of-Words vs. TF-IDF</button>
        <button class="tab-button" data-tab="p4s8ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s8ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s8ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Turning Words into Numbers</h4>
        <p><strong>Feature Extraction</strong> (or vectorization) is the process of converting preprocessed text into a numerical format that can be used as input for a machine learning model. The most common output is a **Document-Term Matrix (DTM)**, where each row represents a document, each column represents a word (term) from the entire vocabulary, and each cell contains a numerical value.</p>
        <p><strong>Analogy:</strong> This process is like creating a **detailed nutritional label for a recipe**. The raw recipe (the text) is a list of ingredients. The DTM is a table that shows, for each recipe (document), exactly how much of each possible ingredient (term) it contains.</p>
    </div>

    <div id="p4s8ss2-methods" class="tab-pane">
        <h4 class="subsection-title">How to Fill the Cells?</h4>
        <p>The key question is what number to put in each cell of the DTM. The two most common methods are:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Bag-of-Words (Term Frequency)</strong>
                <p>The simplest method. The cell value is simply the raw **count** of how many times a term appears in a document. It's called "bag-of-words" because it ignores grammar and word order, treating each document as just a bag of its words.</p>
                <p><strong>Problem:</strong> Very common words (like "data" in a data science corpus) will get high counts everywhere and might be considered more important than they really are.</p>
            </div>
            <div class="decision-branch">
                <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>
                <p>A more advanced method that fixes the problem with Bag-of-Words. It calculates a weighted score that is high for words that are frequent in a single document but rare across all documents. It automatically down-weights common words and highlights the terms that are uniquely important to a specific document.</p>
                <div class="math-foundation" style="margin-top:0.5rem;">$$ \\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t) $$</div>
            </div>
        </div>
    </div>

    <div id="p4s8ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing TF-IDF in Action</h4>
        <p>This plot shows the top TF-IDF words for several classic novels. Notice how the identified words are highly characteristic of each book's specific topic.</p>
        <div class="plot-container">
            <div id="p4s8ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The TF-IDF score identifies words that are uniquely important to each book. For "Moby Dick," the top words are about whaling ("whale," "sea"). For "Pride and Prejudice," they are about social interactions ("darcy," "elizabeth," "mrs"). The algorithm successfully extracts the most distinguishing terms for each document.</p>
        </div>
    </div>
    
    <div id="p4s8ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have a collection of book chapters and we want to find the most important term for each chapter using TF-IDF.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Tokenize the Text.</strong> We first convert our text into a tidy, one-word-per-row format.
                <div class="code-container">
                    <pre><code class="language-r">library(tidytext)
library(janeaustenr) # For sample data
# Create a tidy data frame of words per book and chapter
austen_words <- austen_books() %>%
  unnest_tokens(word, text)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Calculate Term Frequencies.</strong> We count how many times each word appears in each document (in this case, a book is our document).
                <div class="code-container">
                    <pre><code class="language-r">word_counts <- austen_words %>%
  count(book, word, sort = TRUE)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Calculate TF-IDF.</strong> The <code>bind_tf_idf()</code> function takes the tidy word count data frame and calculates tf, idf, and tf-idf.
                <div class="code-container">
                    <pre><code class="language-r">book_tf_idf <- word_counts %>%
  bind_tf_idf(term = word, document = book, n = n)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Analyze the Results.</strong> We can now see the most important words for each book.
                <div class="code-container">
                    <pre><code class="language-r"># Look at the words with the highest tf-idf
book_tf_idf %>%
  arrange(desc(tf_idf))</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Word Embeddings (The Modern Approach)</h3>
    <div class="scenario-content">
        <p>While TF-IDF is a powerful technique, the state-of-the-art approach for representing words is **word embeddings** (like Word2Vec, GloVe, or those from BERT). These are dense vector representations of words where the vectors capture semantic meaning. For example, in a well-trained embedding space, the vector for "king" minus "man" plus "woman" would be very close to the vector for "queen". These rich representations are the standard for most modern NLP tasks.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Follow the step-by-step example using the <code>austen_books()</code> data.</li>
            <li>After calculating the book_tf_idf, use <code>group_by(book)</code> and <code>slice_max(tf_idf, n = 5)</code> to find the top 5 most important words for each of Jane Austen's novels.</li>
            <li>Do these words seem to accurately capture the unique themes of each book?</li>
        </ul>
    </div>
</div>
`,
          p4s8ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A news organization in Karachi wants to automatically categorize incoming articles into topics like "Sports," "Business," and "Technology." 🇵🇰 Manually reading and tagging thousands of articles per day is impossible. They need to build a **Text Classification** model that can learn from a set of pre-labeled examples to perform this task automatically.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s8ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s8ss3-viz">The End-to-End Workflow</button>
        <button class="tab-button" data-tab="p4s8ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s8ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Assigning Labels to Text</h4>
        <p><strong>Text Classification</strong> is a supervised machine learning task where the goal is to assign a predefined category or label to a piece of text. It's one of the most common and commercially valuable applications of NLP.</p>
        <p><strong>Analogy:</strong> Building a text classifier is like training a **librarian's assistant**. You first show the assistant hundreds of examples of books and tell them which shelf each one belongs on ("this is a Science Fiction book," "this is a History book"). After learning from these examples, the assistant can look at a brand new, unseen book and correctly place it on the appropriate shelf.</p>
    </div>
    
    <div id="p4s8ss3-viz" class="tab-pane">
        <h4 class="subsection-title">A Blueprint for Text Classification</h4>
        <p>Every text classification project follows the same fundamental workflow, combining the preprocessing and feature extraction steps we've already learned with a standard machine learning model.</p>
        <div class="plot-container">
            <div id="p4s8ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The process flows from left to right. Raw text is first cleaned and standardized through **Preprocessing**. The clean text is then converted into a numerical Document-Term Matrix (DTM) via **Feature Extraction** (e.g., TF-IDF). This DTM is then used as the input to train a standard **Classification Model** (like Naive Bayes or a Regularized Regression), which can then make predictions on new, unseen text.</p>
        </div>
    </div>
    
    <div id="p4s8ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We will build a simple Naive Bayes model to classify news articles into one of 20 different newsgroups using the <code>newsgroup</code> dataset from the <code>textdata</code> package.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load and Prepare Data.</strong> We load the data, create a Document-Term Matrix using TF-IDF, and split into training/testing sets.
                <div class="code-container">
                    <pre><code class="language-r"># install.packages(c("textdata", "tidymodels"))
library(tidymodels)
library(textdata)
# ... code to load and split data ...
# ... code to create a TF-IDF recipe ...
# (This is a complex workflow, so we'll focus on the concepts)
# Let's assume we have a 'train_dtm' and 'test_dtm'
                    </code></pre>
                </div>
            </li>
            <li><strong>Step 2: Define and Train the Model.</strong> We define a Naive Bayes model specification using <code>tidymodels</code>.
                <div class="code-container">
                    <pre><code class="language-r">nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes") # Use the naivebayes package

nb_workflow <- workflow() %>%
  add_recipe(your_tfidf_recipe) %>%
  add_model(nb_spec)

nb_fit <- fit(nb_workflow, data = training_data)
                    </code></pre>
                </div>
            </li>
            <li><strong>Step 3: Evaluate the Model.</strong> We use the fitted model to make predictions on the test set and calculate accuracy.
                <div class="code-container">
                    <pre><code class="language-r">test_predictions <- predict(nb_fit, new_data = test_data)
# ... code to calculate accuracy and confusion matrix ...
                    </code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> By following the standard NLP workflow, we can train a model that effectively classifies text into predefined categories. The <code>tidymodels</code> framework provides a powerful and consistent interface for building these complex pipelines.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use a <code>tidymodels</code> Workflow</h3>
    <div class="scenario-content">
        <p>Text classification pipelines can be complex, involving many preprocessing and feature engineering steps. The <code>tidymodels</code> framework is perfect for managing this complexity. You can create a <code>recipe</code> that handles all the tokenization and TF-IDF steps, and then bundle it with a model specification into a single <code>workflow()</code> object. This ensures that the exact same preprocessing steps are applied consistently to your training, validation, and testing data, preventing data leakage and making your results reliable.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are building a spam filter. What is your target variable? What are your features?</li>
            <li>Describe the three main steps of the workflow you would use to build this classifier.</li>
            <li>Which classification model that we've learned about would be a particularly good fit for this problem, and why?</li>
        </ul>
    </div>
</div>
`,
          p4s8ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A company has collected thousands of open-ended survey responses. They don't have any predefined categories to classify them into, but they want to discover the main, underlying themes or topics that customers are talking about. This is an unsupervised task that is perfect for **Topic Modeling**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s8ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s8ss4-viz">Interpreting Topics</button>
        <button class="tab-button" data-tab="p4s8ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s8ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Discovering Latent Themes</h4>
        <p><strong>Topic Modeling</strong> is an unsupervised machine learning technique used to discover the abstract "topics" that occur in a collection of documents. The most common algorithm for this is **Latent Dirichlet Allocation (LDA)**.</p>
        <p>LDA is a generative statistical model that treats every document as a mixture of topics, and every topic as a mixture of words.</p>
        <p><strong>Analogy:</strong> Imagine you have a collection of newspaper articles and you want to discover the main topics (e.g., "Politics", "Sports", "Finance"). LDA is like a **librarian who has lost the labels for the sections**. They assume each article is a mix of topics and each topic uses certain words frequently. By looking at the patterns of which words appear together across all the articles, they can reconstruct the original sections, figuring out that articles with "government" and "election" belong to "Politics," and those with "touchdown" and "playoffs" belong to "Sports."</p>
    </div>
    
    <div id="p4s8ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Outputs of an LDA Model</h4>
        <p>An LDA model produces two key outputs: the per-topic word probabilities (beta) and the per-document topic probabilities (gamma).</p>
        <div class="plot-container">
            <div id="p4s8ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This plot shows the **per-topic word probabilities**. Each bar chart represents one of the discovered topics. The bars show the words that are most probable (most important) for that topic. By looking at the top words, the human analyst can interpret the theme of the topic. For example, Topic 1 is clearly about "Politics," while Topic 2 is about "Sports."</p>
        </div>
    </div>
    
    <div id="p4s8ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to discover the latent topics in a dataset of Associated Press news articles using the <code>topicmodels</code> package.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Prepare the Document-Term Matrix (DTM).</strong> LDA requires a DTM as input, where rows are documents, columns are terms, and cells are word counts.
                <div class="code-container">
                    <pre><code class="language-r">library(topicmodels)
data("AssociatedPress")
# The AssociatedPress object is already a DocumentTermMatrix
dtm <- AssociatedPress</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the LDA Model.</strong> We specify the DTM and the number of topics (<code>k</code>) we want to find. Choosing <code>k</code> is a key challenge, often done through experimentation.
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42)
lda_model <- LDA(dtm, k = 4, control = list(seed = 1234))</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Interpret the Topics (Beta).</strong> We use <code>tidytext::tidy()</code> to extract the per-topic-per-word probabilities and find the top terms for each topic.
                <div class="code-container">
                    <pre><code class="language-r">library(tidytext)
topics <- tidy(lda_model, matrix = "beta")
# Find top 10 terms for each topic
top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)
# A ggplot of this 'top_terms' data frame would create the visualization from the Viz tab.</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Choosing <code>k</code></h3>
    <div class="scenario-content">
        <p>How do you choose the number of topics, <code>k</code>? This is one of the hardest parts of topic modeling. It's often a mix of art and science. You can use statistical metrics like "perplexity" to guide your choice, but often the best approach is to try several different values of <code>k</code> and choose the one that produces the most coherent and interpretable topics from a human perspective.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Follow the step-by-step example to fit an LDA model to the <code>AssociatedPress</code> data.</li>
            <li>Use the code provided to extract the <code>top_terms</code> for each of the 4 topics.</li>
            <li>Look at the top words for each topic. Can you give each topic a human-readable name based on its most probable words? (e.g., "Government," "Finance," etc.).</li>
        </ul>
    </div>
</div>
`,
          p4s8ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A company wants to track customer sentiment about their brand on social media in real-time. Manually reading thousands of tweets is impossible. They need an automated system that can take a piece of text (like a tweet) and assign it a sentiment score (e.g., positive, negative, or neutral).</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s8ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s8ss5-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s8ss5-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s8ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Determining the Emotional Tone of Text</h4>
        <p><strong>Sentiment Analysis</strong> is a subfield of NLP that deals with identifying and categorizing opinions expressed in a piece of text. The most common form is **polarity detection**, which classifies text as positive, negative, or neutral.</p>
        <p><strong>Analogy:</strong> A sentiment analysis model is like a **critic reading a movie review**. The critic scans the text, identifies emotionally charged words ("amazing," "brilliant," "dull," "terrible"), and aggregates them to produce a final verdict: "Fresh" (positive) or "Rotten" (negative).</p>
        <p>The simplest and most common approach is **lexicon-based sentiment analysis**, which uses a pre-defined dictionary (a "lexicon") of words that have been scored as positive or negative.</p>
    </div>
    
    <div id="p4s8ss5-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Word Contributions</h4>
        <p>A powerful way to visualize sentiment is to create a bar chart showing the most common positive and negative words in a corpus of text.</p>
        <div class="plot-container">
            <div id="p4s8ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This plot shows the top 10 words contributing to positive and negative sentiment in a collection of Jane Austen's novels. The words are stemmed to their root form. It clearly shows that words like "good," "love," and "happy" are the main drivers of positive sentiment, while words like "miss" and "poor" drive negative sentiment.</p>
        </div>
    </div>
    
    <div id="p4s8ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to calculate the overall sentiment for each of Jane Austen's novels using a lexicon-based approach with <code>tidytext</code>.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Tokenize the Text.</strong> We start by getting our data into a tidy format of one word per row.
                <div class="code-container">
                    <pre><code class="language-r">library(tidytext)
library(janeaustenr)
library(dplyr)
tidy_books <- austen_books() %>%
  unnest_tokens(word, text)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Join with a Sentiment Lexicon.</strong> <code>tidytext</code> provides several sentiment lexicons. We will use the "bing" lexicon, which labels words as either "positive" or "negative".
                <div class="code-container">
                    <pre><code class="language-r">bing_lexicon <- get_sentiments("bing")
book_sentiments <- tidy_books %>%
  inner_join(bing_lexicon, by = "word")</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Calculate Net Sentiment.</strong> We count the number of positive and negative words for each book, and then calculate the difference.
                <div class="code-container">
                    <pre><code class="language-r">library(tidyr) # For spread()
net_sentiment <- book_sentiments %>%
  count(book, sentiment) %>%
  spread(sentiment, n, fill = 0) %>% # Make 'positive' and 'negative' their own columns
  mutate(net_sentiment = positive - negative)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The final <code>net_sentiment</code> data frame gives us a simple score for the overall emotional tone of each novel, with higher scores being more positive. This simple lexicon-based approach provides a powerful and fast baseline for sentiment analysis.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Handling Negation</h3>
    <div class="scenario-content">
        <p>A major weakness of simple lexicon-based methods is their inability to handle negation. The sentence "This movie was not good" contains the positive word "good," and a simple model would incorrectly score it as positive. More advanced techniques involve analyzing n-grams (sequences of words) to detect patterns like "not good" and reverse the sentiment score accordingly.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Follow the step-by-step example to calculate the <code>book_sentiments</code>.</li>
            <li>Use <code>count(word, sentiment, sort = TRUE)</code> on the <code>book_sentiments</code> data frame. What is the most common positive word in Jane Austen's novels? What is the most common negative word?</li>
            <li>Create the plot from the "Visualization" tab by filtering for the top 10 positive and negative words and using <code>ggplot2</code> to create a faceted bar chart.</li>
        </ul>
    </div>
</div>
`,
          p4s8ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst wants to build a chatbot that can understand and generate human-like sentences. A simple Bag-of-Words model is not enough; the model must understand grammar, context, and the sequential order of words. The sentence "dog bites man" means something completely different from "man bites dog." This requires advanced **Sequence Models** like RNNs or Transformers.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s8ss6-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s8ss6-viz">RNN vs. Transformer</button>
        <button class="tab-button" data-tab="p4s8ss6-usecase">Using Pre-trained Models</button>
    </div>

    <div id="p4s8ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">Understanding Context and Order</h4>
        <p><strong>Sequence Models</strong> are a class of deep learning models designed specifically to handle sequential data where order is critical, such as text or time series.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>RNNs / LSTMs</strong>
                <p>Recurrent Neural Networks process data sequentially, one token at a time, maintaining a "memory" (hidden state) that is passed from one step to the next. They are good at capturing local context.</p>
            </div>
            <div class="decision-branch">
                <strong>Transformers (State-of-the-Art)</strong>
                <p>A more modern and powerful architecture that processes all tokens in a sequence at the same time. It uses a mechanism called **self-attention** to weigh the importance of all other words in the input when processing a given word. This allows it to capture complex, long-range dependencies and context far more effectively than RNNs.</p>
                <p><strong>Analogy:</strong> An RNN reads a book one word at a time. A Transformer reads the entire page at once, allowing it to understand how a word at the beginning of a paragraph relates to a word at the end.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s8ss6-viz" class="tab-pane">
        <h4 class="subsection-title">Sequential vs. Parallel Processing</h4>
        <p>This diagram shows the conceptual difference in how RNNs and Transformers process a sentence.</p>
        <div class="plot-container">
            <div id="p4s8ss6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **RNN** processes the sentence one word at a time, passing information forward sequentially. The **Transformer**, via its self-attention mechanism, is able to consider the relationships between all words simultaneously, leading to a much richer contextual understanding.</p>
        </div>
    </div>
    
    <div id="p4s8ss6-usecase" class="tab-pane">
        <h4 class="subsection-title">Using a Pre-trained Transformer in R</h4>
        <p>Training a large transformer model from scratch is computationally massive. The common practice is to use powerful **pre-trained models** that have already been trained on huge text corpora by companies like Google or OpenAI. The <code>transformers</code> package from Hugging Face provides access to thousands of these models.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# This requires installing Python and the transformers library.
# The R 'reticulate' package allows R to talk to Python.
# install.packages("reticulate")
# reticulate::install_miniconda()
# reticulate::py_install("transformers")

library(reticulate)
transformers <- import("transformers")

# Load a pre-trained model and tokenizer for sentiment analysis
classifier <- transformers$pipeline("sentiment-analysis")

# Use the model to classify text
classifier("R is a fantastic language for data science.")
# Expected Output: [[{'label': 'POSITIVE', 'score': 0.99...}]]

classifier("The documentation was very confusing.")
# Expected Output: [[{'label': 'NEGATIVE', 'score': 0.99...}]]
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Fine-Tuning</h3>
    <div class="scenario-content">
        <p>The real power of pre-trained models comes from **fine-tuning**. You can take a large language model that has been trained on general text from the internet and continue its training on your own specific, domain-related data (e.g., legal documents or medical transcripts). This adapts the general-purpose model to become a specialist expert in your domain, achieving state-of-the-art performance with much less data than training from scratch.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's use a different pre-trained pipeline from the <code>transformers</code> library.</p>
        <ul class="prose-list">
            <li>Following the setup from the use case example, load the <code>transformers</code> library.</li>
            <li>Load a pre-trained pipeline for text generation: <code>generator <- transformers$pipeline("text-generation", model="distilgpt2")</code>.</li>
            <li>Use the generator to complete a sentence. The first argument is the starting text, and <code>max_length</code> controls how long the output can be: <code>generator("In the field of data science, R is", max_length = 30)</code>.</li>
            <li>Run it a few times. What kind of text does the pre-trained model generate?</li>
        </ul>
    </div>
</div>
`,
          p4s10ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A marketing manager in Karachi runs a promotional email campaign. 🇵🇰 They observe that customers who received the email had an average spend of Rs. 2,500, while those who didn't spent Rs. 1,500. Can they conclude the campaign *caused* a Rs. 1,000 increase in spending? Not necessarily. It's possible that the email was only sent to already engaged, high-spending customers. To untangle correlation from causation, we need the rigorous logic of the <strong>Potential Outcomes Framework</strong>.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s10ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s10ss1-viz">The Fundamental Problem</button>
        <button class="tab-button" data-tab="p4s10ss1-methods">Experimental vs. Observational Data</button>
    </div>

    <div id="p4s10ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Thinking in Parallel Universes</h4>
        <p>The Potential Outcomes Framework, also known as the Rubin Causal Model, is the foundation of modern causal inference. It asks us to think about two potential states for each individual in our study:</p>
        <ul class="prose-list">
            <li><strong>Potential Outcome if Treated ($Y_i(1)$):</strong> What would have happened to this individual if they received the treatment (e.g., saw the ad)?</li>
            <li><strong>Potential Outcome if Untreated ($Y_i(0)$):</strong> What would have happened to this same individual if they did *not* receive the treatment (the counterfactual)?</li>
        </ul>
        <p>The **causal effect** for an individual is the difference between these two potential outcomes: $Y_i(1) - Y_i(0)$.</p>
        <p><strong>Analogy:</strong> This framework is like the movie **"Sliding Doors."** For one person, we want to know the outcome in two parallel universes: one where they caught the train (were treated) and one where they missed it (were not treated). The difference in their life's path is the causal effect of catching the train.</p>
    </div>

    <div id="p4s10ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Fundamental Problem of Causal Inference</h4>
        <p>The core challenge is that for any given individual, we can only ever observe **one** of their potential outcomes. We can never simultaneously see what would have happened both with and without the treatment. This is the "fundamental problem" that all causal inference methods try to solve.</p>
        <div class="plot-container">
            <div id="p4s10ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> For each customer, one of their potential outcomes is factual (the green checkmark) and the other is a counterfactual that we can never observe (the grey question mark). Because we can't calculate the individual causal effect, our goal is to estimate the **Average Treatment Effect (ATE)** across the entire population by creating comparable groups.</p>
        </div>
    </div>
    
    <div id="p4s10ss1-methods" class="tab-pane">
        <h4 class="subsection-title">Two Types of Data</h4>
        <div class="decision-branches" style="align-items- flex-start;">
            <div class="decision-branch">
                <strong><i class="fas fa-vials"></i> Experimental Data (Randomized Controlled Trials)</strong>
                <p>In an RCT or A/B test, we randomly assign individuals to the treatment or control group. This randomization ensures that, on average, the two groups are identical in all ways (both observed and unobserved) *except for the treatment*. Therefore, any difference in their average outcomes can be confidently attributed as a causal effect.</p>
            </div>
            <div class="decision-branch">
                <strong><i class="fas fa-binoculars"></i> Observational Data</strong>
                <p>In observational data, we do not control who receives the treatment. Individuals self-select or are selected based on other factors. The treatment and control groups are likely to be different in many ways (e.g., the customers who received the email were already more active). We cannot simply compare their means. We must use advanced quasi-experimental methods to try and control for these confounding variables.</p>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: "No Causation Without Manipulation"</h3>
    <div class="scenario-content">
        <p>A core tenet of this framework is that you can only measure the causal effect of something that is, in principle, manipulable. You can measure the causal effect of a drug (you can give it or not give it). You cannot, however, measure the causal effect of a person's birthplace, because it's an immutable characteristic that cannot be randomly assigned. Attributes like race or gender are often used as proxies for the causal effects of societal systems or biases, but they are not themselves "treatments."</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>A study finds that people who own a home gym are, on average, healthier than people who don't.</li>
            <li>In the Potential Outcomes Framework, what is the "treatment"? What is the outcome?</li>
            <li>Is this experimental or observational data?</li>
            <li>Why would it be wrong to conclude that buying a home gym *causes* better health based on this simple comparison? Name at least two potential confounding variables.</li>
        </ul>
    </div>
</div>
`,
          p4s10ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is studying the effect of a new training program on employee productivity using observational data. They know that employees who chose to enroll in the program are likely different from those who didn't (e.g., more motivated, higher-skilled). A simple comparison would be biased. To estimate the true causal effect, they need to create a "control group" of non-enrolled employees that is as similar as possible to the "treatment group" across all other observable characteristics.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s10ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s10ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s10ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s10ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Creating an Apples-to-Apples Comparison</h4>
        <p><strong>Matching</strong> is a technique used in observational studies to reduce bias by creating treatment and control groups that are balanced on a set of observed confounding variables (covariates). **Propensity Score Matching (PSM)** is the most common method.</p>
        <p><strong>Analogy:</strong> PSM is like being a **casting director for a clinical trial**. You have a group of people who chose to take a new drug (the treatment group). To create a fair control group, you can't just pick random people. Instead, you search through a large pool of people who *didn't* take the drug and, for each person in your treatment group, you find their "twin" or "doppelgänger"—someone with the same age, gender, health status, etc. This creates a balanced control group, allowing for a more credible comparison.</p>
    </div>
    
    <div id="p4s10ss2-viz" class="tab-pane">
        <h4 class="subsection-title">The Propensity Score</h4>
        <p>Instead of matching on dozens of individual covariates, PSM collapses them all into a single number: the **propensity score**.
            <ul class="prose-list"></ul>
                <li>First, you build a predictive model (usually logistic regression) to predict the probability of an individual receiving the treatment, based on all their pre-treatment characteristics.</li>
                <li>The predicted probability from this model is the individual's propensity score.</li>
                <li>You can then match treated individuals to untreated individuals who had a very similar propensity score.</li>
            </ul>
        </p>
        <div class="plot-container">
            <div id="p4s10ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The plot shows the distribution of propensity scores for the treatment (blue) and control (orange) groups **before and after matching**. Before matching, the distributions are very different, indicating the groups are not comparable. After matching, the distributions are nearly identical, showing that we have successfully created a control group that is balanced and comparable to our treatment group.</p>
        </div>
    </div>
    
    <div id="p4s10ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to estimate the causal effect of a hypothetical treatment using the <code>lalonde</code> dataset, which contains data from a real job training program. We will use the <code>MatchIt</code> package.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("MatchIt")
library(MatchIt)
data(lalonde)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Perform the Matching.</strong> We use the <code>matchit()</code> function, specifying a formula where the treatment variable is on the left and the covariates to match on are on the right.
                <div class="code-container">
                    <pre><code class="language-r"># treat=1 are the treated individuals, treat=0 are the controls
match_result <- matchit(treat ~ age + educ + race + married + re74 + re75,
                        data = lalonde, method = "nearest")</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Check for Balance.</strong> We use the <code>summary()</code> function on the result to see if the matching was successful in balancing the covariates. A good result will show "Std. Mean Diff." values close to zero.
                <div class="code-container">
                    <pre><code class="language-r">summary(match_result)
plot(summary(match_result)) # A plot to visualize the balance</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Extract Matched Data and Estimate Effect.</strong> We get the new, balanced dataset and can now compare the outcomes.
                <div class="code-container">
                    <pre><code class="language-r">matched_data <- match.data(match_result)
# Fit a simple model on the matched data to get the treatment effect
effect_model <- lm(re78 ~ treat, data = matched_data)
summary(effect_model)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The coefficient for <code>treat</code> in the final model represents our estimate of the Average Treatment Effect on the Treated (ATT). By matching, we have created a more credible estimate of the program's true causal impact on 1978 earnings than a naive comparison would have provided.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Unconfoundedness Assumption</h3>
    <div class="scenario-content">
        <p>Propensity score matching relies on a critical, untestable assumption called **unconfoundedness** (or "selection on observables"). It assumes that you have measured and included all the confounding variables that affect both treatment assignment and the outcome. If there is an important *unmeasured* confounder (e.g., "motivation"), your results can still be biased. This is the fundamental limitation of all methods based on observational data.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>lalonde</code> dataset and the <code>match_result</code> object from the example.</li>
            <li>The <code>plot(summary(match_result))</code> command creates a "Love plot." What does this plot show? What do you want to see in this plot to be confident that your matching was successful?</li>
            <li>Extract the <code>matched_data</code> again. Run a t-test to compare the 1978 earnings (<code>re78</code>) between the treatment and control groups: <code>t.test(re78 ~ treat, data = matched_data)</code>. What is the estimated difference in means? Is it statistically significant?</li>
        </ul>
    </div>
</div>
`,
          p4s10ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst wants to estimate the causal effect of a new law on city-wide accident rates. They can't run a randomized experiment. However, they know the law was only passed in some cities, and they have data from before and after the law was enacted for all cities. This specific setup allows them to use a powerful **quasi-experimental method** called Difference-in-Differences to get a credible causal estimate.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s10ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s10ss3-methods">Key Methods</button>
    </div>

    <div id="p4s10ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Finding Natural Experiments</h4>
        <p><strong>Quasi-experimental methods</strong> are a set of techniques used to estimate causal effects from observational data by finding a "natural experiment"—a situation where some external event or rule creates a source of variation that is "as if" random, without the researcher actually performing a randomization.</p>
        <p><strong>Analogy:</strong> Imagine you are an **astronomer**. You can't perform experiments on stars. You can't randomly assign one star to explode and another to be a control. Instead, you must patiently observe the universe and wait for a "natural experiment" to happen, like a supernova. By carefully observing the "before" and "after" and comparing it to other stars where nothing happened, you can make causal inferences about the effects of the supernova.</p>
    </div>
    
    <div id="p4s10ss3-methods" class="tab-pane">
        <h4 class="subsection-title">A Toolkit for Natural Experiments</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Difference-in-Differences (DiD)</strong>
                <p><strong>Use Case:</strong> You have data from before and after an intervention for both a treatment group and a control group. DiD estimates the effect by comparing the "before-after" difference in the treatment group to the "before-after" difference in the control group.</p>
            </div>
            <div class="decision-branch">
                <strong>Regression Discontinuity (RD)</strong>
                <p><strong>Use Case:</strong> The treatment is assigned based on a sharp cutoff in a continuous variable (e.g., students with a test score >= 80 get a scholarship). RD estimates the effect by comparing the outcomes of individuals just barely above and below the cutoff.</p>
            </div>
            <div class="decision-branch">
                <strong>Instrumental Variables (IV)</strong>
                <p><strong>Use Case:</strong> You have an unmeasured confounder, but you can find an "instrument"—a variable that affects the treatment but is not otherwise related to the outcome. IV is a more advanced technique to control for unobserved bias.</p>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Parallel Trends Assumption</h3>
    <div class="scenario-content">
        <p>The key assumption for a Difference-in-Differences design is **parallel trends**. This means that, in the absence of the treatment, the treatment and control groups would have followed the same trend over time. You can (and should) visually check this assumption by plotting the trends for both groups in the pre-treatment period. If the lines are parallel before the intervention, your DiD estimate is much more credible.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>A university wants to know the causal effect of receiving a prestigious scholarship on future income. They have data on all applicants. The scholarship is awarded to everyone with an entrance exam score of 95 or higher. Which quasi-experimental method would be most appropriate here? Why?</li>
            <li>The city of Karachi introduces a new traffic law, but the neighboring city of Hyderabad does not. You have accident data for both cities for the years before and after the law was introduced. Which quasi-experimental method could you use to estimate the causal effect of the law? What is the critical assumption you would need to check?</li>
        </ul>
    </div>
</div>
`,
          p4s10ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A public health researcher knows that a mandatory education program (the treatment) leads to a decrease in smoking rates (the outcome). However, they want to understand the *mechanism*. Does the program work by increasing people's knowledge of health risks? Or does it work by increasing their willpower? They need to test if the effect of the program is **mediated** by an intermediate variable like "health knowledge."</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s10ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s10ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s10ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s10ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Understanding the Causal Pathway</h4>
        <p><strong>Mediation Analysis</strong> is a statistical technique used to investigate the underlying mechanism or process by which one variable influences another. It helps to explain *how* or *why* an independent variable (X) affects a dependent variable (Y) by introducing a third, **mediator** variable (M).</p>
        <p><strong>Analogy:</strong> Mediation analysis is like understanding a **chain of dominoes**. You know that pushing the first domino (X) causes the last domino (Y) to fall. Mediation analysis is the process of checking to see if the first domino caused the *second* domino (the mediator, M) to fall, which in turn caused the last domino to fall. It tests if the effect is transmitted *through* the mediator.</p>
    </div>
    
    <div id="p4s10ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Causal Diagram</h4>
        <p>The relationships in a mediation analysis are typically represented by a path diagram.</p>
        <div class="plot-container">
            <div id="p4s10ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This diagram shows the three key pathways. **Path c** is the **total effect** of X on Y. **Path a** is the effect of X on the mediator M. **Path b** is the effect of the mediator M on Y, controlling for X. The **mediated effect** (or indirect effect) is the product of paths a and b (a*b). **Path c'** (c-prime) is the **direct effect** of X on Y after accounting for the mediator.</p>
        </div>
    </div>
    
    <div id="p4s10ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to test if the effect of education on income is mediated by job skill level, using simulated data. We will use the classic Baron-Kenny approach, which involves fitting a series of regression models.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Test the Total Effect (Path c).</strong> First, we fit a regression to see if there is a significant total effect of X on Y.
                <div class="code-container">
                    <pre><code class="language-r"># H0: Education has no total effect on income.
model_c <- lm(income ~ education, data = simulated_data)
summary(model_c) # Path c must be significant to proceed.</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Test Path a.</strong> We check if X has a significant effect on the mediator M.
                <div class="code-container">
                    <pre><code class="language-r"># H0: Education has no effect on job skills.
model_a <- lm(job_skills ~ education, data = simulated_data)
summary(model_a) # Path a must be significant.</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Test Path b and Path c'.</strong> We fit a final model that includes both X and M as predictors of Y.
                <div class="code-container">
                    <pre><code class="language-r"># H0_b: Job skills have no effect on income, controlling for education.
# H0_c_prime: Education has no direct effect on income, controlling for job skills.
model_b_c_prime <- lm(income ~ education + job_skills, data = simulated_data)
summary(model_b_c_prime) # Path b must be significant.</code></pre>
                </div>
            </li>
             <li><strong>Step 4: Formulate a Conclusion.</strong>
                <div class="plot-description">
                    <p><strong>Conclusion:</strong> If paths a, b, and c are all significant, we have evidence of mediation. If the effect of <code>education</code> (path c') becomes non-significant or much smaller in the final model compared to the first model, it suggests **full mediation**—the effect of education on income flows almost entirely *through* job skills. If path c' remains significant but is smaller, it suggests **partial mediation**.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use the <code>mediation</code> Package</h3>
    <div class="scenario-content">
        <p>The Baron-Kenny method is a good way to learn, but the modern and more statistically powerful approach is to use a package specifically designed for mediation, like <code>mediation</code>. It can estimate the indirect and direct effects more formally and provides confidence intervals for the mediated effect using bootstrapping, giving you a more robust result.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the built-in <code>mtcars</code> dataset, let's test a plausible mediation hypothesis: The effect of the number of cylinders (<code>cyl</code>) on fuel efficiency (<code>mpg</code>) is mediated by the car's weight (<code>wt</code>). The logic is that more cylinders lead to a heavier car, which in turn leads to lower MPG.</p>
        <ul class="prose-list">
            <li>Follow the four steps of the Baron-Kenny method:
                <ul class="prose-list"></ul>
                    <li>Fit a model for path c: <code>lm(mpg ~ cyl, data = mtcars)</code>. Is it significant?</li>
                    <li>Fit a model for path a: <code>lm(wt ~ cyl, data = mtcars)</code>. Is it significant?</li>
                    <li>Fit the final model for paths b and c': <code>lm(mpg ~ cyl + wt, data = mtcars)</code>. Is the <code>wt</code> coefficient (path b) significant?</li>
                    <li>Compare the coefficient for <code>cyl</code> in the first and third models. Does it get smaller? What can you conclude about the role of <code>wt</code> as a mediator?</li>
                </ul>
            </li>
        </ul>
    </div>
</div>
`,
          p4s10ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A junior analyst uses propensity score matching and finds a significant causal effect. However, they failed to check for covariate balance after matching and did not state the critical "unconfoundedness" assumption. A senior colleague reviews the work and explains that causal claims require a higher standard of evidence and transparency than purely predictive modeling, and must be presented with humility and a clear statement of their underlying assumptions.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Credible Causal Claims</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-gavel"></i> Causal Inference Best Practices</div>
            <div class="scenario">Making claims about cause and effect is a serious endeavor. Following these practices ensures your analysis is as robust and honest as possible.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-tasks"></i> 1. State Your Assumptions Clearly</div>
                        <div class="scenario"><strong>The Rule:</strong> All quasi-experimental methods rely on strong, untestable assumptions (e.g., parallel trends for DiD, unconfoundedness for matching). You **must** explicitly state these assumptions in your report. Be honest about the limitations of your data.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chart-bar"></i> 2. Perform Diagnostic Checks</div>
                        <div class="scenario"><strong>The Rule:</strong> Don't just run the model and report the result. Perform the relevant diagnostic checks for your chosen method. For matching, check for covariate balance. For RD, check for manipulation around the cutoff. For DiD, plot the parallel trends.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-question"></i> 3. Conduct Sensitivity Analyses</div>
                        <div class="scenario"><strong>The Rule:</strong> A robust causal analysis includes a sensitivity analysis. This involves testing how your results would change if your key assumptions were slightly violated. It shows how fragile or robust your conclusions are.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-balance-scale-right"></i> 4. Prefer Simpler, More Transparent Methods</div>
                        <div class="scenario"><strong>The Rule:</strong> When possible, a well-designed quasi-experimental method like RD or DiD is often more credible than a complex "kitchen sink" regression or matching model with dozens of covariates, because its identifying assumptions are clearer and easier to evaluate.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use Causal Diagrams (DAGs)</h3>
    <div class="scenario-content">
        <p>Before you start any causal analysis, it's an excellent practice to draw a **Directed Acyclic Graph (DAG)**. A DAG is a visual representation of your assumptions about the causal relationships between your variables. It helps you identify potential confounding variables that you must control for, and it helps you communicate your causal model to others. The <code>ggdag</code> package in R is a great tool for creating these diagrams.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You are tasked with estimating the causal effect of attending a private university vs. a public university on future income. You have observational data on thousands of students.</p>
        <ul class="prose-list">
            <li>What is the fundamental problem with simply comparing the average incomes of the two groups?</li>
            <li>If you were to use propensity score matching, what are some of the key confounding variables you would need to include in your propensity score model to satisfy the unconfoundedness assumption?</li>
            <li>Why would a Regression Discontinuity design be a very powerful method if you knew that one of the universities had a strict SAT score cutoff for admission?</li>
        </ul>
    </div>
</div>
`,
          p4s9ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A telecom company in Karachi wants to understand customer churn, but they need to answer a more nuanced question than just "if" a customer will churn. 🇵🇰 They want to know **when** they are most likely to churn. Is the risk highest in the first month? Does it decrease after the first year? This focus on "time-to-event" is the domain of **Survival Analysis**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s9ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s9ss1-viz">Censoring: The Key Challenge</button>
    </div>

    <div id="p4s9ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Analyzing Time-to-Event Data</h4>
        <p><strong>Survival Analysis</strong> is a branch of statistics for analyzing the expected duration of time until one or more events happen. It's often used in medicine (time until death or recovery), engineering (time until equipment failure), and business (time until customer churn).</p>
        <p><strong>Analogy:</strong> Survival analysis is like being a **mechanic studying the lifespan of car engines**. You want to know the probability that an engine will survive past 100,000 miles. You also need to account for the cars that are still running perfectly when your study ends; you can't just ignore them. This is the central challenge of survival analysis.</p>
        <h5 class="subsection-title" style="font-size: 1.1rem; border: none; margin-top: 1.5rem;">Key Functions:</h5>
        <ul class="prose-list">
            <li><strong>Survival Function, S(t):</strong> The probability that an individual survives longer than a specific time, t.</li>
            <li><strong>Hazard Function, h(t):</strong> The instantaneous risk of the event occurring at time t, given that the individual has survived up to time t.</li>
        </ul>
    </div>

    <div id="p4s9ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Problem of Censored Data</h4>
        <p>The defining feature of survival data is **censoring**. This occurs when we have some information about an individual's survival time, but we don't know it exactly. The most common type is **right-censoring**.</p>
        <div class="plot-container">
            <div id="p4s9ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This timeline shows data for five patients in a clinical trial. For Patients A, B, and D, we observe the "death" event. Their survival times are exact. For Patient C, they dropped out of the study at day 15. For Patient E, the study ended at day 30 while they were still alive. For C and E, their data is **right-censored**. We know they survived *at least* that long, but we don't know their true, full survival time. Survival analysis methods are specifically designed to correctly use this partial information.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>Surv</code> Object</h3>
    <div class="scenario-content">
        <p>In R, survival data is stored in a special object created by the <code>Surv()</code> function from the <code>survival</code> package. You typically provide two arguments: the time to the event or censoring, and an event indicator (usually 1 if the event was observed, 0 if it was censored). This <code>Surv</code> object is then used as the response variable in all survival analysis models.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>A study tracks 100 light bulbs to see how long they last. The study runs for 1,000 hours.</li>
            <li>A bulb that burns out at 500 hours: is this data point censored?</li>
            <li>A bulb that is still working at the 1,000-hour mark when the study ends: is this data point censored? What type of censoring is it?</li>
            <li>What is the value of the survival function, S(t), at time t=0? (Hint: What is the probability of surviving past time zero?)</li>
        </ul>
    </div>
</div>
`,
          p4s9ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A medical researcher is analyzing data from a clinical trial for a new cancer drug. They want to visualize the survival probability of patients over time. Furthermore, they want to compare the survival curve for the group that received the new drug to the group that received a placebo to see if there is a significant difference.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s9ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s9ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s9ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s9ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">A Non-Parametric Estimate of Survival</h4>
        <p>The <strong>Kaplan-Meier (K-M) Estimator</strong> is a non-parametric statistic used to estimate the survival function from time-to-event data. It is often visualized as a step-function.</p>
        <p>"Non-parametric" means it does not make any assumptions about the underlying distribution of the survival times. It's a way of directly observing the survival probabilities from the data.</p>
        <p><strong>Analogy:</strong> The K-M curve is like a **live-action "survivor" count in a reality TV show**. At the start of the show (time=0), 100% of the contestants are "surviving." After the first challenge, one person is voted off, and the survival probability drops slightly. It stays at that level until the next person is voted off, causing another drop. The K-M curve is this step-by-step plot of the percentage of contestants still in the game over time.</p>
    </div>
    
    <div id="p4s9ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Reading a Kaplan-Meier Curve</h4>
        <p>The K-M plot shows the probability of survival on the y-axis and time on the x-axis. It is a descending step function.</p>
        <div class="plot-container">
            <div id="p4s9ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The curve starts at a survival probability of 1.0 (100%). Each downward "step" in the curve corresponds to an event (a death or failure). The small vertical ticks on the curve represent censored observations; we know the individual survived up to that point, but we don't see an event, so the curve does not drop. The shaded area represents the 95% confidence interval for the survival probability.</p>
        </div>
    </div>

    <div id="p4s9ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to compare the survival curves of lung cancer patients based on their sex, using the built-in <code>lung</code> dataset from the <code>survival</code> package.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Prepare Data.</strong> The <code>status</code> column is coded as 1=censored, 2=dead. We need to convert it to the standard 0/1 format.
                <div class="code-container">
                    <pre><code class="language-r"># install.packages(c("survival", "survminer"))
library(survival)
library(survminer)
lung_data <- lung
lung_data$status <- ifelse(lung_data$status == 2, 1, 0)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Create the Survival Object and Fit the Model.</strong> We create the <code>Surv</code> object and then use the <code>survfit()</code> function to fit the K-M model. The formula <code>Surv(...) ~ sex</code> tells it to create separate curves for each level of <code>sex</code>.
                <div class="code-container">
                    <pre><code class="language-r">km_fit <- survfit(Surv(time, status) ~ sex, data = lung_data)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Visualize the Curves.</strong> The <code>ggsurvplot()</code> function from the <code>survminer</code> package creates a beautiful, publication-quality plot.
                <div class="code-container">
                    <pre><code class="language-r">ggsurvplot(km_fit, data = lung_data, pval = TRUE, conf.int = TRUE)</code></pre>
                </div>
            </li>
             <li><strong>Step 4: Formulate a Conclusion.</strong>
                <div class="plot-description">
                    <p><strong>Conclusion:</strong> The plot shows the two survival curves for males and females. The <code>pval = TRUE</code> argument automatically runs a **log-rank test** to check if the difference between the curves is statistically significant. A low p-value would allow us to conclude that there is a significant difference in survival between the two groups.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Log-Rank Test</h3>
    <div class="scenario-content">
        <p>When you have two or more Kaplan-Meier curves, you often want to know if the difference between them is statistically significant. The **log-rank test** is a hypothesis test used for this purpose. Its null hypothesis is that there is no difference in survival between the groups. The <code>survminer::ggsurvplot()</code> function conveniently runs this test and adds the p-value to your plot automatically when you set <code>pval = TRUE</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>lung_data</code> from the example.</li>
            <li>Fit a new Kaplan-Meier model, this time comparing survival based on the patient's ECOG performance score (<code>ph.ecog</code>), which is a categorical variable.</li>
            <li>Use <code>ggsurvplot()</code> to visualize the survival curves for the different performance scores.</li>
            <li>Based on the plot and the log-rank p-value, is there a significant difference in survival among patients with different performance scores? Which group appears to have the best survival prognosis?</li>
        </ul>
    </div>
</div>
`,
          p4s9ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A medical researcher wants to go beyond just comparing survival curves. They want to build a regression model that can quantify the effect of several predictors (e.g., age, treatment type, tumor size) on a patient's risk of death, while correctly handling censored data. This is the classic use case for the **Cox Proportional Hazards Model**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s9ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s9ss3-math">The Hazard Ratio</button>
        <button class="tab-button" data-tab="p4s9ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s9ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">A Regression Model for Survival</h4>
        <p>The <strong>Cox Proportional Hazards (PH) Model</strong> is a semi-parametric regression model used to investigate the association between predictor variables and time-to-event data.</p>
        <p>Unlike other models that predict the time itself, the Cox model predicts the **hazard** at a given time t for an individual. The output is a **Hazard Ratio (HR)**.</p>
        <p><strong>Analogy:</strong> A Cox model is like being a **life insurance actuary**. The actuary doesn't predict the exact date a person will die. Instead, they build a model that calculates a person's *risk factor* (their hazard ratio) based on predictors like age, smoking status, and health. A person with a high hazard ratio has a higher risk of the event happening at any given time compared to a baseline person.</p>
    </div>

    <div id="p4s9ss3-math" class="tab-pane">
        <h4 class="subsection-title">Interpreting the Hazard Ratio</h4>
        <p>The coefficients ($\\beta$) from a Cox model are on a log-hazard scale. We must exponentiate them to get the Hazard Ratio (HR): $HR = e^{\\beta}$.</p>
        <ul class="prose-list">
            <li><strong>HR = 1:</strong> The predictor has no effect on the hazard.</li>
            <li><strong>HR > 1:</strong> The predictor is associated with an **increased** hazard (risk) of the event. An HR of 1.5 means a 50% increase in risk.</li>
            <li><strong>HR < 1:</strong> The predictor is associated with a **decreased** hazard (risk) of the event. An HR of 0.7 means a 30% decrease in risk (it is protective).</li>
        </ul>
    </div>
    
    <div id="p4s9ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to build a model to assess the effect of sex, age, and ECOG performance score on the survival of lung cancer patients.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Prepare Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(survival)
library(dplyr)
# Prepare data as before
lung_data <- lung %>% mutate(status = ifelse(status == 2, 1, 0))</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the Cox PH Model.</strong> We use the <code>coxph()</code> function, with a <code>Surv</code> object as the response.
                <div class="code-container">
                    <pre><code class="language-r">cox_model <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data = lung_data)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Model Summary.</strong> The summary provides the coefficients, hazard ratios (<code>exp(coef)</code>), and p-values.
                <div class="code-container">
                    <pre><code class="language-r">summary(cox_model)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong>
                <div class="plot-description">
                    <p><strong>Interpretation of a hypothetical result for <code>sex</code>:</strong> "The summary shows a hazard ratio for <code>sex</code> of approximately 0.59, with a significant p-value. This means that, holding age and performance score constant, being female (coded as 2) is associated with a 41% reduction in the hazard of death compared to being male (the baseline). This suggests a significant protective effect."</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Check the Proportional Hazards Assumption</h3>
    <div class="scenario-content">
        <p>The single most important assumption of the Cox model is that the hazards are **proportional** over time. This means that the effect of a predictor (the hazard ratio) is constant over the entire follow-up period. You **must** test this assumption. The <code>survival::cox.zph()</code> function is the standard tool for this. A significant p-value from this test indicates that the assumption is violated and your model may be unreliable.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>lung_data</code> from the example.</li>
            <li>Fit a Cox model to predict survival using only the <code>wt.loss</code> (weight loss in last six months) variable.</li>
            <li>Use <code>summary()</code> on the model. What is the hazard ratio for <code>wt.loss</code>?</li>
            <li>Write a single sentence interpreting this hazard ratio. Is weight loss a good or a bad sign for survival prognosis, according to the model?</li>
        </ul>
    </div>
</div>
`,
          p4s9ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A reliability engineer is studying the lifespan of a specific mechanical component. They have strong theoretical reasons to believe that the failure times should follow a known statistical distribution, like the Weibull distribution. Instead of using a non-parametric K-M curve, they choose to fit a **Parametric Survival Model**, which can provide more efficient estimates and smoother survival curves if the distributional assumption is correct.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s9ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s9ss4-methods">Common Distributions</button>
        <button class="tab-button" data-tab="p4s9ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s9ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Assuming a Shape for Survival</h4>
        <p><strong>Parametric Survival Models</strong> differ from the Kaplan-Meier and Cox PH models by assuming that the survival time follows a specific, known probability distribution. By making this strong assumption, you can often get more precise estimates of survival, especially with smaller datasets.</p>
        <p><strong>Analogy:</strong> A non-parametric K-M curve is like **tracing a silhouette by hand**. It follows the exact jagged contours of your data. A parametric model is like using a **pre-made stencil (a distribution)** that you believe matches the general shape of the silhouette. If you pick the right stencil, you can get a cleaner, smoother, and often more accurate outline.</p>
    </div>

    <div id="p4s9ss4-methods" class="tab-pane">
        <h4 class="subsection-title">A Toolkit of Distributions</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Exponential Distribution</strong>
                <p>The simplest model. It assumes a **constant hazard** over time. This is often unrealistic but can be a useful baseline.</p>
            </div>
            <div class="decision-branch">
                <strong>Weibull Distribution</strong>
                <p>A very flexible and widely used distribution. It can model hazards that are increasing, decreasing, or constant over time, making it much more adaptable than the Exponential.</p>
            </div>
            <div class="decision-branch">
                <strong>Log-Normal & Log-Logistic</strong>
                <p>Other flexible distributions that can model different shapes of the hazard function, particularly useful for non-monotonic (e.g., bathtub-shaped) hazards.</p>
            </div>
        </div>
    </div>

    <div id="p4s9ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to fit a parametric survival model to the <code>lung</code> dataset and compare it to the non-parametric Kaplan-Meier curve.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Prepare Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(survival)
library(dplyr)
lung_data <- lung %>% mutate(status = ifelse(status == 2, 1, 0))
surv_obj <- Surv(lung_data$time, lung_data$status)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit a Parametric Model.</strong> We use the <code>survreg()</code> function, specifying the distribution we assume the survival times follow (e.g., "weibull").
                <div class="code-container">
                    <pre><code class="language-r"># Note: survreg models log(T), so the interpretation is different from coxph
parametric_model <- survreg(surv_obj ~ 1, data = lung_data, dist = "weibull")</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Compare with Kaplan-Meier.</strong> We can plot the K-M curve and overlay the smooth survival curve predicted by our parametric model to visually assess the fit.
                <div class="code-container">
                    <pre><code class="language-r"># Fit the non-parametric KM model for comparison
km_fit <- survfit(surv_obj ~ 1, data = lung_data)
# Plot the KM curve
plot(km_fit, conf.int = FALSE, main="Parametric vs. Non-Parametric Fit")
# Add the parametric curve
lines(predict(parametric_model, type = "quantile", p = seq(0.01, 0.99, 0.01)), 
      1 - seq(0.01, 0.99, 0.01), col = "red")
legend("topright", legend = c("Kaplan-Meier", "Weibull"), col = c("black", "red"), lty = 1)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The plot allows us to visually assess how well our chosen parametric distribution (Weibull, in red) fits the empirical survival curve estimated by the Kaplan-Meier method (in black). A close fit suggests our distributional assumption was reasonable.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Model Selection with AIC</h3>
    <div class="scenario-content">
        <p>How do you choose the best distribution? You can fit several different parametric models (e.g., Weibull, exponential, log-normal) and compare them using the Akaike Information Criterion (AIC). The model with the lowest AIC is generally considered the best fit among the candidates. You can extract the AIC for a <code>survreg</code> model using the <code>extractAIC()</code> function.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>lung_data</code> from the example.</li>
            <li>Fit two different parametric models using <code>survreg()</code>: one with <code>dist = "weibull"</code> and another with <code>dist = "exponential"</code>.</li>
            <li>Use the <code>extractAIC()</code> function on both fitted models.</li>
            <li>Which model has the lower AIC value? What does this suggest about which distribution is a better fit for this data?</li>
        </ul>
    </div>
</div>
`,
          p4s9ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A hospital is studying patient outcomes after a specific surgery. The primary event of interest is "death from surgical complications." However, some patients may die from other, unrelated causes (like a car accident), or they may receive a heart transplant, which fundamentally changes their survival prognosis. These are **competing risks** that prevent the primary event from occurring and must be modeled correctly.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s9ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s9ss5-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s9ss5-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s9ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">When There's More Than One Way Out</h4>
        <p><strong>Competing Risks</strong> occur in survival analysis when a subject can experience one of several different types of events, and the occurrence of one type of event prevents any other type of event from happening.</p>
        <p>Using a standard Kaplan-Meier estimator in this situation is incorrect and will lead to biased results. A K-M curve estimates the probability of the event if it were the *only* event possible. When competing risks exist, we need to estimate the **Cumulative Incidence Function (CIF)**.</p>
        <p><strong>Analogy:</strong> Imagine a video game character with three ways their game can end: "winning the game," "losing to the main boss," or "falling into a pit." These are competing risks. The probability of "winning" is not just the inverse of "not losing"; it depends on the probability of the other events happening first.</p>
    </div>
    
    <div id="p4s9ss5-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Cumulative Incidence</h4>
        <p>A Cumulative Incidence Function (CIF) plot shows the cumulative probability of a specific event occurring over time, *in the presence of the other competing events*.</p>
        <div class="plot-container">
            <div id="p4s9ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This plot shows the cumulative incidence for two different event types. Unlike a Kaplan-Meier curve which goes down, a CIF curve always goes up. The height of the curve at any time point represents the total probability that that specific event has occurred by that time. The top line represents the overall survival probability from all causes combined.</p>
        </div>
    </div>
    
    <div id="p4s9ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We will model a dataset of bone marrow transplant patients, where the competing risks are "relapse" and "death from other causes." We will use the <code>cmprsk</code> package.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Prepare Data.</strong> The event status needs to be a factor. 0=censored, 1=event of interest, 2=competing risk.
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("cmprsk")
library(cmprsk)
# This uses a sample dataset from the package's documentation
# ... data preparation code would go here ...
# Assume we have 'time', and 'status' (0, 1, 2)
# and a group variable 'transplant_type'
                    </code></pre>
                </div>
            </li>
            <li><strong>Step 2: Calculate the Cumulative Incidence.</strong> We use the <code>cuminc()</code> function.
                <div class="code-container">
                    <pre><code class="language-r"># cuminc(ftime = time, fstatus = status, group = transplant_type)
                    </code></pre>
                </div>
            </li>
            <li><strong>Step 3: Plot the CIF Curves.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># plot(cuminc_result)
                    </code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> By plotting the CIF curves, we can separately analyze the probability of relapse and the probability of death from other causes over time for each transplant type. This provides a much more nuanced and accurate picture of patient outcomes than a simple Kaplan-Meier analysis would.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Fine-Gray Models</h3>
    <div class="scenario-content">
        <p>What is the equivalent of a Cox model for competing risks? The **Fine-Gray subdistribution hazards model** is a popular choice. It allows you to build a regression model to estimate the effect of covariates on the cumulative incidence of one specific event type, while correctly accounting for the other competing risks. You can fit these models in R using the <code>crr()</code> function from the <code>cmprsk</code> package.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are studying customer churn for a subscription service. Your primary event is "voluntary cancellation." What is a potential competing risk in this scenario? (Hint: What is another way a customer's subscription could end that is not a voluntary cancellation?)</li>
            <li>Why would a standard Kaplan-Meier curve of voluntary cancellation be biased in this case? What would it be over- or under-estimating?</li>
        </ul>
    </div>
</div>
`,
          p4s9ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst fits a Cox Proportional Hazards model and finds a significant result. However, they failed to check the crucial **proportional hazards assumption**. A diagnostic plot reveals that the effect of the treatment is not constant over time—it is very strong initially but wears off after a few months. This violation invalidates the standard model results and requires a more advanced approach.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Robust Survival Modeling</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Survival Analysis Best Practices</div>
            <div class="scenario">Ensuring your time-to-event analysis is statistically sound and your conclusions are reliable.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chart-line"></i> 1. Visualize First</div>
                        <div class="scenario"><strong>The Rule:</strong> Always start by plotting the Kaplan-Meier curves for your groups. This gives you a crucial non-parametric look at the data and helps you form initial hypotheses before fitting a complex regression model.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-tasks"></i> 2. Check the Proportional Hazards Assumption</div>
                        <div class="scenario"><strong>The Rule:</strong> If you are using a Cox model, you **must** test the proportional hazards assumption. This can be done formally with <code>survival::cox.zph()</code> or visually by plotting the scaled Schoenfeld residuals over time.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-project-diagram"></i> 3. Choose the Right Model for the Question</div>
                        <div class="scenario"><strong>The Rule:</strong> Don't use a standard survival model if competing risks are present. If you have competing events, you must use methods designed for them, like Cumulative Incidence Functions and Fine-Gray models, to avoid biased results.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-info-circle"></i> 4. Be Clear About Censoring</div>
                        <div class="scenario"><strong>The Rule:</strong> Be explicit about what constitutes an "event" and what constitutes "censoring" in your analysis. Your results are highly dependent on the correct definition of the event status variable.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Stratification for Non-Proportionality</h3>
    <div class="scenario-content">
        <p>What if the proportional hazards assumption is violated for one of your key categorical predictors (e.g., treatment group)? You don't have to abandon the Cox model! You can use **stratification**. By adding a <code>strata(variable_name)</code> term to your model formula, you allow the baseline hazard function to be different for each level of that variable, which resolves the violation. The model will then estimate the effects of the other covariates *within* each stratum.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's practice checking the proportional hazards assumption.</p>
        <ul class="prose-list">
            <li>Using the <code>lung</code> dataset from the <code>survival</code> package, fit a Cox model: <code>fit <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data = lung)</code>.</li>
            <li>Run the formal test for the assumption: <code>test_ph <- cox.zph(fit)</code>.</li>
            <li>Print the <code>test_ph</code> object. Look at the p-values in the table. Is there any variable that has a p-value less than 0.05? What does this suggest?</li>
            <li>Use <code>plot(test_ph)</code> to visualize the scaled Schoenfeld residuals for each variable. A non-horizontal line suggests a violation of the assumption.</li>
        </ul>
    </div>
</div>
`,
          p4s11ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst in Karachi is 95% confident that the new ad campaign increased signups by between 2 and 22 per day. 🇵🇰 A manager asks, "So... what's the probability that the campaign actually *increased* signups by more than 10?" Frequentist statistics can't directly answer this question. To talk about the probability of a parameter having a certain value, we must enter the world of **Bayesian Inference**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s11ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s11ss1-viz">The Bayesian Workflow</button>
        <button class="tab-button" data-tab="p4s11ss1-mcmc">How it Works: MCMC</button>
    </div>

    <div id="p4s11ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Updating Beliefs with Data</h4>
        <p><strong>Bayesian Inference</strong> is a paradigm of statistics that treats model parameters not as fixed, unknown constants, but as random variables about which we can have beliefs. The core of Bayesian analysis is using data to update these beliefs.</p>
        <p><strong>Analogy:</strong> Bayesian inference is the mathematical formalization of **how a detective thinks**. The detective starts with an initial belief about who the suspect might be (the **Prior**). They then discover new evidence—fingerprints, an alibi (the **Data/Likelihood**). They use this evidence to update their belief about the suspect's guilt (the **Posterior**). The more evidence they gather, the stronger their posterior belief becomes.</p>
    </div>
    
    <div id="p4s11ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Components of Bayes' Theorem</h4>
        <div class="mindmap-container">
            <div class="mindmap-root">
                <div class="title"><i class="fas fa-balance-scale-right"></i> Posterior ∝ Likelihood × Prior</div>
                <div class="scenario">Bayesian inference combines your prior knowledge with the information from your data to produce an updated, posterior belief.</div>
            </div>
            <div class="mindmap-branches-container">
                <div class="mindmap-branches">
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-seedling"></i> 1. The Prior</div>
                            <div class="scenario"><strong>What it is:</strong> A probability distribution representing your belief about a parameter *before* seeing the data. It can be "uninformative" (a flat line, expressing ignorance) or "informative" (a curve based on previous research).</div>
                        </div>
                    </div>
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-dna"></i> 2. The Likelihood</div>
                            <div class="scenario"><strong>What it is:</strong> A function that describes the probability of observing your data, given a specific value of the parameter. This is the component that brings your data into the calculation.</div>
                        </div>
                    </div>
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-tree"></i> 3. The Posterior</div>
                            <div class="scenario"><strong>What it is:</strong> The final result. It's a new probability distribution for the parameter that represents your updated belief *after* accounting for the evidence in your data. It is a compromise between your prior belief and the likelihood.</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <div id="p4s11ss1-mcmc" class="tab-pane">
        <h4 class="subsection-title">How Do We Find the Posterior?</h4>
        <p>For most interesting problems, the posterior distribution is mathematically impossible to calculate directly. Instead, we approximate it by drawing thousands of samples from it using algorithms like **Markov Chain Monte Carlo (MCMC)**.</p>
        <p><strong>Analogy:</strong> MCMC is like a **drunken cartographer mapping a mountain range in the dark**. They can't see the whole landscape (the posterior distribution). So, they take a step in a random direction. If the step is uphill, they accept it. If it's downhill, they might accept it with some probability. After stumbling around the mountains for thousands of steps, the path they've walked will have mapped out the shape of the landscape—they will have spent more time in the high-altitude areas (high-probability regions of the posterior) and less time in the valleys.</p>
        <div class="plot-container">
            <div id="p4s11ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
             <p><strong>How to observe this plot:</strong> This shows the result of MCMC. The **trace plot** on the left shows the path of the "drunken cartographer" exploring the parameter value over thousands of iterations. The **density plot** on the right is a histogram of all those steps, which forms our final approximation of the posterior distribution for that parameter.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Prior Predictive Checks</h3>
    <div class="scenario-content">
        <p>Before you even fit your model to data, it's a good practice to perform a **prior predictive check**. This involves generating simulated data *from your priors alone*. This helps you see if the priors you've chosen generate plausible-looking data. If your prior on a regression slope generates absurdly steep lines, it's a sign that your prior is poorly chosen and needs to be made more restrictive.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are a frequentist statistician and you perform a t-test. Your 95% confidence interval for the mean is [10, 20]. Can you say "There is a 95% probability that the true mean is between 10 and 20?" Why or why not? (Hint: In frequentist stats, the true mean is a fixed constant, not a random variable).</li>
            <li>You are a Bayesian statistician and you fit a model. Your 95% credible interval for the mean is [10, 20]. Can you say "There is a 95% probability that the true mean is between 10 and 20?" Why or why not?</li>
            <li>This difference in interpretation is the most fundamental distinction between the two paradigms.</li>
        </ul>
    </div>
</div>
`,
          p4s11ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has fit a standard linear regression and a logistic regression. The output gives them a single point-estimate for each coefficient (e.g., $\\beta_1 = 3.93$) and a p-value. A manager asks, "Okay, but how certain are we about that 3.93? What's the probability that the effect is actually bigger than 5?" To answer these questions and get a full probability distribution for each coefficient, the analyst turns to **Bayesian Regression**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s11ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s11ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s11ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s11ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">From Point Estimates to Distributions</h4>
        <p>Bayesian regression applies the principles of Bayesian inference to regression models. Instead of finding a single "best" value for each coefficient, it estimates the entire **posterior probability distribution** for each one.</p>
        <p><strong>Analogy:</strong> A frequentist regression gives you a **single photograph** of the "best" coefficient value. A Bayesian regression gives you a **full 3D hologram**. It shows you not just the most likely value, but the entire landscape of plausible values, allowing you to see the uncertainty and make direct probabilistic statements.</p>
    </div>
    
    <div id="p4s11ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Posterior Distribution</h4>
        <p>The primary output of a Bayesian regression is a posterior distribution for each parameter. We can visualize this distribution to understand our estimate and its uncertainty.</p>
        <div class="plot-container">
            <div id="p4s11ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This shows the posterior distribution for a single regression coefficient. The peak of the distribution is the most likely value (the posterior mean). The width of the distribution represents our uncertainty. The shaded area is the 95% **Credible Interval**, which means there is a 95% probability that the true value of the coefficient lies within this range. We can also calculate the probability of the effect being greater than zero by finding the area of the curve to the right of the zero line.</p>
        </div>
    </div>
    
    <div id="p4s11ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to fit a Bayesian linear regression model to the <code>cars</code> dataset to understand the relationship between speed and stopping distance, and to quantify our uncertainty about the effect.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Data.</strong> The <code>rstanarm</code> package provides an easy-to-use interface for fitting Bayesian models.
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("rstanarm")
library(rstanarm)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the Bayesian Model.</strong> The syntax is almost identical to <code>lm()</code>. We use <code>stan_lm()</code>. We can specify our prior beliefs about the coefficients, but the default "weakly informative" priors are often a good start.
                <div class="code-container">
                    <pre><code class="language-r"># The model will run an MCMC sampler (it might take a moment)
bayes_model <- stan_lm(dist ~ speed, data = cars, seed = 42)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Posterior Distributions.</strong> We can print the model for a summary or plot it to see the full distributions.
                <div class="code-container">
                    <pre><code class="language-r">print(bayes_model)
# plot() on a stanreg object gives beautiful visualizations
plot(bayes_model)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Make Probabilistic Statements.</strong> We can extract the posterior draws (the MCMC samples) to answer specific questions.
                <div class="code-container">
                    <pre><code class="language-r">posterior_draws <- as.data.frame(bayes_model)
# What's the probability the effect of speed is greater than 3.5?
mean(posterior_draws$speed > 3.5)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> Instead of just a single estimate for the <code>speed</code> coefficient, we now have a full probability distribution. We can calculate the 95% credible interval and directly answer probabilistic questions like "What is the probability the effect is positive?", providing a much richer and more intuitive summary of our findings to the manager.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use <code>stan_glm()</code> for GLMs</h3>
    <div class="scenario-content">
        <p>The <code>rstanarm</code> package provides a Bayesian counterpart for nearly every common frequentist model. If you want to run a Bayesian logistic or Poisson regression, you simply use the <code>stan_glm()</code> function and specify the <code>family</code> argument, just like you would with <code>glm()</code>. This provides a consistent and powerful interface for a wide range of Bayesian models.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, fit a Bayesian logistic regression model using <code>stan_glm()</code> to predict <code>am</code> from <code>hp</code>. Remember to set <code>family = binomial</code>.</li>
            <li>Plot the model object to see the posterior distribution for the <code>hp</code> coefficient.</li>
            <li>Extract the posterior draws into a data frame.</li>
            <li>Calculate the 90% credible interval for the <code>hp</code> coefficient. (Hint: use <code>quantile(draws$hp, probs = c(0.05, 0.95))</code>).</li>
            <li>Calculate the probability that the effect of horsepower on being manual is positive (i.e., that the coefficient is > 0).</li>
        </ul>
    </div>
</div>
`,
          p4s11ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A researcher is studying student test scores across many different schools. They could fit a separate regression model for each school, but that would be inefficient and ignore the fact that schools are related. They could also fit one giant model for all students, but that would ignore the fact that students within the same school are more similar to each other than students from different schools. They need a **Hierarchical Model** that can do both: estimate a separate effect for each school while "pooling" information across all schools to get more stable estimates.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s11ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s11ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s11ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s11ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Modeling Nested and Grouped Data</h4>
        <p><strong>Hierarchical Models</strong> (also known as multilevel models or mixed-effects models) are a type of model specifically designed for data that has a nested or grouped structure. They are incredibly powerful in the Bayesian framework.</p>
        <p><strong>Analogy:</strong> A hierarchical model is like a **corporate management structure**. Instead of having one CEO who micromanages every single employee (a pooled model), or having every employee be their own boss (an unpooled model), you have a hierarchy. The CEO sets the overall company-wide goals. Department managers take those goals and adapt them for their specific department. The hierarchical model similarly has a "global" parameter and then "group-level" parameters that are variations on that global theme.</p>
    </div>
    
    <div id="p4s11ss3-viz" class="tab-pane">
        <h4 class="subsection-title">No Pooling vs. Partial Pooling</h4>
        <p>The magic of hierarchical models comes from **partial pooling**. They find a statistically principled compromise between trusting each group's data entirely (no pooling) and trusting the overall average entirely (complete pooling).</p>
        <div class="plot-container">
            <div id="p4s11ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This shows estimates for several groups. The **No Pooling** estimates (blue) are noisy, especially for small groups. The **Complete Pooling** estimate (red) is the same for everyone and misses group-level variation. The **Partial Pooling** estimates (green) from the hierarchical model are the perfect compromise: they have "shrunk" the noisy individual estimates towards the overall average, providing more stable and believable results.</p>
        </div>
    </div>
    
    <div id="p4s11ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to model the relationship between a student's study hours and their exam score, but we have data from multiple different schools. We will fit a hierarchical model to estimate a separate intercept for each school.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Data.</strong> We use the <code>lme4</code> syntax within <code>rstanarm</code>. The formula for a hierarchical model includes a "random effect" term.
                <div class="code-container">
                    <pre><code class="language-r">library(rstanarm)
# Assume 'student_data' has 'score', 'study_hours', and 'school_id'</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Fit the Hierarchical Model.</strong> The term <code>(1 | school_id)</code> tells the model to fit a separate intercept (<code>1</code>) for each level of school_id, and that these intercepts are drawn from a common distribution.
                <div class="code-container">
                    <pre><code class="language-r"># This is a random intercepts model
hierarchical_model <- stan_glmer(score ~ study_hours + (1 | school_id), 
                                 data = student_data, seed = 42)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Results.</strong> The summary will now show estimates for the "fixed effects" (like <code>study_hours</code>) and the variance of the "random effects" (the variation in intercepts across schools).
                <div class="code-container">
                    <pre><code class="language-r">summary(hierarchical_model)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Random Slopes</h3>
    <div class="scenario-content">
        <p>You can make your hierarchical models even more flexible by allowing the *slopes* to vary by group, in addition to the intercepts. The formula syntax for this would be <code>(study_hours | school_id)</code>. This would fit a separate intercept AND a separate slope for the effect of <code>study_hours</code> for each school. This is called a random intercepts and random slopes model.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the built-in <code>sleep</code> dataset, which shows the extra hours of sleep for 10 subjects who took two different drugs.</p>
        <ul class="prose-list">
            <li>The <code>ID</code> column represents the subject. This is our grouping variable.</li>
            <li>Fit a hierarchical model using <code>stan_glmer()</code> to predict the extra hours of sleep (<code>extra</code>) from the type of drug (<code>group</code>). Allow the intercept to vary for each subject. The formula will be <code>extra ~ group + (1 | ID)</code>.</li>
            <li>Use <code>summary()</code> on your model. What is the estimated fixed effect for the <code>group</code> variable?</li>
        </ul>
    </div>
</div>
`,
          p4s11ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has fit a complex Bayesian model using MCMC. The sampler ran without errors, but how can they be sure the results are trustworthy? The "drunken cartographer" might have gotten stuck in a small valley and failed to explore the whole mountain range. The analyst must perform **MCMC diagnostics** to check for convergence and use **posterior predictive checks** to see if the model is a good fit for the data.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s11ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s11ss4-viz">Visual Diagnostics</button>
        <button class="tab-button" data-tab="p4s11ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s11ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Trust, but Verify</h4>
        <p>Because Bayesian models are fit using a stochastic simulation (MCMC), we can't just trust the output. We must perform checks to ensure the simulation worked correctly and that the resulting model makes sense.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>MCMC Convergence Diagnostics</strong>
                <p><strong>The Question:</strong> "Did the algorithm that estimates the posterior distribution work correctly?" We need to check if the MCMC chains have "converged" to a stable distribution.</p>
                <p><strong>Tools:</strong> Trace plots and the R-hat ($\hat{R}$) statistic.</p>
            </div>
            <div class="decision-branch">
                <strong>Posterior Predictive Checks (PPCs)</strong>
                <p><strong>The Question:</strong> "Does my fitted model generate data that looks like my real data?" This is a check of the model's goodness-of-fit.</p>
                <p><strong>Tools:</strong> Visual comparisons of replicated data from the model to the original data.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s11ss4-viz" class="tab-pane">
        <h4 class="subsection-title">Good vs. Bad MCMC Chains</h4>
        <p>A trace plot shows the value of a parameter at each iteration of the MCMC sampler for several different chains. It's the most important visual tool for diagnosing convergence.</p>
        <div class="plot-container">
            <div id="p4s11ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Good (Converged)** plot on the left is what we want to see. The different colored chains are all mixed together and look like a "fat, hairy caterpillar," indicating they have all found the same stable distribution. The **Bad (Non-Converged)** plot on the right shows a clear problem. The chains have not mixed and are exploring different parts of the parameter space, indicating the algorithm has failed to converge and the results are not trustworthy.</p>
        </div>
    </div>
    
    <div id="p4s11ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have fit a Bayesian model using <code>rstanarm</code>. Now we must check its convergence and perform a posterior predictive check.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Fit a Model.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(rstanarm)
bayes_model <- stan_lm(mpg ~ wt, data = mtcars, seed = 42)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Check for Convergence.</strong> We can plot the trace plots for the parameters. The <code>bayesplot</code> package is excellent for this. We also check the R-hat values from the summary.
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("bayesplot")
library(bayesplot)
plot(bayes_model, plotfun = "trace") # Creates trace plots
summary(bayes_model) # Look for Rhat values. They should all be close to 1.0.</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Perform a Posterior Predictive Check.</strong> We use the model to simulate new datasets and compare their distributions to the original data's distribution.
                <div class="code-container">
                    <pre><code class="language-r"># pp_check() creates a plot comparing the density of the real data (y)
# to the densities of several simulated datasets (y_rep) from the model.
pp_check(bayes_model)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> If the trace plots look like fat, hairy caterpillars, R-hat values are ~1.0, and the <code>pp_check</code> plot shows that the simulated data distributions look similar to the real data's distribution, we can be confident that our MCMC simulation has converged and our model provides a good fit to the data.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: What is R-hat ($\hat{R}$)?</h3>
    <div class="scenario-content">
        <p>The R-hat statistic is a numerical diagnostic that compares the variance *within* each MCMC chain to the variance *between* the chains. A value close to 1.0 indicates that all chains have converged to the same distribution. As a rule of thumb, you should be concerned about any R-hat value greater than 1.01.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Fit the <code>bayes_model</code> from the example above.</li>
            <li>Use the <code>posterior_interval()</code> function to get the 90% credible interval for the model's coefficients.</li>
            <li>The <code>pp_check()</code> function can create many different types of plots. Look at the help file (<code>?pp_check</code>) and create a PPC plot that compares the mean of the replicated datasets to the mean of the original data. (Hint: <code>pp_check(bayes_model, plotfun = "stat", stat = "mean")</code>).</li>
        </ul>
    </div>
</div>
`,
          p4s12ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is studying the social network of characters in a novel. They need a formal way to represent the data: who are the characters, and which characters interact with each other? This requires the fundamental language of **Graph Theory**, which uses nodes and edges to model relationships.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s12ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s12ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s12ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s12ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Mathematics of Connections</h4>
        <p><strong>Graph Theory</strong> is a branch of mathematics that studies networks of connected points. A graph consists of two fundamental components:</p>
        <ul class="prose-list">
            <li><strong>Nodes (or Vertices):</strong> These represent the individual entities in the network (e.g., people, cities, websites).</li>
            <li><strong>Edges (or Links):</strong> These represent the connections or relationships between the nodes (e.g., friendships, flight paths, hyperlinks).</li>
        </ul>
        <p><strong>Analogy:</strong> A graph is like a **map of an airline's routes**. The cities are the **nodes**, and the flight paths between them are the **edges**. The entire collection of cities and routes is the graph.</p>
    </div>
    
    <div id="p4s12ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Types of Graphs</h4>
        <p>Graphs can have different properties that change how they are analyzed.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Undirected vs. Directed</strong>
                <p>An **undirected** edge is a symmetric, two-way relationship (e.g., being Facebook friends). A **directed** edge is a one-way relationship (e.g., following someone on Twitter).</p>
            </div>
            <div class="decision-branch">
                <strong>Unweighted vs. Weighted</strong>
                <p>An **unweighted** edge simply shows that a connection exists. A **weighted** edge has a numerical value associated with it that represents the strength of the connection (e.g., the number of flights per day between two cities).</p>
            </div>
        </div>
        <div class="plot-container">
            <div id="p4s12ss1-plot1" class="plotly-chart"></div>
        </div>
    </div>
    
    <div id="p4s12ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We need to create and visualize a simple, undirected social network of five friends using the <code>igraph</code> package.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Define the Edges.</strong> We first define the connections in the network as a data frame.
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("igraph")
library(igraph)
# Create a data frame where each row is a connection (an edge)
edge_list <- data.frame(
  from = c("Ali", "Ali", "Bilal", "Chirag", "Dania"),
  to = c("Bilal", "Dania", "Chirag", "Dania", "Esha")
)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Create the Graph Object.</strong> We use <code>graph_from_data_frame()</code> to convert our edge list into a formal graph object that <code>igraph</code> can work with.
                <div class="code-container">
                    <pre><code class="language-r"># d = FALSE specifies that this is an undirected graph
friend_graph <- graph_from_data_frame(edge_list, directed = FALSE)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Graph Object.</strong>
                <div class="code-container">
                    <pre><code class="language-r">print(friend_graph)
# V(g) gives the vertices (nodes), E(g) gives the edges
V(friend_graph)
E(friend_graph)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Visualize the Graph.</strong>
                <div class="code-container">
                    <pre><code class="language-r">plot(friend_graph)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Representing Graphs as Matrices</h3>
    <div class="scenario-content">
        <p>Under the hood, graphs are often stored mathematically as an **Adjacency Matrix**. This is a square matrix where the rows and columns are the nodes of the graph. The cell <code>A[i, j]</code> is 1 if there is an edge from node i to node j, and 0 otherwise. For a weighted graph, the cell would contain the edge weight. You can get this matrix in <code>igraph</code> with the <code>as_adjacency_matrix()</code> function.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create an edge list data frame for a simple, **directed** graph representing a food chain: Grass -> Rabbit, Rabbit -> Fox, Fox -> Wolf.</li>
            <li>Use <code>graph_from_data_frame()</code> to create a directed graph object from your edge list.</li>
            <li>Plot your graph. Does the plot correctly show the one-way arrows of the food chain?</li>
        </ul>
    </div>
</div>
`,
          p4s12ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An intelligence agency is analyzing a network of suspected terrorists. They need to identify the most important or influential individuals in the network. Is it the person with the most direct contacts? Or the person who acts as a crucial "bridge" between different sub-groups? To answer these questions, they need to calculate different **network centrality metrics**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s12ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s12ss2-methods">Key Centrality Measures</button>
        <button class="tab-button" data-tab="p4s12ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s12ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s12ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Finding the Most Important Nodes</h4>
        <p><strong>Network Metrics</strong> are quantitative measures that describe the properties of a network or the importance of its individual nodes and edges. **Centrality** is a family of metrics that aims to identify the most "important" nodes in a network.</p>
        <p><strong>Analogy:</strong> Centrality metrics are like different ways of defining "popularity" in a high school. Is the most popular person the one who knows the most people (**degree**)? The one who is the best at spreading gossip between different cliques (**betweenness**)? Or the one who is, on average, closest to everyone else (**closeness**)?</p>
    </div>

    <div id="p4s12ss2-methods" class="tab-pane">
        <h4 class="subsection-title">Different Definitions of "Important"</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Degree Centrality</strong>
                <p>The simplest measure: the number of edges connected to a node. It measures local influence or popularity.</p>
            </div>
            <div class="decision-branch">
                <strong>Betweenness Centrality</strong>
                <p>Measures how often a node lies on the shortest path between two other nodes. High betweenness indicates a "bridge" or "broker" role.</p>
            </div>
            <div class="decision-branch">
                <strong>Closeness Centrality</strong>
                <p>Measures the average shortest distance from a node to all other nodes in the network. High closeness indicates a node that can reach others efficiently.</p>
            </div>
             <div class="decision-branch">
                <strong>Eigenvector Centrality</strong>
                <p>A more sophisticated measure. A node's importance is determined by the importance of its neighbors. It's a measure of influence within the network. Google's PageRank is a variant of this.</p>
            </div>
        </div>
    </div>
    
    <div id="p4s12ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Centrality</h4>
        <p>A powerful way to visualize centrality is to map the metric to a visual property of the nodes, like their size. This makes the most important nodes instantly stand out.</p>
        <div class="plot-container">
            <div id="p4s12ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> In this network, the size of each node is proportional to its **Betweenness Centrality**. The node "Dania" is clearly the largest. Even though "Ali" has the same number of direct connections (degree), "Dania" is more important because she acts as the critical bridge connecting Ali and Bilal's group to Esha. Without Dania, the network would be disconnected.</p>
        </div>
    </div>
    
    <div id="p4s12ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> Using the Zachary's Karate Club dataset (a famous social network), we want to find the most important individuals based on degree and betweenness centrality.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(igraph)
karate <- make_graph("Zachary") # Load the built-in dataset</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Calculate Centrality Metrics.</strong> The <code>igraph</code> package has functions for all major metrics.
                <div class="code-container">
                    <pre><code class="language-r">deg <- degree(karate)
bet <- betweenness(karate)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Visualize the Network with Sized Nodes.</strong> We pass the centrality scores to the <code>vertex.size</code> argument in the <code>plot()</code> function.
                <div class="code-container">
                    <pre><code class="language-r"># Set plot layout
l <- layout_with_fr(karate)
# Plot with node size based on betweenness
plot(karate, layout = l, vertex.size = bet * 0.2, vertex.label.cex = 0.8)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use <code>ggraph</code> for <code>ggplot2</code>-style Network Plots</h3>
    <div class="scenario-content">
        <p>While <code>igraph</code>'s base <code>plot()</code> function is useful for quick checks, the <code>ggraph</code> package allows you to create beautiful, publication-quality network visualizations using the familiar Grammar of Graphics from <code>ggplot2</code>. It gives you full control over aesthetics like node color, size, edge width, and annotations.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>karate</code> graph from the example.</li>
            <li>Calculate the degree centrality using <code>degree()</code>. Which two nodes have the highest degree? (These are the instructor and the club administrator).</li>
            <li>Calculate the betweenness centrality using <code>betweenness()</code>. Do the same two nodes also have the highest betweenness?</li>
            <li>Create a plot where the node size is mapped to degree centrality.</li>
        </ul>
    </div>
</div>
`,
          p4s12ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A social media analyst is studying a large network of user interactions. They hypothesize that the network is not one big, homogenous group, but is instead composed of several distinct "communities" or "cliques" of users who interact much more frequently with each other than with users outside their group. They need an algorithm for **Community Detection** to automatically find these clusters.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s12ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s12ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p4s12ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s12ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Finding Clusters in a Network</h4>
        <p><strong>Community Detection</strong> (or graph clustering) is the task of partitioning a network's nodes into groups where the density of connections *within* a group is high, and the density of connections *between* different groups is low.</p>
        <p><strong>Analogy:</strong> Community detection is like looking at a **map of a country at night from space**. You can't see the state borders (the pre-defined labels), but you can clearly see the clusters of lights that represent major cities. The lights within a city are dense and close together, while the areas between cities are dark and sparse. Community detection algorithms find these "cities" of nodes in your network data.</p>
    </div>
    
    <div id="p4s12ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Discovered Communities</h4>
        <p>The best way to visualize the output of a community detection algorithm is to color the nodes in the network plot according to their assigned community.</p>
        <div class="plot-container">
            <div id="p4s12ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This shows the Zachary's Karate Club network. The community detection algorithm has successfully found two main communities (colored in orange and blue). Historically, this is exactly what happened: a conflict between the club's administrator (node 1) and instructor (node 34) caused the club to split into two separate factions, which the algorithm rediscovered purely from the network structure.</p>
        </div>
    </div>
    
    <div id="p4s12ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to apply the Walktrap community detection algorithm to the Zachary's Karate Club network to see if we can find the two historical factions.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(igraph)
karate <- make_graph("Zachary")</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Run the Community Detection Algorithm.</strong> The <code>igraph</code> package has many algorithms. <code>cluster_walktrap()</code> is a popular one based on random walks.
                <div class="code-container">
                    <pre><code class="language-r">wtc <- cluster_walktrap(karate)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Inspect the Community Object.</strong> The resulting object contains information about the found communities. <code>modularity()</code> is a key metric that measures how good the community structure is (higher is better).
                <div class="code-container">
                    <pre><code class="language-r">modularity(wtc)
membership(wtc) # Get the community ID for each node
length(wtc)       # Get the number of communities found</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Visualize with Community Colors.</strong> We pass the community membership vector to the <code>vertex.color</code> argument in the <code>plot()</code> function.
                <div class="code-container">
                    <pre><code class="language-r">plot(karate, vertex.color = membership(wtc), 
     vertex.label.cex = 0.8, layout = layout_with_fr)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Choosing an Algorithm</h3>
    <div class="scenario-content">
        <p>There are many different community detection algorithms (e.g., Louvain, Girvan-Newman, Infomap), and they can sometimes give different results. There is no single "best" algorithm for all networks. The Louvain method (<code>cluster_louvain()</code> in <code>igraph</code>) is a very popular and fast choice for large networks, and is often a great starting point.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>karate</code> graph and the <code>wtc</code> result from the example.</li>
            <li>How many communities did the Walktrap algorithm find by default?</li>
            <li>The <code>sizes()</code> function tells you how many nodes are in each community. Run <code>sizes(wtc)</code>. What are the sizes of the discovered communities?</li>
            <li>Look at the plot. Do the community assignments seem visually sensible?</li>
        </ul>
    </div>
</div>
`,
          p4s12ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A team of data scientists has mastered the tools of network analysis. Now they need to apply these tools to solve real-world business and scientific problems, from identifying influential people on social media in Karachi to understanding protein interactions in a cell.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">From Theory to Impact</h3>
    <p>Network analysis is not just an abstract mathematical field; it's a powerful lens for understanding a wide range of complex, interconnected systems.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-satellite-dish"></i> Real-World Applications</div>
            <div class="scenario">The core goal is to use the structure of the network to gain insights that wouldn't be visible from looking at the individual entities alone.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-users"></i> Social Network Analysis</div>
                        <div class="scenario">This is the classic application. We can identify influential individuals (high centrality), find communities or "echo chambers" (community detection), and model the spread of information or rumors through the network.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-brain"></i> Biological Networks</div>
                        <div class="scenario">In bioinformatics, nodes can be genes or proteins, and edges can be interactions. Network analysis helps identify critical proteins in a disease pathway (high centrality) and find functional modules or protein complexes (community detection).</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-dollar-sign"></i> Financial Systems</div>
                        <div class="scenario">Nodes can be banks and edges can be loans between them. Network analysis can help identify "systemically important" institutions whose failure could trigger a cascade through the entire financial system (contagion modeling).</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-truck"></i> Transportation & Logistics</div>
                        <div class="scenario">Analyzing road networks or flight paths as graphs allows for optimizing routes (shortest path algorithms), identifying critical infrastructure hubs (high centrality), and modeling traffic flow.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Dynamic Networks</h3>
    <div class="scenario-content">
        <p>Many real-world networks are not static; they evolve over time. **Dynamic network analysis** is an advanced field that studies how network structure and metrics change. This is crucial for understanding processes like the growth of a social network, the spread of a disease, or the changing patterns of global trade.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You are a data scientist for an e-commerce company. You build a network where the nodes are your products and you draw an edge between two products if they are frequently bought together (based on association rule mining).</p>
        <ul class="prose-list">
            <li>What business question could you answer by calculating the **degree centrality** of each product in this network?</li>
            <li>What business insight could you gain by running a **community detection** algorithm on this network?</li>
        </ul>
    </div>
</div>
`,
          p4s13ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A marketing manager sends a discount offer to 10,000 customers. They see that 1,000 of them made a purchase. A standard classification model might focus on finding more people like these 1,000. But this is flawed. Some of those customers would have purchased anyway, without the discount! The real goal is to find the **Persuadables**: the customers who only purchased *because* they received the offer. This requires **Uplift Modeling**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s13ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s13ss1-viz">The Four Personas</button>
        <button class="tab-button" data-tab="p4s13ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s13ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Modeling the Net Impact of an Intervention</h4>
        <p><strong>Uplift Modeling</strong> (also known as incremental response, true lift, or net lift modeling) is a predictive modeling technique that aims to estimate the causal effect of an intervention on an individual's behavior. Instead of predicting the outcome, it predicts the **change** in the outcome due to the treatment.</p>
        <p><strong>Analogy:</strong> A standard model is like a **doctor who predicts if a patient will recover**. An uplift model is like a doctor who predicts **how much *more likely* a patient is to recover if they are given a specific drug versus a placebo**. It models the incremental benefit of the treatment itself.</p>
    </div>
    
    <div id="p4s13ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The Four Quadrants of Uplift</h4>
        <p>Uplift modeling categorizes the population into four groups based on how they would behave with and without the treatment.</p>
        <div class="plot-container">
            <div id="p4s13ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The goal of uplift modeling is to find and target the **Persuadables**, as this is the only group where marketing spend generates a positive ROI. Targeting "Sure Things" is a waste of money, and targeting "Sleeping Dogs" can actually hurt your business.</p>
        </div>
    </div>
    
    <div id="p4s13ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have data from a randomized A/B test (a "treatment" and "control" group) and we want to build a model to predict the uplift score for each customer.</p>
        </div>
        <p>The simplest method is the **Two-Model Approach**:</p>
        <ol class="prose-list">
            <li><strong>Step 1: Split the Data.</strong> Separate your dataset into a treatment group and a control group.
                <div class="code-container">
                    <pre><code class="language-r">treatment_group <- filter(ab_test_data, treatment == 1)
control_group <- filter(ab_test_data, treatment == 0)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Train Two Separate Models.</strong> Train one classification model on the treatment group to predict P(Outcome | Treated). Train a second model on the control group to predict P(Outcome | Control).
                <div class="code-container">
                    <pre><code class="language-r"># Assume we are using a random forest
model_treat <- randomForest(outcome ~ ., data = treatment_group)
model_control <- randomForest(outcome ~ ., data = control_group)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Calculate Uplift Score.</strong> For any new customer, get a prediction from *both* models. The uplift score is the difference.
                <div class="code-container">
                    <pre><code class="language-r">prob_treat <- predict(model_treat, newdata = new_customer, type = "prob")
prob_control <- predict(model_control, newdata = new_customer, type = "prob")
uplift_score <- prob_treat - prob_control</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> Customers with the highest positive uplift scores are the "Persuadables" and should be targeted. Customers with scores near zero are "Sure Things" or "Lost Causes". Customers with negative scores are the "Sleeping Dogs" and should be avoided.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Requires Randomized Experiments</h3>
    <div class="scenario-content">
        <p>Uplift modeling is fundamentally a causal inference technique. To build a reliable uplift model, you **must** have data from a randomized controlled trial (an A/B test) where the marketing intervention was randomly assigned. Without this randomization, you cannot disentangle the treatment effect from selection bias, and your uplift scores will be unreliable.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>A customer has a predicted probability of purchasing of 0.80 if they get an offer, and a predicted probability of purchasing of 0.75 if they do not get an offer. What is their uplift score? Which of the four personas do they belong to? Would you send them the offer?</li>
            <li>Another customer has a predicted probability of purchasing of 0.30 if they get an offer, and a predicted probability of purchasing of 0.05 if they do not. What is their uplift score? Which persona do they belong to? Would you send them the offer?</li>
        </ul>
    </div>
</div>
`,
          p4s13ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has built an uplift model that scores every customer on how likely they are to be "persuaded" by a marketing offer. Now they need to evaluate it. Standard classification metrics like ROC AUC won't work, because they can't distinguish between "Sure Things" (who convert anyway) and "Persuadables" (who convert because of the offer). They need specialized tools like the **Uplift Curve** or **Qini Curve**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s13ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p4s13ss2-viz">The Uplift Curve</button>
        <button class="tab-button" data-tab="p4s13ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s13ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Measuring Incremental Gain</h4>
        <p>Uplift model evaluation metrics are designed to answer the business question: "If we target the top X% of customers as ranked by our model, what is the total incremental lift in conversions we can expect compared to a random targeting strategy?"</p>
        <p><strong>Analogy:</strong> Evaluating an uplift model is like being a **political campaign manager**. Your goal isn't just to find people who will vote for you; it's to find the undecided "swing voters" whose minds you can actually change. An uplift curve shows you how much more effective your campaign is at persuading these swing voters compared to just canvassing randomly.</p>
    </div>
    
    <div id="p4s13ss2-viz" class="tab-pane">
        <h4 class="subsection-title">The Uplift and Qini Curves</h4>
        <p>The **Uplift Curve** plots the incremental gain in conversions as you target progressively larger portions of the population, ranked by their uplift score. The **Qini Curve** is a closely related plot that is often preferred because it is less sensitive to the balance between the treatment and control groups.</p>
        <div class="plot-container">
            <div id="p4s13ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The x-axis is the proportion of the population targeted. The y-axis is the cumulative incremental gain. The dashed diagonal line represents a random targeting strategy. The blue curve is our uplift model. The **area between the model curve and the random line** is the "Qini Coefficient," which is analogous to AUC for an ROC curve. A larger area means a better model. The plot shows that by targeting the top 20% of the population, our model generates a much higher lift than a random approach.</p>
        </div>
    </div>
    
    <div id="p4s13ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have predictions from an uplift model and the actual outcomes from an A/B test. We need to evaluate the model's performance by creating a Qini curve using the <code>uplift</code> package.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Prepare Data.</strong> The evaluation function requires separate vectors for the predicted uplift scores, the treatment indicator, and the outcome indicator.
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("uplift")
library(uplift)
# Assume 'test_data' has columns: uplift_score, treatment (1/0), outcome (1/0)
                    </code></pre>
                </div>
            </li>
            <li><strong>Step 2: Use the <code>performance()</code> function.</strong> This function calculates the necessary points for plotting the curve.
                <div class="code-container">
                    <pre><code class="language-r"># We specify which group is treatment (ct=1) and which is control (cc=0)
perf <- performance(test_data$uplift_score, 
                    test_data$treatment, 
                    test_data$outcome, 
                    ct = 1, cc = 0)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Plot the Qini Curve.</strong> We use the <code>plot()</code> method for the performance object.
                <div class="code-container">
                    <pre><code class="language-r">plot(perf, "qini")</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Qini Coefficient</h3>
    <div class="scenario-content">
        <p>Just like AUC for an ROC curve, the **Qini Coefficient** is a single number that summarizes the performance of your uplift model. It is the area between your model's Qini curve and the random chance diagonal. A larger Qini coefficient means a more effective uplift model. This is the primary metric you would use to compare different uplift models during a tuning process.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You show your manager a Qini curve for your new uplift model. The curve is almost identical to the random chance diagonal line. What does this tell you about your model's ability to identify "Persuadables"? Is it a useful model?</li>
            <li>You build a second model, and its Qini curve is much higher, with a large area between it and the random line. However, the curve goes flat after targeting the top 30% of the population. What does this tell you about the size of the "Persuadable" segment in your customer base?</li>
        </ul>
    </div>
</div>
`,
          p4s13ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A marketing team in Karachi has built and validated a powerful uplift model. 🇵🇰 Now, they need to put it to work to solve real-world business problems, from optimizing their marketing budget to personalizing the customer experience.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">From Uplift Scores to Business Impact</h3>
    <p>The output of an uplift model—a score for each customer representing their likelihood of being positively influenced—is the key to a range of powerful business applications.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-store"></i> Business Applications of Uplift Modeling</div>
            <div class="scenario">The core goal is to use the uplift scores to allocate resources more efficiently and personalize interactions.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-dollar-sign"></i> Marketing Campaign Targeting & ROI</div>
                        <div class="scenario">This is the classic application. Instead of sending a discount to everyone, you only send it to customers with high uplift scores (the "Persuadables"). This maximizes the number of incremental conversions while minimizing the cost of sending offers to "Sure Things" (who would have bought anyway) and "Lost Causes."</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-minus"></i> Customer Retention & Churn Prevention</div>
                        <div class="scenario">Instead of a sales offer, the "treatment" could be a retention offer (like a special discount or a call from a support agent). The uplift model identifies the "at-risk" customers who can actually be saved by the intervention, allowing the company to focus its retention efforts where they will have the most impact.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-friends"></i> Personalized Recommendations</div>
                        <div class="scenario">An uplift model can be used to recommend the product that will have the highest *incremental* impact on a user's likelihood to purchase, rather than just recommending the most popular item or the item they are already most likely to buy.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Move Beyond a Single Treatment</h3>
    <div class="scenario-content">
        <p>The concepts of uplift modeling can be extended to situations with multiple treatment options. Instead of a simple Treatment vs. Control A/B test, you can run an experiment with Treatment A, Treatment B, Treatment C, and a Control group. You can then build a model that predicts the uplift for *each* treatment for every customer. This allows you to move from "Should we contact this customer?" to "What is the *best* way to contact this customer?"—a true personalization engine.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You are a data scientist for a non-profit organization that is running a fundraising campaign. The "treatment" is a phone call from a volunteer asking for a donation. Phone calls are expensive and time-consuming.</p>
        <ul class="prose-list">
            <li>Describe the four personas in the context of this fundraising campaign (Sure Things, Persuadables, Sleeping Dogs, Lost Causes).</li>
            <li>Why would an uplift model be more valuable for this campaign than a standard classification model that just predicts the likelihood of donating?</li>
        </ul>
    </div>
</div>
`,
          p5s1ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst trains a model on their entire dataset and achieves 99% accuracy. They are thrilled. However, when the model is deployed to make predictions on new, real-world data, it performs terribly. The model didn't learn the true underlying patterns; it simply **memorized** the training data, including its noise. This is called overfitting, and it's the single biggest pitfall that data splitting strategies are designed to prevent.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s1ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s1ss1-viz">The Workflow</button>
        <button class="tab-button" data-tab="p5s1ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p5s1ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Never Test on Data You've Trained On</h4>
        <p>The golden rule of model validation is that the final performance of a model must be evaluated on data it has **never seen before**. To achieve this, we split our available data into distinct subsets.</p>
        <p><strong>Analogy:</strong> This process is like being a responsible student preparing for a final exam. You use the textbook and old homework problems (the **training set**) to learn the material. You then test your knowledge on a practice exam from a previous year (the **validation set**) to see how you're doing and decide if you need to study more. Finally, you take the real final exam (the **test set**), which you have never seen before, to get your true final score.</p>
        <ul class="prose-list">
            <li><strong>Training Set:</strong> The largest portion of the data, used to teach the model and fit its parameters.</li>
            <li><strong>Testing Set:</strong> A smaller portion of the data, held out until the very end. It's used only *once* to get a final, unbiased estimate of the model's performance on unseen data.</li>
            <li><strong>Validation Set:</strong> An optional split of the training data used during the modeling process to tune hyperparameters and select the best model, without "contaminating" the final test set.</li>
        </ul>
    </div>
    
    <div id="p5s1ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Data Split</h4>
        <p>This diagram shows how the original dataset is partitioned into the three key subsets for a robust validation workflow.</p>
        <div class="plot-container">
            <div id="p5s1ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The original data is first split into a large Training Set and a smaller Test Set. The Test Set is locked away and not touched. The Training Set is then further subdivided for the modeling process, allowing for model tuning on a validation set while still preserving the final test set as truly unseen data.</p>
        </div>
    </div>

    <div id="p5s1ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We need to prepare the <code>iris</code> dataset for a classification model by creating a simple train/test split. The <code>rsample</code> package from <code>tidymodels</code> is the modern standard for this.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("rsample")
library(rsample)
library(dplyr)
data(iris)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Create the Split Object.</strong> The <code>initial_split()</code> function creates an object that *contains the instructions* for the split. It doesn't actually split the data yet. The <code>prop</code> argument specifies the proportion of data to be used for training. <code>strata</code> ensures that the proportion of each <code>Species</code> is the same in both the training and testing sets.
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42) # For reproducibility
iris_split <- initial_split(iris, prop = 0.75, strata = Species)
print(iris_split)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Extract the Training and Testing Data.</strong> We use the <code>training()</code> and <code>testing()</code> functions to actually create the data frames.
                <div class="code-container">
                    <pre><code class="language-r">train_data <- training(iris_split)
test_data <- testing(iris_split)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Verify the Split.</strong> We check the dimensions and class proportions of our new datasets.
                <div class="code-container">
                    <pre><code class="language-r"># Check dimensions
cat("Training data dimensions:", dim(train_data), "\n")
cat("Testing data dimensions:", dim(test_data), "\n")

# Check class proportions
train_data %>% count(Species) %>% mutate(prop = n / sum(n))
test_data %>% count(Species) %>% mutate(prop = n / sum(n))</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> We have successfully and safely split our data. The training set contains about 75% of the data, and the test set contains 25%. Because we used stratified sampling, the proportion of each species is nearly identical in both sets, ensuring our test set is a fair and representative sample for evaluation.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Stratified Sampling is Crucial</h3>
    <div class="scenario-content">
        <p>For classification problems, always use **stratified sampling** when you split your data. This ensures that the distribution of your outcome variable (the classes) is the same in your training and testing sets. If you have an imbalanced dataset and do a simple random split, you might accidentally end up with zero examples of the rare class in your test set, making it impossible to evaluate your model's performance on that class.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset.</li>
            <li>Use <code>initial_split()</code> to create a split object where 80% of the data is for training. Since this is a regression problem (predicting <code>mpg</code>), we don't need to stratify.</li>
            <li>Extract the training and testing data frames.</li>
            <li>Use <code>nrow()</code> to confirm that your training and testing sets have the correct number of rows (approximately 80% and 20% of the original 32 rows).</li>
        </ul>
    </div>
</div>
`,
          p5s1ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst performs a single train/test split to evaluate their model and gets 85% accuracy. But what if this was just a "lucky" split where the test set happened to be unusually easy? To get a more robust and reliable estimate of the model's true performance on unseen data, they should use **k-fold Cross-Validation**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s1ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s1ss2-viz">The Workflow</button>
        <button class="tab-button" data-tab="p5s1ss2-types">Key Methods</button>
        <button class="tab-button" data-tab="p5s1ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p5s1ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">A More Robust Approach to Validation</h4>
        <p><strong>Cross-Validation (CV)</strong> is a resampling procedure used to evaluate machine learning models on a limited data sample. It provides a more stable and less biased estimate of how the model is likely to perform on unseen data compared to a single train/test split.</p>
        <p><strong>Analogy:</strong> A single train/test split is like a **single practice exam**. You might score well because you got lucky with the questions. Cross-validation is like taking **multiple different practice exams**. Your average score across all of them is a much more reliable indicator of your true knowledge and how you'll perform on the final exam.</p>
    </div>

    <div id="p5s1ss2-viz" class="tab-pane">
        <h4 class="subsection-title">The k-Fold Cross-Validation Process</h4>
        <p>This diagram shows the process for 5-fold cross-validation. The training data is split into 5 "folds," and the model is trained and evaluated 5 times.</p>
        <div class="plot-container">
            <div id="p5s1ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> In **Fold 1**, the model is trained on folds 2-5 and evaluated on fold 1. In **Fold 2**, it's trained on folds 1, 3, 4, and 5 and evaluated on fold 2. This process repeats for all 5 folds. The final cross-validation score is the average of the 5 individual evaluation scores, giving a robust estimate of the model's performance.</p>
        </div>
    </div>
    
    <div id="p5s1ss2-types" class="tab-pane">
        <h4 class="subsection-title">A Family of Resampling Methods</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>k-Fold Cross-Validation</strong>
                <p>The most common method. The data is split into <code>k</code> folds (typically 5 or 10). The model is trained <code>k</code> times, each time using a different fold as the hold-out set.</p>
            </div>
            <div class="decision-branch">
                <strong>Stratified k-Fold</strong>
                <p>An extension of k-fold for classification problems. It ensures that each fold has approximately the same percentage of samples of each target class as the complete set.</p>
            </div>
             <div class="decision-branch">
                <strong>Leave-One-Out (LOOCV)</strong>
                <p>An extreme version of k-fold where <code>k</code> is equal to the number of data points. The model is trained on all but one data point and tested on that single point. This is repeated for every data point. It's computationally very expensive.</p>
            </div>
        </div>
    </div>

    <div id="p5s1ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to get a robust estimate of a random forest model's accuracy on the <code>iris</code> dataset using 10-fold cross-validation. We will use the <code>tidymodels</code> framework.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Define Folds.</strong> We use <code>vfold_cv()</code> to create the resampling plan.
                <div class="code-container">
                    <pre><code class="language-r">library(tidymodels)
set.seed(42)
iris_folds <- vfold_cv(iris, v = 10, strata = Species)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Define the Model and Workflow.</strong> We create a model specification and bundle it with a formula into a workflow object.
                <div class="code-container">
                    <pre><code class="language-r">rf_spec <- rand_forest(trees = 100) %>%
  set_engine("randomForest") %>%
  set_mode("classification")

rf_workflow <- workflow() %>%
  add_formula(Species ~ .) %>%
  add_model(rf_spec)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Fit the Model to the Resamples.</strong> The <code>fit_resamples()</code> function does the hard work of training and evaluating the model on all 10 folds.
                <div class="code-container">
                    <pre><code class="language-r">rf_results <- fit_resamples(rf_workflow, resamples = iris_folds)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Collect and Analyze the Metrics.</strong> We use <code>collect_metrics()</code> to see the performance from each fold and the overall average.
                <div class="code-container">
                    <pre><code class="language-r">collect_metrics(rf_results)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The output will show the accuracy and ROC AUC for each of the 10 folds, along with the mean performance. This mean value is our robust, cross-validated estimate of how the model will perform on new data. It is much more reliable than a single train/test split score.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Cross-Validation for Tuning</h3>
    <div class="scenario-content">
        <p>The primary use of cross-validation in practice is not just to get a final performance estimate, but to perform **hyperparameter tuning**. You create a grid of possible hyperparameter values (e.g., different values of <code>k</code> for a k-NN model), and you use cross-validation to estimate the performance for *each* value. You then choose the hyperparameter value that gave the best average cross-validated performance.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset.</li>
            <li>Use the <code>vfold_cv()</code> function from <code>rsample</code> to create 5 cross-validation folds.</li>
            <li>Define a simple linear model specification using <code>tidymodels</code>: <code>lm_spec <- linear_reg() %>% set_engine("lm")</code>.</li>
            <li>Create a workflow that combines this model with the formula <code>mpg ~ wt + hp</code>.</li>
            <li>Use <code>fit_resamples()</code> to fit your model to the 5 folds.</li>
            <li>Use <code>collect_metrics()</code> to see the results. What is the average cross-validated RMSE (Root Mean Squared Error) of your model?</li>
        </ul>
    </div>
</div>
`,
          p5s1ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has fit a linear regression model. The summary provides a single point-estimate for a coefficient. They know this estimate is not perfect and has some uncertainty. They want to quantify this uncertainty by creating a 95% confidence interval for the coefficient. The **Bootstrap** is a powerful resampling method that can do this without relying on complex mathematical formulas or assumptions about the data's distribution.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s1ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s1ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p5s1ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p5s1ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Creating New Datasets from Your Dataset</h4>
        <p><strong>Resampling Methods** are a class of techniques that involve repeatedly drawing samples from a training set and refitting a model on each sample. This process allows us to obtain additional information about the fitted model that is not available from fitting it only once.</p>
        <p><strong>Analogy:</strong> Resampling is like having a single **photograph of a crowd** and wanting to know the average height. You can't survey everyone. So, you create hundreds of new "pseudo-crowds" by repeatedly taking random samples (with replacement) from the faces in your one photograph. By calculating the average height in each of your pseudo-crowds, you get a good sense of the distribution and uncertainty of the true average height.</p>
        <p>The most common and important resampling method is the **Bootstrap**.</p>
    </div>
    
    <div id="p5s1ss3-viz" class="tab-pane">
        <h4 class="subsection-title">The Bootstrap Process</h4>
        <p>This diagram shows how we can use our single training set to generate many new "bootstrap" training sets and build a distribution of our statistic of interest.</p>
        <div class="plot-container">
            <div id="p5s1ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> We start with our original training data. We sample from it *with replacement* to create many new bootstrap datasets of the same size. We then calculate our statistic (e.g., the median) for each bootstrap sample. The histogram of these thousands of calculated statistics forms the **bootstrap distribution**, which approximates the true sampling distribution of our statistic.</p>
        </div>
    </div>
    
    <div id="p5s1ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to find a 95% confidence interval for the median horsepower of cars in the <code>mtcars</code> dataset. The median is a difficult statistic to get a standard confidence interval for, so bootstrapping is a perfect tool.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Package and Prepare Data.</strong> We will use the <code>rsample</code> package to create our bootstrap samples.
                <div class="code-container">
                    <pre><code class="language-r">library(rsample)
library(dplyr)
library(purrr)
data(mtcars)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Generate Bootstrap Samples.</strong> The <code>bootstraps()</code> function creates an object containing our resamples.
                <div class="code-container">
                    <pre><code class="language-r">set.seed(42)
# Create 1000 bootstrap samples
boot_samples <- bootstraps(mtcars, times = 1000)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Calculate the Statistic for Each Sample.</strong> We use <code>purrr::map()</code> to iterate through each bootstrap sample, calculate the median hp, and return the results.
                <div class="code-container">
                    <pre><code class="language-r"># 'analysis()' extracts the data frame from each split object
median_estimates <- boot_samples %>%
  mutate(
    median_hp = map_dbl(splits, ~ median(analysis(.x)$hp))
  )</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Calculate the Confidence Interval.</strong> We use the percentile method on our vector of bootstrap estimates.
                <div class="code-container">
                    <pre><code class="language-r">quantile(median_estimates$median_hp, probs = c(0.025, 0.975))</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The output gives us a robust 95% confidence interval for the true median horsepower of all cars. This result is based on the empirical data and does not rely on the assumption that the data is normally distributed.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Permutation Tests</h3>
    <div class="scenario-content">
        <p>Another powerful resampling method is the **permutation test**. It's used to conduct hypothesis tests. To test if there's a difference in means between two groups, you can randomly shuffle the group labels thousands of times and re-calculate the difference in means for each shuffled dataset. This builds the null distribution of the test statistic, and you can see how extreme your actual observed difference is compared to this random distribution to get a p-value.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>iris</code> dataset.</li>
            <li>Create 1,000 bootstrap samples using the <code>bootstraps()</code> function.</li>
            <li>Use the <code>map_dbl()</code> workflow from the example to calculate the *correlation* between <code>Sepal.Length</code> and <code>Sepal.Width</code> for each bootstrap sample. (Hint: inside the map function, you'll need something like <code>cor(analysis(.x)$Sepal.Length, analysis(.x)$Sepal.Width)</code>).</li>
            <li>Calculate the 95% confidence interval for this correlation coefficient using the percentile method.</li>
        </ul>
    </div>
</div>
`,
          p5s1ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is building a k-Nearest Neighbors model. The choice of <code>k</code> (the number of neighbors) has a huge impact on the model's performance. They also need to decide whether to use a "uniform" or "distance-weighted" kernel. Manually testing every possible combination is tedious and error-prone. They need an automated process for **Hyperparameter Tuning** to find the best settings.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s1ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s1ss4-methods">Search Strategies</button>
        <button class="tab-button" data-tab="p5s1ss4-viz">Visualization</button>
        <button class="tab-button" data-tab="p5s1ss4-usecase">Step-by-Step Example</button>
    </div>

    <div id="p5s1ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Finding the Model's Best Settings</h4>
        <p><strong>Hyperparameters</strong> are the "knobs" or settings of a machine learning algorithm that are set by the analyst *before* the training process begins. They are not learned from the data like the model's internal parameters (e.g., regression coefficients).</p>
        <p><strong>Hyperparameter Tuning</strong> is the process of systematically searching for the combination of hyperparameter values that results in the best model performance, as measured by cross-validation.</p>
        <p><strong>Analogy:</strong> Tuning a model is like **tuning a radio**. The model is the radio. The training process is the antenna picking up the signal (the pattern in the data). The hyperparameters are the **tuning knobs** (frequency, volume). Your job is to turn these knobs to find the combination that gives you the clearest possible station (the best-performing model).</p>
    </div>

    <div id="p5s1ss4-methods" class="tab-pane">
        <h4 class="subsection-title">How to Search for the Best Knobs</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Grid Search</strong>
                <p>You specify a grid of discrete values for each hyperparameter. The algorithm then exhaustively tests every single combination of these values using cross-validation. It's thorough but can be very slow.</p>
            </div>
            <div class="decision-branch">
                <strong>Random Search</strong>
                <p>Instead of a fixed grid, you specify a range or distribution for each hyperparameter. The algorithm then randomly samples a fixed number of combinations from this search space. It is often more efficient than grid search.</p>
            </div>
            <div class="decision-branch">
                <strong>Bayesian Optimization</strong>
                <p>A "smarter" search method. It uses the results from previous trials to inform where to search next, focusing on the most promising areas of the hyperparameter space. It is the most computationally efficient method.</p>
            </div>
        </div>
    </div>
    
    <div id="p5s1ss4-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Search Space</h4>
        <p>This plot compares how Grid Search and Random Search explore the hyperparameter space for two parameters. Often, only one parameter is truly important.</p>
        <div class="plot-container">
            <div id="p5s1ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> In this example, the "Important Parameter" has a much larger impact on performance. **Grid Search** (left) wastes many of its trials testing different levels of the unimportant parameter. **Random Search** (right), by sampling randomly, is more likely to test a wider and more effective range of values for the truly important parameter, often finding a better model with fewer trials.</p>
        </div>
    </div>
    
    <div id="p5s1ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to find the optimal number of neighbors (<code>k</code>) for a k-NN model on the <code>iris</code> dataset using a cross-validated grid search.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Prepare Folds.</strong>
                <div class="code-container">
                    <pre><code class="language-r">library(tidymodels)
set.seed(42)
iris_folds <- vfold_cv(iris, v = 10, strata = Species)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Define a Tunable Model Specification.</strong> We create a model specification, but instead of a fixed value for neighbors, we use the <code>tune()</code> placeholder.
                <div class="code-container">
                    <pre><code class="language-r">knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Create a Tuning Grid.</strong> We define the specific values of <code>k</code> we want to test.
                <div class="code-container">
                    <pre><code class="language-r">knn_grid <- grid_regular(neighbors(range = c(1, 15)), levels = 8)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Run the Grid Search.</strong> The <code>tune_grid()</code> function combines the model, the data folds, and the grid, and runs the entire cross-validated tuning process.
                <div class="code-container">
                    <pre><code class="language-r">knn_tune_results <- tune_grid(
  knn_spec,
  Species ~ .,
  resamples = iris_folds,
  grid = knn_grid
)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> We can use <code>show_best(knn_tune_results, metric = "accuracy")</code> to see the best-performing <code>k</code> value. By automating the search, we have found the optimal hyperparameter for our model in a statistically robust way.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Start with Random Search</h3>
    <div class="scenario-content">
        <p>For complex models with many hyperparameters (like XGBoost or neural networks), a grid search is computationally impossible. In these cases, **Random Search** is almost always preferred. It is more efficient and often finds a better model in less time. The <code>tune</code> package supports this with the <code>grid_random()</code> function.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Follow the step-by-step example to run the grid search for the k-NN model.</li>
            <li>Use <code>collect_metrics(knn_tune_results)</code> to see the performance for every value of <code>k</code> that was tested.</li>
            <li>Use <code>autoplot(knn_tune_results)</code> to create a plot of the results. What value of <code>k</code> provides the highest accuracy?</li>
        </ul>
    </div>
</div>
`,
          p5s1ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst fits a 10th-degree polynomial regression to a dataset with only 12 data points. The line passes perfectly through every single point, achieving a training error of zero. However, the line is incredibly "wiggly" and will make terrible predictions for any new data. This model has low bias but extremely high variance—it is severely **overfit**. Conversely, a simple straight line through the same data is too simple and misses the true pattern. It is **underfit**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s1ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s1ss5-viz">Visualization</button>
    </div>

    <div id="p5s1ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Two Types of Model Error</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Underfitting (High Bias)</strong>
                <p>An underfit model is too simple. It has failed to capture the underlying signal in the data. It will have poor performance on both the training data and the test data.</p>
                <p><strong>Analogy:</strong> This is a student who didn't study at all. They perform poorly on the practice exams and also on the final exam. They haven't learned the material.</p>
            </div>
            <div class="decision-branch">
                <strong>Overfitting (High Variance)</strong>
                <p>An overfit model is too complex. It has not only learned the signal but has also learned the random noise in the training data. It will have excellent performance on the training data but poor performance on new, unseen test data.</p>
                <p><strong>Analogy:</strong> This is a student who has memorized the exact answers to the practice exams but hasn't learned the underlying concepts. They get 100% on the practice tests but fail the final exam because it has slightly different questions.</p>
            </div>
        </div>
    </div>
    
    <div id="p5s1ss5-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Fit</h4>
        <p>This plot shows three different models fit to the same data: one that is too simple, one that is too complex, and one that is "just right."</p>
        <div class="plot-container">
            <div id="p5s1ss5-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Underfit** model (a straight line) fails to capture the curved nature of the data. The **Overfit** model (a very high-degree polynomial) wiggles erratically to pass through every single point, modeling the noise. The **Good Fit** model (a simple curve) captures the underlying trend without being overly sensitive to the individual noisy data points.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Diagnosing with Learning Curves</h3>
    <div class="scenario-content">
        <p>A powerful way to diagnose overfitting and underfitting is to plot **learning curves**. These plots show the model's performance on the training set and the validation set as a function of the training set size. If the training and validation scores are both poor and have converged, the model is likely underfitting. If there is a large gap between a high training score and a low validation score, the model is overfitting.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You train a decision tree model. On your training data, it has an accuracy of 99.8%. On your held-out test data, it has an accuracy of 75%. Is this model more likely to be underfit or overfit? Why?</li>
            <li>You train a simple logistic regression model. Its accuracy on the training data is 60%, and its accuracy on the test data is 61%. Is this model more likely to be underfit or overfit? What might you try next to improve performance?</li>
        </ul>
    </div>
</div>
`,
          p5s1ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is choosing between a simple, interpretable linear model and a complex, "black-box" gradient boosting model. The simple model has a slightly higher error on the test set, but is much faster and easier to explain. The complex model is more accurate, but slow and opaque. This is the **Bias-Variance Tradeoff** in action: the fundamental challenge of balancing model complexity with its ability to generalize to new data.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s1ss6-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s1ss6-viz">Visualization</button>
    </div>

    <div id="p5s1ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Three Sources of Prediction Error</h4>
        <p>The total expected error of any supervised learning model can be decomposed into three parts:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Bias</strong>
                <p>The error introduced by approximating a real-world problem, which may be very complicated, by a much simpler model. High-bias models are often **underfit**.</p>
            </div>
            <div class="decision-branch">
                <strong>Variance</strong>
                <p>The amount by which your model's prediction would change if you trained it on a different training dataset. High-variance models are often **overfit**.</p>
            </div>
            <div class="decision-branch">
                <strong>Irreducible Error</strong>
                <p>The error that cannot be reduced by any model. It's the inherent noise or randomness in the system you are trying to model.</p>
            </div>
        </div>
        <p style="margin-top:1.5rem;"><strong>The Tradeoff:</strong> As you increase the flexibility and complexity of your model, its bias will decrease, but its variance will increase. The goal of a good data scientist is to find the sweet spot that minimizes the *total* error, not just one component.</p>
    </div>
    
    <div id="p5s1ss6-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Tradeoff</h4>
        <p>This classic plot shows how the three components of error change as model complexity increases.</p>
        <div class="plot-container">
            <div id="p5s1ss6-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> On the left, a simple model has **high bias** but low variance. On the right, a very complex model has **high variance** but low bias. The total error (the red curve) is minimized at a point of moderate complexity. This "sweet spot" is the model that will generalize best to new, unseen data.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Ensemble Methods Manage the Tradeoff</h3>
    <div class="scenario-content">
        <p>Ensemble methods are powerful specifically because they are designed to manage the bias-variance tradeoff:</p>
        <ul class="prose-list">
            <li><strong>Bagging (e.g., Random Forest):</strong> This technique primarily reduces **variance**. It takes many high-variance, low-bias models (deep decision trees) and averages them together. The averaging process cancels out the noise and results in a single model with low variance.</li>
            <li><strong>Boosting (e.g., Gradient Boosting):</strong> This technique primarily reduces **bias**. It starts with a simple, high-bias model and sequentially adds new models that correct the errors, progressively reducing the overall bias of the ensemble.</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>A linear regression model is generally considered a low-complexity model. Does it typically have high bias or high variance?</li>
            <li>A very deep, unpruned decision tree is a high-complexity model. Does it typically have high bias or high variance?</li>
            <li>You have a model with high bias and low variance. What is one specific thing you could do to your features or your model to try and improve its performance? (Hint: Think about increasing complexity).</li>
            <li>You have a model with low bias and high variance. What is one specific technique you could use to improve its performance? (Hint: Think about penalizing complexity).</li>
        </ul>
    </div>
</div>
`,
          p5s1ss7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is comparing two models for predicting sales. Model A is a simple linear model. Model B is a more complex polynomial model. Model B has a slightly higher R-squared, but it's also more complex. How can the analyst make a principled decision about which model is truly better, penalizing the second model for its added complexity? They can use an **Information Criterion**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s1ss7-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s1ss7-methods">AIC vs. BIC</button>
        <button class="tab-button" data-tab="p5s1ss7-usecase">Step-by-Step Example</button>
    </div>

    <div id="p5s1ss7-concepts" class="tab-pane active">
        <h4 class="subsection-title">Balancing Fit and Complexity</h4>
        <p>An **Information Criterion** is a statistical measure used for model selection. It provides a score that represents a tradeoff between a model's goodness-of-fit and its simplicity (parsimony).</p>
        <p>The goal is always to find the model with the **lowest** information criterion score.</p>
        <p><strong>Analogy:</strong> Information criteria are like a **golf score**. In golf, you want the lowest possible score to win. A model's "score" is calculated by its error (how many shots you took) plus a penalty for complexity (a handicap for using more advanced clubs). A model that fits the data perfectly but is overly complex might have a higher (worse) score than a slightly less accurate but much simpler model.</p>
    </div>
    
    <div id="p5s1ss7-methods" class="tab-pane">
        <h4 class="subsection-title">Two Common Criteria</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>AIC (Akaike Information Criterion)</strong>
                <p>The most common criterion. It is focused on finding the model that is best for **prediction**.</p>
                <div class="math-foundation" style="margin-top:0.5rem;">$$ AIC = 2k - 2\\ln(\\hat{L}) $$</div>
                <div class="plot-description"><p>Where $k$ is the number of parameters and $\\hat{L}$ is the maximum likelihood of the model.</p></div>
            </div>
            <div class="decision-branch">
                <strong>BIC (Bayesian Information Criterion)</strong>
                <p>The BIC applies a stronger penalty for complexity than the AIC, as its penalty term depends on the number of data points. It is focused on finding the model that is the **best approximation of the true data-generating process**.</p>
                 <div class="math-foundation" style="margin-top:0.5rem;">$$ BIC = k\ln(n) - 2\ln(\\hat{L}) $$</div>
                 <div class="plot-description"><p>Where $n$ is the number of data points.</p></div>
            </div>
        </div>
        <div class="plot-description" style="margin-top:1.5rem;"><p><strong>Rule of Thumb:</strong> Because of its stronger penalty, BIC tends to favor simpler models than AIC.</p></div>
    </div>
    
    <div id="p5s1ss7-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to decide if adding a quadratic term to our regression of <code>dist</code> on <code>speed</code> in the <code>cars</code> dataset is justified. We will compare the AIC of the simple and polynomial models.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Fit the Candidate Models.</strong>
                <div class="code-container">
                    <pre><code class="language-r">model_simple <- lm(dist ~ speed, data = cars)
model_poly <- lm(dist ~ poly(speed, 2), data = cars)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Calculate the AIC for Each Model.</strong> We use the base R <code>AIC()</code> function.
                <div class="code-container">
                    <pre><code class="language-r">aic_simple <- AIC(model_simple)
aic_poly <- AIC(model_poly)
print(paste("Simple Model AIC:", round(aic_simple, 2)))
print(paste("Polynomial Model AIC:", round(aic_poly, 2)))</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Compare the Scores and Formulate a Conclusion.</strong>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The AIC for the polynomial model (e.g., 419.4) is significantly lower than the AIC for the simple model (e.g., 421.9). This tells us that the improved goodness-of-fit from adding the quadratic term is worth the extra complexity. We should choose the more complex polynomial model.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: AIC is Relative, Not Absolute</h3>
    <div class="scenario-content">
        <p>The absolute value of an AIC score is meaningless. It is only useful for **comparing** a set of candidate models that have been fit to the exact same dataset. You cannot compare the AIC of a model fit to dataset A with the AIC of a model fit to dataset B.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, fit three different models to predict <code>mpg</code>:
                <ul class="prose-list"></ul>
                    <li>A simple model using only <code>wt</code>.</li>
                    <li>A model using wt and hp.</li>
                    <li>A model using wt, hp, and qsec.</li>
                </ul>
            </li>
            <li>Calculate the AIC for all three models.</li>
            <li>Calculate the BIC for all three models using the <code>BIC()</code> function.</li>
            <li>Based on both AIC and BIC, which model is the best among these three candidates?</li>
        </ul>
    </div>
</div>
`,
          p5s1ss8: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst in Karachi is about to start a new modeling project. 🇵🇰 To ensure their work is credible, reproducible, and avoids common pitfalls, they follow a systematic checklist of best practices, from splitting their data correctly at the beginning to choosing the right evaluation metric at the end.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Trustworthy Modeling</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Model Validation Best Practices</div>
            <div class="scenario">A robust workflow that moves from data preparation to a final, validated model.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-columns"></i> 1. Split Data First</div>
                        <div class="scenario"><strong>The Rule:</strong> The very first step, before any preprocessing or modeling, is to split your data into training and testing sets. The test set should not be touched again until the final evaluation.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-secret"></i> 2. Prevent Data Leakage</div>
                        <div class="scenario"><strong>The Rule:</strong> Any "learning" step in your preprocessing pipeline (e.g., calculating means for scaling, learning imputation models, finding principal components) must be done on the **training data only**. The <code>recipes</code> package is designed to enforce this.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-random"></i> 3. Use Cross-Validation for Tuning</div>
                        <div class="scenario"><strong>The Rule:</strong> Do not tune your hyperparameters based on performance on the test set. Use k-fold cross-validation on your *training set* to find the best hyperparameters. This gives a much more reliable estimate of performance.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-tachometer-alt"></i> 4. Use a Simple Baseline</div>
                        <div class="scenario"><strong>The Rule:</strong> Always start with a simple, interpretable model (like a linear or logistic regression) as a baseline. This gives you a performance score to beat and helps you understand if the complexity of a more advanced model is actually justified.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>tidymodels</code> Framework</h3>
    <div class="scenario-content">
        <p>All of these best practices are baked into the design of the <code>tidymodels</code> ecosystem. It provides a unified, consistent framework for data splitting (<code>rsample</code>), preprocessing (<code>recipes</code>), modeling (<code>parsnip</code>), tuning (<code>tune</code>), and evaluation (<code>yardstick</code>). While it has a learning curve, investing in <code>tidymodels</code> is the single best way to ensure your modeling workflows are robust, reproducible, and statistically sound.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You have a dataset. Your goal is to build and evaluate a random forest model.</p>
        <ul class="prose-list">
            <li>What is the very first thing you should do with your data?</li>
            <li>You need to scale your numeric features. Should you calculate the mean and standard deviation from the full dataset, or just the training set?</li>
            <li>You want to find the best value for the <code>mtry</code> hyperparameter in your random forest. Should you use your test set to do this? If not, what should you use?</li>
            <li>After you have selected your final, best model, what is the purpose of the test set you held out at the beginning?</li>
        </ul>
    </div>
</div>
`,
          p5s2ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst builds a model to detect a rare disease. The model achieves 99% accuracy. This sounds fantastic, but the disease is only present in 1% of the population. A useless model that simply *always* predicts "no disease" would also have 99% accuracy. This illustrates why accuracy alone is a terrible metric for imbalanced classes and why a more nuanced set of evaluation tools is required.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p4s2ss7-matrix">The Confusion Matrix</button>
        <button class="tab-button" data-tab="p4s2ss7-metrics">Key Metrics</button>
        <button class="tab-button" data-tab="p4s2ss7-viz">The ROC Curve</button>
        <button class="tab-button" data-tab="p4s2ss7-usecase">Step-by-Step Example</button>
    </div>

    <div id="p4s2ss7-matrix" class="tab-pane active">
        <h4 class="subsection-title">The Foundation of Evaluation</h4>
        <p>The **Confusion Matrix** is a table that breaks down the performance of a classification model. It compares the model's predictions to the actual, true classes.</p>
        <div class="plot-container">
            <div id="p5s2ss1-plot1" class="plotly-chart"></div>
        </div>
        <ul class="prose-list" style="margin-top:1rem;">
            <li><strong>True Positives (TP):</strong> The model correctly predicted the positive class. (Predicted disease, patient has disease).</li>
            <li><strong>True Negatives (TN):</strong> The model correctly predicted the negative class. (Predicted no disease, patient has no disease).</li>
            <li><strong>False Positives (FP):</strong> The model incorrectly predicted the positive class. (Predicted disease, patient has no disease). Also called a **Type I Error**.</li>
            <li><strong>False Negatives (FN):</strong> The model incorrectly predicted the negative class. (Predicted no disease, patient has disease). Also called a **Type II Error**.</li>
        </ul>
    </div>
    
    <div id="p4s2ss7-metrics" class="tab-pane">
        <h4 class="subsection-title">Beyond Accuracy</h4>
        <p>All the key classification metrics are derived from the four quadrants of the confusion matrix.</p>
        <ul class="prose-list">
            <li><strong>Accuracy:</strong> $(TP + TN) / \\text{Total}$. The overall percentage of correct predictions. Can be very misleading on imbalanced datasets.</li>
            <li><strong>Precision (Positive Predictive Value):</strong> $TP / (TP + FP)$. Of all the times the model predicted positive, what proportion was correct? It measures the cost of false positives.</li>
            <li><strong>Recall (Sensitivity or True Positive Rate):</strong> $TP / (TP + FN)$. Of all the actual positive cases, what proportion did the model correctly identify? It measures the cost of false negatives.</li>
            <li><strong>F1-Score:</strong> The harmonic mean of Precision and Recall. It provides a single score that balances both metrics.</li>
        </ul>
    </div>
    
    <div id="p4s2ss7-viz" class="tab-pane">
        <h4 class="subsection-title">The ROC Curve</h4>
        <p>An **ROC (Receiver Operating Characteristic) Curve** is a plot that shows the performance of a classification model at all possible classification thresholds. It plots the True Positive Rate (Recall) against the False Positive Rate.</p>
        <p>The **AUC (Area Under the Curve)** is the single number that summarizes the ROC curve. An AUC of 1.0 represents a perfect model. An AUC of 0.5 represents a useless model that is no better than random guessing.</p>
        <div class="plot-container">
            <div id="p5s2ss1-plot2" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The dashed diagonal line represents a random-chance model (AUC = 0.5). A good model (the blue curve) has an ROC curve that bows up towards the top-left corner, indicating a high True Positive Rate and a low False Positive Rate. The area under this blue curve (AUC) will be close to 1.0, indicating excellent discriminatory power.</p>
        </div>
    </div>

    <div id="p4s2ss7-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have a set of model predictions and the true values. We need to generate a comprehensive report of the model's performance.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Prepare the Data.</strong> We create factor variables for our actual and predicted values. It is critical that they have the same levels.
                <div class="code-container">
                    <pre><code class="language-r">actual <- factor(c("Yes", "No", "Yes", "Yes", "No", "No", "Yes"), levels = c("Yes", "No"))
predicted <- factor(c("Yes", "Yes", "Yes", "No", "No", "No", "Yes"), levels = c("Yes", "No"))</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Generate the Confusion Matrix.</strong> We use the <code>caret::confusionMatrix()</code> function, making sure to specify which class is "positive".
                <div class="code-container">
                    <pre><code class="language-r">library(caret)
conf_matrix <- confusionMatrix(data = predicted, reference = actual, positive = "Yes")</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Interpret the Output.</strong> We print the full results object.
                <div class="code-container">
                    <pre><code class="language-r">print(conf_matrix)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Formulate a Conclusion.</strong>
                <div class="plot-description">
                    <p><strong>Conclusion:</strong> "The model achieved an overall accuracy of 71.4%. For the 'Yes' class, the model had a Precision of 0.75 (meaning 75% of the time it predicted 'Yes', it was correct) and a Recall of 0.75 (meaning it successfully found 75% of all the actual 'Yes' cases). The F1-score provides a balanced summary of this performance."</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Precision-Recall Tradeoff</h3>
    <div class="scenario-content">
        <p>Precision and Recall are often in tension. If you want to increase Recall (find every possible case of the disease), you might have to lower your prediction threshold, which will likely lead to more False Positives and lower Precision. If you want to increase Precision (be very sure every positive prediction is correct), you might raise your threshold, which will lead to more False Negatives and lower Recall. The choice of which metric to optimize for is a business decision based on the costs of the two types of errors.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Consider a spam filter with the following results on 100 emails:</p>
        <ul class="prose-list">
            <li>True Positives (Spam correctly flagged): 8</li>
            <li>False Positives (Not-Spam flagged as Spam): 2</li>
            <li>True Negatives (Not-Spam correctly ignored): 88</li>
            <li>False Negatives (Spam missed): 2</li>
        </ul>
        <li>Calculate the Accuracy, Precision, and Recall for this model.</li>
        <li>Is accuracy a good metric here? Why or why not?</li>
        <li>Which error (False Positive or False Negative) do you think is more costly for a spam filter user? Which metric (Precision or Recall) does that correspond to?</li>
    </div>
</div>
`,
          p5s2ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst builds two different models to predict house prices in Karachi. Model A has an average error of Rs. 500,000. Model B has an average error of Rs. 450,000. To decide which model is better, they need a standardized way to quantify the prediction error. <strong>Regression Metrics</strong> provide this.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s2ss2-concepts">Key Metrics</button>
        <button class="tab-button" data-tab="p5s2ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p5s2ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p5s2ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Quantifying Prediction Error</h4>
        <p>Regression metrics measure the difference between the model's predicted values and the actual, observed values.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>MAE (Mean Absolute Error)</strong>
                <p>The average of the absolute differences between predictions and actuals. It's easy to interpret as the "typical" prediction error, in the original units of the target.</p>
                <div class="math-foundation" style="margin-top:0.5rem;">$$ MAE = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$</div>
            </div>
            <div class="decision-branch">
                <strong>RMSE (Root Mean Squared Error)</strong>
                <p>The square root of the average of the squared differences. It's the most common regression metric. Because it squares the errors, it penalizes large errors more heavily than MAE.</p>
                <div class="math-foundation" style="margin-top:0.5rem;">$$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$</div>
            </div>
            <div class="decision-branch">
                <strong>R-squared ($R^2$)</strong>
                <p>Measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with 1 being a perfect fit. It answers, "How well does my model explain the data?"</p>
            </div>
        </div>
    </div>
    
    <div id="p5s2ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Residuals</h4>
        <p>A residual plot is a scatter plot of the model's prediction errors (residuals) versus the predicted values. It's a key diagnostic tool and helps to visualize the error metrics.</p>
        <div class="plot-container">
            <div id="p5s2ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> Each point is a single observation. Its y-value is the residual ($y - \hat{y}$). MAE is the average absolute vertical distance from the points to the zero line. RMSE is the square root of the average *squared* vertical distance. For a good model, this plot should look like a random cloud of points centered on zero with no discernible patterns.</p>
        </div>
    </div>
    
    <div id="p5s2ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have fit a linear model to predict <code>mpg</code>. Now we need to calculate its performance metrics on the test data using the <code>yardstick</code> package from <code>tidymodels</code>.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Prepare Data.</strong> We need to split our data and fit a model to get predictions.
                <div class="code-container">
                    <pre><code class="language-r">library(tidymodels)
set.seed(42)
split <- initial_split(mtcars, prop = 0.8)
train_data <- training(split)
test_data <- testing(split)
model <- lm(mpg ~ wt, data = train_data)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Get Predictions.</strong> We create a tibble with the true values and the model's predictions.
                <div class="code-container">
                    <pre><code class="language-r">results <- test_data %>%
  select(mpg) %>%
  mutate(predicted_mpg = predict(model, newdata = test_data))</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Calculate Metrics.</strong> We use the metric functions from <code>yardstick</code>.
                <div class="code-container">
                    <pre><code class="language-r"># Calculate RMSE
rmse(results, truth = mpg, estimate = predicted_mpg)
# Calculate R-squared
rsq(results, truth = mpg, estimate = predicted_mpg)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Adjusted R-squared</h3>
    <div class="scenario-content">
        <p>A problem with the standard R-squared is that it will *always* increase if you add more predictors to your model, even if they are useless. The **Adjusted R-squared** corrects for this by penalizing the score for the number of predictors in the model. It provides a more honest assessment of the model's explanatory power and is preferred when comparing models with different numbers of features.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Follow the step-by-step example.</li>
            <li>Calculate the MAE for the model using the <code>mae()</code> function from <code>yardstick</code>.</li>
            <li>Now, fit a more complex model: <code>model2 <- lm(mpg ~ wt + hp, data = train_data)</code>.</li>
            <li>Generate predictions for this new model and calculate its RMSE, R-squared, and MAE.</li>
            <li>Compare the metrics for the two models. Which model is better?</li>
        </ul>
    </div>
</div>
`,
          p5s2ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst runs k-Means on a dataset with <code>k=3</code> and gets three nice-looking clusters. They then run it again with <code>k=4</code> and get four clusters that also look reasonable. Since clustering is unsupervised (there are no "true" labels to check against), how can they quantitatively measure which set of clusters is better?</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s2ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s2ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p5s2ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p5s2ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Evaluating Groups Without Labels</h4>
        <p>Since we don't have ground truth labels, clustering evaluation relies on **intrinsic metrics**. These metrics measure the quality of the clusters based on how compact and well-separated they are.</p>
        <p><strong>Analogy:</strong> Evaluating clusters is like judging a **sorting competition**. You don't have a master answer key. Instead, you judge the quality of the sorting by looking at the piles themselves. Good sorting results in piles that are **tightly packed** (high intra-cluster similarity) and **far apart from each other** (low inter-cluster similarity).</p>
        <p>The most common metric for this is the **Silhouette Score**.</p>
    </div>
    
    <div id="p5s2ss3-viz" class="tab-pane">
        <h4 class="subsection-title">The Silhouette Score Explained</h4>
        <p>The Silhouette score is calculated for each individual data point and measures how well it fits into its assigned cluster.</p>
        <div class="plot-container">
            <div id="p5s2ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> For the green point, <code>a</code> is the average distance to all other points *in its own cluster*. <code>b</code> is the average distance to all points in the *nearest neighboring cluster*. The Silhouette Score is $(b-a) / \max(a,b)$. A score close to +1 means the point is well-clustered. A score near 0 means it's on the border between two clusters. A negative score means it might be in the wrong cluster.</p>
        </div>
    </div>
    
    <div id="p5s2ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to find the optimal number of clusters for the <code>iris</code> dataset by comparing the average Silhouette score for different values of <code>k</code>.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Prepare Data.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># install.packages("factoextra")
library(factoextra)
iris_scaled <- scale(iris[, 1:4])</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Use <code>fviz_nbclust()</code> to Automate the Process.</strong> The <code>factoextra</code> package provides a wonderful function that automates the process of running k-Means for multiple <code>k</code> and plotting the resulting validation metric.
                <div class="code-container">
                    <pre><code class="language-r"># This will run kmeans for k=1 through 10 and plot the average silhouette width
fviz_nbclust(iris_scaled, kmeans, method = "silhouette")</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Interpret the Plot.</strong>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The plot will show the average silhouette score on the y-axis for each number of clusters <code>k</code> on the x-axis. The function will also draw a dashed vertical line at the value of <code>k</code> that maximizes the score. For the <code>iris</code> dataset, this will be at k=2, although <code>k=3</code> is also very good. This provides a quantitative justification for choosing a specific number of clusters.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Adjusted Rand Index for Labeled Data</h3>
    <div class="scenario-content">
        <p>In the rare case where you have the true labels for your data but are using clustering as an exploratory tool, you can use an **extrinsic metric** to evaluate performance. The **Adjusted Rand Index (ARI)** measures the similarity between your predicted clusters and the true class labels, corrected for chance. A score of 1 is a perfect match, and 0 is what you'd expect from random guessing. You can calculate it in R with <code>mclust::adjustedRandIndex()</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the scaled <code>mpg</code> and <code>wt</code> columns from the <code>mtcars</code> dataset.</li>
            <li>Use the <code>fviz_nbclust()</code> function with <code>method = "silhouette"</code> to generate a silhouette score plot. Based on the plot, what is the optimal number of clusters for this data?</li>
            <li>Now, generate an "Elbow plot" by changing the argument to <code>method = "wss"</code>. Does it suggest the same number of clusters?</li>
        </ul>
    </div>
</div>
`,
          p5s2ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist at a search engine company in Karachi needs to evaluate a new search algorithm. 🇵🇰 A simple accuracy metric isn't enough; the model's success depends on whether the most relevant results appear at the **top** of the list. They need specialized **Ranking Metrics** to evaluate the quality of their ordered search results.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s2ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s2ss4-methods">Key Metrics</button>
    </div>

    <div id="p5s2ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Evaluating Ordered Lists</h4>
        <p>Many machine learning problems are not about classification or regression, but about producing an optimal **ranked list** of items. This is common in information retrieval (search engines) and recommendation systems.</p>
        <p><strong>Analogy:</strong> Evaluating a ranking model is like judging a **librarian**. A good librarian doesn't just tell you that the book you want is "somewhere in the library." They tell you the exact aisle and shelf. An even better librarian puts the most relevant books right at the front of the shelf so you see them first. Ranking metrics reward the model for putting the most relevant items at the top of the list.</p>
    </div>
    
    <div id="p5s2ss4-methods" class="tab-pane">
        <h4 class="subsection-title">A Toolkit for Ranking</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Precision@k / Recall@k</strong>
                <p>Simple and intuitive. Precision@k asks: "Of the top <code>k</code> items recommended, what proportion are relevant?" Recall@k asks: "Of all the possible relevant items, what proportion did we find in our top <code>k</code> recommendations?"</p>
            </div>
            <div class="decision-branch">
                <strong>Mean Average Precision (MAP)</strong>
                <p>A popular metric that provides a single score for the quality of a ranked list. It's the mean of the Average Precision scores across all queries, where Average Precision heavily rewards models that place relevant documents early in the list.</p>
            </div>
            <div class="decision-branch">
                <strong>NDCG (Normalized Discounted Cumulative Gain)</strong>
                <p>A sophisticated and widely used metric, especially in search. It's powerful because it can handle multiple levels of relevance (e.g., "perfect," "good," "fair") and it applies a logarithmic discount, giving much more weight to items at the very top of the list.</p>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>Metrics</code> Package</h3>
    <div class="scenario-content">
        <p>While <code>yardstick</code> has some ranking metrics, the specialized <code>Metrics</code> package in R is an excellent resource that has easy-to-use implementations for many of these, including <code>apk()</code> (Average Precision at k) and <code>mapk()</code> (Mean Average Precision at k).</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's calculate Precision@k manually.</p>
        <p>A user searches for "Karachi food". There are 5 truly relevant restaurants in the database.</p>
        <p>Your model returns the following ranked list of 10 results (1=Relevant, 0=Not Relevant):<br><code>[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]</code></p>
        <ul class="prose-list">
            <li>What is the Precision@3 for this model? (Hint: How many of the top 3 were relevant?)</li>
            <li>What is the Recall@10 for this model? (Hint: How many of the 5 total relevant items did it find in the top 10?)</li>
        </ul>
    </div>
</div>
`,
          p5s2ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst presents a model with 95% accuracy to their manager. The manager, however, asks critical questions: "Is 95% good enough for our use case? What is the performance of the simple baseline model we are currently using? What are the business costs of a false positive versus a false negative?" The analyst realizes that a metric is meaningless without context.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Responsible Metric Selection</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Metrics Best Practices</div>
            <div class="scenario">Choosing and interpreting metrics correctly is a business decision, not just a technical one.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-bullseye"></i> 1. Align Metrics with Business Goals</div>
                        <div class="scenario"><strong>The Rule:</strong> The most important step is to choose the metric that best reflects the actual business objective. For a spam filter, minimizing false positives (Precision) is key. For a cancer screening test, minimizing false negatives (Recall) is paramount.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-balance-scale-left"></i> 2. Use Multiple Metrics</div>
                        <div class="scenario"><strong>The Rule:</strong> Never rely on a single metric. A single number can hide important trade-offs. Always look at a suite of metrics (e.g., Accuracy, Precision, Recall, and AUC) to get a complete picture of your model's performance.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-ruler-horizontal"></i> 3. Compare to a Baseline</div>
                        <div class="scenario"><strong>The Rule:</strong> A model's performance is only meaningful when compared to a simple, common-sense baseline. Is your complex model actually better than just predicting the majority class? Or better than a simple linear model?</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-secret"></i> 4. Evaluate on the Test Set</div>
                        <div class="scenario"><strong>The Rule:</strong> The final, reported metric must be the one calculated on the held-out, completely unseen test set. Reporting metrics from your training or validation set is misleading and will be overly optimistic.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>tidymodels</code> <code>metric_set()</code></h3>
    <div class="scenario-content">
        <p>The <code>yardstick</code> package from <code>tidymodels</code> makes it easy to manage and calculate multiple metrics at once. You can create a "metric set" using the <code>metric_set()</code> function, listing all the metrics you care about (e.g., <code>my_metrics <- metric_set(accuracy, precision, recall)</code>). You can then pass this set to your tuning or evaluation functions, and <code>tidymodels</code> will automatically calculate all of them for you.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are building a regression model to predict the price of a house. The business goal is to be as accurate as possible, but large over-predictions are particularly bad as they can lead to financial losses. Which metric would be more appropriate to optimize for: MAE or RMSE? Why?</li>
            <li>You are building a classification model for a dataset with 98% of the data belonging to Class A and 2% belonging to Class B. Why is Accuracy a poor choice for your primary evaluation metric? What metric would be a better choice?</li>
        </ul>
    </div>
</div>
`,
          p5s3ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A bank uses a complex XGBoost model to approve or deny loan applications. The model is highly accurate, but it's a "black box." When a customer is denied, the loan officer has no idea why. To comply with regulations and provide transparency, the bank needs to understand which features were most influential in the model's decisions overall. This calls for **Feature Importance** methods.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s3ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s3ss1-methods">Key Methods</button>
        <button class="tab-button" data-tab="p5s3ss1-viz">Visualization</button>
        <button class="tab-button" data-tab="p5s3ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p5s3ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Ranking Features by Influence</h4>
        <p><strong>Feature Importance</strong> refers to a class of techniques for assigning a score to each input feature of a model that indicates its relative importance in making predictions. It helps answer the question: "What were the most influential factors in the model's decisions, on average, across all predictions?"</p>
        <p><strong>Analogy:</strong> Feature importance is like asking a **championship basketball coach** which factors were most important to their team's success over the whole season. The coach might say, "Our top scorer's points-per-game was the most important factor, followed by our center's rebound rate, and then our point guard's assist-to-turnover ratio." It provides a high-level, ranked list of what mattered most.</p>
    </div>

    <div id="p5s3ss1-methods" class="tab-pane">
        <h4 class="subsection-title">Different Ways to Measure Importance</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Permutation Importance</strong>
                <p>A model-agnostic method. To find the importance of a feature, it randomly shuffles the values of just that feature's column and measures how much the model's performance drops. A big drop means the feature was very important.</p>
            </div>
            <div class="decision-branch">
                <strong>SHAP (SHapley Additive exPlanations)</strong>
                <p>A sophisticated, game-theory based approach that is currently the state-of-the-art. It calculates the marginal contribution of each feature to the prediction for every single data point, providing both global and local explanations.</p>
            </div>
            <div class="decision-branch">
                <strong>LIME (Local Interpretable Model-agnostic Explanations)</strong>
                <p>A method for explaining individual predictions (local importance). It builds a simple, interpretable model (like a linear regression) in the local neighborhood of a single data point to approximate the behavior of the complex model at that point.</p>
            </div>
        </div>
    </div>
    
    <div id="p5s3ss1-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing Global Importance</h4>
        <p>A bar chart is the most effective way to visualize and communicate global feature importance scores.</p>
        <div class="plot-container">
            <div id="p5s3ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This SHAP summary plot shows the features ranked by their average impact on the model's output magnitude. It clearly identifies that <code>bill_length_mm</code> and <code>flipper_length_mm</code> were the two most influential features for the model's predictions, while <code>year</code> had almost no impact. This provides a clear, high-level summary of what the model learned.</p>
        </div>
    </div>
    
    <div id="p5s3ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have trained a random forest model to classify penguin species. We now want to calculate and visualize the feature importance using the powerful SHAP method.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Train a Model.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># install.packages(c("randomForest", "fastshap", "ggplot2"))
library(randomForest)
library(fastshap)
library(palmerpenguins)
library(tidyr)
penguins_clean <- penguins %>% drop_na()
rf_model <- randomForest(species ~ ., data = penguins_clean)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Define the Prediction Wrapper.</strong> SHAP needs a specific function that takes the model and data and returns a prediction.
                <div class="code-container">
                    <pre><code class="language-r">pfun <- function(object, newdata) {
  predict(object, newdata = newdata, type = "prob")
}</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Calculate SHAP Values.</strong> We use the <code>explain()</code> function from <code>fastshap</code>. This can take some time on large datasets.
                <div class="code-container">
                    <pre><code class="language-r">shap_values <- explain(rf_model, X = penguins_clean[, -1], pred_wrapper = pfun, nsim = 30)</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Visualize the Importance.</strong> The package comes with a built-in <code>autoplot()</code> function for <code>ggplot2</code>.
                <div class="code-container">
                    <pre><code class="language-r"># This will generate the plot seen in the Visualization tab
autoplot(shap_values)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Local vs. Global Explanations</h3>
    <div class="scenario-content">
        <p>It's crucial to distinguish between global and local importance. **Global importance** (what this topic covers) tells you which features are important for the model *on average*. **Local importance** (covered in the next topics) explains *why a single, specific prediction was made*. SHAP is powerful because it can provide both.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>mtcars</code> dataset, fit a random forest model to predict <code>mpg</code> from all other variables.</li>
            <li>The <code>randomForest</code> package has a built-in importance measure. After fitting the model (e.g., <code>rf <- randomForest(...)</code>), you can get the scores with <code>importance(rf)</code> and create a bar chart with <code>varImpPlot(rf)</code>.</li>
            <li>Run this process. Which two variables does the model consider most important for predicting a car's fuel efficiency?</li>
        </ul>
    </div>
</div>
`,
          p5s3ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A bank's loan approval model has high global feature importance for "income." However, a specific high-income customer was denied a loan, and the loan officer needs to understand why. Was it due to their high debt-to-income ratio? Their short credit history? To explain this single decision, they need to visualize the model's behavior locally using tools like **Partial Dependence Plots** and **ICE Plots**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s3ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s3ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p5s3ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p5s3ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Understanding "What If" Scenarios</h4>
        <p>Model behavior visualization techniques help us understand *how* a model's prediction changes as we change the value of a single input feature, while holding all other features constant.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Partial Dependence Plots (PDP)</strong>
                <p>Shows the **average** effect of a feature on the model's prediction. It answers the question: "On average, how does the prediction change as this one feature changes?"</p>
            </div>
            <div class="decision-branch">
                <strong>Individual Conditional Expectation (ICE) Plots</strong>
                <p>An ICE plot is a disaggregated version of a PDP. It shows one line for *each individual observation* in the dataset, showing how that specific individual's prediction would change if you varied one feature. This can reveal heterogeneous effects that are hidden by the PDP's average.</p>
            </div>
        </div>
        <p><strong>Analogy:</strong> A PDP is like a **company-wide policy manual**. It tells you the average effect of a policy (e.g., "On average, a 10% price increase leads to a 5% drop in sales"). An ICE plot is a set of **individual case files**. It shows you how that policy might affect each customer differently (e.g., for Customer A, a 10% price increase might cause a 50% drop in their purchases, while for loyal Customer B, it has no effect).</p>
    </div>
    
    <div id="p5s3ss2-viz" class="tab-pane">
        <h4 class="subsection-title">PDP and ICE Plots Compared</h4>
        <p>This plot shows both a PDP and the underlying ICE curves for the same feature. The PDP shows the average trend, while the ICE curves reveal the individual-level heterogeneity.</p>
        <div class="plot-container">
            <div id="p5s3ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The thin grey lines are the **ICE curves**, one for each data point. The thick blue line is the **PDP curve**, which is simply the average of all the ICE curves. While the average trend (PDP) is slightly positive, the ICE curves show that for some individuals, the relationship is flat, and for others, it's strongly positive. This level of detail is hidden by the PDP alone.</p>
        </div>
    </div>
    
    <div id="p5s3ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have a random forest model predicting car MPG. We want to understand the average effect of horsepower (<code>hp</code>) on the prediction and see if that effect is consistent across all cars.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Load Packages and Train a Model.</strong>
                <div class="code-container">
                    <pre><code class="language-r"># install.packages(c("randomForest", "pdp"))
library(randomForest)
library(pdp)
set.seed(42)
rf_model <- randomForest(mpg ~ ., data = mtcars)</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Generate the PDP and ICE Curves.</strong> The <code>pdp</code> package's <code>partial()</code> function calculates the necessary values. We set <code>ice = TRUE</code> to get the individual curves.
                <div class="code-container">
                    <pre><code class="language-r"># This calculates the predictions for each observation at different values of hp
ice_curves <- partial(rf_model, pred.var = "hp", ice = TRUE, grid.resolution = 30)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Visualize the Plots.</strong> The package provides an <code>autoplot()</code> function.
                <div class="code-container">
                    <pre><code class="language-r">autoplot(ice_curves, alpha = 0.3) + 
  labs(title = "ICE and PDP for Horsepower on MPG Prediction")</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> The resulting plot shows that, on average (the PDP), as horsepower increases, the predicted MPG decreases. The individual ICE curves are mostly parallel, suggesting that this negative relationship is fairly consistent for all the different types of cars in the dataset.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Centered ICE Plots</h3>
    <div class="scenario-content">
        <p>A standard ICE plot can look like a messy plate of spaghetti. A **centered ICE (c-ICE) plot** is a variation that makes patterns easier to see. It shifts each ICE curve vertically so they all start at zero on the left side of the plot. This makes it much easier to compare the *slopes* and *shapes* of the individual curves. You can create one with <code>autoplot(..., center = TRUE)</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>rf_model</code> from the step-by-step example.</li>
            <li>Use the <code>partial()</code> function to calculate the partial dependence of the <code>wt</code> (weight) feature.</li>
            <li>Use <code>autoplot()</code> to create just the PDP (the default when <code>ice=FALSE</code>). How would you describe the relationship between a car's weight and its predicted MPG?</li>
            <li>Now, re-run <code>partial()</code> with <code>ice = TRUE</code> and plot the ICE curves as well. Is the effect of weight consistent across all cars?</li>
        </ul>
    </div>
</div>
`,
          p5s3ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has explained that "income" is globally the most important feature for their loan prediction model. A manager then asks, "Okay, but why was this *specific* customer, Mr. Khan from Karachi, denied his loan?" Answering this requires a **local explanation**, one that is specific to a single prediction, not the model as a whole. This is the difference between **global** and **local** explanation scope.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The "What" vs. the "Why" of a Prediction</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-search"></i> The Scope of Explanation</div>
            <div class="scenario">You must choose the right tool based on whether you are trying to understand the entire model or just one decision.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-globe-americas"></i> Global Explanations</div>
                        <div class="scenario"><strong>The Question:</strong> "How does this model work in general?"
                        <br><strong>What it does:</strong> Describes the average behavior of the model across the entire dataset.
                        <br><strong>Tools:</strong> Feature Importance, Partial Dependence Plots (PDP).</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-map-marker-alt"></i> Local Explanations</div>
                        <div class="scenario"><strong>The Question:</strong> "Why was this *one* prediction made?"
                        <br><strong>What it does:</strong> Explains a single prediction by showing how each feature contributed to that specific outcome.
                        <br><strong>Tools:</strong> SHAP values, LIME, Individual Conditional Expectation (ICE) plots.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: SHAP for Both</h3>
    <div class="scenario-content">
        <p>The SHAP (SHapley Additive exPlanations) framework is exceptionally powerful because it unifies both global and local explanations. A SHAP analysis first calculates the contribution of each feature for *every single prediction* (the local explanations). You can then get the global feature importance simply by taking the average absolute SHAP value for each feature across all the predictions. This makes it a one-stop-shop for feature importance.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>A hospital uses a model to predict a patient's risk of readmission. A doctor wants to understand why a specific patient, Mrs. Devi, has been flagged as high-risk. Should the data scientist provide a global or a local explanation? Which specific tool (e.g., LIME, PDP, Feature Importance) would be most appropriate?</li>
            <li>The hospital's management wants to understand the top five risk factors for readmission across their entire patient population to guide policy changes. Should the data scientist provide a global or a local explanation? Which tool would be best?</li>
        </ul>
    </div>
</div>
`,
          p5s3ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> For a critical medical diagnosis, a hospital has two models. Model A is a highly complex "black-box" neural network with 95% accuracy. Model B is a simple, interpretable logistic regression model with 93% accuracy. A doctor is more likely to trust and use Model B, because they can understand *why* it's making its predictions. This highlights the tradeoff between model accuracy and **transparency**.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Spectrum of Interpretability</h3>
    <p>Models exist on a spectrum from transparent "white-box" models to opaque "black-box" models.</p>
    <div class="decision-branches" style="align-items: flex-start;">
        <div class="decision-branch">
            <strong>White-Box Models (Interpretable by Design)</strong>
            <p>These are models whose internal mechanics are inherently understandable to humans. We can look at their parameters and understand exactly how they arrive at a decision.</p>
            <p><strong>Examples:</strong> Linear Regression, Logistic Regression, Decision Trees.</p>
            <p><strong>Pros:</strong> Highly transparent and easy to explain.
            <br><strong>Cons:</strong> Often less accurate than more complex models.</p>
        </div>
        <div class="decision-branch">
            <strong>Black-Box Models (Require Post-Hoc Explanation)</strong>
            <p>These are models whose internal workings are too complex for a human to reasonably understand. Their high performance comes from learning intricate, non-linear patterns.</p>
            <p><strong>Examples:</strong> Gradient Boosting Machines (XGBoost), Random Forests, Deep Neural Networks.</p>
            <p><strong>Pros:</strong> Often achieve state-of-the-art accuracy.
            <br><strong>Cons:</strong> Require XAI methods (like SHAP or LIME) to explain their behavior after they have been trained.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Start Simple</h3>
    <div class="scenario-content">
        <p>A fundamental best practice in machine learning is to **always start with a simple, interpretable model**. Build a logistic or linear regression first. This provides you with a strong, understandable baseline. Only if the performance of this simple model is not sufficient for your business needs should you move on to more complex, black-box methods. Often, the small gain in accuracy from a complex model is not worth the loss in interpretability.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are building a model for a credit card company to automatically approve or deny credit limit increase requests. The company has a legal requirement to provide a clear reason for every denial. Which type of model (white-box or black-box) would be a better choice here, and why?</li>
            <li>You are participating in a Kaggle competition where the only goal is to get the highest possible accuracy score on an image classification task. Which type of model would you likely choose?</li>
        </ul>
    </div>
</div>
`,
          p5s3ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst presents a SHAP plot to a manager, saying "This proves that being a long-time customer *causes* them to have a lower churn risk." This is a dangerous misinterpretation. The XAI tool has only shown a **correlation** the model has learned, not a causal link. Adhering to best practices means communicating these results with precision and intellectual honesty.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Responsible Explanation</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> XAI Best Practices</div>
            <div class="scenario">Using explainability tools correctly and communicating their results responsibly.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-exchange-alt"></i> 1. Correlation is Not Causation</div>
                        <div class="scenario"><strong>The Rule:</strong> XAI methods show which features the model found to be associated with the outcome. They do **not** explain the underlying causal relationships in the real world. Never use causal language (e.g., "impact," "effect," "causes") when describing their output.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-glasses"></i> 2. The Explanation is Not the Model</div>
                        <div class="scenario"><strong>The Rule:</strong> An explanation (like a LIME or SHAP summary) is an *approximation* of the model's behavior. It is itself a simpler model of the complex model. Be aware that the explanation can sometimes be an imperfect simplification of the true model logic.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-bullseye"></i> 3. Choose the Right Scope</div>
                        <div class="scenario"><strong>The Rule:</strong> Use the right tool for the right question. Use global methods (Feature Importance, PDPs) to explain the model's average behavior. Use local methods (LIME, SHAP force plots) to explain a single, specific prediction.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-trash"></i> 4. Garbage In, Garbage Out</div>
                        <div class="scenario"><strong>The Rule:</strong> An XAI tool can only explain what the model has learned. If your model was trained on biased or poor-quality data, the explanation will simply reflect those biases. XAI is not a substitute for good data and proper model validation.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Explain the Model, Not the World</h3>
    <div class="scenario-content">
        <p>A crucial distinction to maintain when communicating results is that you are explaining the **model's behavior**, not the **world's behavior**. Instead of saying "Higher income causes lower risk," a more accurate and responsible statement is, "The model has learned a strong negative relationship between income and its prediction of risk." This small change in language is critical for intellectual honesty.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You build a model to predict employee promotions. A SHAP analysis reveals that the feature <code>number_of_meetings_with_ceo</code> has the highest feature importance.</p>
        <ul class="prose-list">
            <li>What is the incorrect, causal conclusion someone might jump to?</li>
            <li>What is a more likely, non-causal explanation for this relationship? (Hint: Think about confounding variables).</li>
            <li>How would you responsibly phrase this finding to a manager?</li>
        </ul>
    </div>
</div>
`,
          p5s4ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A historical dataset used to train a hiring model shows that in the past, mostly men were hired for engineering roles. The model learns this pattern and now, when given equally qualified male and female candidates, it consistently ranks the male candidate higher. The model is simply reflecting the **bias** present in its training data, and if deployed, it will perpetuate and even amplify this historical inequity.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">How Bias Enters AI Systems</h3>
    <p>Algorithmic bias occurs when a computer system reflects the implicit biases of the humans who created it, the data it was trained on, or the societal context in which it is deployed. It is not something that happens maliciously; it is an emergent property of using historical data to make future decisions.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-exclamation-triangle"></i> Common Sources of Bias</div>
            <div class="scenario">Bias can be introduced at any stage of the machine learning lifecycle.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-database"></i> 1. Historical & Societal Bias</div>
                        <div class="scenario">The data itself reflects existing prejudices in the world. This is the most common and difficult source of bias to address.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-users"></i> 2. Representation Bias</div>
                        <div class="scenario">The data used to train the model is not representative of the population it will be used on (e.g., a facial recognition system trained primarily on light-skinned faces will perform poorly on dark-skinned faces).</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-pen-square"></i> 3. Measurement & Label Bias</div>
                        <div class="scenario">The way a feature is measured or the labels are created can be inconsistent across different groups (e.g., using "arrests" as a proxy for "criminality" introduces bias because different communities are policed at different rates).</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: "Fairness Through Unawareness" is a Myth</h3>
    <div class="scenario-content">
        <p>A common but dangerously naive approach to fairness is to simply remove protected attributes like <code>race</code> or <code>gender</code> from the training data. This is called "fairness through unawareness" and it **does not work**. Other variables in the dataset (like <code>zip_code</code> or <code>first_name</code>) are often highly correlated with the protected attributes and act as proxies, allowing the model to learn the same biases anyway.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You are building a model to predict student success in college using high school GPA and standardized test scores as features.</p>
        <ul class="prose-list">
            <li>What is a potential source of **measurement bias** related to standardized test scores? (Hint: Think about access to test prep resources).</li>
            <li>If your training data comes only from wealthy, private high schools but the model will be used for students from all high schools, what kind of bias might you be introducing?</li>
        </ul>
    </div>
</div>
`,
          p5s4ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A bank's loan approval model is audited. The auditors want to know if the model is "fair." But what does "fair" mean mathematically? Does it mean that the approval rate should be the same for all demographic groups? Or does it mean that among qualified applicants, the approval rate should be the same? These different definitions of fairness can be mutually exclusive, highlighting the complexity of choosing a **Fairness Metric**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s4ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s4ss2-methods">Key Metrics</button>
    </div>

    <div id="p5s4ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Defining "Fairness" Mathematically</h4>
        <p><strong>Fairness Metrics** are quantitative measures used to evaluate a model for algorithmic bias. They assess whether the model's predictions are equitable across different subgroups of the population (e.g., groups based on race, gender, or other protected attributes).</p>
        <p><strong>Analogy:</strong> Fairness metrics are like the different clauses in an **anti-discrimination law**. One clause might mandate equal opportunity (like Demographic Parity), while another might mandate equal outcomes for equally qualified people (like Equalized Odds). You can't always satisfy all of them simultaneously, so you must choose the one that best reflects the societal goal.</p>
    </div>
    
    <div id="p5s4ss2-methods" class="tab-pane">
        <h4 class="subsection-title">Common Group Fairness Definitions</h4>
        <p>These are just a few of the many mathematical definitions of fairness.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Demographic Parity (Statistical Parity)</strong>
                <p><strong>The Goal:</strong> The model's predictions are independent of the sensitive attribute. The proportion of positive predictions (e.g., "loan approved") should be the same for all groups.
                <br><strong>Pro/Con:</strong> Simple to understand, but can lead to unqualified candidates being accepted to meet a quota.</p>
            </div>
            <div class="decision-branch">
                <strong>Equalized Odds</strong>
                <p><strong>The Goal:</strong> The model should have equal True Positive Rates and equal False Positive Rates across all groups. Among qualified applicants, the approval rate should be the same for all groups.
                <br><strong>Pro/Con:</strong> A very strong and often desirable definition of fairness, but can be difficult to achieve.</p>
            </div>
             <div class="decision-branch">
                <strong>Equal Opportunity</strong>
                <p><strong>The Goal:</strong> A relaxed version of Equalized Odds. It only requires that the True Positive Rate be equal across all groups. It ensures that the model correctly identifies positive outcomes at the same rate for everyone.</p>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>fairmodels</code> Package</h3>
    <div class="scenario-content">
        <p>Auditing a model for fairness can be complex. The <code>fairmodels</code> package in R provides a comprehensive toolkit for this. You can create a "fairness object," and the package will automatically calculate a wide range of fairness metrics for your model. It also provides powerful visualizations to help you compare the performance and fairness of multiple models at once.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>A company uses a model to screen resumes for a software engineering job. The model is found to have a much higher True Positive Rate for male candidates than for female candidates.</p>
        <ul class="prose-list">
            <li>Which fairness metric has this model violated?</li>
            <li>What is the real-world consequence of this violation?</li>
        </ul>
    </div>
</div>
`,
          p5s4ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An audit has revealed that a bank's loan approval model is biased; it has a significantly lower True Positive Rate for a minority group. The bank cannot deploy this model. They must now apply a **Bias Mitigation** strategy to create a fairer model, even if it means sacrificing a small amount of overall accuracy.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Three-Pronged Approach to Mitigation</h3>
    <p>Bias mitigation techniques can be applied at three different stages of the modeling pipeline.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-tools"></i> Bias Mitigation Strategies</div>
            <div class="scenario">Choosing the right strategy depends on where in the process you want to intervene.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-database"></i> 1. Pre-processing</div>
                        <div class="scenario"><strong>What it is:</strong> Modifying the training data itself to remove the underlying biases before training the model.
                        <br><strong>Methods:</strong> Re-sampling (e.g., over-sampling the minority group), re-weighting data points.
                        <br><strong>Pro/Con:</strong> Easy to implement, but can distort the original data.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-cogs"></i> 2. In-processing</div>
                        <div class="scenario"><strong>What it is:</strong> Modifying the model's learning algorithm to incorporate fairness constraints directly into the optimization process.
                        <br><strong>Methods:</strong> Adding a penalty term to the loss function for unfair outcomes.
                        <br><strong>Pro/Con:</strong> Can be very effective, but is often more complex to implement.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-edit"></i> 3. Post-processing</div>
                        <div class="scenario"><strong>What it is:</strong> Taking the predictions from a trained model and adjusting them to improve fairness.
                        <br><strong>Methods:</strong> Adjusting the classification threshold differently for different groups.
                        <br><strong>Pro/Con:</strong> Simple and doesn't require retraining the model, but might not address the root cause of the bias.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>fairness</code> R Package</h3>
    <div class="scenario-content">
        <p>The <code>fairness</code> package in R provides implementations for several common bias mitigation techniques, including re-weighting (a pre-processing method) and threshold adjustment (a post-processing method). It's a great tool to start experimenting with these strategies in your own workflows.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You have a trained model that is biased. The business cannot afford to retrain the model from scratch due to time constraints. Which category of mitigation strategy (pre-, in-, or post-processing) would be the most practical choice?</li>
            <li>You have a dataset where a minority group is severely under-represented. You decide to use a pre-processing mitigation technique. What specific technique could you use?</li>
        </ul>
    </div>
</div>
`,
          p5s4ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A hospital wants to release a dataset of patient records for research purposes. However, the dataset contains Personally Identifiable Information (PII) like names, addresses, and national ID numbers. Simply removing these columns isn't enough, as a combination of other data (like zip code, age, and diagnosis) could potentially be used to re-identify an individual. The hospital must apply rigorous **anonymization** techniques to protect patient privacy.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s4ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s4ss4-methods">Key Methods</button>
    </div>

    <div id="p5s4ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Protecting Individual Identity</h4>
        <p><strong>Data Privacy** is the area of data management that deals with the proper handling of sensitive data. It involves consent, notice, and regulatory obligations.</p>
        <p><strong>Analogy:</strong> Handling private data is like being a **secret keeper**. You have been entrusted with sensitive information. Your primary duty is to protect that information from being revealed, both directly and indirectly. You must take active measures to obscure the identities of the people who shared their secrets with you.</p>
    </div>
    
    <div id="p5s4ss4-methods" class="tab-pane">
        <h4 class="subsection-title">A Toolkit for Anonymization</h4>
        <div class="decision-branches" style="align-items- flex-start;">
            <div class="decision-branch">
                <strong>De-identification & Masking</strong>
                <p>The most basic step. This involves removing or encrypting direct identifiers like names and social security numbers.</p>
            </div>
            <div class="decision-branch">
                <strong>k-Anonymity</strong>
                <p>A property of a dataset where every individual is indistinguishable from at least k-1 other individuals. This is achieved by suppressing or generalizing quasi-identifiers (like zip code or age).</p>
            </div>
            <div class="decision-branch">
                <strong>Differential Privacy</strong>
                <p>The gold standard. It's a mathematical framework that makes it possible to learn from a dataset as a whole, while making it impossible to learn any information about a single individual in the dataset. It works by adding carefully calibrated statistical noise to the data or the query results.</p>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The <code>sdcMicro</code> Package</h3>
    <div class="scenario-content">
        <p>For serious statistical disclosure control and anonymization in R, the <code>sdcMicro</code> package is a powerful tool. It provides functions to measure disclosure risk and apply a wide range of anonymization techniques, including k-anonymity, suppression, and adding noise, helping you to create a privacy-preserving public use file from your sensitive microdata.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You have a dataset with columns for age, gender, and zip_code. Why are these "quasi-identifiers"?</li>
            <li>If you want to achieve 5-anonymity, what does that mean for every individual in your released dataset? What is one way you could modify the <code>age</code> column to help achieve this? (Hint: Think about binning).</li>
        </ul>
    </div>
</div>
`,
          p5s4ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A large tech company is developing a new AI system. To guide their development and ensure the final product is trustworthy and beneficial, they establish a set of **Responsible AI Principles**. These principles act as a constitution or a code of ethics for their entire AI development lifecycle, covering everything from fairness and privacy to accountability and transparency.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Guiding Framework for Trustworthy AI</h3>
    <p>Responsible AI is a governance framework for ensuring that AI systems are developed and used in a way that is ethical, transparent, and accountable. Many organizations have published their own set of principles, but they often revolve around a common set of core ideas.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-shield-alt"></i> Core Principles of Responsible AI</div>
            <div class="scenario">These principles provide a high-level guide for building ethical and trustworthy AI systems.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-balance-scale"></i> Fairness & Non-Discrimination</div>
                        <div class="scenario">The system should not create or reinforce unfair bias against individuals or groups, particularly those with sensitive characteristics.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-eye"></i> Transparency & Explainability</div>
                        <div class="scenario">It should be possible to understand how an AI system makes its decisions. The system should be interpretable to its users and developers.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-shield"></i> Privacy & Security</div>
                        <div class="scenario">The system must respect user privacy, obtain proper consent for data, and be robust against security threats that could compromise sensitive data.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-check"></i> Accountability & Human Oversight</div>
                        <div class="scenario">There should be clear lines of human responsibility for the outcomes of an AI system. Humans should be able to intervene and override the system's decisions, especially in high-stakes situations.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Principles are a Starting Point</h3>
    <div class="scenario-content">
        <p>Publishing a set of AI principles is an important first step, but it is not enough. The real work is in translating these high-level principles into concrete practices, technical tools, and governance processes within an organization. This involves creating internal review boards, developing checklists for data scientists, and implementing tools for bias detection and model explainability.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>A self-driving car's AI system must make a decision in a no-win accident scenario. The company that built the AI has a strong "Accountability" principle.</p>
        <ul class="prose-list">
            <li>How does this principle apply here? Who should be held responsible for the decision the AI makes? The owner of the car? The programmer? The company?</li>
            <li>This is a classic ethical dilemma in AI. How does the principle of "Human Oversight" factor into this scenario?</li>
        </ul>
    </div>
</div>
`,
          p5s4ss6: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist working for a healthcare startup in Karachi is building a model that uses patient data. 🇵🇰 They must be acutely aware of data privacy laws like the EU's **General Data Protection Regulation (GDPR)**, as their company might have customers who are European citizens. Failure to comply with these regulations can result in massive fines and loss of trust.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p5s4ss6-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p5s4ss6-methods">Key Regulations</button>
    </div>

    <div id="p5s4ss6-concepts" class="tab-pane active">
        <h4 class="subsection-title">The Legal Landscape of Data</h4>
        <p>As data becomes more central to the economy, governments around the world are enacting strict regulations to protect citizens' privacy and control how their personal data is used. Data scientists must be aware of and comply with these legal frameworks.</p>
        <p><strong>Analogy:</strong> Data regulations are like the **building codes for construction**. You can't just build whatever you want. You must follow a strict set of rules that govern safety, materials, and design to ensure your building is safe for the public. Similarly, you can't just use data however you want; you must follow the legal codes that govern privacy and consent.</p>
    </div>
    
    <div id="p5s4ss6-methods" class="tab-pane">
        <h4 class="subsection-title">Major Global Regulations</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>GDPR (General Data Protection Regulation)</strong>
                <p>A landmark regulation from the European Union. It gives EU citizens significant rights over their personal data, including the right to access, the right to erasure ("right to be forgotten"), and requires explicit consent for data processing.</p>
            </div>
            <div class="decision-branch">
                <strong>CCPA / CPRA (California Consumer Privacy Act / Privacy Rights Act)</strong>
                <p>California's privacy law, which gives consumers similar rights to GDPR, including the right to know what personal information is being collected about them and the right to opt-out of the sale of their personal information.</p>
            </div>
            <div class="decision-branch">
                <strong>HIPAA (Health Insurance Portability and Accountability Act)</strong>
                <p>A US federal law that provides data privacy and security provisions for safeguarding medical information.</p>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The "Right to Explanation"</h3>
    <div class="scenario-content">
        <p>While still a matter of legal debate, regulations like GDPR are often interpreted as containing a "right to explanation." This means that if an automated system (like an AI model) makes a significant decision about a person (like denying them a loan), that person has the right to receive a meaningful explanation of the logic involved. This is one of the primary legal drivers for the field of Explainable AI (XAI).</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>Your company wants to collect user location data to improve its recommendation service. Under GDPR, what is the most important thing you must do before you can start collecting this data?</li>
            <li>A user from California submits a request to your company asking to have all their personal data deleted. Which regulation gives them this right?</li>
        </ul>
    </div>
</div>
`,
          p5s4ss7: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data science team is about to start a new project to build a predictive model. To ensure the project is developed responsibly from the start, they follow a checklist of best practices. They begin by conducting an ethical review to identify potential risks, and they create a plan for engaging with stakeholders throughout the project lifecycle.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Responsible Data Science</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Ethical AI Best Practices</div>
            <div class="scenario">Integrating ethical considerations into the entire data science workflow.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-clipboard-check"></i> 1. Conduct an Ethical Review</div>
                        <div class="scenario"><strong>The Rule:</strong> Before you write a single line of code, assess the potential ethical risks and societal impacts of your project. Who could be harmed by this model? What are the potential unintended consequences?</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-users"></i> 2. Engage Diverse Stakeholders</div>
                        <div class="scenario"><strong>The Rule:</strong> Involve people from different backgrounds and disciplines (including ethicists, social scientists, and members of affected communities) in the development process to get diverse perspectives on the project's potential impact.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-search"></i> 3. Audit for Bias Proactively</div>
                        <div class="scenario"><strong>The Rule:</strong> Don't wait until the end to check for fairness. Audit your data for biases at the beginning, and audit your model's predictions for fairness as part of your standard evaluation process.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-book-open"></i> 4. Document Everything</div>
                        <div class="scenario"><strong>The Rule:</strong> Maintain clear and transparent documentation about your data sources, cleaning decisions, modeling choices, and evaluation results. This is crucial for accountability and for allowing others to audit your work.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Model Cards</h3>
    <div class="scenario-content">
        <p>A **Model Card** is a short, standardized document that provides key information about a trained machine learning model. It's like a "nutrition label" for an AI model. It typically includes details about the model's intended use cases, its performance metrics across different demographic groups, and its ethical considerations and limitations. Creating model cards is a powerful best practice for promoting transparency and responsible AI deployment.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You are a data scientist at a large social media company in Karachi. Your team is tasked with building a new algorithm to automatically moderate and remove "harmful" content.</p>
        <ul class="prose-list">
            <li>What is the first step you should take before starting to build the model, according to the best practices checklist?</li>
            <li>Who are some of the key "stakeholders" you would want to engage with during this project's development?</li>
            <li>Why is documenting the definition of "harmful" content so critically important for this project's accountability?</li>
        </ul>
    </div>
</div>
`,
          p6s2ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst at a Karachi-based retailer has built a powerful predictive model, but their non-technical managers can't use it. 🇵🇰 The managers want to explore the model's predictions by changing inputs themselves (e.g., "What would the predicted sales be if we increase the marketing budget by 20%?"). To bridge this gap, the analyst needs to build an interactive web application—a <strong>Shiny App</strong>—that provides a user-friendly interface to their R code.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s2ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s2ss1-viz">The Architecture</button>
        <button class="tab-button" data-tab="p6s2ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p6s2ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Web Apps Straight from R</h4>
        <p><strong>Shiny</strong> is an R package that makes it incredibly easy to build interactive web applications directly from R. It allows you to create dashboards, data explorers, and model interfaces without needing to know HTML, CSS, or JavaScript.</p>
        <p><strong>Analogy:</strong> Shiny is like a set of **interactive LEGO bricks for your R code**. You have your analysis (a LEGO creation). Shiny provides special bricks with buttons, sliders, and plot windows that you can snap onto your creation. When a user interacts with a button, it sends a signal that automatically re-runs the relevant part of your R code and updates the display, all without you having to be a web developer.</p>
    </div>
    
    <div id="p6s2ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The UI and Server Logic</h4>
        <p>Every Shiny app has two main components that communicate with each other.</p>
        <div class="mindmap-container">
            <div class="mindmap-root">
                <div class="title"><i class="fas fa-desktop"></i> The Shiny App</div>
                <div class="scenario">The User Interface (UI) and the Server work together to create a reactive experience.</div>
            </div>
            <div class="mindmap-branches-container">
                <div class="mindmap-branches">
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-paint-brush"></i> UI (User Interface)</div>
                            <div class="scenario"><strong>The "What":</strong> This is the front-end part of the app. It defines the layout and appearance of the application—where the sliders, buttons, and plots will go. It controls what the user sees.</div>
                        </div>
                    </div>
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-cogs"></i> Server</div>
                            <div class="scenario"><strong>The "How":</strong> This is the back-end part of the app. It contains all the R code and instructions for *how* to build the outputs. It listens for changes in the UI's inputs and re-runs the necessary calculations to update the outputs.</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="plot-description">
             <p>This separation of concerns is key. The UI defines the look, and the Server provides the brainpower.</p>
        </div>
    </div>
    
    <div id="p6s2ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We want to build a simple app that allows a user to explore the <code>mtcars</code> dataset by creating a scatter plot of any two variables they choose from dropdown menus.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Create a New Shiny App File.</strong> In RStudio, go to <code>File > New File > Shiny Web App...</code>. Give it a name and choose "Single File (app.R)". This will create a template file.</li>
            <li><strong>Step 2: Define the UI.</strong> We create a layout with a sidebar containing two dropdown menus (<code>selectInput</code>) for the x and y variables.
                <div class="code-container">
                    <pre><code class="language-r">ui <- fluidPage(
    titlePanel("MTCars Explorer"),
    sidebarLayout(
        sidebarPanel(
            selectInput("x_var", "X-axis variable:", choices = names(mtcars)),
            selectInput("y_var", "Y-axis variable:", choices = names(mtcars), selected = "mpg")
        ),
        mainPanel(
           plotOutput("car_plot")
        )
    )
)</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Define the Server Logic.</strong> The server function takes <code>input</code> and <code>output</code> as arguments. We use <code>renderPlot</code> to create a reactive plot. The plot will automatically re-render whenever <code>input$x_var</code> or <code>input$y_var</code> changes.
                <div class="code-container">
                    <pre><code class="language-r">server <- function(input, output) {
    output$car_plot <- renderPlot({
        ggplot(mtcars, aes_string(x = input$x_var, y = input$y_var)) +
            geom_point(size = 3, color = "blue") +
            theme_minimal(base_size = 16)
    })
}</code></pre>
                </div>
            </li>
            <li><strong>Step 4: Run the App.</strong> We combine the UI and Server to launch the application.
                <div class="code-container">
                    <pre><code class="language-r">shinyApp(ui = ui, server = server)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Understanding Reactivity</h3>
    <div class="scenario-content">
        <p>The magic of Shiny is **reactivity**. You don't tell the plot *when* to update; you only declare the relationship between the inputs and the output. Shiny builds a "reactive graph" behind the scenes. When it detects that an input value has changed, it automatically finds all the outputs that depend on that input and re-executes only those specific pieces of code. This makes building complex, responsive applications surprisingly simple.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the Shiny app template from the example.</li>
            <li>Modify the <code>sidebarPanel</code> in the UI. Add a <code>sliderInput</code> that allows the user to control the size of the points on the scatter plot. Give it an ID of <code>point_size</code>, a label, and a min/max/value range (e.g., 1 to 10, with a default of 3).</li>
            <li>Modify the <code>geom_point()</code> call in the server logic. Change <code>size = 3</code> to <code>size = input$point_size</code>.</li>
            <li>Run your app. You should now be able to interactively change the point size with your new slider!</li>
        </ul>
    </div>
</div>
`,
          p6s2ss2:
            `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A sales manager needs a high-level overview of the company's performance. They don't want to interact with a complex app; they want a clean, simple, and professional-looking **dashboard** with key performance indicators (KPIs), a main trend chart, and a data table, all presented in a clear, grid-based layout. The analyst decides to use <code>flexdashboard</code> to quickly create this from their existing R Markdown analysis.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s2ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s2ss2-viz">Anatomy of a Dashboard</button>
        <button class="tab-button" data-tab="p6s2ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p6s2ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Layouts and Components for Reporting</h4>
        <p>A **dashboard** is a specific type of user interface that provides an at-a-glance view of key metrics and data relevant to a particular objective or business process. Unlike a general-purpose Shiny app, a dashboard is focused on presenting pre-defined information in a clear, organized layout.</p>
        <p><strong>Analogy:</strong> If a Shiny app is a fully-equipped **workshop** where you can build anything, a dashboard is a perfectly organized **toolbox**. The tools are laid out in a specific, predictable way, allowing you to find the information you need instantly.</p>
        <p>The <code>flexdashboard</code> package is the perfect tool for this, allowing you to define a grid-based layout using simple R Markdown headers.</p>
    </div>
    
    <div id="p6s2ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Common Dashboard Components</h4>
        <p>This diagram shows the typical layout of a professional dashboard, with interactive controls on the side and reactive outputs in the main panel.</p>
        <div class="plot-container">
            <div id="p6s2ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The key components are the **input controls** (like sliders and dropdowns) in the sidebar and the **outputs** (like plots and value boxes) in the main area. The magic of a dashboard is that changing an input control automatically re-runs the necessary R code and updates the outputs in real-time.</p>
        </div>
    </div>
    
    <div id="p6s2ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We will create the dashboard from the scenario, using <code>flexdashboard</code> to create the layout and Shiny components to make it interactive.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Create a New R Markdown File.</strong> In RStudio, go to <code>File > New File > R Markdown...</code>. From the "From Template" section, choose "Flex Dashboard".</li>
            <li><strong>Step 2: Define the Layout with Markdown Headers.</strong> We use <code>---</code> to define columns and <code>###</code> to define individual boxes within the grid.
            </li>
            <li><strong>Step 3: Add Inputs and Outputs.</strong> We add Shiny input controls and reactive outputs just like in a standard Shiny app. The code below is a complete, working <code>flexdashboard</code> Rmd file.
                <div class="code-container">
                    <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>` +
            '<pre><code class="language-markdown">' +
            `---
title: "Iris Species Dashboard"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
---

` +
            "`" +
            "``{r setup, include=FALSE}\n" +
            `library(flexdashboard)
library(ggplot2)
library(plotly)
library(dplyr)
` +
            "``" +
            "`" +
            `

Column {data-width=350}
-----------------------------------------------------------------------

### Controls

` +
            "``" +
            "`{r}\n" +
            `selectInput("species_select", "Select Species:",
            choices = c("All", unique(iris$Species)), selected = "All")
` +
            "``" +
            "`" +
            `

### Key Metrics

` +
            "``" +
            "`{r}\n" +
            `# A reactive value box
renderValueBox({
  filtered_data <- if(input$species_select == "All") {
    iris
  } else {
    filter(iris, Species == input$species_select)
  }
  
  avg_petal_len <- round(mean(filtered_data$Petal.Length), 2)
  
  valueBox(avg_petal_len, icon = "fa-leaf", color = "primary")
})
` +
            "``" +
            "`" +
            `

Column {data-width=650}
-----------------------------------------------------------------------

### Sepal Dimensions Plot

` +
            "``" +
            "`{r}\n" +
            `renderPlotly({
  filtered_data <- if(input$species_select == "All") {
    iris
  } else {
    filter(iris, Species == input$species_select)
  }
  
  p <- ggplot(filtered_data, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
    geom_point(size=3, alpha=0.7) +
    theme_minimal(base_size=16)
  
  ggplotly(p)
})
` +
            "``" +
            "`" +
            "</code></pre>" +
            `</div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: <code>bslib</code> for Theming</h3>
    <div class="scenario-content">
        <p>You can easily customize the look and feel of your Shiny apps and <code>flexdashboard</code> dashboards using the <code>bslib</code> package. It allows you to apply modern Bootstrap themes (like a dark mode) and change fonts and colors with just a few lines of code in your YAML header, making your reports look highly professional.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Using the <code>flexdashboard</code> example code above.</li>
            <li>Add another input control to the sidebar: a <code>sliderInput</code> that controls the <code>alpha</code> (transparency) of the points on the scatter plot. Give it an ID of <code>alpha_slider</code>, a label, and a range from 0 to 1.</li>
            <li>Modify the <code>geom_point()</code> call in the plot's code chunk to use the value from your new slider: <code>geom_point(alpha = input$alpha_slider)</code>.</li>
            <li>Run the document. You should now be able to interactively control the transparency of the points!</li>
        </ul>
    </div>
</div>
`,
          p6s2ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst creates a powerful, interactive Shiny dashboard. However, the input controls are confusingly labeled, there's too much information packed into one screen, and the color scheme is jarring and hard to read. Users find it overwhelming and don't use it. The application fails not because the analysis is wrong, but because it provides a poor **User Experience (UX)**.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for User-Centric Design</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-user-friends"></i> User Experience (UX) Design</div>
            <div class="scenario">Designing an application is not about what *you* find easy to use, but what your *target audience* finds intuitive.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-tasks"></i> 1. Clarity and Simplicity</div>
                        <div class="scenario"><strong>The Rule:</strong> Less is more. Don't overwhelm the user with dozens of controls and plots. Each dashboard page should have a single, clear purpose. Use whitespace and logical groupings to guide the user's eye.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-text-width"></i> 2. Use Clear Language</div>
                        <div class="scenario"><strong>The Rule:</strong> Use plain, simple language for all titles, labels, and help text. Avoid technical jargon. Instead of <code>var_x1_scaled</code>, use "Customer Satisfaction Score (1-5)".</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-universal-access"></i> 3. Ensure Accessibility</div>
                        <div class="scenario"><strong>The Rule:</strong> Design for everyone. Use color-blind friendly palettes, ensure sufficient color contrast, and make sure text is large enough to be easily readable.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-mobile-alt"></i> 4. Consider Responsiveness</div>
                        <div class="scenario"><strong>The Rule:</strong> Your dashboard might be viewed on a laptop, a tablet, or a phone. Use a framework like <code>flexdashboard</code> that is "responsive" and automatically adapts its layout to different screen sizes.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Get User Feedback Early and Often</h3>
    <div class="scenario-content">
        <p>You are not your user. The best way to improve the UX of your application is to watch a real user interact with it. Create a simple prototype and ask a stakeholder to "think out loud" as they try to use it. You will be amazed at what you learn. Their confusion is not their fault; it's a signal that your design needs to be improved.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>Review the <code>flexdashboard</code> code from the previous topic. Let's critique and improve its UX.</p>
        <ul class="prose-list">
            <li>The title is "Iris Species Dashboard". How could you make this more informative and action-oriented for a botanist?</li>
            <li>The slider label is "Minimum Petal Length:". How could you add more context, for example, by including the units?</li>
            <li>Imagine a non-technical user. Is it clear what the plot is showing? What one sentence could you add as a caption below the plot to explain its key insight?</li>
        </ul>
    </div>
</div>
`,
          p6s2ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst's Shiny app works perfectly on their machine. When they send it to a colleague, it crashes because the colleague is missing a required package. Later, they deploy it to a server, and it's incredibly slow because a single reactive calculation is re-running unnecessarily. These issues highlight the need for best practices in writing **production-quality** application code.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">From Prototype to Production</h3>
    <p>Building a robust and performant Shiny app requires moving beyond a single app.R file and adopting software engineering best practices.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> Shiny Best Practices</div>
            <div class="scenario">A checklist for creating applications that are maintainable, performant, and secure.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-box"></i> 1. Use a Modular Structure</div>
                        <div class="scenario"><strong>The Rule:</strong> For any non-trivial app, split your code. Put your UI logic in a <code>ui.R</code> file and your server logic in a <code>server.R</code> file. For very complex apps, use **Shiny Modules** to break down your app into smaller, reusable, and independently testable components.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-rocket"></i> 2. Optimize Reactivity</div>
                        <div class="scenario"><strong>The Rule:</strong> Understand the reactive graph to avoid unnecessary recalculations. Use functions like <code>reactive()</code> to cache calculations that are used by multiple outputs. Use <code>isolate()</code> to prevent a reactive expression from re-running when one of its inputs changes.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-lock"></i> 3. Prioritize Security</div>
                        <div class="scenario"><strong>The Rule:</strong> If your app involves user input that is used to build a SQL query, you are vulnerable to **SQL Injection**. Always use parameterized queries (e.g., with the <code>sqlInterpolate()</code> function from DBI) to prevent this. Sanitize all user inputs.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-clipboard-list"></i> 4. Manage Dependencies</div>
                        <div class="scenario"><strong>The Rule:</strong> Your app will crash if the server it's deployed on is missing a package. Use a dependency management tool like <code>renv</code> to create a lockfile that records the exact versions of all the packages your app needs, ensuring a reproducible environment.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Profile Your App with profvis</h3>
    <div class="scenario-content">
        <p>If your Shiny app is slow, don't guess where the bottleneck is. The <code>profvis</code> package can be used to profile a running Shiny app. You can wrap your app launch code in <code>profvis()</code> (<code>profvis(shinyApp(...))</code>), and it will generate an interactive flame graph that shows you exactly which reactive calculations are taking the most time. This allows you to target your optimization efforts effectively.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise about optimizing reactivity.</p>
        <p>You have a Shiny app with two outputs: a plot and a summary table. Both of them depend on a single, complex data processing step that takes 30 seconds to run. The user can change an input that only affects the title of the plot.</p>
        <ul class="prose-list">
            <li>In a poorly designed app, what happens when the user changes the plot title?</li>
            <li>How would you use a <code>reactive()</code> expression to fix this inefficiency? Where would you put the 30-second data processing step?</li>
        </ul>
    </div>
</div>
`,
          p6s1ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst presents a series of disconnected charts and statistical outputs to their team. While the analysis is technically correct, the audience is left confused, asking "So what? What's the main takeaway?" The analyst needs to learn how to move from simply presenting data to telling a compelling story with it.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Data Analysis Story Arc</h3>
    <p>A data narrative follows the same structure as any good story: a beginning, a middle, and an end. Framing your analysis in this structure turns a dry report into a persuasive and memorable argument.</p>
    <p><strong>Analogy:</strong> A data storyteller is a **tour guide**. They don't just dump a pile of maps and photos (your charts and tables) on you and walk away. They lead you on a planned journey. They start by setting the scene (the context), point out the important landmarks along the way (the key findings), and bring you to a final, memorable destination (the conclusion and recommendation).</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-book-open"></i> The Data Narrative</div>
            <div class="scenario">A structured analysis that guides an audience from a question to a conclusion.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-flag"></i> The Beginning: Context & Question</div>
                        <div class="scenario"><strong>Goal:</strong> Hook your audience. Explain the business context and the specific question your analysis will answer. Why does this matter? What is the core problem we are trying to solve?</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chart-line"></i> The Middle: The Analysis</div>
                        <div class="scenario"><strong>Goal:</strong> Build your case. Present your findings in a logical sequence. Each piece of evidence (a plot, a table, a test result) should build on the last, leading the audience step-by-step through your thought process.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-flag-checkered"></i> The End: Conclusion & Recommendation</div>
                        <div class="scenario"><strong>Goal:</strong> Deliver the "So what?". State your main conclusion clearly and directly. Answer the original question. Provide an actionable recommendation or suggest next steps based on your findings.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Know Your Audience</h3>
    <div class="scenario-content">
        <p>The most important part of storytelling is tailoring the story to your audience. A presentation for a technical audience can be filled with detailed charts and statistical outputs. A presentation for a senior executive should be high-level, focusing on 1-2 key visuals and the final business recommendation. Before you build your narrative, always ask: "Who am I talking to, and what do they care about?"</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Imagine you are an analyst for a subscription service. Your task is to investigate customer churn.</p>
        <ul class="prose-list">
            <li><strong>The Beginning:</strong> How would you frame the business context and the core question for your analysis?</li>
            <li><strong>The Middle:</strong> What are 2-3 key pieces of evidence (e.g., plots or summaries) you would present to build your case?</li>
            <li><strong>The End:</strong> Based on your hypothetical findings, what would be a sample conclusion and an actionable recommendation?</li>
        </ul>
    </div>
</div>
`,
          p6s1ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has a clear narrative in their head, but their visuals don't support it. They show a complex, exploratory plot with all data points when a simple, explanatory bar chart highlighting the key takeaway is needed. They need to learn how to design and sequence their plots specifically to advance their story and guide the audience's attention.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s1ss2-concepts">Key Concepts</button>
        <button class="tab-button" data-tab="p6s1ss2-viz">From Exploration to Explanation</button>
        <button class="tab-button" data-tab="p6s1ss2-usecase">Implementation in R</button>
    </div>

    <div id="p6s1ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Designing for an Audience</h4>
        <ul class="prose-list">
            <li><strong>Exploration vs. Explanation:</strong> Exploratory plots are for you, the analyst. They can be dense and complex as you search for patterns. Explanatory plots are for your audience. They must be simple, clean, and focus on communicating a single, clear message.</li>
            <li><strong>The Power of Annotation:</strong> Don't make your audience work to find the insight. Use titles, subtitles, text labels, and arrows to explicitly point out the most important parts of your visualization. Guide their eye to the story.</li>
            <li><strong>Logical Sequencing:</strong> Don't just show plots in a random order. Arrange them in a sequence that builds your narrative. Start with a high-level overview, then drill down into the specific findings that support your main point.</li>
        </ul>
    </div>
    
    <div id="p6s1ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Transforming a Plot for Storytelling</h4>
        <p>This shows how a standard, exploratory scatter plot can be transformed into a powerful, explanatory graphic that tells a clear story.</p>
        <div class="plot-container">
            <div id="p6s1ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Exploratory Plot** on the left is a standard <code>ggplot</code> output. It's useful for the analyst but doesn't tell a story. The **Explanatory Plot** on the right is designed for an audience. It has a clear title that states the main finding. It uses color and annotations to highlight the specific group of interest (the high-MPG, 4-cylinder cars) and fades the other data into the background, focusing the viewer's attention on the key insight.</p>
        </div>
    </div>

    <div id="p6s1ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Adding a Narrative Layer with ggplot2</h4>
        <p>The <code>labs()</code> and <code>annotate()</code> functions are your primary storytelling tools in ggplot2.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(ggplot2)
library(dplyr)

# Create a 'highlight' column for our story
mtcars_highlight <- mtcars %>%
  mutate(highlight = if_else(cyl == 4 & mpg > 30, "highlight", "default"))

# Create the explanatory plot
ggplot(mtcars_highlight, aes(x = wt, y = mpg)) +
  # Draw the faded points first
  geom_point(data = filter(mtcars_highlight, highlight == "default"), color = "grey70") +
  # Draw the highlighted points on top
  geom_point(data = filter(mtcars_highlight, highlight == "highlight"), color = "#ef4444", size = 3) +
  # Add the annotation
  annotate(
    "text", x = 2.4, y = 33, 
    label = "Lightweight 4-cylinder models\nachieve the highest efficiency.",
    hjust = 0, color = "#ef4444", fontface = "bold"
  ) +
  # Add storytelling labs
  labs(
    title = "Fuel Efficiency is Driven by Lightweight 4-Cylinder Cars",
    x = "Weight (1000 lbs)",
    y = "Miles per Gallon (MPG)",
    caption = "Source: 1974 Motor Trend US magazine"
  ) +
  theme_minimal()
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The ggrepel Package</h3>
    <div class="scenario-content">
        <p>When you have many points to label on a scatter plot, using <code>geom_text()</code> can result in a cluttered mess of overlapping labels. The <code>ggrepel</code> package provides an amazing drop-in replacement, <code>geom_text_repel()</code>, that automatically moves labels around to prevent them from overlapping, resulting in a much cleaner and more readable plot.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Using the iris dataset:</p>
        <ul class="prose-list">
            <li>Create a scatter plot of <code>Petal.Length</code> vs. <code>Petal.Width</code>, colored by Species.</li>
            <li>Add a title that tells a story, such as "Petal Dimensions Clearly Separate Iris Species."</li>
            <li>Use <code>annotate()</code> to draw a rectangle around the setosa cluster, which is clearly distinct from the other two. (Hint: <code>annotate("rect", xmin=..., xmax=..., ymin=..., ymax=..., fill="blue", alpha=0.1)</code>).</li>
        </ul>
    </div>
</div>
`,
          p6s1ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has crafted a compelling data story. Now they must choose the right medium to deliver it. Should they create a static, reproducible report for a technical appendix? An interactive dashboard for an executive who wants to explore the data? Or a slide deck for a live presentation to the department?</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">Choosing Your Medium</h3>
    <p>The final step of communication is packaging your narrative into the right format for your audience and purpose. R provides a world-class ecosystem for creating a variety of professional outputs from a single analytical script.</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-bullseye"></i> What is the goal of your communication?</div>
            <div class="scenario">The answer determines the best tool for the job.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-file-alt"></i> Goal: A Reproducible Report</div>
                        <div class="scenario"><strong>Audience:</strong> Technical peers, future self, journal reviewers.<br><strong>Tool:</strong> <strong>R Markdown</strong> or <strong>Quarto</strong> to produce static HTML, PDF, or Word documents. This is the gold standard for reproducible research.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-desktop"></i> Goal: An Interactive Dashboard</div>
                        <div class="scenario"><strong>Audience:</strong> Executives, business stakeholders, non-technical users.<br><strong>Tool:</strong> <strong>Shiny</strong> or <strong>flexdashboard</strong> to create web-based applications that allow users to explore the data and change parameters in real-time.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chalkboard-teacher"></i> Goal: A Live Presentation</div>
                        <div class="scenario"><strong>Audience:</strong> Department meeting, conference, stakeholders.<br><strong>Tool:</strong> <strong>Quarto</strong> can produce beautiful slide decks (similar to PowerPoint) directly from your R code, ensuring your presentation is in sync with your analysis.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Quarto for Everything</h3>
    <div class="scenario-content">
        <p>If you're starting a new project today, consider using <strong>Quarto</strong> (<code>.qmd</code> files) as your default. It is the next generation of R Markdown and is designed from the ground up to be a single, consistent framework for creating all of these outputs—reports, dashboards, presentations, websites, and books—from a single source, and it works with multiple languages (R, Python, etc.).</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's create a simple set of slides with Quarto.</p>
        <ul class="prose-list">
            <li>In RStudio (you may need to update to a recent version), go to <code>File > New File > Quarto Presentation</code>.</li>
            <li>Give it a title and author. Choose "Beamer" for PDF output or "Revealjs" for HTML slides.</li>
            <li>Click the "Render" button. RStudio will process the template file and produce a professional-looking slide deck.</li>
            <li>Try editing the Markdown. Use <code>##</code> to create a new slide, and include a code chunk to put a plot on one of your slides.</li>
        </ul>
    </div>
</div>
`,
          p6s1ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is presenting their findings on customer churn to a mixed audience. In the room are data scientists, marketing managers, and senior executives. The analyst makes the mistake of presenting the same dense, technical slides to everyone. The executives get lost in the details, while the data scientists are bored by the high-level summary. The presentation fails because it wasn't tailored to the **audience**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s1ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s1ss4-viz">Audience Segmentation</button>
    </div>

    <div id="p6s1ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Speaking Your Audience's Language</h4>
        <p><strong>Audience Engagement** is the practice of tailoring your communication style, content, and level of detail to the specific audience you are addressing. A successful data communicator is like a translator, converting complex statistical findings into the language that their audience understands and cares about.</p>
        <p><strong>Analogy:</strong> A data communicator is like a **doctor explaining a diagnosis**. To another specialist, the doctor might use precise medical terminology and discuss complex test results. To the patient, the doctor will use simple language, clear analogies, and focus on the main takeaway and the recommended course of action. The underlying medical facts are the same, but the presentation is radically different.</p>
    </div>
    
    <div id="p6s1ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Technical vs. Executive Axis</h4>
        <p>You can think of your audience on a spectrum from highly technical to high-level executive.</p>
        <div class="mindmap-container">
            <div class="mindmap-root">
                <div class="title"><i class="fas fa-users"></i> Know Your Audience</div>
                <div class="scenario">Tailor your message to what they know and what they need to know.</div>
            </div>
            <div class="mindmap-branches-container">
                <div class="mindmap-branches">
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-microscope"></i> Technical Audience</div>
                            <div class="scenario"><strong>Who:</strong> Fellow data scientists, statisticians, engineers.<br><strong>What they care about:</strong> Methodological rigor, model diagnostics, code quality, statistical significance.<br><strong>How to communicate:</strong> Detailed reports, code appendices, precise statistical language.</div>
                        </div>
                    </div>
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-user-tie"></i> Executive Audience</div>
                            <div class="scenario"><strong>Who:</strong> C-suite, VPs, Directors.<br><strong>What they care about:</strong> The "So what?", the bottom line, business impact, ROI, actionable recommendations.<br><strong>How to communicate:</strong> High-level summaries, one or two key visuals, clear and concise recommendations.</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The "Pyramid Principle"</h3>
    <div class="scenario-content">
        <p>When communicating with a business audience, use the Pyramid Principle: **Start with the answer first**. Lead with your main conclusion and recommendation in the first 30 seconds. Then, spend the rest of the presentation providing the supporting evidence. This respects the time of busy executives and ensures your key message is heard, even if they have to leave the meeting early.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You have just completed a complex A/B test analysis for a new website feature. Your main finding is that the new feature increased user engagement by 5% (p < 0.01), with a 95% confidence interval of [2%, 8%].</p>
        <ul class="prose-list">
            <li>How would you present this finding in a single sentence for a **technical audience**?</li>
            <li>How would you present this finding in a single sentence for an **executive audience**? What key piece of business context would you need to add to make it actionable for them?</li>
        </ul>
    </div>
</div>
`,
          p6s1ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst working for a news organization in Karachi is investigating a new dataset on local air quality. 🇵🇰 Instead of just presenting a dry report, they want to create a compelling, narrative-driven article for the public. They need to find the human story within the data, create clear and powerful visualizations, and present their findings in a way that is engaging and easy to understand. This is the essence of **Data Journalism**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s1ss5-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s1ss5-viz">The Workflow</button>
    </div>

    <div id="p6s1ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Applying Journalistic Principles to Data</h4>
        <p><strong>Data Journalism** is a field that combines data analysis with the principles of journalism. It's about finding, analyzing, and visualizing data to tell a compelling story about a real-world issue.</p>
        <p><strong>Analogy:</strong> A data journalist is a **detective who is also a great writer**. They don't just solve the case (the analysis); they write a captivating true-crime novel about it. They find the human angle, build a narrative arc, and present the evidence in a way that captivates and informs the reader.</p>
    </div>
    
    <div id="p6s1ss5-viz" class="tab-pane">
        <h4 class="subsection-title">The Data Journalist's Workflow</h4>
        <p>The process combines the skills of a data scientist with the narrative skills of a journalist.</p>
        <div class="mindmap-container">
            <div class="mindmap-root">
                <div class="title"><i class="fas fa-newspaper"></i> The Data Journalism Process</div>
            </div>
            <div class="mindmap-branches-container">
                <div class="mindmap-branches">
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-search"></i> 1. Find the Story</div>
                            <div class="scenario">Start with a question or a hypothesis. Dig into the data (often through extensive EDA) to find the core narrative. What is the most surprising or important finding?</div>
                        </div>
                    </div>
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-check-double"></i> 2. Verify and Contextualize</div>
                            <div class="scenario">This is crucial. Fact-check your data and your findings. Add context to the numbers. Why is a 5% increase in pollution significant? What are the real-world impacts?</div>
                        </div>
                    </div>
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-pen-nib"></i> 3. Write the Narrative</div>
                            <div class="scenario">Structure your findings into a clear story with a beginning, middle, and end. Use clear language and focus on the human impact of the data.</div>
                        </div>
                    </div>
                     <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-chart-pie"></i> 4. Visualize and Publish</div>
                            <div class="scenario">Create clean, powerful, and explanatory visualizations that support your narrative. Publish your work in an accessible format, like an R Markdown or Quarto article.</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: theme_ipsum and hrbrthemes</h3>
    <div class="scenario-content">
        <p>The visual style of publications like The Economist or the BBC is often clean, clear, and uses excellent typography. The <code>hrbrthemes</code> package, created by Bob Rudis, provides a set of ggplot2 themes (like <code>theme_ipsum()</code>) that make it easy to give your plots this professional, publication-ready aesthetic with a single line of code.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>You are a data journalist with access to the <code>mtcars</code> dataset from 1974.</p>
        <ul class="prose-list">
            <li>What is a potential story or narrative you could explore with this data? (e.g., comparing American vs. Japanese cars, the impact of weight on efficiency).</li>
            <li>What would be your "headline" or the main title of your data story?</li>
            <li>What single visualization would be the centerpiece of your article to support your headline?</li>
        </ul>
    </div>
</div>
`,
          p6s3ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is exploring a new dataset. Their R script is just a long series of commands with a few comments. When they revisit the script a month later to answer a follow-up question, they can't remember their original thought process, why they made certain data cleaning decisions, or what their initial hypotheses were. Their analysis is not a coherent narrative, just a list of instructions.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss3-concepts">The Core Philosophy</button>
        <button class="tab-button" data-tab="p1s7ss3-comparison">Comparison: Script vs. Notebook</button>
    </div>

    <div id="p1s7ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Programming as an Act of Explanation</h4>
        <p><strong>Literate Programming</strong> is a philosophy introduced by Donald Knuth. It argues that a program should be written as an essay for human beings, explaining its logic in natural language, with executable code snippets included as illustrations. The focus is on the *explanation* of the thought process, not just the final code.</p>
        <p><strong>R Notebooks</strong> are RStudio's interactive implementation of this philosophy. They allow you to work in an environment where you can easily interweave text, code, and output, creating a "lab notebook" that documents your analysis as you perform it.</p>
        <p><strong>Analogy:</strong> A traditional script is like a list of cooking instructions from a machine. A literate programming notebook is like a page from a <strong>chef's personal journal</strong>. It includes the recipe (the code), but also their notes on why they chose certain ingredients, observations they made during the process, and their thoughts on the final dish (the interpretation of the results).</p>
    </div>
    
    <div id="p1s7ss3-comparison" class="tab-pane">
        <h4 class="subsection-title">Two Ways of Working</h4>
        <p>This comparison shows how the same simple analysis can be represented in a standard R script versus an R Notebook. The notebook format encourages a narrative and captures the analytical thought process.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Traditional R Script (<code>.R</code>)</strong>
                <p>Focused purely on the code. Explanations are limited to comments. Output appears separately in the console or plots pane.</p>
                <div class="code-container">
                    <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
                    <pre><code class="language-r">
# Load libraries
library(dplyr)
library(ggplot2)

# Filter data
versicolor_data <- filter(iris, Species == "versicolor")

# Plot
ggplot(versicolor_data, aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point()

# Model
lm(Sepal.Width ~ Sepal.Length, data = versicolor_data)
</code></pre>
                </div>
            </div>
            <div class="decision-branch">
                <strong>R Notebook (<code>.Rmd</code>)</strong>
                <p>Interweaves formatted text with executable code chunks. Output is displayed inline, directly beneath the code that generated it, creating a logical, readable flow.</p>
                <div class="code-container"><code> +
            '<pre><code class="language-markdown">' +
            </code>### Analysis of Versicolor Species

First, we load the necessary libraries and filter the <code>iris</code> dataset to keep only the <code>versicolor</code> species.

<code> +
            "</code><code>" +
            "</code>{r setup-and-filter}\n" +
            <code>library(dplyr)
library(ggplot2)
versicolor_data <- filter(iris, Species == "versicolor")
</code> +
            "<code></code>" +
            "<code>" +
            </code>

Next, let's visualize the relationship between sepal length and width for this species.

<code> +
            "</code><code>" +
            "</code>{r plot-versicolor}\n" +
            <code>ggplot(versicolor_data, aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point()
</code> +
            "<code></code>" +
            "<code>" +
            </code>

The plot shows a slight positive relationship. We can quantify this by fitting a simple linear model.

<code> +
            "</code><code>" +
            "</code>{r model-versicolor}\n" +
            <code>lm(Sepal.Width ~ Sepal.Length, data = versicolor_data)
</code> +
            "<code></code>" +
            "<code>" +
            "</code></pre>" +
            </code></div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Inline Code for Truly Dynamic Text</h3>
    <div class="scenario-content">
        <p>R Markdown and Notebooks allow you to insert small snippets of R code directly into your text. This is done by enclosing the code in backticks with an <code>r</code> at the start, like this: <code><code> +
            "</code>r R_CODE<code>" +
            </code></code>. This is incredibly powerful for making your narrative truly dynamic. For example, you can write: "The average sepal length was <code><code> +
            "</code>r mean(iris$Sepal.Length)<code>" +
            </code></code> cm." If your data changes, the number in your final rendered sentence will update automatically.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>In RStudio, create a new R Notebook (<code>File > New File > R Notebook</code>).</li>
            <li>In the first text chunk, write a sentence describing the <code>mtcars</code> dataset.</li>
            <li>In a code chunk, calculate the average miles per gallon (<code>mpg</code>) for cars with 4 cylinders versus 8 cylinders. (Hint: use <code>group_by(cyl)</code> and <code>summarise(avg_mpg = mean(mpg))</code>).</li>
            <li>In a new text chunk below, use inline R code to report the average MPG for 4-cylinder cars directly in your sentence. For example: "The average MPG for 4-cylinder cars was <code><code> +
            "</code>r # YOUR CODE HERE<code>" +
            </code></code>."</li>
        </ul>
    </div>
</div>
`,
          p6s3ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has created an R Markdown report that analyzes sales for their Karachi store. 🇵🇰 Now, their manager wants the identical report but for the Lahore and Islamabad stores. The analyst could copy and paste the <code>.Rmd</code> file three times and change the city name manually, but this is inefficient and a maintenance nightmare. A **Parameterized Report** is the correct, professional solution.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s3ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s3ss2-viz">The Workflow</button>
        <button class="tab-button" data-tab="p6s3ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p6s3ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Reports as Functions</h4>
        <p>A **Parameterized Report** is a dynamic document (R Markdown or Quarto) that is designed like a function. It accepts input arguments (**parameters**) that can be used to change the content of the final rendered report. This allows you to generate many customized versions of the same report from a single template.</p>
        <p><strong>Analogy:</strong> A parameterized report is like a **form letter**. You write the main body of the letter once (the <code>.Rmd</code> template), leaving placeholders for things like <code>[Customer Name]</code> and <code>[Order Number]</code> (the parameters). You can then automatically generate thousands of personalized letters by feeding a list of names and order numbers into your template.</p>
    </div>

    <div id="p6s3ss2-viz" class="tab-pane">
        <h4 class="subsection-title">One Template, Many Outputs</h4>
        <p>This diagram shows how a single, parameterized R Markdown template can be combined with different input parameters to automate the creation of multiple, unique reports.</p>
        <div class="plot-container">
            <div id="p6s3ss2-plot1" class="plotly-chart"></div>
        </div>
    </div>
    
    <div id="p6s3ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We will create a single report template that can generate a summary plot for any of the three species in the <code>iris</code> dataset.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: Define Parameters in the YAML Header.</strong> In your <code>.Rmd</code> file, you add a <code>params</code> section to the header to declare your input arguments and their default values.
                <div class="code-container"><code> +
                    '<pre><code class="language-yaml">' +
</code>---
title: "Iris Species Report"
output: html_document
params:
  species_name: "setosa"
---
<code> +
                    '</code></pre>' +
                </code></div>
            </li>
            <li><strong>Step 2: Access Parameters in Your Code.</strong> You can access these parameters anywhere in your R code chunks via a special read-only list called <code>params</code>.
                <div class="code-container"><code> +
                    '<pre><code class="language-markdown">' +
                    "This report is for the species: r params$species_name" +
                    </code>

<code> +
            "</code><code>" +
            "</code>{r}\n" +
            <code>library(ggplot2)
library(dplyr)

# Filter the data using the parameter
filtered_data <- filter(iris, Species == params$species_name)

# Create a plot based on the filtered data
ggplot(filtered_data, aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point() +
  ggtitle(paste("Sepal Dimensions for", params$species_name))
</code> +
            "<code></code>" +
            "<code>" +
                    '</code></pre>' +
                </code></div>
            </li>
            <li><strong>Step 3: Render the Report with New Parameters.</strong> You can knit the report with the default value, or you can use the <code>rmarkdown::render()</code> function from a separate R script to generate new versions.
                <div class="code-container">
                    <pre><code class="language-r"># This code would be in a separate R script
rmarkdown::render(
  "my_report.Rmd",
  output_file = "virginica_report.html",
  params = list(species_name = "virginica")
)</code></pre>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Version Your Outputs</h3>
    <div class="scenario-content">
        <p>When you parameterize reports, you often generate many output files. It's a good practice to include a version number or a timestamp in the output file name to keep them organized. For example: <code>output_file = paste0("report-", Sys.Date(), ".html")</code>. This prevents you from accidentally overwriting old reports and creates a clear historical record.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Create a parameterized R Markdown report.</li>
            <li>Define one parameter in the YAML header called <code>num_bins</code>, with a default value of 10.</li>
            <li>In a code chunk, create a histogram of the <code>mpg</code> column from the <code>mtcars</code> dataset. Use the parameter to control the number of bins: <code>geom_histogram(bins = params$num_bins)</code>.</li>
            <li>Knit the report to see the default version.</li>
            <li>Use <code>rmarkdown::render()</code> to generate a new version with 30 bins.</li>
        </ul>
    </div>
</div>
`,
          p6s3ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist writes a brilliant but complex function to calculate a custom metric. A colleague on their team wants to use it, but has no idea what arguments it takes, what data type each argument should be, or what the function returns. The function is effectively unusable without proper documentation.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss5-concepts">Levels of Documentation</button>
        <button class="tab-button" data-tab="p1s7ss5-usecase">Function Documentation with <code>roxygen2</code></button>
    </div>

    <div id="p1s7ss5-concepts" class="tab-pane active">
        <h4 class="subsection-title">Explaining Your Work</h4>
        <p>Good documentation is essential for collaboration, reproducibility, and your own future sanity. There are several levels of documentation for an R project:</p>
        <ul class="prose-list">
            <li><strong>Comments:</strong> Inline notes to explain the "why" of a specific line of code.</li>
            <li><strong>Function Documentation:</strong> A structured explanation of what a function does, what its arguments are, and what it returns. The standard for this in R is <code>roxygen2</code>.</li>
            <li><strong><code>README.md</code>:</strong> A file in your project's root directory that gives a high-level overview of the project: its purpose, how to install and run the analysis, and where to find key files.</li>
            <li><strong>Vignettes:</strong> Long-form documents that act as a tutorial or guide for a package, showing how to use its functions together to solve a real problem.</li>
        </ul>
        <p><strong>Analogy:</strong> Think of an appliance. Inline comments are the small warning labels. The <strong><code>roxygen2</code> documentation</strong> is the technical manual that describes what each button and knob does. The <strong><code>README</code></strong> is the "Quick Start" guide. A <strong>vignette</strong> is a recipe book showing you how to use the appliance to cook full meals.</p>
    </div>
    
    <div id="p1s7ss5-usecase" class="tab-pane">
        <h4 class="subsection-title">The <code>roxygen2</code> Standard</h4>
        <p><code>roxygen2</code> is a system that allows you to write documentation in special comments directly above your function definition. RStudio can then process these comments to automatically create the official R help files (the ones you see when you type <code>?function_name</code>).</p>
        <p>A <code>roxygen2</code> block starts with <code>#'</code>. Special tags starting with <code>@</code> are used to define different parts of the documentation.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Function Without Documentation</strong>
                <div class="code-container">
                    <pre><code class="language-r">
calc_bmi <- function(w, h) {
  if (!is.numeric(w) || !is.numeric(h)) {
    stop("Inputs must be numeric.")
  }
  return(w / h^2)
}
</code></pre>
                </div>
            </div>
            <div class="decision-branch">
                <strong>Same Function With <code>roxygen2</code> Docs</strong>
                 <div class="code-container">
                    <pre><code class="language-r">
#' Calculate Body Mass Index (BMI)
#'
#' This function calculates BMI based on a person's
#' weight in kilograms and height in meters.
#'
#' @param w A numeric value for weight in kilograms.
#' @param h A numeric value for height in meters.
#'
#' @return A numeric value representing the BMI.
#' @export
#'
#' @examples
#' calc_bmi(w = 75, h = 1.8)
calc_bmi <- function(w, h) {
  if (!is.numeric(w) || !is.numeric(h)) {
    stop("Inputs must be numeric.")
  }
  return(w / h^2)
}
</code></pre>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: RStudio Integration</h3>
    <div class="scenario-content">
        <p>RStudio has excellent integration with <code>roxygen2</code>. After writing a function, you can place your cursor inside it and go to <code>Code > Insert Roxygen Skeleton</code>. RStudio will automatically insert a template with <code>@param</code> tags for each of your function's arguments, saving you time and ensuring you don't miss any.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Write a simple function called <code>convert_fahr_to_celsius</code> that takes one argument, <code>temp_f</code>, and returns the temperature in Celsius. The formula is <code>(temp_f - 32) * 5 / 9</code>.</li>
            <li>Use the RStudio shortcut (<code>Code > Insert Roxygen Skeleton</code>) or manually write a <code>roxygen2</code> documentation block above your function.</li>
            <li>Fill in the title, description, the <code>@param temp_f</code> tag, the <code>@return</code> tag, and an <code>@examples</code> tag.</li>
        </ul>
    </div>
</div>
`,
          p6s3ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A complex analysis involves multiple, dependent R scripts: <code>01_download_data.R</code>, <code>02_clean_data.R</code>, <code>03_train_model.R</code>, and <code>04_generate_report.Rmd</code>. Running these manually is tedious. More importantly, if a raw data file from step 1 changes, the analyst must remember to manually re-run all subsequent steps. A <strong>pipeline tool</strong> can automate this entire workflow and intelligently re-run only the necessary parts.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s7ss4-viz">Visualizing the Pipeline</button>
        <button class="tab-button" data-tab="p1s7ss4-usecase">The <code>targets</code> Package</button>
    </div>

    <div id="p1s7ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Managing Dependencies</h4>
        <p>An automation pipeline tool views your entire analysis as a network of dependencies. Each step (a "target") is an object that depends on other objects or files. The tool builds a <strong>Directed Acyclic Graph (DAG)</strong> to map these relationships.</p>
        <p>When you run the pipeline, the tool checks which targets are "out of date"—meaning their dependencies (code or data) have changed since the last run. It then intelligently runs *only* the targets that are out of date, and any downstream targets that depend on them.</p>
        <p><strong>Analogy:</strong> A pipeline tool is like a **smart general contractor building a house**. They have a blueprint (the pipeline plan) of all dependencies (the foundation must be laid before the walls go up; the plumbing must be installed before the drywall). If you decide to change the kitchen faucet (one piece of "code"), the contractor knows they don't need to rebuild the entire foundation. They only need to re-do the specific plumbing work affected by that change.</p>
    </div>

    <div id="p1s7ss4-viz" class="tab-pane">
        <h4 class="subsection-title">The Analysis as a Graph</h4>
        <p>This diagram shows a simple analysis workflow visualized as a dependency graph. An arrow from A to B means that B depends on A. If <code>clean_data.R</code> changes, the pipeline knows it must re-run the <code>clean_data</code>, <code>model</code>, and <code>report</code> targets, but it can skip re-running the <code>raw_data</code> target.</p>
        <div class="plot-container">
            <div id="p6s3ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This graph shows the flow of dependencies. The final report depends on the model and a plot. The model depends on the cleaned data. The cleaned data depends on the raw data. This map of relationships allows the pipeline tool to make smart decisions about what to re-run when something changes.</p>
        </div>
    </div>
    
    <div id="p1s7ss4-usecase" class="tab-pane">
        <h4 class="subsection-title">A Simple <code>_targets.R</code> File</h4>
        <p>The <code>targets</code> package is the modern, standard choice for building pipelines in R. You define your entire workflow in a single script, typically named <code>_targets.R</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
# Load the necessary packages
library(targets)

# Load user-defined functions (best practice)
source("R/functions.R") 

# Define the pipeline
list(
  # Target 1: A file path to the raw data
  tar_target(
    name = raw_data_file,
    command = "data/raw_data.csv",
    format = "file" 
  ),
  # Target 2: A clean data frame, depends on the file from step 1
  tar_target(
    name = clean_data,
    command = process_data(raw_data_file)
  ),
  # Target 3: A plot, which depends on the clean data
  tar_target(
    name = my_plot,
    command = create_plot(clean_data)
  )
)
</code></pre>
        </div>
        <p>To run the pipeline, you simply open R and run <code>tar_make()</code> in the console. To visualize the dependency graph, you run <code>tar_visnetwork()</code>.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Functions over Scripts</h3>
    <div class="scenario-content">
        <p>The best practice for building <code>targets</code> pipelines is to put all your actual analysis code into functions within a separate <code>R/</code> folder. The <code>_targets.R</code> file should only be used for "calling the plays"—connecting these functions together to define the pipeline steps. This keeps your pipeline definition clean and your analysis code modular and reusable.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise to think about dependencies.</p>
        <ul class="prose-list">
            <li>Imagine an analysis with these steps: <code>download_data</code>, <code>clean_data</code>, <code>model_A</code>, <code>model_B</code>, <code>compare_models</code>.</li>
            <li>Draw out the dependency graph for this pipeline. Which steps depend on which other steps?</li>
            <li>If you change the code for <code>model_A</code>, which steps need to be re-run by the pipeline tool?</li>
            <li>If you change the code for <code>clean_data</code>, which steps need to be re-run?</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p6s4ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist has trained a final, validated random forest model to predict customer churn. Now, they need to deploy this model so that the company's main web application can send it new customer data and get a churn prediction in real-time. This requires two steps: saving the trained model object (**serialization**) and loading it into a script that can receive web requests (**serving**).</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Deployment Workflow</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-cogs"></i> From R Session to Production</div>
            <div class="scenario">The goal is to take a model object that exists in your R session and make it accessible to other applications over a network.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-save"></i> 1. Model Serialization</div>
                        <div class="scenario"><strong>What:</strong> The process of converting your trained model object into a format that can be saved to a file. In R, the standard function is <code>saveRDS()</code>, which creates a compressed <code>.rds</code> file.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-server"></i> 2. Model Serving via API</div>
                        <div class="scenario"><strong>What:</strong> Creating a web server that loads the saved model file and exposes an **API endpoint**. Other applications can send data to this endpoint, and the server will use the loaded model to make a prediction and send it back.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">Step-by-Step Example</h3>
    <div class="decision-branches" style="align-items: flex-start;">
        <div class="decision-branch">
            <strong>Part A: Training and Saving the Model</strong>
            <p>This is done in your analysis script.</p>
            <div class="code-container">
                <pre><code class="language-r">
# 1. Train your final model
final_model <- lm(mpg ~ wt, data = mtcars)

# 2. Serialize and save the model to a file
# It's good practice to save this in a 'model' subfolder.
saveRDS(final_model, "model/mpg_model.rds")
                </code></pre>
            </div>
        </div>
        <div class="decision-branch">
            <strong>Part B: Creating the API with <code>plumber</code></strong>
            <p>This is a new, separate script (e.g., <code>plumber.R</code>).</p>
            <div class="code-container">
                <pre><code class="language-r">
# install.packages("plumber")
library(plumber)

# 1. Load the serialized model object from the file
mpg_model <- readRDS("model/mpg_model.rds")

#* @apiTitle MPG Prediction API

#* Predict MPG from a car's weight
#* @param car_weight The weight of the car in 1000s of lbs
#* @get /predict
function(car_weight) {
  # 2. Prepare the new data
  new_data <- data.frame(wt = as.numeric(car_weight))
  
  # 3. Use the loaded model to make a prediction
  prediction <- predict(mpg_model, newdata = new_data)
  
  # 4. Return the prediction
  return(list(predicted_mpg = prediction))
}
                </code></pre>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use <code>vetiver</code> for a Full Workflow</h3>
    <div class="scenario-content">
        <p>The <code>plumber</code> package is a great low-level tool. For a more complete, MLOps-focused workflow, the <code>vetiver</code> package (from <code>tidymodels</code>) is the modern standard. It helps you version, share, and deploy your models. You create a "vetiver model object" that bundles your model with information about its input data prototype. <code>vetiver</code> can then automatically create a <code>plumber</code> API, a Dockerfile for containerization, and provides tools for deploying to services like Posit Connect.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Train a simple linear model on the <code>iris</code> dataset to predict <code>Sepal.Length</code> from <code>Petal.Length</code>.</li>
            <li>Use <code>saveRDS()</code> to save your trained model to a file named <code>iris_model.rds</code>.</li>
            <li>In your R session, remove the model object you just created with <code>rm(my_model)</code>.</li>
            <li>Use <code>readRDS()</code> to load the model back from the file into a new object. Verify that it's the same model by using it to make a prediction.</li>
        </ul>
    </div>
</div>
`,
          p6s4ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> The analyst has deployed their prediction model using <code>plumber</code>. Now, a front-end web developer, who uses JavaScript, needs to communicate with this model. They need a standardized, language-agnostic "contract" that defines how to request a prediction and what the response will look like. This contract is a **REST API**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s4ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s4ss2-viz">Visualization</button>
        <button class="tab-button" data-tab="p6s4ss2-usecase">Step-by-Step Example</button>
    </div>

    <div id="p6s4ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">A Universal Language for Web Services</h4>
        <p>A **REST (REpresentational State Transfer) API** is an architectural style for building web services. It provides a set of rules and conventions that make it easy for different computer systems to communicate with each other over the internet.</p>
        <p><strong>Analogy:</strong> A REST API is like a **restaurant menu and a well-trained waiter**.
            <ul>
                <li><strong>The Menu (API Documentation):</strong> It lists all the available dishes (<strong>endpoints</strong> like <code>/predict</code> or <code>/user_info</code>) and what you need to provide for each (<strong>parameters</strong>).</li>
                <li><strong>The Waiter (The API Server):</strong> You give the waiter a clear order from the menu (a <strong>request</strong>). They take it to the kitchen (the R model), which prepares the dish. The waiter then brings the finished dish back to your table (the <strong>response</strong>, usually in JSON format).</li>
            </ul>
        </p>
    </div>

    <div id="p6s4ss2-viz" class="tab-pane">
        <h4 class="subsection-title">The Request-Response Cycle</h4>
        <p>This diagram shows the flow of information when a client application makes a request to a Plumber API.</p>
        <div class="plot-container">
            <div id="p6s4ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **Client** (e.g., a web browser or another server) constructs an HTTP request to a specific **endpoint** on the API server. The API server receives the request, runs the corresponding R function, and sends back a structured **JSON response**.</p>
        </div>
    </div>
    
    <div id="p6s4ss2-usecase" class="tab-pane">
        <h4 class="subsection-title">Defining Endpoints in <code>plumber</code></h4>
        <p>In <code>plumber</code>, you use special comments (called "annotations") starting with <code>#*</code> to define your API endpoints.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-r">
library(plumber)

#* Log some information about the request
#* @filter logger
function(req){
  cat(as.character(Sys.time()), "-", req$REQUEST_METHOD, req$PATH_INFO, "\n")
  plumber::forward()
}

#* Return a simple "hello world" message
#* @get /echo
function(msg=""){
  list(msg = paste0("The message is: '", msg, "'"))
}

#* Return the sum of two numbers
#* @param a The first number
#* @param b The second number
#* @post /sum
function(a, b){
  as.numeric(a) + as.numeric(b)
}
</code></pre>
        </div>
         <div class="plot-description">
            <p><strong>How to interpret this code:</strong></p>
             <ul class="prose-list">
                <li><code>#* @get /echo</code>: This creates a GET endpoint at the path <code>/echo</code>. You could call this from a browser with a URL like <code>http://127.0.0.1:8000/echo?msg=hello</code>.</li>
                <li><code>#* @post /sum</code>: This creates a POST endpoint at <code>/sum</code>, which is typically used for sending data in the request body.</li>
                <li><code>#* @param ...</code>: These comments document the parameters for the endpoint, which Plumber uses to automatically generate interactive documentation (using OpenAPI/Swagger).</li>
            </ul>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Auto-Generated Documentation</h3>
    <div class="scenario-content">
        <p>One of the best features of <code>plumber</code> is that it uses your special <code>@</code> comments to automatically generate interactive API documentation. When you run your Plumber API, you can navigate to the <code>/__docs__/</code> endpoint (e.g., <code>http://127.0.0.1:8000/__docs__/</code>) in your web browser to see a beautiful Swagger UI. This interface lists all your endpoints, shows their parameters, and even lets you test them live from the browser.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Take the <code>plumber.R</code> script from the previous topic (Model Serialization & Serving).</li>
            <li>Add a new endpoint that provides some basic information about the model.</li>
            <li>It should be a GET endpoint at the path <code>/info</code>.</li>
            <li>The function should not take any arguments.</li>
            <li>It should return a list with some information, like <code>list(model_name = "Linear Regression", model_version = "1.0")</code>.</li>
            <li>Run your API and test the new <code>/info</code> endpoint in your browser.</li>
        </ul>
    </div>
</div>
`,
          p6s4ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A bank deploys a fraud detection model that works perfectly. Six months later, its performance has degraded significantly. The reason: fraudsters have adapted their strategies, and the patterns of fraud have changed. The real-world data no longer looks like the data the model was trained on. The model is "stale" and needs to be retrained. This highlights the critical need for **Model Monitoring**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s4ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s4ss3-viz">Visualization</button>
        <button class="tab-button" data-tab="p6s4ss3-usecase">Step-by-Step Example</button>
    </div>

    <div id="p6s4ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Preventing "Model Rot"</h4>
        <p><strong>Model Monitoring</strong> is the process of continuously tracking the performance and behavior of a machine learning model in production to detect degradation and ensure it remains effective over time.</p>
        <p><strong>Analogy:</strong> A deployed model is like a **car**. When it's new, it runs perfectly. But over time, parts wear out and performance degrades. Model monitoring is the **dashboard warning light**. It continuously checks the engine's health (the model's performance) and alerts you when something is wrong (like data drift) so you can take it in for a tune-up (retraining).</p>
        <p>The two main problems to monitor for are:</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Data Drift</strong>
                <p>The statistical properties of the input data the model receives in production have changed significantly from the data it was trained on. The model is now seeing a new kind of data it wasn't prepared for.</p>
            </div>
            <div class="decision-branch">
                <strong>Concept Drift</strong>
                <p>The relationship between the input features and the target variable has changed over time. The fundamental patterns the model learned are no longer valid.</p>
            </div>
        </div>
    </div>

    <div id="p6s4ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Detecting Data Drift</h4>
        <p>A common way to detect data drift is to monitor the distribution of the model's input features over time and compare it to the distribution of the training data.</p>
        <div class="plot-container">
            <div id="p6s4ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> This plot shows the distribution of a single feature. The blue area is the distribution of the original training data. The orange histogram represents the distribution of the live, production data from the last 24 hours. There is a clear and significant shift to the right, indicating **data drift**. The model is now seeing values it rarely encountered during training, which will likely degrade its performance.</p>
        </div>
    </div>
    
    <div id="p6s4ss3-usecase" class="tab-pane">
        <h4 class="subsection-title">A Simple Drift Detection Workflow</h4>
        <p>We can use a statistical test, like the Kolmogorov-Smirnov (K-S) test, to formally check if two samples come from the same distribution.</p>
        <ol class="prose-list">
            <li><strong>Step 1: Store the Training Data Distribution.</strong> When you train your model, save the training data used.
                <div class="code-container">
                    <pre><code class="language-r"># Assume 'training_data' is the data used to train your deployed model
training_feature_x <- training_data$feature_x</code></pre>
                </div>
            </li>
            <li><strong>Step 2: Collect a Sample of Live Data.</strong> On a regular schedule (e.g., daily), collect the new data that your model is seeing in production.
                <div class="code-container">
                    <pre><code class="language-r"># Assume 'live_data_sample' is the data from the last 24 hours
live_feature_x <- live_data_sample$feature_x</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Perform a Statistical Test.</strong> The K-S test's null hypothesis is that the two samples are from the same distribution. A small p-value indicates drift.
                <div class="code-container">
                    <pre><code class="language-r">ks.test(training_feature_x, live_feature_x)</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> If the p-value from the K-S test drops below a predefined threshold (e.g., 0.01), this can trigger an automated alert to the MLOps team, signaling that data drift has been detected and the model may need to be investigated or retrained.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Monitor Model Predictions Too</h3>
    <div class="scenario-content">
        <p>In addition to the input features, you should also monitor the distribution of your model's **output predictions**. A sudden shift in the distribution of predicted probabilities can be a powerful and early indicator of concept drift, even before you have new ground-truth labels to calculate accuracy.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>Let's simulate and detect data drift.</p>
        <ul class="prose-list">
            <li>Create a "training" data sample: <code>training_data <- rnorm(1000, mean = 10, sd = 2)</code>.</li>
            <li>Create a "live" data sample that has *not* drifted: <code>live_data_no_drift <- rnorm(1000, mean = 10, sd = 2)</code>.</li>
            <li>Create another "live" data sample that *has* drifted: <code>live_data_drifted <- rnorm(1000, mean = 13, sd = 2)</code>.</li>
            <li>Run <code>ks.test()</code> comparing the training data to the non-drifted data. Is the p-value high or low?</li>
            <li>Run <code>ks.test()</code> comparing the training data to the drifted data. Is the p-value high or low? What does this tell you?</li>
        </ul>
    </div>
</div>
`,
          p6s4ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A company's churn model, deployed last year, is now stale due to concept drift. The data science team needs to retrain it on new data. However, they need a systematic process for this. How do they version the new model? How do they test it before replacing the old one? And how can they quickly roll back to the old version if the new one has a bug? This requires **Model Lifecycle Management**.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The MLOps Cycle</h3>
    <p>Model Lifecycle Management is the process of governing a machine learning model's entire journey, from its creation to its retirement. It is a core component of MLOps (Machine Learning Operations).</p>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-sync-alt"></i> The Model Lifecycle</div>
            <div class="scenario">A continuous, iterative process of development, deployment, and maintenance.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-drafting-compass"></i> 1. Develop & Train</div>
                        <div class="scenario">The data scientist experiments, trains a model, and validates its performance on historical data.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-box"></i> 2. Package & Version</div>
                        <div class="scenario">The final model is serialized and stored in a **model registry**, along with its version, metadata, and performance metrics.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-rocket"></i> 3. Deploy & Serve</div>
                        <div class="scenario">The versioned model is deployed as an API into the production environment to serve live predictions.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-chart-line"></i> 4. Monitor & Alert</div>
                        <div class="scenario">The model's performance and data inputs are continuously monitored. Alerts are triggered if drift is detected.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-redo"></i> 5. Retrain & Repeat</div>
                        <div class="scenario">When an alert is triggered or on a regular schedule, the model is retrained on new data, starting the cycle over again.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Model Registry is Your Single Source of Truth</h3>
    <div class="scenario-content">
        <p>A **model registry** is a central repository for managing your trained models. It's the most critical piece of infrastructure for a mature MLOps workflow. It allows you to track model versions, store their performance metrics, manage their stage (e.g., "development," "staging," "production"), and easily retrieve a specific version for deployment or rollback. The <code>vetiver</code> package in R provides excellent tools for creating and interacting with a model registry.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You have a sales forecasting model in production (Version 1.0).</li>
            <li>You retrain the model on six new months of data. What version number might you give this new model?</li>
            <li>You want to test the new version without affecting all users. What is a common deployment strategy to do this? (Hint: Think about A/B testing or a "canary" release).</li>
            <li>Your monitoring system shows that the new model is producing strange predictions. What is the immediate, safe action to take?</li>
        </ul>
    </div>
</div>
`,
          p6s4ss5: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst deploys their model API. It works, but it's fragile. The R code is in one giant script, there are no automated tests, and the environment is not reproducible. A small change to the code could break the entire system. They need to adopt MLOps best practices to create a robust, reliable, and maintainable production service.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Production-Grade MLOps</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> MLOps Best Practices</div>
            <div class="scenario">The key principles that separate a fragile data science project from a robust production system.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-code-branch"></i> 1. Version Everything</div>
                        <div class="scenario"><strong>The Rule:</strong> Use Git for your code. Use a model registry (like <code>vetiver</code>) for your models. Use a data versioning tool (like DVC) for your data. Every component should have a version number, creating a fully reproducible audit trail.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-vials"></i> 2. Automate Testing</div>
                        <div class="scenario"><strong>The Rule:</strong> Your project should have a suite of automated tests. This includes unit tests for individual functions (using <code>testthat</code>) and integration tests to ensure the whole pipeline works. These should be run automatically by a CI/CD system.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-sync-alt"></i> 3. Implement CI/CD</div>
                        <div class="scenario"><strong>The Rule:</strong> Use a Continuous Integration / Continuous Deployment tool (like GitHub Actions) to automate the process of testing, building, and deploying your model. This reduces manual errors and increases deployment speed.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fab fa-docker"></i> 4. Use Containers</div>
                        <div class="scenario"><strong>The Rule:</strong> Package your model API into a Docker container. This bundles your code, dependencies, and system environment into a single, portable artifact that will run identically on your laptop, a staging server, or in the cloud.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: A Simple CI/CD Workflow with GitHub Actions</h3>
    <div class="scenario-content">
        <p>You can set up a simple CI pipeline for your R project for free using GitHub Actions. You create a YAML file in your repository's <code>.github/workflows/</code> directory. This file can specify a workflow that, for example, triggers on every push to the <code>main</code> branch, sets up an R environment, installs your package dependencies using <code>renv</code>, and then runs your entire test suite using the <code>testthat</code> package. If any test fails, the build fails, and you are notified immediately.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <p>Let's design a simple CI/CD pipeline for a model API.</p>
        <ul class="prose-list">
            <li>What event would trigger the start of the pipeline? (Hint: a <code>git ...</code> command).</li>
            <li>What is the first step the automated pipeline should perform? (Hint: it relates to quality).</li>
            <li>If that step succeeds, what is the next step related to packaging the application?</li>
            <li>If that step also succeeds, what is the final step in the pipeline?</li>
        </ul>
    </div>
</div>
`,
          p6s5ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has developed a Shiny app that works perfectly on their laptop. They send the code to a colleague, but it crashes because the colleague has a different version of a key package. To deploy the app to a server, they need to guarantee that the environment is *exactly* the same as on their machine. This is the problem that **Docker** solves.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s5ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s5ss1-viz">The Workflow</button>
        <button class="tab-button" data-tab="p6s5ss1-usecase">Step-by-Step Example</button>
    </div>

    <div id="p6s5ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Shipping Your Entire Workshop</h4>
        <p><strong>Containerization</strong> is the process of packaging up an application along with all of its dependencies—code, R version, system libraries, and packages—into a single, isolated, and portable unit called a **container**. **Docker** is the most popular technology for creating and running containers.</p>
        <p><strong>Analogy:</strong> A Docker container is like a **shipping container**. Before containers, shipping was chaotic; different-sized crates and barrels were hard to stack and transport. The shipping container standardized everything. You can pack anything you want inside (your R app, its specific package versions, system libraries), and the container can be easily moved and run on any ship or truck (any computer with Docker installed) without having to worry about what's inside.</p>
    </div>

    <div id="p6s5ss1-viz" class="tab-pane">
        <h4 class="subsection-title">From Dockerfile to Running Container</h4>
        <p>The process involves writing a recipe (a Dockerfile), building an image from that recipe, and then running the image as a live container.</p>
        <div class="plot-container">
            <div id="p6s5ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The workflow starts with the <strong>Dockerfile</strong>, a plain text file of instructions. The <code>docker build</code> command reads this file and creates a read-only template called an <strong>Image</strong>. The <code>docker run</code> command then creates a live, running instance of this image, called a <strong>Container</strong>, which is where your application actually executes.</p>
        </div>
    </div>
    
    <div id="p6s5ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">Data Scientist Step-by-Step Example</h4>
        <div class="scenario-content" style="border-color: var(--accent); margin-bottom: 1rem;">
            <p><strong>Problem:</strong> We have a simple Plumber API and we need to write a Dockerfile to containerize it.</p>
        </div>
        <ol class="prose-list">
            <li><strong>Step 1: The Plumber API (<code>plumber.R</code>).</strong> This is our simple R application.
                <div class="code-container">
                    <pre><code class="language-r"># plumber.R
library(plumber)
#* @get /ping
function() {
  return("pong")
}</code></pre>
                </div>
            </li>
            <li><strong>Step 2: The Dockerfile.</strong> This file, named <code>Dockerfile</code>, is the recipe for building our image.
                <div class="code-container">
                    <pre><code class="language-dockerfile">
# Use an official R base image from the Rocker project
FROM rocker/r-ver:4.3.1

# Install the plumber package inside the container
RUN R -e "install.packages('plumber')"

# Copy our local plumber.R file into the container
COPY plumber.R /tmp/plumber.R

# Expose port 8000 to the outside world
EXPOSE 8000

# The command to run when the container starts
ENTRYPOINT ["R", "-e", "pr <- plumber::plumb('/tmp/plumber.R'); pr$run(host='0.0.0.0', port=8000)"]
</code></pre>
                </div>
            </li>
            <li><strong>Step 3: Build and Run from the Terminal.</strong>
                <div class="code-container">
                    <pre><code class="language-bash">
# Build the image from the Dockerfile in the current directory
docker build -t my-r-api .

# Run the image as a container
docker run -p 8000:8000 my-r-api
</code></pre>
                </div>
                 <div class="plot-description">
                    <p><strong>Conclusion:</strong> Your R API is now running inside a container. Anyone with Docker can run these two commands and have your application working perfectly, regardless of their local R version or installed packages.</p>
                </div>
            </li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: The Rocker Project</h3>
    <div class="scenario-content">
        <p>Creating a good base R image for Docker can be complex. The **Rocker Project** provides a set of community-maintained, best-practice Docker images for R. They have images for specific R versions (<code>rocker/r-ver</code>), images with the Tidyverse pre-installed (<code>rocker/tidyverse</code>), and images specifically for deploying Shiny apps (<code>rocker/shiny</code>). You should almost always use a Rocker image as the <code>FROM</code> instruction in your Dockerfiles.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This exercise requires Docker to be installed on your machine.</p>
        <ul class="prose-list">
            <li>Take the <code>plumber.R</code> and <code>Dockerfile</code> from the example.</li>
            <li>Modify the <code>plumber.R</code> file to add a new endpoint that returns the mean of the <code>mpg</code> column from the <code>mtcars</code> dataset.</li>
            <li>Modify the <code>Dockerfile</code>. Add a new <code>RUN</code> command to install the <code>dplyr</code> package (since you might need it).</li>
            <li>Build the new image and run it. Test your new endpoint by navigating to it in a web browser.</li>
        </ul>
    </div>
</div>
`,
          p6s5ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst has built and containerized a Shiny dashboard. Now, they need to make it accessible to their managers worldwide. Running it on their laptop isn't an option. They need to deploy it to a **Cloud Platform**, a service that provides on-demand computing resources, to host their application reliably and scalably.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s5ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s5ss2-methods">Deployment Options</button>
    </div>

    <div id="p6s5ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Renting a Supercomputer</h4>
        <p><strong>Cloud Platforms** (like Amazon Web Services, Google Cloud Platform, and Microsoft Azure) are companies that provide massive data centers and allow you to rent computing resources—servers, storage, databases, etc.—over the internet. For data science, they are the standard for deploying and scaling applications.</p>
        <p><strong>Analogy:</strong> Using a cloud platform is like renting a **professional kitchen** instead of trying to cook a banquet in your tiny apartment kitchen. The professional kitchen (the cloud) provides all the industrial-grade ovens (servers), refrigerators (storage), and workspace you need, and you only pay for it when you're using it. It's more powerful, reliable, and scalable than your home setup.</p>
    </div>
    
    <div id="p6s5ss2-methods" class="tab-pane">
        <h4 class="subsection-title">Where to Deploy Your R Workloads</h4>
        <div class="decision-branches" style="align-items- flex-start;">
            <div class="decision-branch">
                <strong>Platform-as-a-Service (PaaS) for R</strong>
                <p>These are services specifically designed for hosting R applications. They are the easiest and fastest way to get started.</p>
                <p><strong>Examples:</strong> <code>shinyapps.io</code>, Posit Connect.</p>
                <p><strong>Pros:</strong> "Push-button" deployment directly from RStudio. Handles all the server management for you.</p>
                <p><strong>Cons:</strong> Less flexible than general-purpose cloud platforms.</p>
            </div>
            <div class="decision-branch">
                <strong>General-Purpose Cloud (IaaS/PaaS)</strong>
                <p>These are the major cloud providers. You can rent a virtual machine (an "instance") and set up your R environment yourself, or use higher-level services to run your Docker containers.</p>
                <p><strong>Examples:</strong> AWS (EC2, App Runner), Google Cloud (Compute Engine, Cloud Run), Azure (Virtual Machines, App Service).</p>
                <p><strong>Pros:</strong> Infinitely flexible and scalable.</p>
                <p><strong>Cons:</strong> Much steeper learning curve; requires knowledge of system administration and networking.</p>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Start with shinyapps.io</h3>
    <div class="scenario-content">
        <p>For deploying your first Shiny applications, <strong>shinyapps.io</strong> (provided by Posit) is the perfect starting point. It has a free tier and is deeply integrated into RStudio. After connecting your account, you can deploy a running application directly from the RStudio IDE with a single click of a button. It's the simplest way to share your work publicly.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <ul class="prose-list">
            <li>Go to the <a href="https://www.shinyapps.io/" target="_blank">shinyapps.io</a> website and sign up for a free account.</li>
            <li>Follow their getting started guide to install the <code>rsconnect</code> package and configure it with the token from your account.</li>
            <li>Take any simple Shiny app you've built (like the one from the "Shiny Applications" topic).</li>
            <li>Click the blue "Publish" button at the top of your script editor in RStudio and follow the prompts.</li>
            <li>Congratulations! Your R application is now live on the internet for anyone to see.</li>
        </ul>
    </div>
</div>
`,
          p6s5ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A deployed model API becomes hugely popular. A single Docker container running the API is overwhelmed with requests and crashes. The development team needs an automated system that can manage a fleet of containers, automatically scale up the number of containers when traffic is high, scale down when it's low, and handle failures by automatically restarting crashed containers. This is the job of a **Container Orchestrator** like Kubernetes.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s5ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s5ss3-viz">The Architecture</button>
    </div>

    <div id="p6s5ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Managing Fleets of Containers</h4>
        <p><strong>Container Orchestration</strong> is the automated management, deployment, scaling, and networking of containers. **Kubernetes (K8s)** is the open-source, industry-standard platform for this.</p>
        <p><strong>Analogy:</strong> If Docker is a single shipping container, Kubernetes is the entire **automated port and logistics system**. It's the fleet of cranes, trucks, and the central control tower that decides which container goes on which ship, how many ships are needed, and automatically reroutes traffic if a crane breaks down. It manages the entire fleet at scale.</p>
    </div>
    
    <div id="p6s5ss3-viz" class="tab-pane">
        <h4 class="subsection-title">A High-Level View of Kubernetes</h4>
        <p>This diagram shows a simplified view of a Kubernetes cluster, with a Control Plane managing several worker Nodes.</p>
        <div class="plot-container">
            <div id="p6s5ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The user interacts with the **Control Plane** (the "brain"), defining the desired state (e.g., "I want 3 replicas of my R API running at all times"). The Control Plane then automatically manages the **Worker Nodes** (the servers) to make this state a reality. It schedules your containers (in units called **Pods**) to run on the nodes. If a node fails, the Control Plane automatically moves its pods to a healthy node, ensuring your application stays online.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Managed Kubernetes Services</h3>
    <div class="scenario-content">
        <p>Setting up and managing a Kubernetes cluster from scratch is incredibly complex. All major cloud providers offer **Managed Kubernetes Services** that handle this complexity for you. You simply ask for a cluster, and they manage the entire Control Plane for you. Examples include Amazon EKS, Google Kubernetes Engine (GKE), and Azure Kubernetes Service (AKS). Using a managed service is the standard for almost all production workloads.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You have deployed your Shiny app in a single Docker container on a single server. What is the single point of failure in this setup?</li>
            <li>How would using Kubernetes to run three replicas of your container solve this problem?</li>
            <li>Your app experiences a massive traffic spike every evening. How could Kubernetes' auto-scaling feature help you save money while ensuring your app remains responsive?</li>
        </ul>
    </div>
</div>
`,
          p6s5ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst is building a production application on a cloud platform. Storing their data on the virtual machine's local disk is risky and not scalable. They need to use dedicated, managed cloud services for their data: a **Cloud Database** (like Amazon RDS) for their structured user data and **Object Storage** (like Amazon S3) for their large raw data files and model artifacts.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s5ss4-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s5ss4-viz">A Common Architecture</button>
    </div>

    <div id="p6s5ss4-concepts" class="tab-pane active">
        <h4 class="subsection-title">Decoupling Compute and Storage</h4>
        <p>A fundamental principle of cloud architecture is to separate your application's logic (the compute) from its data (the storage). This allows you to scale, manage, and secure each component independently.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Managed Cloud Databases</strong>
                <p>These are services where the cloud provider manages the database for you. They handle backups, patching, and scaling, freeing you to focus on your application.</p>
                <p><strong>Examples:</strong> Amazon RDS, Google Cloud SQL, Azure SQL Database.</p>
            </div>
            <div class="decision-branch">
                <strong>Object Storage</strong>
                <p>A highly scalable and durable system for storing large, unstructured files. It's not a database, but a massive key-value store. It's the perfect place to store raw data files, logs, and serialized model objects.</p>
                <p><strong>Examples:</strong> Amazon S3, Google Cloud Storage, Azure Blob Storage.</p>
            </div>
        </div>
        <p><strong>Analogy:</strong> This is like separating your **kitchen (compute)** from your **pantry (storage)**. Your kitchen can be small or large depending on how many chefs are working, but it always connects to the same, massive, well-organized pantry where all the ingredients are stored safely and durably.</p>
    </div>

    <div id="p6s5ss4-viz" class="tab-pane">
        <h4 class="subsection-title">An Example Cloud Data Architecture</h4>
        <p>This diagram shows how different data types are handled by different, specialized storage services in a typical cloud architecture.</p>
        <div class="plot-container">
            <div id="p6s5ss4-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The application servers (the "compute") are stateless. They load large raw files and model artifacts from **Object Storage (S3)**. They read and write structured, transactional data to the **Managed SQL Database (RDS)**. This separation makes the entire system scalable and robust.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use IAM for Secure Access</h3>
    <div class="scenario-content">
        <p>Never give your application server a permanent password to access your database. Instead, use the cloud provider's **Identity and Access Management (IAM)** service. You can create a "role" with specific, limited permissions (e.g., "read-only access to the sales table") and assign this role to your application server. This is far more secure and allows you to enforce the principle of least privilege.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You have 10 terabytes of raw image data that you will use to train a model. Which cloud service would be the appropriate place to store this data: a managed SQL database or object storage? Why?</li>
            <li>Your deployed Shiny app needs to store user account information (usernames, hashed passwords, etc.). Which cloud service would you use for this? Why would object storage be a very bad choice?</li>
        </ul>
    </div>
</div>
`,
          p6s6ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A team of data scientists is collaborating on a project. Every time a team member pushes new code to GitHub, they have to manually pull the changes, run a series of tests to make sure nothing broke, and then check the code style. This is slow and unreliable. They need to set up **Continuous Integration (CI)** to automate this entire process.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s6ss1-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s6ss1-viz">The CI Workflow</button>
        <button class="tab-button" data-tab="p6s6ss1-usecase">Example with GitHub Actions</button>
    </div>

    <div id="p6s6ss1-concepts" class="tab-pane active">
        <h4 class="subsection-title">Automating Quality Control</h4>
        <p><strong>Continuous Integration (CI)</strong> is the practice of automating the integration of code changes from multiple contributors into a single software project. It is a core part of modern software development and MLOps.</p>
        <p><strong>Analogy:</strong> A CI pipeline is like an **automated editor and fact-checker for a newspaper**. Every time a reporter (a developer) submits a new article (pushes code), the system automatically runs a spell check (a linter), verifies all the sources (runs unit tests), and checks for plagiarism (integration tests). If any check fails, the article is sent back for revisions. This ensures a consistent level of quality for everything that gets published.</p>
    </div>

    <div id="p6s6ss1-viz" class="tab-pane">
        <h4 class="subsection-title">The CI Feedback Loop</h4>
        <p>This diagram shows the typical workflow triggered by a developer pushing code to a repository with a CI pipeline configured.</p>
        <div class="plot-container">
            <div id="p6s6ss1-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The developer pushes code to GitHub. This automatically triggers a webhook that notifies the **CI Server** (like GitHub Actions). The CI server spins up a clean environment, checks out the code, and runs a series of pre-defined **Jobs** (like installing dependencies, linting, and testing). It then reports the status (pass or fail) back to GitHub, giving the team immediate feedback.</p>
        </div>
    </div>
    
    <div id="p6s6ss1-usecase" class="tab-pane">
        <h4 class="subsection-title">A Simple GitHub Actions Workflow for R</h4>
        <p>You can set up a CI pipeline by creating a YAML file in your project at <code>.github/workflows/my-workflow.yml</code>.</p>
        <div class="code-container">
            <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
            <pre><code class="language-yaml">
name: R-CMD-check

# Trigger the workflow on pushes to the main branch
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.3.1'

      - name: Install dependencies
        run: |
          install.packages(c("remotes", "rcmdcheck"))
          remotes::install_deps(dependencies = TRUE)
        shell: Rscript {0}

      - name: Check package
        run: rcmdcheck::rcmdcheck(args = "--no-manual", error_on = "warning")
        shell: Rscript {0}
</code></pre>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use Pre-Built Actions</h3>
    <div class="scenario-content">
        <p>You don't need to write CI scripts from scratch. The GitHub Actions Marketplace has thousands of pre-built actions for common tasks. The <code>r-lib/actions</code> repository contains a set of official, best-practice actions specifically for setting up R and running checks on R packages, making it incredibly easy to get started.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You are working on a team. You push some new code that passes all the tests on your machine, but the CI build fails. Why is the CI build more trustworthy than the tests you ran on your own computer?</li>
            <li>What are two specific checks or tests (besides just running the code) that you would include in a CI pipeline for a data science project to ensure code quality?</li>
        </ul>
    </div>
</div>
`,
          p6s6ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A team has a robust CI pipeline that automatically tests their Shiny app. However, after the tests pass, the deployment process is still a manual, multi-step, and error-prone checklist. They need to automate this final step by setting up **Continuous Deployment (CD)**, so that any code that passes all the tests is automatically deployed to their production server.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s6ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s6ss2-viz">The Full CI/CD Pipeline</button>
    </div>

    <div id="p6s6ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Automating the Release Process</h4>
        <p><strong>Continuous Deployment (CD)** is the practice of automatically deploying code changes to a production environment after they have successfully passed the automated tests in the Continuous Integration (CI) stage.</p>
        <p><strong>Analogy:</strong> If CI is the automated editor, CD is the **automated printing press**. As soon as an article passes all the editorial checks, the CD system automatically prints thousands of copies and distributes them to the newsstands. It automates the entire release process from code commit to live production.</p>
    </div>

    <div id="p6s6ss2-viz" class="tab-pane">
        <h4 class="subsection-title">From Code to Production</h4>
        <p>This diagram shows the full, end-to-end CI/CD pipeline, extending the CI workflow with the final deployment steps.</p>
        <div class="plot-container">
            <div id="p6s6ss2-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The pipeline flows from left to right. The **CI** part (Build, Test) ensures code quality. If it succeeds, it triggers the **CD** part. The CD pipeline builds the production artifact (like a Docker image), pushes it to a registry, and then deploys this new version to the production environment (e.g., a Kubernetes cluster), making the changes live for users.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Continuous Delivery vs. Continuous Deployment</h3>
    <div class="scenario-content">
        <p>These two terms are often confused. **Continuous Delivery** means the pipeline automates everything *up to* the final deployment, but the final push to production requires a manual button-press by a human. **Continuous Deployment** takes the final step and automates the deployment as well. For high-stakes applications, Continuous Delivery is often preferred as it keeps a human in the loop for the final release decision.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>Your team wants to implement CI/CD for a Shiny app that is hosted on <code>shinyapps.io</code>.</li>
            <li>What is the trigger for your workflow?</li>
            <li>What is one test you would run in the CI stage?</li>
            <li>If the tests pass, what is the key "secret" or credential your CD job would need to be able to deploy to your <code>shinyapps.io</code> account? (Hint: Think about API tokens).</li>
        </ul>
    </div>
</div>
`,
          p6s6ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data science team has a CI/CD pipeline that automatically retrains and deploys their sales forecasting model every week. How can they be sure the newly deployed model is actually any good? They need to extend their pipeline to include **Automated Reporting**—a step that generates a diagnostic report comparing the new model to the old one—and **Automated Monitoring** to alert them if the live model's performance ever degrades.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s6ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s6ss3-viz">The Monitoring Loop</button>
    </div>

    <div id="p6s6ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Closing the Loop</h4>
        <p>A mature MLOps pipeline doesn't just deploy models; it continuously monitors them and uses that information to trigger new updates, creating a closed-loop system.</p>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Automated Reporting</strong>
                <p>This involves adding a step to your CI/CD pipeline that automatically generates a report (e.g., an R Markdown or Quarto document) after a new model is trained. This report can compare the new model's performance on a test set to the currently deployed model, check for biases, and provide other key diagnostics.</p>
            </div>
            <div class="decision-branch">
                <strong>Automated Monitoring & Alerting</strong>
                <p>This is the process of having a separate service that continuously watches your production model. It can track data drift, concept drift, or operational metrics (like latency). If a metric crosses a predefined threshold, the system automatically sends an alert (e.g., via Slack or email) to the data science team.</p>
            </div>
        </div>
    </div>

    <div id="p6s6ss3-viz" class="tab-pane">
        <h4 class="subsection-title">The Full MLOps Cycle</h4>
        <p>This diagram shows how CI/CD and monitoring work together to create a full, automated lifecycle for a machine learning model.</p>
        <div class="plot-container">
            <div id="p6s6ss3-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The **CI/CD Pipeline** handles the automated training and deployment of new models. Once deployed, the **Monitoring System** watches the live model. If it detects a problem (like data drift), it triggers an alert. This alert signals the need to retrain the model, thus starting the CI/CD pipeline all over again with new data.</p>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use a Scheduler</h3>
    <div class="scenario-content">
        <p>You can use your CI/CD service (like GitHub Actions) as a simple scheduler. Most CI systems allow you to trigger a workflow not just on a <code>git push</code>, but also on a time-based schedule (using a "cron" job). You can set up a workflow to automatically run your R Markdown report every morning at 9 AM and email the result to stakeholders.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You have a model in production that predicts customer churn. What is one key metric you would want your automated monitoring system to track?</li>
            <li>Your monitoring system detects significant data drift in the <code>income</code> feature being sent to the model. What should happen next?</li>
        </ul>
    </div>
</div>
`,
          p6s6ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An analyst sets up a CI/CD pipeline. However, the tests are "flaky" (they sometimes pass and sometimes fail for no reason), there's no way to roll back a bad deployment, and the pipeline is a single, monolithic script that is hard to debug. They are using the tools of CI/CD but not the **best practices**, resulting in an unreliable system.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">A Checklist for Robust Automation</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-check-double"></i> CI/CD Best Practices</div>
            <div class="scenario">Moving from a simple, fragile pipeline to a robust, production-grade automation system.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-box"></i> 1. Make Jobs Modular</div>
                        <div class="scenario"><strong>The Rule:</strong> Don't put everything into one giant script. Break your pipeline into logical, separate jobs (e.g., a "test" job, a "build" job, a "deploy" job). If one job fails, you can re-run just that one without starting over.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-history"></i> 2. Implement a Rollback Strategy</div>
                        <div class="scenario"><strong>The Rule:</strong> Your deployment process must have a plan for what to do when things go wrong. A good CD system can automatically and instantly roll back to the previous, stable version of the application if it detects that the new deployment is failing.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-bell"></i> 3. Use Notifications</div>
                        <div class="scenario"><strong>The Rule:</strong> The pipeline should notify the team of its status. Set up integrations to automatically post a message to a team chat (like Slack) when a build succeeds or, more importantly, when it fails.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-user-secret"></i> 4. Manage Secrets Securely</div>
                        <div class="scenario"><strong>The Rule:</strong> Never store passwords, API keys, or other secrets directly in your CI/CD configuration file. All CI/CD platforms (like GitHub Actions) provide a secure way to store these as encrypted "secrets" that can be safely accessed by your pipeline at runtime.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Run CI on Pull Requests</h3>
    <div class="scenario-content">
        <p>A powerful best practice is to configure your CI pipeline to run automatically on every **pull request**. You can even set up your GitHub repository to *block* a pull request from being merged until all the CI checks have passed. This ensures that no code that breaks the tests can ever enter your main branch, keeping it stable and production-ready at all times.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>You push a new commit, and five minutes later you get a notification in your team's Slack channel that the "CI build failed." Why is this a good thing? What does it prevent from happening?</li>
            <li>Your team deploys a new version of a model, and your monitoring system immediately detects a spike in prediction errors. What is the first thing your automated CD system should do?</li>
        </ul>
    </div>
</div>
`,
          p6s7ss1: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> Two analysts are working on the same project. If everyone pushes their changes directly to the main project branch, it could become unstable and break the analysis for everyone else. They need a structured, safe workflow that allows developers to work on new features in isolation and have their code reviewed by a teammate before it's merged into the official project.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p1s7ss8-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p1s7ss8-viz">Branching Visualization</button>
        <button class="tab-button" data-tab="p1s7ss8-usecase">The Pull Request Workflow</button>
    </div>

    <div id="p1s7ss8-concepts" class="tab-pane active">
        <h4 class="subsection-title">Isolating and Reviewing Work</h4>
        <ul class="prose-list">
            <li><strong>Branching:</strong> A branch is an independent line of development. You can create a new branch from the main branch to work on a new feature or bug fix without affecting the stable, main version of the project.</li>
            <li><strong>Pull Request (PR):</strong> A pull request is a formal request to merge your changes from your feature branch into another branch (usually the main branch). It's a central place on GitHub to discuss the proposed changes.</li>
            <li><strong>Code Review:</strong> A process where a teammate reviews the code in your pull request. They can ask questions, suggest improvements, and must approve the changes before they can be merged. This is a critical practice for maintaining code quality.</li>
        </ul>
        <p><strong>Analogy:</strong> The <code>main</code> branch is the <strong>master copy</strong> of a book in a library. When you want to edit a chapter, you don't write directly in the master copy. You create a clean photocopy (you create a <strong>branch</strong>). You make all your edits on your photocopy. When you're done, you submit your edited chapter to the lead editor for review (you open a <strong>Pull Request</strong>). Only after the editor approves it are your changes carefully merged into the master copy.</p>
    </div>

    <div id="p1s7ss8-viz" class="tab-pane">
        <h4 class="subsection-title">Visualizing the Workflow</h4>
        <p>This diagram shows the lifecycle of a feature branch. It is created from <code>main</code>, work is done on it in isolation, and then it is merged back into <code>main</code> after a pull request and review.</p>
        <div class="plot-container">
            <div id="p7s7ss8-plot1" class="plotly-chart"></div>
        </div>
        <div class="plot-description">
            <p><strong>How to observe this plot:</strong> The blue line represents the stable <code>main</code> branch. A new <code>feature</code> branch (orange) is created to work on a new feature. Several commits are made to this branch independently. Once the feature is complete, a Pull Request is opened, and upon approval, the feature branch is merged back into <code>main</code>, incorporating the new work into the official project history.</p>
        </div>
    </div>
    
    <div id="p1s7ss8-usecase" class="tab-pane">
        <h4 class="subsection-title">A Data Scientist's PR Workflow</h4>
        <ol class="prose-list">
            <li><strong>Create a Branch:</strong> In RStudio's Git pane, click the purple branch icon to create and switch to a new branch (e.g., <code>feature/add-new-plot</code>).</li>
            <li><strong>Do Your Work:</strong> Write your code, make your plot, and commit your changes to this new branch as usual. Push the branch to GitHub.</li>
            <li><strong>Open a Pull Request:</strong> On GitHub.com, you'll see a prompt to open a pull request for your new branch. Click it.</li>
            <li><strong>Describe Your Changes:</strong> Give the PR a clear title and write a description of what you did, why you did it, and how to test it. Tag a teammate to review it.</li>
            <li><strong>Code Review:</strong> Your teammate will review your code on GitHub, leave comments, and suggest changes. You can make more commits on your branch to address their feedback.</li>
            <li><strong>Merge:</strong> Once your teammate approves the PR, you (or a project lead) can click the "Merge Pull Request" button on GitHub. Your changes are now safely part of the <code>main</code> branch!</li>
            <li><strong>Clean Up:</strong> Pull the updated <code>main</code> branch to your local machine and delete the old feature branch.</li>
        </ol>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Link Commits to Issues</h3>
    <div class="scenario-content">
        <p>Most professional projects use an issue tracker (like GitHub Issues) to manage tasks. You can automatically link your commits and pull requests to a specific issue. If you're working on issue #42, simply include "fixes #42" or "closes #42" in your commit message or PR description. When the PR is merged, GitHub will automatically close the corresponding issue, keeping your project management tidy.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This exercise builds on the previous one.</p>
        <ul class="prose-list">
            <li>In the RStudio project you created and linked to GitHub, create a new branch called <code>update-greeting</code>.</li>
            <li>On this new branch, change the text in your R script from <code>"Hello, Git!"</code> to <code>"Hello, Collaboration!"</code>.</li>
            <li>Stage, commit, and push this change *on the new branch*.</li>
            <li>Go to GitHub and open a pull request to merge <code>update-greeting</code> into your <code>main</code> branch.</li>
            <li>Since you're the only one on the project, you can review and merge your own pull request. Click the "Merge" button.</li>
            <li>Your <code>main</code> branch on GitHub is now updated!</li>
        </ul>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">R Code for Visualization</h3>
    <div class="code-container">
        <button class="copy-btn" title="Copy to clipboard"><i class="fas fa-copy"></i></button>
        <pre><code class="language-r">
# This is a conceptual diagram that cannot be easily generated with ggplot.
# The JS implementation will create it directly.
# This R code block is a placeholder to satisfy the guidelines.
library(ggplot2)
library(plotly)
p <- ggplot() + theme_void() + labs(title="Flow Diagram Placeholder")
ggplotly(p)
</code></pre>
    </div>
</div>
`,
          p6s7ss2: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data science team has many ongoing projects and tasks, but no central system for tracking them. Work gets lost, priorities are unclear, and deadlines are missed. They need a lightweight **Project Management Methodology** to visualize their work, limit work-in-progress, and improve their flow.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s7ss2-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s7ss2-viz">Kanban vs. Agile</button>
    </div>

    <div id="p6s7ss2-concepts" class="tab-pane active">
        <h4 class="subsection-title">Bringing Order to Chaos</h4>
        <p>Project management methodologies provide a structured framework for planning, executing, and managing team projects. While many exist, the most popular in modern software and data science teams are based on **Agile** principles.</p>
        <p><strong>Analogy:</strong> A project management system is like the **traffic control system for a city**. It provides a set of rules (lanes, stoplights, speed limits) that prevent chaos and ensure everyone can get to their destination efficiently and safely.</p>
    </div>

    <div id="p6s7ss2-viz" class="tab-pane">
        <h4 class="subsection-title">Two Agile Flavors</h4>
        <div class="decision-branches" style="align-items: flex-start;">
            <div class="decision-branch">
                <strong>Kanban</strong>
                <p>A visual system for managing work as it moves through a process. The core is the **Kanban board**, which has columns representing stages of the workflow (e.g., "To Do", "In Progress", "In Review", "Done").</p>
                <p><strong>Focus:</strong> Visualizing work, limiting work-in-progress (WIP) to prevent bottlenecks, and improving the flow of tasks. It is continuous and not time-boxed.</p>
            </div>
            <div class="decision-branch">
                <strong>Scrum (a type of Agile)</strong>
                <p>A more structured framework where work is done in short, time-boxed iterations called **sprints** (usually 1-4 weeks). At the start of a sprint, the team commits to a specific set of tasks from a backlog. At the end, they deliver a potentially shippable increment of work.</p>
                <p><strong>Focus:</strong> Predictability, delivering value in regular increments, and continuous feedback through regular meetings (stand-ups, retrospectives).</p>
            </div>
        </div>
        <div class="plot-container">
            <div id="p6s7ss2-plot1" class="plotly-chart"></div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Use GitHub Projects for Kanban</h3>
    <div class="scenario-content">
        <p>GitHub has a powerful, built-in project management tool called **GitHub Projects**. It allows you to create a Kanban-style board directly within your repository. You can convert your GitHub Issues into cards on the board and drag and drop them between columns like "To Do", "In Progress", and "Done". It's a fantastic, lightweight way to manage your data science projects right where your code lives.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>Go to any public GitHub repository (e.g., <code>tidyverse/dplyr</code>). Click on the "Projects" tab at the top.</li>
            <li>Explore their project boards. How do they name their columns? What kind of information do they put on their task cards?</li>
            <li>Your team's main problem is that developers are starting too many tasks at once and nothing is getting finished. Which methodology, Kanban or Scrum, has a core principle that directly addresses this problem?</li>
        </ul>
    </div>
</div>
`,
          p6s7ss3: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> An academic researcher completes a groundbreaking analysis. To ensure the scientific community can trust, verify, and build upon their work, they don't just publish a PDF of their paper. They also publish their full, reproducible code, the raw data, and detailed documentation in an **open-source** format, embracing the principles of **Open Science**.</p>
    </div>
</div>
<div class="subsection">
    <div class="tabs">
        <button class="tab-button active" data-tab="p6s7ss3-concepts">The Core Idea</button>
        <button class="tab-button" data-tab="p6s7ss3-viz">The Benefits</button>
    </div>

    <div id="p6s7ss3-concepts" class="tab-pane active">
        <h4 class="subsection-title">Science as a Community Endeavor</h4>
        <p><strong>Open Science** is a movement to make scientific research, data, and dissemination accessible to all levels of society. For a data scientist, this means embracing transparency and reproducibility in your work.</p>
        <p><strong>Analogy:</strong> Open science is like a **community cookbook**. Instead of keeping recipes secret, chefs share them openly. This allows other chefs to test the recipes (reproducibility), improve upon them (collaboration), and for everyone to learn and collectively advance the art of cooking.</p>
    </div>

    <div id="p6s7ss3-viz" class="tab-pane">
        <h4 class="subsection-title">Why Share Your Work?</h4>
        <div class="mindmap-container">
            <div class="mindmap-root">
                <div class="title"><i class="fas fa-unlock-alt"></i> Benefits of Open Science</div>
                <div class="scenario">Sharing your work openly benefits both the community and you personally.</div>
            </div>
            <div class="mindmap-branches-container">
                <div class="mindmap-branches">
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-check-double"></i> For the Community</div>
                            <div class="scenario">It increases the credibility and transparency of scientific findings, accelerates discovery by allowing others to build on your work, and promotes a more inclusive and equitable scientific culture.</div>
                        </div>
                    </div>
                    <div class="mindmap-branch">
                        <div class="content">
                            <div class="title"><i class="fas fa-user-graduate"></i> For You</div>
                            <div class="scenario">It forces you to write cleaner, more reproducible code. It allows you to get feedback from a global community, improving your work. Most importantly, it's the single best way to build a professional portfolio and demonstrate your skills to potential employers.</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Choose a License</h3>
    <div class="scenario-content">
        <p>If you share your code on GitHub, it is not technically "open source" unless you include a license file. A license (like MIT, GPL, or Apache 2.0) is a legal document that tells others how they are allowed to use, modify, and distribute your work. The MIT license is a very common and permissive choice for data science projects. GitHub makes it easy to add a license when you create a new repository.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual exercise.</p>
        <ul class="prose-list">
            <li>Find a data science project on your GitHub profile that you are proud of.</li>
            <li>Write a high-quality <code>README.md</code> file for it. Your README should include: a high-level summary of the project's goal, instructions on how to run your analysis, and a summary of your key findings.</li>
            <li>Add a license to your repository.</li>
            <li>You have now taken the first steps toward creating a professional, open-source portfolio project.</li>
        </ul>
    </div>
</div>
`,
          p6s7ss4: `
<div class="subsection">
    <div class="scenario-content" style="border-color: var(--primary);">
        <p><strong>Scenario:</strong> A data scientist joins a new team. They are expected to adhere to the team's established **professional standards**: their code must follow a specific style guide, all new work must be submitted via a pull request and receive a code review, and their analyses must be documented and reproducible. These standards ensure the team can work together effectively and produce high-quality, trustworthy work.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">The Hallmarks of a Professional Data Scientist</h3>
    <div class="mindmap-container">
        <div class="mindmap-root">
            <div class="title"><i class="fas fa-user-tie"></i> Professional Standards</div>
            <div class="scenario">Moving beyond just writing code that works to being an effective, collaborative, and responsible member of a data science team.</div>
        </div>
        <div class="mindmap-branches-container">
            <div class="mindmap-branches">
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-pencil-ruler"></i> Technical Craftsmanship</div>
                        <div class="scenario">Writing code that is not just correct, but also clean, readable, well-documented, and efficient. Following team style guides and best practices for reproducibility.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-users"></i> Collaboration</div>
                        <div class="scenario">Working effectively with a team using tools like Git and GitHub. Actively participating in code reviews (both giving and receiving feedback constructively) and using project management tools to track work.</div>
                    </div>
                </div>
                <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-comments"></i> Communication</div>
                        <div class="scenario">Being able to clearly explain complex technical concepts to both technical and non-technical audiences. Tailoring your message and being able to build a compelling narrative around your findings.</div>
                    </div>
                </div>
                 <div class="mindmap-branch">
                    <div class="content">
                        <div class="title"><i class="fas fa-balance-scale"></i> Ethical Responsibility</div>
                        <div class="scenario">Understanding and taking responsibility for the societal impact of your work. Proactively auditing for bias, protecting user privacy, and being transparent about your model's limitations.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">⭐ Pro Tip: Be a Lifelong Learner</h3>
    <div class="scenario-content">
        <p>The field of data science is constantly evolving. New packages, algorithms, and best practices emerge every year. A key professional standard is a commitment to continuous learning. Follow blogs, attend webinars, read papers, and always be curious about new tools and techniques to keep your skills sharp and relevant.</p>
    </div>
</div>
<div class="subsection">
    <h3 class="subsection-title">✍️ Practice Exercise</h3>
    <div class="scenario-content">
        <p>This is a conceptual self-reflection exercise.</p>
        <ul class="prose-list">
            <li>Look at a data analysis project you have completed recently.</li>
            <li>Review it against the four pillars of professionalism described above (Technical Craftsmanship, Collaboration, Communication, Ethical Responsibility).</li>
            <li>What is one specific thing you could do to improve the project in each of these four areas?</li>
        </ul>
    </div>
</div>
`,
        };

        const plotFunctions = {
          "p1s5ss1t1-plot1": () => {
            // Simulate right-skewed data generation from R script
            const salesData = [];
            for (let i = 0; i < 500; i++) {
              // A simple way to generate skewed data in JS
              salesData.push(Math.round(Math.exp(3 + Math.random() * 2.5)));
            }

            const mean =
              salesData.reduce((a, b) => a + b, 0) / salesData.length;
            const sortedSales = [...salesData].sort((a, b) => a - b);
            const median = sortedSales[Math.floor(sortedSales.length / 2)];

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
              meanColor: "#f59e0b",
              medianColor: "#10b981",
            };

            Plotly.newPlot(
              "p1s5ss1t1-plot1",
              [
                {
                  x: salesData,
                  type: "histogram",
                  name: "Distribution",
                  marker: {
                    color: "#3b82f6",
                    line: { color: themeColors.bgColor, width: 1 },
                  },
                },
              ],
              {
                title: "Distribution of Daily Sales",
                showlegend: true,
                legend: { x: 0.7, y: 0.95 },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  gridcolor: themeColors.gridColor,
                  title: "Daily Sales ($)",
                },
                yaxis: { gridcolor: themeColors.gridColor, title: "Frequency" },
                shapes: [
                  {
                    type: "line",
                    x0: mean,
                    y0: 0,
                    x1: mean,
                    y1: 1,
                    yref: "paper",
                    line: {
                      color: themeColors.meanColor,
                      width: 2.5,
                      dash: "dash",
                    },
                    name: "Mean",
                  },
                  {
                    type: "line",
                    x0: median,
                    y0: 0,
                    x1: median,
                    y1: 1,
                    yref: "paper",
                    line: {
                      color: themeColors.medianColor,
                      width: 2.5,
                      dash: "dot",
                    },
                    name: "Median",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s5ss1t3-plot1": () => {
            // Simulate Binomial PMF data (n=20, p=0.5) from R script
            const x = Array.from({ length: 21 }, (_, i) => i);
            const factorial = (n) => (n <= 1 ? 1 : n * factorial(n - 1));
            const combinations = (n, k) =>
              factorial(n) / (factorial(k) * factorial(n - k));
            const y = x.map(
              (k) =>
                combinations(20, k) * Math.pow(0.5, k) * Math.pow(0.5, 20 - k)
            );

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p1s5ss1t3-plot1",
              [
                {
                  x: x,
                  y: y,
                  type: "bar",
                  name: "PMF",
                  marker: { color: "#8b5cf6" },
                },
              ],
              {
                title: "Binomial Distribution (n=20, p=0.5)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  gridcolor: themeColors.gridColor,
                  title: "Number of Successes (Heads in 20 Flips)",
                },
                yaxis: {
                  gridcolor: themeColors.gridColor,
                  title: "Probability",
                },
              },
              { responsive: true }
            );
          },
          "p1s5ss1t3-plot2": () => {
            // Poisson
            const x = Array.from({ length: 26 }, (_, i) => i);
            const lambda = 10;
            const factorial = (n) => (n <= 1 ? 1 : n * factorial(n - 1));
            const y = x.map(
              (k) => (Math.pow(lambda, k) * Math.exp(-lambda)) / factorial(k)
            );
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p1s5ss1t3-plot2",
              [{ x, y, type: "bar", marker: { color: "#ef4444" } }],
              {
                title: "Poisson PMF (λ=10)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  gridcolor: themeColors.gridColor,
                  title: "Number of Events",
                },
                yaxis: {
                  gridcolor: themeColors.gridColor,
                  title: "Probability",
                },
              },
              { responsive: true }
            );
          },
          "p1s5ss1t3-plot3": () => {
            // Normal
            const x = [];
            const y = [];
            for (let i = -4; i <= 4; i += 0.1) {
              x.push(i);
              y.push((1 / Math.sqrt(2 * Math.PI)) * Math.exp(-0.5 * i * i));
            }
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p1s5ss1t3-plot3",
              [
                {
                  x,
                  y,
                  type: "scatter",
                  mode: "lines",
                  fill: "tozeroy",
                  line: { color: "#3b82f6" },
                },
              ],
              {
                title: "Standard Normal PDF (μ=0, σ=1)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { gridcolor: themeColors.gridColor, title: "Value" },
                yaxis: { gridcolor: themeColors.gridColor, title: "Density" },
              },
              { responsive: true }
            );
          },
          "p1s5ss1t3-plot4": () => {
            // Exponential
            const x = [];
            const y = [];
            const rate = 1;
            for (let i = 0; i <= 5; i += 0.1) {
              x.push(i);
              y.push(rate * Math.exp(-rate * i));
            }
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p1s5ss1t3-plot4",
              [
                {
                  x,
                  y,
                  type: "scatter",
                  mode: "lines",
                  fill: "tozeroy",
                  line: { color: "#f59e0b" },
                },
              ],
              {
                title: "Exponential PDF (λ=1)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { gridcolor: themeColors.gridColor, title: "Time" },
                yaxis: { gridcolor: themeColors.gridColor, title: "Density" },
              },
              { responsive: true }
            );
          },
          "p1s5ss1t4-plot1": () => {
            // Simulate bootstrap estimates from R script
            const bootstrap_means = [];
            for (let i = 0; i < 5000; i++) {
              // A simple way to generate normally distributed random numbers
              const rand =
                Math.random() +
                Math.random() +
                Math.random() +
                Math.random() +
                Math.random() +
                Math.random();
              bootstrap_means.push(0.52 + (rand - 3) / 30); // Center around 0.52
            }
            bootstrap_means.sort((a, b) => a - b);
            const ci_lower = bootstrap_means[Math.floor(0.025 * 5000)];
            const ci_upper = bootstrap_means[Math.floor(0.975 * 5000)];

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
              ciColor: "#ef4444",
            };

            Plotly.newPlot(
              "p1s5ss1t4-plot1",
              [
                {
                  x: bootstrap_means,
                  type: "histogram",
                  marker: { color: "#3b82f6" },
                  nbinsx: 50,
                },
              ],
              {
                title: "Distribution of 5,000 Bootstrap Approval Ratings",
                xaxis: { title: "Bootstrap Sample Mean (Approval Rate)" },
                yaxis: { title: "Frequency" },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                shapes: [
                  {
                    type: "line",
                    x0: ci_lower,
                    y0: 0,
                    x1: ci_lower,
                    y1: 1,
                    yref: "paper",
                    line: { color: themeColors.ciColor, dash: "dash" },
                  },
                  {
                    type: "line",
                    x0: ci_upper,
                    y0: 0,
                    x1: ci_upper,
                    y1: 1,
                    yref: "paper",
                    line: { color: themeColors.ciColor, dash: "dash" },
                  },
                ],
                annotations: [
                  {
                    x: ci_lower,
                    y: 0.8,
                    yref: "paper",
                    text: "2.5th %ile",
                    showarrow: true,
                    ax: -40,
                    ay: -30,
                    font: { color: themeColors.ciColor },
                  },
                  {
                    x: ci_upper,
                    y: 0.8,
                    yref: "paper",
                    text: "97.5th %ile",
                    showarrow: true,
                    ax: 40,
                    ay: -30,
                    font: { color: themeColors.ciColor },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s5ss1t5-plot1": () => {
            // Statistical Significance
            const x = [];
            const y = [];
            for (let i = -4; i <= 4; i += 0.05) {
              x.push(i);
              y.push((1 / Math.sqrt(2 * Math.PI)) * Math.exp(-0.5 * i * i));
            }
            const critical_value = 1.645; // qnorm(0.95)
            const observed_statistic = 2.1;

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p1s5ss1t5-plot1",
              [
                {
                  x,
                  y,
                  type: "scatter",
                  mode: "lines",
                  line: { color: "#3b82f6" },
                  name: "Null Distribution",
                },
                {
                  x: x.filter((val) => val >= observed_statistic),
                  y: y.slice(x.findIndex((val) => val >= observed_statistic)),
                  type: "scatter",
                  mode: "lines",
                  fill: "tozeroy",
                  fillcolor: "rgba(239, 68, 68, 0.7)",
                  line: { color: "transparent" },
                  name: "p-value",
                },
              ],
              {
                title: "P-value vs. Significance Level (α)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  gridcolor: themeColors.gridColor,
                  title: "Test Statistic",
                },
                yaxis: { gridcolor: themeColors.gridColor, title: "Density" },
                showlegend: false,
                shapes: [
                  {
                    type: "line",
                    x0: critical_value,
                    y0: 0,
                    x1: critical_value,
                    y1: 0.25,
                    line: { color: "#f59e0b", width: 2, dash: "dash" },
                  },
                ],
                annotations: [
                  {
                    x: critical_value + 0.1,
                    y: 0.26,
                    text: "α = 0.05",
                    showarrow: false,
                    font: { color: "#f59e0b" },
                    xanchor: "left",
                  },
                  {
                    x: observed_statistic + 0.5,
                    y: 0.05,
                    text: "p-value",
                    showarrow: true,
                    arrowhead: 2,
                    ax: 40,
                    ay: -30,
                    font: { color: "#ef4444" },
                  },
                  {
                    x: observed_statistic,
                    y: -0.05,
                    text: "Observed<br>Result",
                    showarrow: true,
                    arrowhead: 7,
                    ax: 0,
                    ay: -40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s5ss1t6-plot1": () => {
            // Type I and II Errors
            const x = [];
            const y_h0 = [];
            const y_ha = [];
            const mean_h0 = 0;
            const mean_ha = 3;
            const sd = 1.5;
            const decision_boundary = 2.467; // qnorm(0.95, 0, 1.5)

            for (let i = -4; i <= 8; i += 0.1) {
              x.push(i);
              y_h0.push(
                (1 / (sd * Math.sqrt(2 * Math.PI))) *
                  Math.exp(-0.5 * Math.pow((i - mean_h0) / sd, 2))
              );
              y_ha.push(
                (1 / (sd * Math.sqrt(2 * Math.PI))) *
                  Math.exp(-0.5 * Math.pow((i - mean_ha) / sd, 2))
              );
            }

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p1s5ss1t6-plot1",
              [
                {
                  x,
                  y: y_h0,
                  type: "scatter",
                  mode: "lines",
                  name: "H0 is True (No Effect)",
                  line: { color: "#3b82f6" },
                },
                {
                  x,
                  y: y_ha,
                  type: "scatter",
                  mode: "lines",
                  name: "HA is True (Real Effect)",
                  line: { color: "#eab308" },
                },
                {
                  x: x.filter((val) => val >= decision_boundary),
                  y: y_h0.slice(x.findIndex((val) => val >= decision_boundary)),
                  type: "scatter",
                  mode: "lines",
                  fill: "tozeroy",
                  fillcolor: "rgba(239, 68, 68, 0.5)",
                  line: { color: "transparent" },
                  name: "Type I Error (α)",
                },
                {
                  x: x.filter((val) => val < decision_boundary),
                  y: y_ha.slice(
                    0,
                    x.findIndex((val) => val >= decision_boundary) + 1
                  ),
                  type: "scatter",
                  mode: "lines",
                  fill: "tozeroy",
                  fillcolor: "rgba(245, 158, 11, 0.5)",
                  line: { color: "transparent" },
                  name: "Type II Error (β)",
                },
              ],
              {
                title: "Type I and Type II Errors",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  gridcolor: themeColors.gridColor,
                  title: "Observed Result",
                },
                yaxis: { gridcolor: themeColors.gridColor, title: "Density" },
                legend: {
                  x: 0.5,
                  y: -0.2,
                  xanchor: "center",
                  orientation: "h",
                },
                shapes: [
                  {
                    type: "line",
                    x0: decision_boundary,
                    y0: 0,
                    x1: decision_boundary,
                    y1: 0.28,
                    line: {
                      color: isDark ? "white" : "black",
                      width: 2,
                      dash: "dash",
                    },
                  },
                ],
                annotations: [
                  {
                    x: decision_boundary,
                    y: 0.29,
                    text: "Decision Boundary",
                    showarrow: false,
                    yanchor: "bottom",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s5ss1t7-plot1": () => {
            // Correlation Scatter Plot
            const temperature = [];
            const sales = [];
            for (let i = 0; i < 30; i++) {
              const temp = 20 + (15 / 29) * i;
              temperature.push(temp);
              sales.push(50 + temp * 15 + (Math.random() - 0.5) * 80);
            }

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            // Basic linear regression
            const n = temperature.length;
            const sum_x = temperature.reduce((a, b) => a + b, 0);
            const sum_y = sales.reduce((a, b) => a + b, 0);
            const sum_xy = temperature
              .map((x, i) => x * sales[i])
              .reduce((a, b) => a + b, 0);
            const sum_xx = temperature
              .map((x) => x * x)
              .reduce((a, b) => a + b, 0);
            const slope =
              (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x * sum_x);
            const intercept = (sum_y - slope * sum_x) / n;
            const regression_line_y = temperature.map(
              (x) => slope * x + intercept
            );

            Plotly.newPlot(
              "p1s5ss1t7-plot1",
              [
                {
                  x: temperature,
                  y: sales,
                  type: "scatter",
                  mode: "markers",
                  marker: { color: "#ef4444", size: 8, opacity: 0.8 },
                  name: "Daily Data",
                },
                {
                  x: temperature,
                  y: regression_line_y,
                  type: "scatter",
                  mode: "lines",
                  line: { color: "#3b82f6", width: 3 },
                  name: "Trend Line",
                },
              ],
              {
                title: "Ice Cream Sales vs. Temperature",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  gridcolor: themeColors.gridColor,
                  title: "Temperature (°C)",
                },
                yaxis: {
                  gridcolor: themeColors.gridColor,
                  title: "Daily Sales ($)",
                },
              },
              { responsive: true }
            );
          },
          "p1s5ss1t8-plot1": () => {
            // Descriptive vs Inferential
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const fig = {
              data: [
                {
                  type: "bar",
                  x: ["Jan", "Feb", "Mar"],
                  y: [250, 250, 285],
                  name: "Descriptive",
                  marker: { color: "#10b981" },
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  type: "scatter",
                  mode: "markers",
                  x: [1, 2, 3],
                  y: [250, 250, 285],
                  name: "Past Data",
                  marker: { color: "#10b981", size: 8 },
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  type: "scatter",
                  mode: "lines",
                  x: [0.5, 4.5],
                  y: [232.5, 302.5],
                  name: "Model",
                  line: { color: "#3b82f6" },
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  x: [0.5, 4.5, 4.5, 0.5],
                  y: [202.5, 272.5, 332.5, 262.5],
                  fill: "toself",
                  fillcolor: "rgba(59, 130, 246, 0.2)",
                  line: { color: "transparent" },
                  name: "Confidence Interval",
                  showlegend: false,
                  xaxis: "x2",
                  yaxis: "y2",
                },
              ],
              layout: {
                title: "Descriptive (What Was) vs. Inferential (What Will Be)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                xaxis1: { title: "Month", gridcolor: themeColors.gridColor },
                yaxis1: {
                  title: "Total Sales",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "Month",
                  tickvals: [1, 2, 3, 4],
                  ticktext: ["Jan", "Feb", "Mar", "Apr (Pred)"],
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: {
                  title: "Total Sales",
                  gridcolor: themeColors.gridColor,
                  range: [200, 340],
                },
                showlegend: false,
              },
            };
            Plotly.newPlot("p1s5ss1t8-plot1", fig, { responsive: true });
          },
          "p1s5ss2t1-plot1": () => {
            // Core Objects & Operations
            const users = [
              {
                name: "Alice",
                hours_watched: 15,
                sci_fi_rated: 2,
                color: "#ef4444",
              },
              {
                name: "Bob",
                hours_watched: 3,
                sci_fi_rated: 12,
                color: "#3b82f6",
              },
              {
                name: "Carol",
                hours_watched: 16,
                sci_fi_rated: 3,
                color: "#eab308",
              },
            ];
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const data = users.map((user) => ({
              type: "scatter",
              x: [0, user.hours_watched],
              y: [0, user.sci_fi_rated],
              mode: "lines",
              line: { width: 3, color: user.color },
              name: user.name,
            }));

            const layout = {
              title: "User Profiles as Vectors in Feature Space",
              paper_bgcolor: themeColors.bgColor,
              plot_bgcolor: themeColors.bgColor,
              font: { color: themeColors.textColor },
              xaxis: {
                title: "Hours Watched per Week",
                range: [0, 20],
                gridcolor: themeColors.gridColor,
              },
              yaxis: {
                title: "Sci-Fi Movies Rated",
                range: [0, 15],
                gridcolor: themeColors.gridColor,
              },
              showlegend: false,
              annotations: users.map((user) => ({
                x: user.hours_watched,
                y: user.sci_fi_rated,
                text: `<b>${user.name}</b>`,
                showarrow: true,
                arrowhead: 0,
                ax: 15,
                ay: 15,
                font: { color: user.color },
              })),
            };
            Plotly.newPlot("p1s5ss2t1-plot1", data, layout, {
              responsive: true,
            });
          },
          "p1s5ss2t2-plot1": () => {
            // Matrix Decompositions
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };

            const annotations = [
              {
                x: 1,
                y: 4,
                text: "A",
                showarrow: false,
                font: { size: 40, color: "white" },
              },
              {
                x: 1,
                y: 2.7,
                text: "m x n",
                showarrow: false,
                font: { size: 20 },
              },
              { x: 2.5, y: 4, text: "=", showarrow: false, font: { size: 40 } },
              {
                x: 3.75,
                y: 4,
                text: "U",
                showarrow: false,
                font: { size: 40, color: "white" },
              },
              {
                x: 3.75,
                y: 2.7,
                text: "m x k",
                showarrow: false,
                font: { size: 20 },
              },
              {
                x: 5.5,
                y: 4,
                text: "Σ",
                showarrow: false,
                font: { size: 40, color: "white" },
              },
              {
                x: 5.5,
                y: 3.2,
                text: "k x k",
                showarrow: false,
                font: { size: 20 },
              },
              {
                x: 7.5,
                y: 4.25,
                text: "Vᵀ",
                showarrow: false,
                font: { size: 40, color: "white" },
              },
              {
                x: 7.5,
                y: 3.5,
                text: "k x n",
                showarrow: false,
                font: { size: 20 },
              },
            ];

            Plotly.newPlot(
              "p1s5ss2t2-plot1",
              [],
              {
                title: "Conceptual View of SVD",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [-0.5, 9],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [2, 6],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0,
                    y0: 3,
                    x1: 2,
                    y1: 5,
                    fillcolor: "#3b82f6",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 3,
                    x1: 4.5,
                    y1: 5,
                    fillcolor: "#10b981",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 5,
                    y0: 3.5,
                    x1: 6,
                    y1: 4.5,
                    fillcolor: "#f59e0b",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 3.75,
                    x1: 8.5,
                    y1: 4.75,
                    fillcolor: "#ef4444",
                    line: { width: 0 },
                  },
                ],
                annotations: annotations,
              },
              { responsive: true }
            );
          },
          "p1s5ss2t3-plot1": () => {
            // Eigen-analysis
            // Simulate correlated data
            const data_x = [];
            const data_y = [];
            for (let i = 0; i < 100; i++) {
              const x_base = (Math.random() - 0.5) * 10;
              const y_base = x_base * 0.75 + (Math.random() - 0.5) * 2;
              data_x.push(x_base);
              data_y.push(y_base);
            }
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const layout = {
              title: "Principal Components of Correlated Data",
              paper_bgcolor: themeColors.bgColor,
              plot_bgcolor: themeColors.bgColor,
              font: { color: themeColors.textColor },
              xaxis: {
                title: "Feature 1 (e.g., Wingspan)",
                gridcolor: themeColors.gridColor,
              },
              yaxis: {
                title: "Feature 2 (e.g., Beak Length)",
                gridcolor: themeColors.gridColor,
              },
              showlegend: true,
              shapes: [
                // Eigenvectors
                {
                  type: "line",
                  x0: -6,
                  y0: -4.5,
                  x1: 6,
                  y1: 4.5,
                  line: { color: "#ef4444", width: 4 },
                },
                {
                  type: "line",
                  x0: 3.2,
                  y0: -4.2,
                  x1: -3.2,
                  y1: 4.2,
                  line: { color: "#3b82f6", width: 4, dash: "dash" },
                },
              ],
            };

            Plotly.newPlot(
              "p1s5ss2t3-plot1",
              [
                {
                  x: data_x,
                  y: data_y,
                  mode: "markers",
                  type: "scatter",
                  marker: { color: "grey", opacity: 0.6 },
                  name: "Data Points",
                },
                {
                  x: [null],
                  y: [null],
                  mode: "lines",
                  line: { color: "#ef4444", width: 4 },
                  name: "PC1 (Max Variance)",
                },
                {
                  x: [null],
                  y: [null],
                  mode: "lines",
                  line: { color: "#3b82f6", width: 4, dash: "dash" },
                  name: "PC2",
                },
              ],
              layout,
              { responsive: true }
            );
          },
          "p1s5ss2t5-plot1": () => {
            // Vector Space Concepts
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const fig = {
              data: [
                // Correlated
                {
                  type: "scatter",
                  mode: "lines",
                  x: [0, 3],
                  y: [0, 1.5],
                  line: { color: "#f59e0b", width: 4 },
                  name: "Correlated v1",
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  type: "scatter",
                  mode: "lines",
                  x: [0, 2],
                  y: [0, 3],
                  line: { color: "#f59e0b", width: 4 },
                  name: "Correlated v2",
                  xaxis: "x1",
                  yaxis: "y1",
                },
                // Orthogonal
                {
                  type: "scatter",
                  mode: "lines",
                  x: [0, 3],
                  y: [0, 0],
                  line: { color: "#10b981", width: 4 },
                  name: "Ortho b1",
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  type: "scatter",
                  mode: "lines",
                  x: [0, 0],
                  y: [0, 3],
                  line: { color: "#10b981", width: 4 },
                  name: "Ortho b2",
                  xaxis: "x2",
                  yaxis: "y2",
                },
              ],
              layout: {
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Non-Orthogonal (Correlated)",
                  range: [-1, 4],
                  gridcolor: themeColors.gridColor,
                  zeroline: false,
                },
                yaxis1: {
                  range: [-1, 4],
                  gridcolor: themeColors.gridColor,
                  zeroline: false,
                },
                xaxis2: {
                  title: "Orthogonal (Uncorrelated)",
                  range: [-1, 4],
                  gridcolor: themeColors.gridColor,
                  zeroline: false,
                },
                yaxis2: {
                  range: [-1, 4],
                  gridcolor: themeColors.gridColor,
                  zeroline: false,
                },
              },
            };
            Plotly.newPlot("p1s5ss2t5-plot1", fig, { responsive: true });
          },
          "p1s5ss2t6-plot1": () => {
            // Similarity Measures
            const docs = [
              { name: "Doc A (Short)", x: 5, y: 4, color: "#3b82f6" },
              { name: "Doc B (Long)", x: 10, y: 8, color: "#0ea5e9" },
              { name: "Doc C (Different)", x: 2, y: 9, color: "#ef4444" },
            ];
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const data = docs.map((doc) => ({
              type: "scatter",
              x: [0, doc.x],
              y: [0, doc.y],
              mode: "lines",
              line: { width: 3, color: doc.color },
              name: doc.name,
            }));

            const layout = {
              title: "Document Similarity as Vector Angles",
              paper_bgcolor: themeColors.bgColor,
              plot_bgcolor: themeColors.bgColor,
              font: { color: themeColors.textColor },
              xaxis: {
                title: "Count of word 'data'",
                range: [0, 12],
                gridcolor: themeColors.gridColor,
              },
              yaxis: {
                title: "Count of word 'analysis'",
                range: [0, 12],
                gridcolor: themeColors.gridColor,
                scaleanchor: "x",
                scaleratio: 1,
              },
              showlegend: false,
              annotations: docs.map((doc) => ({
                x: doc.x,
                y: doc.y,
                text: `<b>${doc.name}</b>`,
                showarrow: true,
                arrowhead: 0,
                ax: 20,
                ay: -20,
                font: { color: doc.color },
              })),
            };
            Plotly.newPlot("p1s5ss2t6-plot1", data, layout, {
              responsive: true,
            });
          },
          "p1s5ss2t7-plot1": () => {
            // PCA Scree Plot
            const scree_data = {
              Component: ["PC1", "PC2", "PC3", "PC4"],
              Variance: [0.7296, 0.2285, 0.0367, 0.0052],
              Cumulative: [0.7296, 0.9581, 0.9948, 1.0],
            };
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const trace1 = {
              x: scree_data.Component,
              y: scree_data.Variance,
              type: "bar",
              name: "Individual Variance",
              marker: { color: "#3b82f6" },
            };
            const trace2 = {
              x: scree_data.Component,
              y: scree_data.Cumulative,
              type: "scatter",
              mode: "lines+markers",
              name: "Cumulative Variance",
              line: { color: "#ef4444" },
            };

            const layout = {
              title: "Scree Plot for Iris PCA",
              paper_bgcolor: themeColors.bgColor,
              plot_bgcolor: themeColors.bgColor,
              font: { color: themeColors.textColor },
              xaxis: {
                title: "Principal Component",
                gridcolor: themeColors.gridColor,
              },
              yaxis: {
                title: "Proportion of Variance Explained",
                tickformat: ".0%",
                gridcolor: themeColors.gridColor,
              },
              showlegend: true,
              legend: { x: 0.6, y: 0.6 },
              annotations: scree_data.Component.map((c, i) => ({
                x: c,
                y: scree_data.Cumulative[i],
                text: `${(scree_data.Cumulative[i] * 100).toFixed(1)}%`,
                xanchor: "center",
                yanchor: "bottom",
                showarrow: false,
                font: { color: themeColors.textColor },
              })),
            };
            Plotly.newPlot("p1s5ss2t7-plot1", [trace1, trace2], layout, {
              responsive: true,
            });
          },
          "p1s5ss3t1-plot1": () => {
            // Differentiation
            const x = [];
            const y = [];
            for (let i = -3; i <= 3; i += 0.1) {
              x.push(i);
              y.push(i * i);
            }
            const x0 = 1.5;
            const y0 = x0 * x0;
            const slope = 2 * x0;
            const tangent_x = [x0 - 1, x0 + 1];
            const tangent_y = [y0 - slope, y0 + slope];

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p1s5ss3t1-plot1",
              [
                {
                  x: x,
                  y: y,
                  type: "scatter",
                  mode: "lines",
                  name: "f(x) = x²",
                  line: { color: "#3b82f6", width: 3 },
                },
                {
                  x: tangent_x,
                  y: tangent_y,
                  type: "scatter",
                  mode: "lines",
                  name: "Tangent Line",
                  line: { color: "#ef4444", dash: "dash", width: 2 },
                },
                {
                  x: [x0],
                  y: [y0],
                  type: "scatter",
                  mode: "markers",
                  name: "Point of Tangency",
                  marker: { color: "#ef4444", size: 10 },
                },
              ],
              {
                title: "The Derivative as the Slope of the Tangent Line",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "x", gridcolor: themeColors.gridColor },
                yaxis: { title: "f(x)", gridcolor: themeColors.gridColor },
                showlegend: false,
                annotations: [
                  {
                    x: x0,
                    y: y0 + 1,
                    text: `Slope = ${slope}`,
                    showarrow: false,
                    font: { color: "#ef4444" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s5ss3t2-plot1": () => {
            // Integration
            const x = [];
            const y = [];
            const mean = 175;
            const sd = 7;
            for (let i = 150; i <= 200; i += 0.5) {
              x.push(i);
              y.push(
                (1 / (sd * Math.sqrt(2 * Math.PI))) *
                  Math.exp(-0.5 * Math.pow((i - mean) / sd, 2))
              );
            }
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p1s5ss3t2-plot1",
              [
                {
                  x: x,
                  y: y,
                  type: "scatter",
                  mode: "lines",
                  line: { color: "#3b82f6", width: 2 },
                  name: "PDF",
                },
                {
                  x: x.filter((val) => val >= 170 && val <= 180),
                  y: y.slice(
                    x.findIndex((val) => val >= 170),
                    x.findIndex((val) => val > 180)
                  ),
                  type: "scatter",
                  mode: "lines",
                  fill: "tozeroy",
                  fillcolor: "rgba(239, 68, 68, 0.7)",
                  line: { color: "transparent" },
                  name: "Area",
                },
              ],
              {
                title: "Probability as Area Under the Curve",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Height (cm)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Probability Density",
                  gridcolor: themeColors.gridColor,
                },
                showlegend: false,
                annotations: [
                  {
                    x: 175,
                    y: 0.03,
                    text: "Shaded Area = P(170 < Height < 180)",
                    showarrow: true,
                    arrowhead: 2,
                    ax: 0,
                    ay: -40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s5ss3t3-plot1": () => {
            // Function Approximation
            const x = [];
            const y_sin = [];
            const y_t1 = [];
            const y_t3 = [];
            const y_t7 = [];
            const fact = (n) => (n <= 1 ? 1 : n * fact(n - 1));
            for (let i = -Math.PI; i <= Math.PI; i += 0.1) {
              x.push(i);
              y_sin.push(Math.sin(i));
              y_t1.push(i);
              y_t3.push(i - Math.pow(i, 3) / fact(3));
              y_t7.push(
                i -
                  Math.pow(i, 3) / fact(3) +
                  Math.pow(i, 5) / fact(5) -
                  Math.pow(i, 7) / fact(7)
              );
            }
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p1s5ss3t3-plot1",
              [
                {
                  x: x,
                  y: y_sin,
                  type: "scatter",
                  mode: "lines",
                  name: "sin(x)",
                  line: { color: "#3b82f6", width: 4 },
                },
                {
                  x: x,
                  y: y_t1,
                  type: "scatter",
                  mode: "lines",
                  name: "1st Order",
                  line: { color: "#10b981", dash: "dash" },
                },
                {
                  x: x,
                  y: y_t3,
                  type: "scatter",
                  mode: "lines",
                  name: "3rd Order",
                  line: { color: "#8b5cf6", dash: "dot" },
                },
                {
                  x: x,
                  y: y_t7,
                  type: "scatter",
                  mode: "lines",
                  name: "7th Order",
                  line: { color: "#f59e0b", dash: "dot" },
                },
              ],
              {
                title: "Taylor Series Approximation of sin(x)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "x", gridcolor: themeColors.gridColor },
                yaxis: { title: "f(x)", gridcolor: themeColors.gridColor },
                legend: { x: 0.8, y: 0.1 },
              },
              { responsive: true }
            );
          },
          "p1s5ss3t4-plot1": () => {
            // Optimization Theory
            const loss_surface_func = (x, y) => {
              const term1 = Math.pow(x * x + y - 11, 2);
              const term2 = Math.pow(x + y * y - 7, 2);
              return Math.log1p(term1 + term2);
            };
            const x_grid = [];
            for (let i = 0; i <= 6; i += 0.2) x_grid.push(i);
            const y_grid = [];
            for (let i = 0; i <= 6; i += 0.2) y_grid.push(i);
            const z_grid = x_grid.map((x) =>
              y_grid.map((y) => loss_surface_func(x, y))
            );

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p1s5ss3t4-plot1",
              [
                {
                  x: x_grid,
                  y: y_grid,
                  z: z_grid,
                  type: "surface",
                  colorscale: "Blues",
                  reversescale: true,
                },
              ],
              {
                title: "Non-Convex Loss Surface",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                scene: {
                  xaxis: {
                    title: "Parameter 1",
                    backgroundcolor: themeColors.bgColor,
                    gridcolor: themeColors.gridColor,
                  },
                  yaxis: {
                    title: "Parameter 2",
                    backgroundcolor: themeColors.bgColor,
                    gridcolor: themeColors.gridColor,
                  },
                  zaxis: {
                    title: "Loss",
                    backgroundcolor: themeColors.bgColor,
                    gridcolor: themeColors.gridColor,
                  },
                  annotations: [
                    {
                      x: 3,
                      y: 2,
                      z: 1,
                      text: "Global Minimum",
                      textangle: 0,
                      ax: 0,
                      ay: -75,
                      font: { color: isDark ? "white" : "black", size: 14 },
                      arrowcolor: isDark ? "white" : "black",
                      arrowwidth: 2,
                      arrowhead: 1,
                    },
                    {
                      x: 3.58,
                      y: -1.84,
                      z: 2,
                      text: "Local Minimum",
                      textangle: 0,
                      ax: 75,
                      ay: 0,
                      font: { color: isDark ? "white" : "black", size: 14 },
                      arrowcolor: isDark ? "white" : "black",
                      arrowwidth: 2,
                      arrowhead: 1,
                    },
                  ],
                },
              },
              { responsive: true }
            );
          },
          "p1s5ss3t5-plot1": () => {
            // Gradient Descent
            const x_grid = [];
            for (let i = -10; i <= 10; i += 0.5) x_grid.push(i);
            const y_grid = [];
            for (let i = -10; i <= 10; i += 0.5) y_grid.push(i);
            const z_grid = x_grid.map((x) => y_grid.map((y) => x * x + y * y));
            const path = { x: [9], y: [9] };
            for (let i = 0; i < 15; i++) {
              const grad_x = 2 * path.x[i];
              const grad_y = 2 * path.y[i];
              path.x.push(path.x[i] - 0.1 * grad_x);
              path.y.push(path.y[i] - 0.1 * grad_y);
            }

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const trace1 = {
              x: x_grid,
              y: y_grid,
              z: z_grid,
              type: "contour",
              colorscale: "Blues",
              reversescale: true,
              showscale: false,
            };
            const trace2 = {
              x: path.x,
              y: path.y,
              type: "scatter",
              mode: "lines+markers",
              line: { color: "#ef4444", width: 3 },
              marker: { color: "#ef4444", size: 6 },
            };

            Plotly.newPlot(
              "p1s5ss3t5-plot1",
              [trace1, trace2],
              {
                title: "Path of Gradient Descent on a Contour Plot",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Parameter 1",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Parameter 2",
                  gridcolor: themeColors.gridColor,
                },
                showlegend: false,
              },
              { responsive: true }
            );
          },
          "p1s5ss3t7-plot1": () => {
            // Modern Optimizers
            const x_grid = [];
            for (let i = -2; i <= 2; i += 0.1) x_grid.push(i);
            const y_grid = [];
            for (let i = -5; i <= 5; i += 0.1) y_grid.push(i);
            const z_grid = x_grid.map((x) =>
              y_grid.map((y) => x * x + 0.1 * y * y)
            );

            const sgd_path = {
              x: [-1.8, 1.8, -1.7, 1.7, -1.6, 1.6],
              y: [4, 3, 2.8, 1.8, 1.6, 0.8],
            };
            const momentum_path = {
              x: [-1.8, 0.2, -0.4, 0.1, -0.1, 0],
              y: [4, 2.5, 1.5, 0.8, 0.3, 0],
            };
            const adam_path = {
              x: [-1.8, -1, -0.4, -0.1, 0],
              y: [4, 0.5, 0.2, 0.1, 0],
            };

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const trace1 = {
              x: x_grid,
              y: y_grid,
              z: z_grid,
              type: "contour",
              colorscale: "Blues",
              reversescale: true,
              showscale: false,
              contours: { coloring: "lines" },
            };
            const trace_sgd = {
              x: sgd_path.x,
              y: sgd_path.y,
              type: "scatter",
              mode: "lines+markers",
              name: "SGD",
              line: { color: "#a855f7" },
            };
            const trace_momentum = {
              x: momentum_path.x,
              y: momentum_path.y,
              type: "scatter",
              mode: "lines+markers",
              name: "Momentum",
              line: { color: "#f59e0b" },
            };
            const trace_adam = {
              x: adam_path.x,
              y: adam_path.y,
              type: "scatter",
              mode: "lines+markers",
              name: "Adam",
              line: { color: "#10b981" },
            };

            Plotly.newPlot(
              "p1s5ss3t7-plot1",
              [trace1, trace_sgd, trace_momentum, trace_adam],
              {
                title: "Comparing Optimizer Paths",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Parameter 1",
                  zeroline: false,
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Parameter 2",
                  zeroline: false,
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.1, y: 0.9 },
              },
              { responsive: true }
            );
          },
          "p1s6ss1-plot1": () => {
            // Advanced Functional Programming
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s6ss1-plot1",
              [],
              {
                title: "Function Factory Process",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [-0.5, 7.5],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [1.5, 6.5],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0,
                    y0: 3,
                    x1: 2,
                    y1: 5,
                    fillcolor: "#3b82f6",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 5,
                    y0: 4.5,
                    x1: 7,
                    y1: 6,
                    fillcolor: "#10b981",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 5,
                    y0: 2,
                    x1: 7,
                    y1: 3.5,
                    fillcolor: "#f59e0b",
                    line: { width: 0 },
                  },
                  {
                    type: "path",
                    path: "M 2 4.5 L 4.5 5.25",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 2 3.5 L 4.5 2.75",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                ],
                annotations: [
                  {
                    x: 1,
                    y: 4,
                    text: "<b>Factory<br>Function</b>",
                    showarrow: false,
                    font: { color: "white", size: 14 },
                  },
                  {
                    x: 6,
                    y: 5.25,
                    text: "<b>Z-Score<br>Scaler</b>",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 6,
                    y: 2.75,
                    text: "<b>Min-Max<br>Scaler</b>",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 3.25,
                    y: 5.1,
                    text: "Argument:<br>'z-score'",
                    showarrow: false,
                  },
                  {
                    x: 3.25,
                    y: 3,
                    text: "Argument:<br>'min-max'",
                    showarrow: false,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s6ss2-plot1": () => {
            // Evaluation Model
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s6ss2-plot1",
              [],
              {
                title: "Lazy Evaluation Flow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 9.5],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [2, 6.5],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0,
                    y0: 5,
                    x1: 3,
                    y1: 6,
                    fillcolor: "#a855f7",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 3,
                    x1: 1.5,
                    y1: 4,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 1.5,
                    y0: 3,
                    x1: 2.5,
                    y1: 4,
                    fillcolor: "#f43f5e",
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 2,
                    x1: 7,
                    y1: 6,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 8,
                    y0: 3.5,
                    x1: 9,
                    y1: 4.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "path",
                    path: "M 1 5 L 1 4",
                    line: {
                      color: themeColors.textColor,
                      width: 2,
                      dash: "dot",
                    },
                  },
                  {
                    type: "path",
                    path: "M 2 5 L 2 4",
                    line: {
                      color: themeColors.textColor,
                      width: 2,
                      dash: "dot",
                    },
                  },
                  {
                    type: "path",
                    path: "M 1.5 3.5 C 2.5 4, 3.5 4.5, 4 4.5",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 7 4 L 8 4",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    x0: 2.5,
                    y0: 2.5,
                    x1: 1.5,
                    y1: 2.5,
                    type: "line",
                    line: { color: "#f43f5e", width: 3 },
                  },
                ],
                annotations: [
                  {
                    x: 1.5,
                    y: 5.5,
                    text: "my_func(arg1=10, arg2=stop())",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 1,
                    y: 3.5,
                    text: "Promise 1<br>(expr = 10)",
                    showarrow: false,
                    font: { size: 10 },
                  },
                  {
                    x: 2,
                    y: 3.5,
                    text: "Promise 2<br>(expr = stop())",
                    showarrow: false,
                    font: { size: 10 },
                  },
                  {
                    x: 5.5,
                    y: 4,
                    text: "<b>Function Body:</b><br>return(arg1 * 2)",
                    showarrow: false,
                    font: { color: "white", size: 14 },
                  },
                  { x: 2.75, y: 4.2, text: "Accessed!", showarrow: false },
                  {
                    x: 8.5,
                    y: 4,
                    text: "20",
                    showarrow: false,
                    font: { color: "white", size: 20 },
                  },
                  {
                    x: 2,
                    y: 2.2,
                    text: "Never Evaluated!",
                    showarrow: false,
                    font: { color: "#f43f5e" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s6ss3-plot1": () => {
            // Code Debugging & Profiling
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p1s6ss3-plot1",
              [
                {
                  type: "bar",
                  x: [90, 10, 70],
                  y: ["Total Runtime", "Fast Part", "Slow Loop"],
                  base: [0, 10, 20],
                  orientation: "h",
                  marker: { color: ["grey", "#10b981", "#ef4444"] },
                },
              ],
              {
                title: "Simplified Flame Graph",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Time (ms)" },
                yaxis: { title: "Call Stack" },
                showlegend: false,
              },
              { responsive: true }
            );
          },
          "p1s6ss4-plot1": () => {
            // Performance Optimization
            const perf_data = {
              Method: ["Growing in Loop", "Pre-allocated Loop", "Vectorized"],
              Time: [3500, 25, 0.5],
            };
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p1s6ss4-plot1",
              [
                {
                  x: perf_data.Method,
                  y: perf_data.Time,
                  type: "bar",
                  marker: { color: ["#ef4444", "#f59e0b", "#10b981"] },
                },
              ],
              {
                title: "Performance Comparison (Log Scale)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Programming Method" },
                yaxis: {
                  title: "Execution Time (ms)",
                  type: "log",
                  autorange: true,
                  gridcolor: themeColors.gridColor,
                },
              },
              { responsive: true }
            );
          },
          "p1s6ss5-plot1": () => {
            // Error Handling
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s6ss5-plot1",
              [],
              {
                title: "Error Handling Execution Flow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 7],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  // Normal Flow
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 5,
                    x1: 2,
                    y1: 6,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 2.5,
                    y0: 5,
                    x1: 4,
                    y1: 6,
                    fillcolor: "#f43f5e",
                  },
                  {
                    type: "rect",
                    x0: 2,
                    y0: 3.5,
                    x1: 3.5,
                    y1: 4.5,
                    fillcolor: "#7f1d1d",
                  },
                  {
                    type: "path",
                    path: "M 2 5.5 L 2.5 5.5",
                    line: { width: 2, color: themeColors.textColor },
                  },
                  {
                    type: "path",
                    path: "M 3.25 5 L 3.25 4.5",
                    line: { width: 2, color: themeColors.textColor },
                  },
                  // Robust Flow
                  {
                    type: "rect",
                    x0: 6,
                    y0: 5,
                    x1: 9,
                    y1: 6,
                    fillcolor: "#3b82f6",
                    line: { width: 2, color: "#10b981" },
                  },
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 3,
                    x1: 8.5,
                    y1: 4,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 6,
                    y0: 0.5,
                    x1: 9,
                    y1: 1.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "path",
                    path: "M 7.5 5 C 7.5 4.5, 6.5 4.5, 6.5 3.5",
                    line: {
                      width: 2,
                      color: themeColors.textColor,
                      dash: "dot",
                    },
                  },
                  {
                    type: "path",
                    path: "M 8.5 3.5 C 9.5 3.5, 9.5 2, 8.25 1.5",
                    line: { width: 2, color: themeColors.textColor },
                  },
                ],
                annotations: [
                  {
                    x: 2.75,
                    y: 6.5,
                    text: "<b>Normal Flow</b>",
                    showarrow: false,
                  },
                  {
                    x: 1.25,
                    y: 5.5,
                    text: "Step 1",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 3.25,
                    y: 5.5,
                    text: "Error!",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 2.75,
                    y: 4,
                    text: "CRASH",
                    showarrow: false,
                    font: { color: "white", size: 16 },
                  },
                  {
                    x: 7.5,
                    y: 6.5,
                    text: "<b>Robust Flow</b>",
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 5.5,
                    text: "tryCatch{...}",
                    showarrow: false,
                    font: { color: "white", size: 16 },
                  },
                  {
                    x: 7.5,
                    y: 3.5,
                    text: "Error Handler",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 7.5,
                    y: 1,
                    text: "Next Step",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 6,
                    y: 4,
                    text: "Error occurs",
                    showarrow: false,
                    font: { size: 10 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s7ss1-plot1": () => {
            // RStudio Projects (Corrected Version 2)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
              lineColor: isDark ? "#495057" : "#dee2e6",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };

            const fig = {
              data: [], // No data traces are needed for this conceptual plot
              layout: {
                title: "Workflow Comparison",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 20],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  // Bounding box for Disorganized
                  {
                    type: "rect",
                    x0: 1,
                    y0: 1,
                    x1: 9,
                    y1: 9,
                    line: {
                      color: themeColors.lineColor,
                      width: 1,
                      dash: "dot",
                    },
                  },
                  // Bounding boxes for Projects
                  {
                    type: "rect",
                    x0: 10.5,
                    y0: 5,
                    x1: 19.5,
                    y1: 9,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 10.5,
                    y0: 1,
                    x1: 19.5,
                    y1: 5,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                ],
                annotations: [
                  // Titles for each side
                  {
                    x: 5,
                    y: 9.5,
                    text: "Disorganized Folder",
                    showarrow: false,
                    yanchor: "bottom",
                    font: { size: 14 },
                  },
                  {
                    x: 15,
                    y: 9.5,
                    text: "Project-Based",
                    showarrow: false,
                    yanchor: "bottom",
                    font: { size: 14 },
                  },

                  // Disorganized files (styled with monospace font)
                  {
                    x: 2,
                    y: 8,
                    text: "sales.R",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 2,
                    y: 7.2,
                    text: "churn.csv",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 2,
                    y: 6.4,
                    text: "plot.png",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 2,
                    y: 5.6,
                    text: "sales_v2.R",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 2,
                    y: 4.8,
                    text: "churn_model.R",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 2,
                    y: 4,
                    text: "final_report.docx",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },

                  // Project A
                  {
                    x: 11,
                    y: 8.5,
                    text: "<b>Project A: Sales</b>",
                    showarrow: false,
                    xanchor: "left",
                    font: { size: 14 },
                  },
                  {
                    x: 11,
                    y: 7.5,
                    text: "sales.Rproj",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 11,
                    y: 6.7,
                    text: "/data/sales.csv",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 11,
                    y: 5.9,
                    text: "/R/analysis.R",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },

                  // Project B
                  {
                    x: 11,
                    y: 4.5,
                    text: "<b>Project B: Churn</b>",
                    showarrow: false,
                    xanchor: "left",
                    font: { size: 14 },
                  },
                  {
                    x: 11,
                    y: 3.5,
                    text: "churn.Rproj",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 11,
                    y: 2.7,
                    text: "/data/churn.csv",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 11,
                    y: 1.9,
                    text: "/R/modeling.R",
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                ],
              },
            };
            Plotly.newPlot("p1s7ss1-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p1s7ss2-plot1": () => {
            // Dynamic Documents
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s7ss2-plot1",
              [],
              {
                title: "Dynamic Document Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 7],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0,
                    y0: 4,
                    x1: 2,
                    y1: 5,
                    fillcolor: "#eab308",
                  }, // Data
                  {
                    type: "rect",
                    x0: 0,
                    y0: 2,
                    x1: 2,
                    y1: 3,
                    fillcolor: "#a855f7",
                  }, // Rmd
                  {
                    type: "rect",
                    x0: 3.5,
                    y0: 2.5,
                    x1: 5.5,
                    y1: 4.5,
                    fillcolor: "#3b82f6",
                  }, // RStudio
                  {
                    type: "rect",
                    x0: 7,
                    y0: 3,
                    x1: 9,
                    y1: 4,
                    fillcolor: "#10b981",
                  }, // Report
                  {
                    type: "path",
                    path: "M 2 4.5 L 3.5 3.75",
                    line: { width: 2, color: themeColors.textColor },
                  },
                  {
                    type: "path",
                    path: "M 2 2.5 L 3.5 3.25",
                    line: { width: 2, color: themeColors.textColor },
                  },
                  {
                    type: "path",
                    path: "M 5.5 3.5 L 7 3.5",
                    line: { width: 2, color: themeColors.textColor },
                  },
                ],
                annotations: [
                  {
                    x: 1,
                    y: 4.5,
                    text: "Data",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 1,
                    y: 2.5,
                    text: ".Rmd File",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 4.5,
                    y: 3.5,
                    text: "<b>KNIT</b>",
                    showarrow: false,
                    font: { color: "white", size: 20 },
                  },
                  {
                    x: 8,
                    y: 3.5,
                    text: "HTML Report",
                    showarrow: false,
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s7ss4-plot1": () => {
            // Automation & Pipelines
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const nodes = {
              x: [1, 3, 3, 5, 7],
              y: [4, 6, 2, 4, 4],
              text: ["Raw Data", "Clean Data", "Plot", "Model", "Report"],
              color: ["#eab308", "#3b82f6", "#10b981", "#a855f7", "#ef4444"],
            };
            const edges = [
              { x0: 1, y0: 4, x1: 3, y1: 6 }, // Raw -> Clean
              { x0: 1, y0: 4, x1: 3, y1: 2 }, // Raw -> Plot (assuming plot uses raw)
              { x0: 3, y0: 6, x1: 5, y1: 4 }, // Clean -> Model
              { x0: 3, y0: 2, x1: 7, y1: 4 }, // Plot -> Report
              { x0: 5, y0: 4, x1: 7, y1: 4 }, // Model -> Report
            ];
            Plotly.newPlot(
              "p1s7ss4-plot1",
              [
                {
                  type: "scatter",
                  x: nodes.x,
                  y: nodes.y,
                  mode: "markers+text",
                  marker: { size: 40, color: nodes.color },
                  text: nodes.text,
                  textfont: { color: "white", size: 12 },
                },
              ],
              {
                title: "Analysis Dependency Graph (DAG)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                showlegend: false,
                shapes: edges.map((e) => ({
                  type: "line",
                  x0: e.x0,
                  y0: e.y0,
                  x1: e.x1,
                  y1: e.y1,
                  line: { color: themeColors.textColor, width: 2 },
                })),
              },
              { responsive: true }
            );
          },
          "p1s7ss6-plot1": () => {
            // Professional Reporting
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s7ss6-plot1",
              [],
              {
                title: "Anatomy of a Dashboard",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 0.5,
                    x1: 3.5,
                    y1: 9.5,
                    fillcolor: isDark ? "#282c34" : "#f1f5f9",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 5.5,
                    x1: 9.5,
                    y1: 9.5,
                    fillcolor: isDark ? "#282c34" : "#f1f5f9",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 0.5,
                    x1: 6.5,
                    y1: 4.5,
                    fillcolor: isDark ? "#282c34" : "#f1f5f9",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 0.5,
                    x1: 9.5,
                    y1: 4.5,
                    fillcolor: isDark ? "#282c34" : "#f1f5f9",
                    line: { width: 0 },
                  },
                ],
                annotations: [
                  {
                    x: 2,
                    y: 9,
                    text: "<b>Sidebar</b>",
                    showarrow: false,
                    yanchor: "top",
                  },
                  { x: 2, y: 8, text: "Input Controls", showarrow: false },
                  {
                    x: 2,
                    y: 7,
                    text: "(Sliders, Dropdowns)",
                    showarrow: false,
                    font: { size: 10 },
                  },
                  {
                    x: 6.75,
                    y: 9,
                    text: "<b>Main Panel</b>",
                    showarrow: false,
                    yanchor: "top",
                  },
                  { x: 6.75, y: 7.5, text: "Plot Output", showarrow: false },
                  { x: 5.25, y: 2.5, text: "Value Box 1", showarrow: false },
                  { x: 8.25, y: 2.5, text: "Value Box 2", showarrow: false },
                ],
              },
              { responsive: true }
            );
          },
          "p1s7ss7-plot1": () => {
            // Version Control Integration
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s7ss7-plot1",
              [],
              {
                title: "The Git Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 1,
                    y0: 1,
                    x1: 4,
                    y1: 7,
                    fillcolor: "#3b82f6",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 6,
                    y0: 1,
                    x1: 9,
                    y1: 7,
                    fillcolor: "#10b981",
                    line: { width: 0 },
                  },
                ],
                annotations: [
                  {
                    x: 2.5,
                    y: 7.5,
                    text: "<b>Local Computer</b>",
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 7.5,
                    text: "<b>GitHub Repository</b>",
                    showarrow: false,
                  },
                  {
                    x: 5,
                    y: 5.5,
                    text: "<b>PUSH ➔</b>",
                    showarrow: true,
                    ax: -50,
                    ay: 0,
                    font: { size: 16 },
                  },
                  {
                    x: 2.5,
                    y: 4,
                    text: "Edit & Save Files",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 3,
                    text: "Stage & Commit",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 2.5,
                    text: "<b>PULL ￩</b>",
                    showarrow: true,
                    ax: 50,
                    ay: 0,
                    font: { size: 16 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s7ss8-plot1": () => {
            // Collaborative Workflows
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p1s7ss8-plot1",
              [
                {
                  x: [1, 2, 5, 6, 7],
                  y: [1, 1, 1, 1, 1],
                  type: "scatter",
                  mode: "lines+markers",
                  name: "main",
                  line: { color: "#3b82f6", width: 3 },
                },
                {
                  x: [2, 3, 4, 5],
                  y: [1, 2, 2, 1],
                  type: "scatter",
                  mode: "lines+markers",
                  name: "feature",
                  line: { color: "#f59e0b", width: 3, dash: "dash" },
                },
              ],
              {
                title: "Git Branching Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Commit History",
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                showlegend: true,
                legend: { x: 0.1, y: 0.9 },
                annotations: [
                  {
                    x: 2,
                    y: 1.2,
                    text: "Create Branch",
                    showarrow: true,
                    arrowhead: 4,
                    ax: 0,
                    ay: -30,
                  },
                  {
                    x: 3.5,
                    y: 2.2,
                    text: "Work in Isolation",
                    showarrow: false,
                  },
                  {
                    x: 5,
                    y: 1.2,
                    text: "Merge PR",
                    showarrow: true,
                    arrowhead: 4,
                    ax: 0,
                    ay: -30,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s8ss1-plot1": () => {
            // Repository Management
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            Plotly.newPlot(
              "p1s8ss1-plot1",
              [],
              {
                title: "Two Paths to a Local Repository",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4.5,
                    x1: 3.5,
                    y1: 6.5,
                    fillcolor: "#10b981",
                  }, // GitHub Repo
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 3.5,
                    y1: 3,
                    fillcolor: "#3b82f6",
                  }, // Local Repo A
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 1,
                    x1: 9.5,
                    y1: 3,
                    fillcolor: "#3b82f6",
                  }, // Local Repo B
                ],
                annotations: [
                  {
                    x: 2,
                    y: 7,
                    text: "<b>GitHub Remote</b>",
                    showarrow: false,
                  },
                  {
                    x: 2,
                    y: 5.5,
                    text: "Existing Repo",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 4.5,
                    y: 3.5,
                    text: "<b>git clone</b>",
                    showarrow: true,
                    ax: -30,
                    ay: 30,
                    arrowhead: 2,
                  },
                  {
                    x: 2,
                    y: 2,
                    text: "Local Repo A",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  { x: 2, y: 0.5, text: "Path A: Cloning", showarrow: false },
                  {
                    x: 8,
                    y: 2,
                    text: "Local Repo B",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 8,
                    y: 0.5,
                    text: "Path B: Initializing",
                    showarrow: false,
                  },
                  {
                    x: 8,
                    y: 4,
                    text: "<b>git init</b><br>(then connect)",
                    showarrow: false,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s8ss2-plot1": () => {
            // The Git Workflow
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s8ss2-plot1",
              [],
              {
                title: "The Core Git Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 5],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 3,
                    y1: 4,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 1,
                    x1: 6.25,
                    y1: 4,
                    fillcolor: "#f59e0b",
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 1,
                    x1: 9.5,
                    y1: 4,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: 1.75,
                    y: 4.5,
                    text: "<b>Working Directory</b>",
                    showarrow: false,
                  },
                  {
                    x: 5,
                    y: 4.5,
                    text: "<b>Staging Area</b>",
                    showarrow: false,
                  },
                  {
                    x: 8.25,
                    y: 4.5,
                    text: "<b>Repository</b>",
                    showarrow: false,
                  },
                  {
                    x: 3.375,
                    y: 2.5,
                    text: "git add",
                    showarrow: true,
                    ax: -30,
                    ay: 0,
                  },
                  {
                    x: 6.625,
                    y: 2.5,
                    text: "git commit",
                    showarrow: true,
                    ax: -30,
                    ay: 0,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s8ss3-plot1": () => {
            // Remote Collaboration
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s8ss3-plot1",
              [],
              {
                title: "The Git Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 1,
                    y0: 1,
                    x1: 4,
                    y1: 7,
                    fillcolor: "#3b82f6",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 6,
                    y0: 1,
                    x1: 9,
                    y1: 7,
                    fillcolor: "#10b981",
                    line: { width: 0 },
                  },
                ],
                annotations: [
                  {
                    x: 2.5,
                    y: 7.5,
                    text: "<b>Local Computer</b>",
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 7.5,
                    text: "<b>GitHub Repository</b>",
                    showarrow: false,
                  },
                  {
                    x: 5,
                    y: 5.5,
                    text: "<b>PUSH ➔</b>",
                    showarrow: true,
                    ax: -50,
                    ay: 0,
                    font: { size: 16 },
                  },
                  {
                    x: 2.5,
                    y: 4,
                    text: "Edit & Save Files",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 3,
                    text: "Stage & Commit",
                    showarrow: false,
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 2.5,
                    text: "<b>PULL ￩</b>",
                    showarrow: true,
                    ax: 50,
                    ay: 0,
                    font: { size: 16 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s8ss4-plot1": () => {
            // Team Contributions
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s8ss4-plot1",
              [],
              {
                title: "The Fork & Pull Request Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 7.5,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4,
                    x1: 4.5,
                    y1: 6,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 0.5,
                    x1: 4.5,
                    y1: 2.5,
                    fillcolor: "#3b82f6",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 9,
                    text: "Upstream Repo",
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 5,
                    text: "Your Fork on GitHub",
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 1.5,
                    text: "Your Local Computer",
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 6.75,
                    text: "1. Fork",
                    showarrow: true,
                    ax: 0,
                    ay: -20,
                  },
                  {
                    x: 2.5,
                    y: 3.25,
                    text: "2. Clone",
                    showarrow: true,
                    ax: 0,
                    ay: -20,
                  },
                  {
                    x: 6,
                    y: 1.5,
                    text: "3. Push to Fork",
                    showarrow: true,
                    ax: -50,
                    ay: 30,
                    arrowhead: 2,
                  },
                  {
                    x: 6,
                    y: 5,
                    text: "4. Open Pull Request",
                    showarrow: true,
                    ax: 0,
                    ay: 30,
                    arrowhead: 2,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p1s8ss5-plot1": () => {
            // History Management
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p1s8ss5-plot1",
              [
                {
                  x: [1, 2, 3, 5],
                  y: [1, 1, 1, 1],
                  type: "scatter",
                  mode: "lines+markers",
                  name: "main",
                  line: { color: "#3b82f6", width: 3 },
                  marker: { size: 15 },
                },
                {
                  x: [3, 4, 5],
                  y: [1, 2, 1],
                  type: "scatter",
                  mode: "lines",
                  name: "revert",
                  line: { color: "#ef4444", width: 2, dash: "dot" },
                },
              ],
              {
                title: "Reverting a Commit",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 3],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                showlegend: false,
                annotations: [
                  { x: 1, y: 1.3, text: "Commit A", showarrow: false },
                  { x: 2, y: 1.3, text: "Commit B", showarrow: false },
                  {
                    x: 3,
                    y: 1.3,
                    text: "Commit C (Mistake)",
                    showarrow: false,
                  },
                  {
                    x: 5,
                    y: 1.3,
                    text: "Commit D (New Revert Commit)",
                    showarrow: false,
                  },
                  {
                    x: 4,
                    y: 2.2,
                    text: "git revert C",
                    showarrow: false,
                    font: { color: "#ef4444" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s1ss2-plot1": () => {
            // From SQL Databases
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p2s1ss2-plot1",
              [],
              {
                title: "R to SQL Database Architecture",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 6],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 2,
                    x1: 2.5,
                    y1: 4,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 2,
                    x1: 5,
                    y1: 4,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 2,
                    x1: 7.5,
                    y1: 4,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 8,
                    y0: 2,
                    x1: 10,
                    y1: 4,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "path",
                    path: "M 2.5 3 L 3 3",
                    line: { width: 2, color: themeColors.textColor },
                  },
                  {
                    type: "path",
                    path: "M 5 3 L 5.5 3",
                    line: { width: 2, color: themeColors.textColor },
                  },
                  {
                    type: "path",
                    path: "M 7.5 3 L 8 3",
                    line: {
                      width: 2,
                      color: themeColors.textColor,
                      dash: "dot",
                    },
                  },
                ],
                annotations: [
                  { x: 1.5, y: 3, text: "R Session", font: { color: "white" } },
                  { x: 4, y: 3, text: "DBI", font: { color: "white" } },
                  {
                    x: 6.5,
                    y: 3,
                    text: "Driver<br>(odbc)",
                    font: { color: "white" },
                  },
                  { x: 9, y: 3, text: "SQL DB", font: { color: "white" } },
                ],
              },
              { responsive: true }
            );
          },
          "p2s1ss4-plot1": () => {
            // From the Web
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p2s1ss4-plot1",
              [],
              {
                title: "Two Paths for Acquiring Web Data",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  // API Path
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 5,
                    x1: 2,
                    y1: 7,
                    fillcolor: "#3b82f6",
                  }, // R
                  {
                    type: "rect",
                    x0: 4,
                    y0: 5,
                    x1: 5.5,
                    y1: 7,
                    fillcolor: "#a855f7",
                  }, // API
                  {
                    type: "rect",
                    x0: 8,
                    y0: 5,
                    x1: 9.5,
                    y1: 7,
                    fillcolor: "#10b981",
                  }, // JSON
                  // Scraping Path
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 2,
                    y1: 3,
                    fillcolor: "#3b82f6",
                  }, // R
                  {
                    type: "rect",
                    x0: 4,
                    y0: 1,
                    x1: 5.5,
                    y1: 3,
                    fillcolor: "#f43f5e",
                  }, // Website
                  {
                    type: "rect",
                    x0: 8,
                    y0: 1,
                    x1: 9.5,
                    y1: 3,
                    fillcolor: "#eab308",
                  }, // Scraped

                  {
                    type: "path",
                    path: "M 2 6 L 4 6",
                    line: { width: 2, color: themeColors.textColor },
                  },
                  {
                    type: "path",
                    path: "M 5.5 6 L 8 6",
                    line: { width: 2, color: themeColors.textColor },
                  },
                  {
                    type: "path",
                    path: "M 2 2 L 4 2",
                    line: { width: 2, color: themeColors.textColor },
                  },
                  {
                    type: "path",
                    path: "M 5.5 2 L 8 2",
                    line: { width: 2, color: themeColors.textColor },
                  },
                ],
                annotations: [
                  {
                    x: 1.25,
                    y: 6,
                    text: "R Session",
                    font: { color: "white" },
                  },
                  { x: 3, y: 6.3, text: "httr2 request", showarrow: false },
                  { x: 4.75, y: 6, text: "API", font: { color: "white" } },
                  {
                    x: 6.75,
                    y: 6.3,
                    text: "Structured Response",
                    showarrow: false,
                  },
                  { x: 8.75, y: 6, text: "JSON", font: { color: "white" } },

                  {
                    x: 1.25,
                    y: 2,
                    text: "R Session",
                    font: { color: "white" },
                  },
                  { x: 3, y: 2.3, text: "rvest read_html", showarrow: false },
                  { x: 4.75, y: 2, text: "HTML", font: { color: "white" } },
                  {
                    x: 6.75,
                    y: 2.3,
                    text: "Extract Elements",
                    showarrow: false,
                  },
                  { x: 8.75, y: 2, text: "Data", font: { color: "white" } },
                ],
              },
              { responsive: true }
            );
          },
          "p2s2ss1-plot1": () => {
            // Automated Cleaning
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };
            Plotly.newPlot(
              "p2s2ss1-plot1",
              [],
              {
                title: "`janitor::clean_names()` Transformation",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 7],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "path",
                    path: "M 4.8 4 L 5.2 4",
                    line: { color: themeColors.textColor, width: 3 },
                  },
                ],
                annotations: [
                  {
                    x: 2.25,
                    y: 6,
                    text: "<b>Before</b>",
                    align: "center",
                    showarrow: false,
                  },
                  {
                    x: 0.2,
                    y: 5,
                    text: "<u>First Name</u>  <u>Profit ($)</u>  <u>hire-date</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.2,
                    y: 4,
                    text: "Alice        150         2022-03-10",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.2,
                    y: 3,
                    text: "Bob          200         2021-07-22",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5,
                    y: 4.5,
                    text: "➔",
                    showarrow: false,
                    font: { size: 24 },
                  },
                  {
                    x: 7.25,
                    y: 6,
                    text: "<b>After</b>",
                    align: "center",
                    showarrow: false,
                  },
                  {
                    x: 5.5,
                    y: 5,
                    text: "<u>first_name</u>  <u>profit_usd</u>  <u>hire_date</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5.5,
                    y: 4,
                    text: "Alice        150         2022-03-10",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5.5,
                    y: 3,
                    text: "Bob          200         2021-07-22",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s2ss2-plot1": () => {
            // Missing Value Treatment
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const fig = {
              data: [
                {
                  x: ["Ozone Only", "Ozone & Solar.R", "Solar.R Only"],
                  y: [35, 2, 5],
                  type: "bar",
                  marker: { color: themeColors.textColor },
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  x: ["Ozone", "Solar.R"],
                  y: [37, 7],
                  type: "bar",
                  marker: { color: "#3b82f6" },
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  x: [1, 2, 2, 3],
                  y: [2, 1, 2, 2],
                  mode: "markers",
                  marker: { color: themeColors.textColor, size: 8 },
                  xaxis: "x3",
                  yaxis: "y3",
                },
                {
                  x: [1, 2, 3],
                  y: [1, 1, 1],
                  mode: "markers",
                  marker: { color: isDark ? "#4b5563" : "#e5e7eb", size: 8 },
                  xaxis: "x3",
                  yaxis: "y3",
                },
                {
                  x: [1, 3],
                  y: [2, 1],
                  mode: "markers",
                  marker: { color: isDark ? "#4b5563" : "#e5e7eb", size: 8 },
                  xaxis: "x3",
                  yaxis: "y3",
                },
              ],
              layout: {
                title: "Enhanced Upset Plot for Missing Data",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 2, columns: 1, roworder: "top to bottom" },
                showlegend: false,
                yaxis1: {
                  title: "Combination Count",
                  domain: [0.55, 1],
                  gridcolor: themeColors.gridColor,
                },
                xaxis1: { showticklabels: false },
                yaxis2: {
                  title: "Total Missing",
                  domain: [0, 0.25],
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: { gridcolor: themeColors.gridColor },
                yaxis3: {
                  domain: [0.3, 0.5],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                xaxis3: {
                  showgrid: false,
                  zeroline: false,
                  tickvals: [1, 2, 3],
                  ticktext: ["Solar.R Only", "Both", "Ozone Only"],
                  tickangle: -25,
                },
              },
            };
            Plotly.newPlot("p2s2ss2-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p2s2ss3-plot1": () => {
            // Outlier Detection
            const prices = [];
            for (let i = 0; i < 100; i++) {
              prices.push(250000 + (Math.random() - 0.5) * 100000);
            }
            prices.push(10, 550000, 560000); // Add outliers
            prices.sort((a, b) => a - b);
            const q1 = prices[25];
            const q3 = prices[75];
            const iqr = q3 - q1;
            const upper_whisker = Math.min(q3 + 1.5 * iqr, prices[99]);
            const median = prices[50];

            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p2s2ss3-plot1",
              [
                {
                  y: prices,
                  type: "box",
                  name: "House Prices",
                  boxpoints: "outliers",
                  hoverinfo: "y",
                },
              ],
              {
                title: "Annotated Boxplot for Outlier Detection",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { showticklabels: false },
                yaxis: { title: "Price ($)", gridcolor: themeColors.gridColor },
                annotations: [
                  {
                    x: 0.3,
                    y: q3,
                    text: "3rd Quartile (Q3)",
                    showarrow: true,
                    ax: 60,
                    font: { size: 12 },
                  },
                  {
                    x: 0.3,
                    y: median,
                    text: "Median",
                    showarrow: true,
                    ax: 50,
                    font: { size: 12 },
                  },
                  {
                    x: 0.3,
                    y: q1,
                    text: "1st Quartile (Q1)",
                    showarrow: true,
                    ax: 60,
                    font: { size: 12 },
                  },
                  {
                    x: -0.35,
                    y: upper_whisker,
                    text: `Upper Whisker<br>(Q3 + 1.5*IQR)`,
                    showarrow: true,
                    ax: -100,
                    font: { size: 12 },
                  },
                  {
                    x: -0.35,
                    y: 555000,
                    text: "Outliers",
                    showarrow: true,
                    ax: -50,
                    ay: 0,
                    font: { size: 12 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s2ss4-plot1": () => {
            // Data Type Conversion
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };
            Plotly.newPlot(
              "p2s2ss4-plot1",
              [],
              {
                title: "Data Type Conversion Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 7],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "path",
                    path: "M 4 3.5 L 6 3.5",
                    line: { width: 3, color: themeColors.textColor },
                  },
                ],
                annotations: [
                  {
                    x: 2,
                    y: 6.5,
                    text: "<b>Input Tibble</b>",
                    showarrow: false,
                  },
                  {
                    x: 2,
                    y: 5,
                    text: "product_id: <dbl>",
                    font: monoFont,
                    showarrow: false,
                    xanchor: "center",
                  },
                  {
                    x: 2,
                    y: 4,
                    text: "123",
                    font: monoFont,
                    showarrow: false,
                    xanchor: "center",
                  },
                  {
                    x: 2,
                    y: 2,
                    text: "sale_date: <chr>",
                    font: monoFont,
                    showarrow: false,
                    xanchor: "center",
                  },
                  {
                    x: 2,
                    y: 1,
                    text: '"2025-01-10"',
                    font: monoFont,
                    showarrow: false,
                    xanchor: "center",
                  },
                  {
                    x: 8,
                    y: 6.5,
                    text: "<b>Output Tibble</b>",
                    showarrow: false,
                  },
                  {
                    x: 8,
                    y: 5,
                    text: "product_id: <chr>",
                    font: monoFont,
                    showarrow: false,
                    xanchor: "center",
                  },
                  {
                    x: 8,
                    y: 4,
                    text: '"123"',
                    font: monoFont,
                    showarrow: false,
                    xanchor: "center",
                  },
                  {
                    x: 8,
                    y: 2,
                    text: "sale_date: <Date>",
                    font: monoFont,
                    showarrow: false,
                    xanchor: "center",
                  },
                  {
                    x: 8,
                    y: 1,
                    text: "2025-01-10",
                    font: monoFont,
                    showarrow: false,
                    xanchor: "center",
                  },
                  {
                    x: 5,
                    y: 3.8,
                    text: "<code>mutate(...)</code>",
                    showarrow: false,
                    font: { size: 16 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s2ss6-plot1": () => {
            // High-Performance Wrangling
            const perf_data = {
              Method: ["dplyr", "data.table"],
              Time: [120, 5],
            };
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p2s2ss6-plot1",
              [
                {
                  x: perf_data.Method,
                  y: perf_data.Time,
                  type: "bar",
                  marker: { color: ["#3b82f6", "#10b981"] },
                },
              ],
              {
                title: "Performance Benchmark (Log Scale)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Package" },
                yaxis: {
                  title: "Execution Time (seconds)",
                  type: "log",
                  autorange: true,
                  gridcolor: themeColors.gridColor,
                },
                annotations: [
                  {
                    x: "data.table",
                    y: Math.log10(30),
                    text: `<b>~${Math.round(120 / 5)}x Faster</b>`,
                    showarrow: true,
                    arrowhead: 4,
                    ax: 0,
                    ay: -40,
                    font: { size: 16, color: "#10b981" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s2ss7-plot1": () => {
            // Data Reshaping
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };
            Plotly.newPlot(
              "p2s2ss7-plot1",
              [],
              {
                title: "Pivoting from Wide to Long Format",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "path",
                    path: "M 4 4 L 6 4",
                    line: { width: 3, color: themeColors.textColor },
                  },
                  {
                    type: "path",
                    path: "M 2, 4.5 C 3 6.5, 5 6.5, 6 5",
                    line: { color: "#f59e0b", width: 2, dash: "dot" },
                  },
                  {
                    type: "path",
                    path: "M 3, 4.5 C 4 6.5, 6 6.5, 7 4",
                    line: { color: "#ef4444", width: 2, dash: "dot" },
                  },
                ],
                annotations: [
                  {
                    x: 0,
                    y: 7,
                    text: "<b>Wide Table</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 0,
                    y: 6,
                    text: '<u>product</u>  <span style="color:#f59e0b;"><u>q1_sales</u></span>  <span style="color:#ef4444;"><u>q2_sales</u></span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0,
                    y: 5,
                    text: 'A        <span style="color:#f59e0b;">100</span>         <span style="color:#ef4444;">110</span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5,
                    y: 4.5,
                    text: "<code>pivot_longer()</code>",
                    showarrow: false,
                  },
                  {
                    x: 7,
                    y: 7,
                    text: "<b>Long Table</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 7,
                    y: 6,
                    text: "<u>product</u>  <u>quarter</u>   <u>sales</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7,
                    y: 5,
                    text: 'A        <span style="color:#f59e0b;">q1_sales</span>    <span style="color:#f59e0b;">100</span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7,
                    y: 4,
                    text: 'A        <span style="color:#ef4444;">q2_sales</span>    <span style="color:#ef4444;">110</span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s2ss8-plot1": () => {
            // Best Practices
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const nodes = [
              { x: 5, y: 8, text: "Load Raw Data" },
              { x: 8.5, y: 5, text: "Apply Cleaning Steps" },
              { x: 5, y: 2, text: "Validate Output" },
              { x: 1.5, y: 5, text: "Document Choices" },
              { x: 5, y: -1, text: "Save Clean Data" },
            ];
            Plotly.newPlot(
              "p2s2ss8-plot1",
              [
                {
                  x: nodes.map((n) => n.x),
                  y: nodes.map((n) => n.y),
                  mode: "markers",
                  marker: { size: 1, color: themeColors.bgColor },
                },
              ],
              {
                title: "The Iterative Data Cleaning Lifecycle",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor, size: 14 },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [-2, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "path",
                    path: "M 5,7.5 L 8,5.5",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 8,4.5 L 5.5,2.5",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 4.5,2.5 L 2,4.5",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 2,5.5 L 4.5,7.5",
                    line: {
                      color: themeColors.textColor,
                      width: 2,
                      dash: "dot",
                    },
                  },
                  {
                    type: "path",
                    path: "M 5,1.5 L 5, -0.5",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                ],
                annotations: nodes.map((n) => ({
                  x: n.x,
                  y: n.y,
                  text: `<b>${n.text}</b>`,
                  showarrow: false,
                  bgcolor: isDark ? "#2c3e50" : "#ecf0f1",
                  borderpad: 10,
                  bordercolor: themeColors.textColor,
                  borderwidth: 1,
                })),
              },
              { responsive: true }
            );
          },
          "p2s3ss2-plot1": () => {
            // Feature Selection
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p2s3ss2-plot1",
              [],
              {
                title: "Feature Selection Workflows",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  // Filter
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 7,
                    x1: 2,
                    y1: 8,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 2.5,
                    y0: 7,
                    x1: 4,
                    y1: 8,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 4.5,
                    y0: 7,
                    x1: 6,
                    y1: 8,
                    fillcolor: "#10b981",
                  },
                  // Wrapper
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4,
                    x1: 2,
                    y1: 5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 2.5,
                    y0: 4,
                    x1: 4,
                    y1: 5,
                    fillcolor: "#a855f7",
                  },
                  {
                    type: "rect",
                    x0: 4.5,
                    y0: 4,
                    x1: 6,
                    y1: 5,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 4,
                    x1: 8,
                    y1: 5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "path",
                    path: "M 2 4.5 L 2.5 4.5 M 4 4.5 L 4.5 4.5 M 6 4.5 L 6.5 4.5 M 2 4 C 1.5 3.5, 7 3.5, 6.5 4",
                    line: {
                      color: themeColors.textColor,
                      width: 2,
                      dash: "dot",
                    },
                  },
                  // Embedded
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 2,
                    y1: 2,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 2.5,
                    y0: 1,
                    x1: 4,
                    y1: 2,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "rect",
                    x0: 4.5,
                    y0: 1,
                    x1: 6,
                    y1: 2,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: -0.2,
                    y: 7.5,
                    text: "<b>Filter</b>",
                    showarrow: false,
                    xanchor: "right",
                  },
                  { x: 1.25, y: 7.5, text: "Data", font: { color: "white" } },
                  { x: 3.25, y: 7.5, text: "Stats", font: { color: "white" } },
                  {
                    x: 5.25,
                    y: 7.5,
                    text: "Features",
                    font: { color: "white" },
                  },
                  {
                    x: -0.2,
                    y: 4.5,
                    text: "<b>Wrapper</b>",
                    showarrow: false,
                    xanchor: "right",
                  },
                  { x: 1.25, y: 4.5, text: "Data", font: { color: "white" } },
                  { x: 3.25, y: 4.5, text: "Model", font: { color: "white" } },
                  { x: 5.25, y: 4.5, text: "Eval", font: { color: "white" } },
                  {
                    x: 7.25,
                    y: 4.5,
                    text: "Features",
                    font: { color: "white" },
                  },
                  {
                    x: -0.2,
                    y: 1.5,
                    text: "<b>Embedded</b>",
                    showarrow: false,
                    xanchor: "right",
                  },
                  { x: 1.25, y: 1.5, text: "Data", font: { color: "white" } },
                  { x: 3.25, y: 1.5, text: "Model", font: { color: "white" } },
                  {
                    x: 5.25,
                    y: 1.5,
                    text: "Features",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s3ss3-plot1": () => {
            // Feature Transformation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const ad_spend = [100, 200, 500, 1000, 2000, 5000, 10000];
            const sales = [50, 75, 120, 160, 200, 240, 260];
            const log_ad_spend = ad_spend.map((x) => Math.log(x));

            const trace1 = {
              x: ad_spend,
              y: sales,
              mode: "markers",
              type: "scatter",
              name: "Raw Data",
              xaxis: "x1",
              yaxis: "y1",
            };
            const trace2 = {
              x: log_ad_spend,
              y: sales,
              mode: "markers",
              type: "scatter",
              name: "Transformed Data",
              xaxis: "x2",
              yaxis: "y2",
            };

            const fig = {
              data: [trace1, trace2],
              layout: {
                title: "Effect of Log Transformation",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Ad Spend (Raw)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: { title: "Sales", gridcolor: themeColors.gridColor },
                xaxis2: {
                  title: "log(Ad Spend)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: { showticklabels: false },
              },
            };
            Plotly.newPlot("p2s3ss3-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p2s3ss4-plot1": () => {
            // Interaction Features
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const ad_spend = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
            const sales_normal = ad_spend.map(
              (x) => 50 + 5 * x + (Math.random() - 0.5) * 20
            );
            const sales_holiday = ad_spend.map(
              (x) => 50 + 5 * x + 200 + 10 * x + (Math.random() - 0.5) * 20
            );

            Plotly.newPlot(
              "p2s3ss4-plot1",
              [
                {
                  x: ad_spend,
                  y: sales_normal,
                  type: "scatter",
                  mode: "lines+markers",
                  name: "Normal Period",
                  line: { color: "#3b82f6" },
                },
                {
                  x: ad_spend,
                  y: sales_holiday,
                  type: "scatter",
                  mode: "lines+markers",
                  name: "Holiday Period",
                  line: { color: "#f59e0b" },
                },
              ],
              {
                title: "Interaction Effect of Ad Spend and Holiday",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Advertising Spend ($1000s)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: { title: "Sales", gridcolor: themeColors.gridColor },
                legend: { x: 0.05, y: 0.95 },
              },
              { responsive: true }
            );
          },
          "p2s3ss5-plot1": () => {
            // The recipes Package
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p2s3ss5-plot1",
              [],
              {
                title: "The `recipes` Workflow for Preprocessing",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3.5,
                    y0: 8,
                    x1: 6.5,
                    y1: 9.5,
                    fillcolor: "#a855f7",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4,
                    x1: 3.5,
                    y1: 5.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 3.5,
                    y1: 2.5,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 4,
                    x1: 9.5,
                    y1: 5.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 1,
                    x1: 9.5,
                    y1: 2.5,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "<b>1. The Recipe</b><br>(Blueprint)",
                    font: { color: "white" },
                  },
                  {
                    x: 2,
                    y: 4.75,
                    text: "Training Data",
                    font: { color: "white" },
                  },
                  {
                    x: 2,
                    y: 1.75,
                    text: "New Data<br>(e.g., Test Set)",
                    font: { color: "white" },
                  },
                  {
                    x: 8,
                    y: 4.75,
                    text: "Processed<br>Training Data",
                    font: { color: "white" },
                  },
                  {
                    x: 8,
                    y: 1.75,
                    text: "Processed<br>New Data",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 6.5,
                    text: "<b>2. prep(recipe, training_data)</b><br><i>Learns parameters</i>",
                    showarrow: true,
                    ax: 0,
                    ay: -40,
                  },
                  {
                    x: 5,
                    y: 4.75,
                    text: "<b>3. bake(...)</b><br><i>Applies recipe</i>",
                    showarrow: true,
                    ax: 20,
                    ay: 0,
                  },
                  {
                    x: 5,
                    y: 1.75,
                    text: "<b>3. bake(...)</b><br><i>Applies recipe</i>",
                    showarrow: true,
                    ax: 20,
                    ay: 0,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s3ss6-plot1": () => {
            // Dimensionality Reduction
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Simulated data points
            const data = {
              pca: {
                x: [2.5, 2, -1, -1.5, -0.5, 0.5, 1, 0, -2, -3, 1, 1.5],
                y: [1, 1.2, -1, -1.2, 1, -1, 1.5, -1.5, 0, 0, 0.5, -0.5],
                color: [
                  "#ef4444",
                  "#ef4444",
                  "#3b82f6",
                  "#3b82f6",
                  "#10b981",
                  "#10b981",
                  "#ef4444",
                  "#3b82f6",
                  "#3b82f6",
                  "#3b82f6",
                  "#10b981",
                  "#10b981",
                ],
              },
              tsne: {
                x: [8, 8.5, 1, 1.2, 5, 5.5, 8.2, 1.5, 1.8, 1.6, 5.2, 5.8],
                y: [8, 8.5, 2, 2.2, 5, 5.5, 8.8, 2.5, 2.8, 2.6, 5.2, 5.8],
                color: [
                  "#ef4444",
                  "#ef4444",
                  "#3b82f6",
                  "#3b82f6",
                  "#10b981",
                  "#10b981",
                  "#ef4444",
                  "#3b82f6",
                  "#3b82f6",
                  "#3b82f6",
                  "#10b981",
                  "#10b981",
                ],
              },
              umap: {
                x: [9, 9.5, 2, 2.2, 6, 6.5, 9.2, 2.5, 2.8, 2.6, 6.2, 6.8],
                y: [9, 9.5, 3, 3.2, 6, 6.5, 9.8, 3.5, 3.8, 3.6, 6.2, 6.8],
                color: [
                  "#ef4444",
                  "#ef4444",
                  "#3b82f6",
                  "#3b82f6",
                  "#10b981",
                  "#10b981",
                  "#ef4444",
                  "#3b82f6",
                  "#3b82f6",
                  "#3b82f6",
                  "#10b981",
                  "#10b981",
                ],
              },
            };
            const fig = {
              data: [
                {
                  ...data.pca,
                  mode: "markers",
                  type: "scatter",
                  name: "PCA",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: data.pca.color },
                },
                {
                  ...data.tsne,
                  mode: "markers",
                  type: "scatter",
                  name: "t-SNE",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: data.tsne.color },
                },
                {
                  ...data.umap,
                  mode: "markers",
                  type: "scatter",
                  name: "UMAP",
                  xaxis: "x3",
                  yaxis: "y3",
                  marker: { color: data.umap.color },
                },
              ],
              layout: {
                title: "Comparing Dimensionality Reduction for Visualization",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 3, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "PCA",
                  zeroline: false,
                  showticklabels: false,
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: {
                  zeroline: false,
                  showticklabels: false,
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "t-SNE",
                  zeroline: false,
                  showticklabels: false,
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: {
                  zeroline: false,
                  showticklabels: false,
                  gridcolor: themeColors.gridColor,
                },
                xaxis3: {
                  title: "UMAP",
                  zeroline: false,
                  showticklabels: false,
                  gridcolor: themeColors.gridColor,
                },
                yaxis3: {
                  zeroline: false,
                  showticklabels: false,
                  gridcolor: themeColors.gridColor,
                },
              },
            };
            Plotly.newPlot("p2s3ss6-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p2s3ss7-plot1": () => {
            // Best Practices
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p2s3ss7-plot1",
              [],
              {
                title: "Feature Engineering Decision Flow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4,
                    x1: 4.5,
                    y1: 6,
                    fillcolor: "#f59e0b",
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 4,
                    x1: 9.5,
                    y1: 6,
                    fillcolor: "#f59e0b",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 0.5,
                    x1: 4.5,
                    y1: 2,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 0.5,
                    x1: 9.5,
                    y1: 2,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "New Feature Idea",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 7,
                    text: "Does it use the target variable<br>(e.g., avg. target per category)?",
                    showarrow: true,
                    ax: 0,
                    ay: -50,
                  },
                  {
                    x: 2.5,
                    y: 5,
                    text: "<b>YES</b>",
                    font: { color: "white" },
                  },
                  { x: 7.5, y: 5, text: "<b>NO</b>", font: { color: "white" } },
                  {
                    x: 2.5,
                    y: 3,
                    text: "Was it calculated<br>BEFORE train/test split?",
                    showarrow: true,
                    ax: 0,
                    ay: -50,
                  },
                  {
                    x: 2.5,
                    y: 1.25,
                    text: "<b>DANGER:<br>DATA LEAKAGE</b>",
                    font: { color: "white", size: 16 },
                  },
                  {
                    x: 7.5,
                    y: 1.25,
                    text: "<b>SAFE:<br>PROCEED</b>",
                    font: { color: "white", size: 16 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s4ss1-plot1": () => {
            // Standardization
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const original_data = Array.from(
              { length: 1000 },
              () =>
                50 +
                (Math.random() -
                  0.5 +
                  Math.random() -
                  0.5 +
                  Math.random() -
                  0.5) *
                  20
            );
            const mean =
              original_data.reduce((a, b) => a + b) / original_data.length;
            const stdDev = Math.sqrt(
              original_data
                .map((x) => Math.pow(x - mean, 2))
                .reduce((a, b) => a + b) / original_data.length
            );
            const scaled_data = original_data.map((x) => (x - mean) / stdDev);

            const fig = {
              data: [
                {
                  x: original_data,
                  type: "histogram",
                  name: "Before",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#f59e0b" },
                },
                {
                  x: scaled_data,
                  type: "histogram",
                  name: "After",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#10b981" },
                },
              ],
              layout: {
                title: "Effect of Standardization (Z-Score Scaling)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Original Values",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: {
                  title: "Frequency",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "Standardized Values (Z-Scores)",
                  gridcolor: themeColors.gridColor,
                },
                annotations: [
                  {
                    x: mean,
                    y: 0.8,
                    xref: "x1",
                    yref: "paper",
                    text: `Mean ≈ ${mean.toFixed(1)}`,
                    showarrow: true,
                    arrowhead: 2,
                    ax: 0,
                    ay: -30,
                  },
                  {
                    x: 0,
                    y: 0.8,
                    xref: "x2",
                    yref: "paper",
                    text: "Mean = 0",
                    showarrow: true,
                    arrowhead: 2,
                    ax: 0,
                    ay: -30,
                  },
                ],
              },
            };
            Plotly.newPlot("p2s4ss1-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p2s4ss2-plot1": () => {
            // Normalization
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const original_data = Array.from(
              { length: 1000 },
              () => 50 + (Math.random() - 0.5 + Math.random() - 0.5) * 30
            );
            const min = Math.min(...original_data);
            const max = Math.max(...original_data);
            const scaled_data = original_data.map(
              (x) => (x - min) / (max - min)
            );

            const fig = {
              data: [
                {
                  x: original_data,
                  type: "histogram",
                  name: "Before",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#f59e0b" },
                },
                {
                  x: scaled_data,
                  type: "histogram",
                  name: "After",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#10b981" },
                },
              ],
              layout: {
                title: "Effect of Normalization (Min-Max Scaling)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Original Values",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: {
                  title: "Frequency",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "Normalized Values",
                  range: [-0.1, 1.1],
                  gridcolor: themeColors.gridColor,
                },
                annotations: [
                  {
                    x: min,
                    y: 0.8,
                    xref: "x1",
                    yref: "paper",
                    text: `Min ≈ ${min.toFixed(1)}`,
                    showarrow: false,
                    xanchor: "left",
                  },
                  {
                    x: max,
                    y: 0.8,
                    xref: "x1",
                    yref: "paper",
                    text: `Max ≈ ${max.toFixed(1)}`,
                    showarrow: false,
                    xanchor: "right",
                  },
                  {
                    x: 0,
                    y: 0.8,
                    xref: "x2",
                    yref: "paper",
                    text: "Min = 0",
                    showarrow: false,
                    xanchor: "left",
                  },
                  {
                    x: 1,
                    y: 0.8,
                    xref: "x2",
                    yref: "paper",
                    text: "Max = 1",
                    showarrow: false,
                    xanchor: "right",
                  },
                ],
              },
            };
            Plotly.newPlot("p2s4ss2-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p2s4ss3-plot1": () => {
            // Robust Scaling
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            let original_data = Array.from({ length: 1000 }, () =>
              Math.exp(Math.random() * 2.5)
            );
            original_data.push(500); // Add a large outlier

            const mean =
              original_data.reduce((a, b) => a + b) / original_data.length;
            const stdDev = Math.sqrt(
              original_data
                .map((x) => Math.pow(x - mean, 2))
                .reduce((a, b) => a + b) / original_data.length
            );
            const standard_scaled = original_data.map(
              (x) => (x - mean) / stdDev
            );

            const sorted = [...original_data].sort((a, b) => a - b);
            const median = sorted[500];
            const q1 = sorted[250];
            const q3 = sorted[750];
            const iqr = q3 - q1;
            const robust_scaled = original_data.map((x) => (x - median) / iqr);

            const fig = {
              data: [
                {
                  x: standard_scaled,
                  type: "histogram",
                  name: "Standard Scaled",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#ef4444" },
                },
                {
                  x: robust_scaled,
                  type: "histogram",
                  name: "Robust Scaled",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#10b981" },
                },
              ],
              layout: {
                title: "Standard vs. Robust Scaling with Outlier",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Standard Scaled (Z-Score)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: {
                  title: "Frequency",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "Robust Scaled (Median/IQR)",
                  gridcolor: themeColors.gridColor,
                },
                annotations: [
                  {
                    x: -0.5,
                    y: 0.8,
                    xref: "x1",
                    yref: "paper",
                    text: "Bulk of data<br>squished by outlier",
                    showarrow: true,
                    ax: -50,
                    ay: -40,
                  },
                  {
                    x: 0,
                    y: 0.8,
                    xref: "x2",
                    yref: "paper",
                    text: "Bulk of data<br>centered properly",
                    showarrow: true,
                    ax: 50,
                    ay: -40,
                  },
                ],
              },
            };
            Plotly.newPlot("p2s4ss3-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p2s4ss4-plot1": () => {
            // Best Practices (Scaling)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p2s4ss4-plot1",
              [],
              {
                title: "The Correct Preprocessing Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  // Incorrect Path
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 7,
                    x1: 2.5,
                    y1: 8,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 7,
                    x1: 5,
                    y1: 8,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 7,
                    x1: 7.5,
                    y1: 8,
                    fillcolor: "#eab308",
                  },
                  // Correct Path
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 2.5,
                    x1: 2.5,
                    y1: 3.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 2.5,
                    x1: 5,
                    y1: 3.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 1,
                    x1: 7.5,
                    y1: 2,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 4,
                    x1: 7.5,
                    y1: 5,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 9,
                    text: '<b><span style="color: #ef4444;">INCORRECT WORKFLOW</span></b>',
                    showarrow: false,
                  },
                  {
                    x: 1.5,
                    y: 7.5,
                    text: "Full Data",
                    font: { color: "white" },
                  },
                  { x: 4, y: 7.5, text: "Scale", font: { color: "white" } },
                  { x: 6.5, y: 7.5, text: "Split", font: { color: "white" } },
                  {
                    x: 8.5,
                    y: 7.5,
                    text: "<b>Data Leakage!</b>",
                    font: { color: "#ef4444" },
                  },
                  {
                    x: 5,
                    y: 5.5,
                    text: '<b><span style="color: #10b981;">CORRECT WORKFLOW</span></b>',
                    showarrow: false,
                  },
                  { x: 1.5, y: 3, text: "Full Data", font: { color: "white" } },
                  { x: 4, y: 3, text: "Split", font: { color: "white" } },
                  {
                    x: 6.5,
                    y: 4.5,
                    text: "Train Set",
                    font: { color: "white" },
                  },
                  {
                    x: 6.5,
                    y: 1.5,
                    text: "Test Set",
                    font: { color: "white" },
                  },
                  {
                    x: 8.5,
                    y: 3,
                    text: "<b>Safe!</b>",
                    font: { color: "#10b981" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s5ss1-plot1": () => {
            // One-Hot Encoding
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };
            Plotly.newPlot(
              "p2s5ss1-plot1",
              [],
              {
                title: "One-Hot Encoding Transformation",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "path",
                    path: "M 4 4 L 6 4",
                    line: { width: 3, color: themeColors.textColor },
                  },
                ],
                annotations: [
                  {
                    x: 0,
                    y: 7,
                    text: "<b>Before</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 0,
                    y: 6,
                    text: "<u>id</u>  <u>color</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0,
                    y: 5,
                    text: '1    <span style="color:#ef4444;">Red</span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0,
                    y: 4,
                    text: '2    <span style="color:#10b981;">Green</span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0,
                    y: 3,
                    text: '3    <span style="color:#3b82f6;">Blue</span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5,
                    y: 4.5,
                    text: "➔",
                    showarrow: false,
                    font: { size: 24 },
                  },
                  {
                    x: 7,
                    y: 7,
                    text: "<b>After</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 7,
                    y: 6,
                    text: "<u>id</u> <u>c_Red</u> <u>c_Green</u> <u>c_Blue</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7,
                    y: 5,
                    text: '1   <span style="color:#ef4444;">1</span>       <span style="color:#ef4444;">0</span>       <span style="color:#ef4444;">0</span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7,
                    y: 4,
                    text: '2   <span style="color:#10b981;">0</span>       <span style="color:#10b981;">1</span>       <span style="color:#10b981;">0</span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7,
                    y: 3,
                    text: '3   <span style="color:#3b82f6;">0</span>       <span style="color:#3b82f6;">0</span>       <span style="color:#3b82f6;">1</span>',
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s5ss2-plot1": () => {
            // Label & Ordinal Encoding
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };
            Plotly.newPlot(
              "p2s5ss2-plot1",
              [],
              {
                title: "Ordinal Encoding Transformation",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "path",
                    path: "M 4 4 L 6 4",
                    line: { width: 3, color: themeColors.textColor },
                  },
                ],
                annotations: [
                  {
                    x: 0,
                    y: 7,
                    text: "<b>Before</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 0,
                    y: 6,
                    text: "<u>rating</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0,
                    y: 5,
                    text: "Good",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0,
                    y: 4,
                    text: "Poor",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0,
                    y: 3,
                    text: "Excellent",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5,
                    y: 4.5,
                    text: "➔",
                    showarrow: false,
                    font: { size: 24 },
                  },
                  {
                    x: 7,
                    y: 7,
                    text: "<b>After</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 7,
                    y: 6,
                    text: "<u>rating_encoded</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7,
                    y: 5,
                    text: "2",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7,
                    y: 4,
                    text: "1",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7,
                    y: 3,
                    text: "3",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s5ss3-plot1": () => {
            // Target Encoding
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };
            Plotly.newPlot(
              "p2s5ss3-plot1",
              [],
              {
                title: "Target Encoding Transformation",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "path",
                    path: "M 4 4 L 6 4",
                    line: { width: 3, color: themeColors.textColor },
                  },
                ],
                annotations: [
                  {
                    x: 0.5,
                    y: 7,
                    text: "<b>Original Data</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 0.5,
                    y: 6,
                    text: "<u>City</u>     <u>Churn</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 5,
                    text: "Boston     1",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 4,
                    text: "Denver     0",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 3,
                    text: "Boston     1",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 2,
                    text: "Boston     0",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 1,
                    text: "Denver     0",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5,
                    y: 4.5,
                    text: "➔",
                    showarrow: false,
                    font: { size: 24 },
                  },
                  {
                    x: 7.5,
                    y: 7,
                    text: "<b>Encoded Data</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 6,
                    text: "<u>City_Encoded</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 5,
                    text: "0.67",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 4,
                    text: "0.00",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 3,
                    text: "0.67",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 2,
                    text: "0.67",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 1,
                    text: "0.00",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5,
                    y: 6,
                    text: "mean(churn)<br>for Boston = 2/3 = 0.67",
                    showarrow: false,
                  },
                  {
                    x: 5,
                    y: 2.5,
                    text: "mean(churn)<br>for Denver = 0/2 = 0.00",
                    showarrow: false,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s5ss4-plot1": () => {
            // Frequency Encoding
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };
            Plotly.newPlot(
              "p2s5ss4-plot1",
              [],
              {
                title: "Frequency Encoding Transformation",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "path",
                    path: "M 4 4 L 6 4",
                    line: { width: 3, color: themeColors.textColor },
                  },
                ],
                annotations: [
                  {
                    x: 0.5,
                    y: 7,
                    text: "<b>Original Data</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 0.5,
                    y: 6,
                    text: "<u>City</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 5,
                    text: "Boston",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 4,
                    text: "Denver",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 3,
                    text: "Boston",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 2,
                    text: "Boston",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.5,
                    y: 1,
                    text: "Denver",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5,
                    y: 4.5,
                    text: "➔",
                    showarrow: false,
                    font: { size: 24 },
                  },
                  {
                    x: 7.5,
                    y: 7,
                    text: "<b>Encoded Data</b>",
                    align: "left",
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 6,
                    text: "<u>City_Encoded</u>",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 5,
                    text: "3",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 4,
                    text: "2",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 3,
                    text: "3",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 2,
                    text: "3",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 7.5,
                    y: 1,
                    text: "2",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  { x: 5, y: 6, text: "count(Boston) = 3", showarrow: false },
                  { x: 5, y: 2.5, text: "count(Denver) = 2", showarrow: false },
                ],
              },
              { responsive: true }
            );
          },
          "p2s5ss5-plot1": () => {
            // Best Practices for Encoding
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p2s5ss5-plot1",
              [],
              {
                title: "Categorical Encoding Decision Tree",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4.5,
                    x1: 3,
                    y1: 6.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 4.5,
                    x1: 6,
                    y1: 6.5,
                    fillcolor: "#f59e0b",
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 4.5,
                    x1: 9.5,
                    y1: 6.5,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 1,
                    x1: 6,
                    y1: 3,
                    fillcolor: "#ef4444",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "Is the variable ORDINAL?",
                    font: { color: "white" },
                  },
                  {
                    x: 1.75,
                    y: 5.5,
                    text: "YES:<br>Ordinal<br>Encoding",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 5.5,
                    text: "NO (Nominal):<br>Check Cardinality",
                    font: { color: "white" },
                  },
                  {
                    x: 8.25,
                    y: 5.5,
                    text: "LOW:<br>One-Hot<br>Encoding",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 2,
                    text: "HIGH:<br>Target/Freq<br>Encoding",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s6ss1-plot1": () => {
            // Unsupervised Binning
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const skewed_data = Array.from({ length: 1000 }, () =>
              Math.round(Math.exp(10 + Math.random() * 1))
            );

            const fig = {
              data: [
                {
                  x: skewed_data,
                  type: "histogram",
                  name: "Original",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#6366f1" },
                },
                {
                  x: skewed_data,
                  type: "histogram",
                  name: "Equal Width",
                  xaxis: "x2",
                  yaxis: "y2",
                  nbinsx: 4,
                  marker: { color: "#ef4444" },
                },
                {
                  x: skewed_data,
                  type: "histogram",
                  name: "Equal Frequency",
                  xbins: { size: 0, start: 0, end: 0 },
                  autobinx: false,
                  marker: { color: "#10b981" },
                  xaxis: "x3",
                  yaxis: "y3",
                },
              ],
              layout: {
                title: "Equal Width vs. Equal Frequency Binning on Skewed Data",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 3, columns: 1, pattern: "independent" },
                showlegend: false,
                yaxis1: {
                  title: "Original<br>Distribution",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: {
                  title: "Equal<br>Width",
                  gridcolor: themeColors.gridColor,
                },
                yaxis3: {
                  title: "Equal<br>Frequency",
                  gridcolor: themeColors.gridColor,
                },
                xaxis3: { title: "Income", gridcolor: themeColors.gridColor },
                annotations: [
                  {
                    x: 0.95,
                    y: 0.5,
                    xref: "x2 domain",
                    yref: "y2 domain",
                    text: "Outlier forces most data<br>into a single bin",
                    showarrow: false,
                    font: { color: "#ef4444" },
                  },
                  {
                    x: 0.95,
                    y: 0.5,
                    xref: "x3 domain",
                    yref: "y3 domain",
                    text: "Bins have equal counts,<br>capturing the distribution better",
                    showarrow: false,
                    font: { color: "#10b981" },
                  },
                ],
              },
            };
            // Manually set quantile bins for the third plot
            const sorted = [...skewed_data].sort((a, b) => a - b);
            fig.data[2].xbins.start = sorted[0];
            fig.data[2].xbins.end = sorted[sorted.length - 1];
            const boundaries = [
              sorted[0],
              sorted[250],
              sorted[500],
              sorted[750],
              sorted[sorted.length - 1],
            ];
            fig.data[2].xbins.size = boundaries
              .map((b, i) => (i > 0 ? b - boundaries[i - 1] : 0))
              .slice(1);

            Plotly.newPlot("p2s6ss1-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p2s6ss2-plot1": () => {
            // Supervised Binning
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const age = Array.from(
              { length: 200 },
              () => 20 + Math.random() * 50
            );
            const default_status = age
              .map((a) =>
                a < 28 || a > 60
                  ? 1 + (Math.random() - 0.8)
                  : 0 + (Math.random() - 0.2)
              )
              .map((v) => Math.max(0, Math.min(1, v)));

            Plotly.newPlot(
              "p2s6ss2-plot1",
              [
                {
                  x: age,
                  y: default_status,
                  mode: "markers",
                  type: "scatter",
                  marker: {
                    color: default_status,
                    colorscale: [
                      ["0", "#3b82f6"],
                      ["1", "#ef4444"],
                    ],
                    opacity: 0.6,
                  },
                },
              ],
              {
                title: "Supervised Binning Using Decision Tree Splits",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Age", gridcolor: themeColors.gridColor },
                yaxis: {
                  title: "Probability of Default",
                  range: [-0.2, 1.2],
                  gridcolor: themeColors.gridColor,
                },
                showlegend: false,
                shapes: [
                  {
                    type: "line",
                    x0: 28,
                    y0: -0.2,
                    x1: 28,
                    y1: 1.2,
                    line: { color: "#10b981", width: 3, dash: "dash" },
                  },
                  {
                    type: "line",
                    x0: 60,
                    y0: -0.2,
                    x1: 60,
                    y1: 1.2,
                    line: { color: "#10b981", width: 3, dash: "dash" },
                  },
                ],
                annotations: [
                  { x: 28, y: 1.1, text: "Optimal Split 1", ax: -50, ay: -30 },
                  { x: 60, y: 1.1, text: "Optimal Split 2", ax: 50, ay: -30 },
                  {
                    x: 24,
                    y: 0.5,
                    text: "Bin 1<br>(High Risk)",
                    font: { color: "white" },
                    bgcolor: "rgba(239, 68, 68, 0.7)",
                    borderpad: 4,
                  },
                  {
                    x: 44,
                    y: 0.5,
                    text: "Bin 2<br>(Low Risk)",
                    font: { color: "white" },
                    bgcolor: "rgba(59, 130, 246, 0.7)",
                    borderpad: 4,
                  },
                  {
                    x: 65,
                    y: 0.5,
                    text: "Bin 3<br>(High Risk)",
                    font: { color: "white" },
                    bgcolor: "rgba(239, 68, 68, 0.7)",
                    borderpad: 4,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s7ss1-plot1": () => {
            // Parsing & Manipulation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };
            Plotly.newPlot(
              "p2s7ss1-plot1",
              [],
              {
                title: "Parsing Inconsistent Strings to a Standard Date",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 7,
                    y0: 4,
                    x1: 9.5,
                    y1: 6,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: 0,
                    y: 8,
                    text: "<b>Raw Strings</b>",
                    showarrow: false,
                    xanchor: "left",
                  },
                  {
                    x: 0,
                    y: 7,
                    text: '"2025-09-18"',
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 0,
                    y: 5,
                    text: '"09/18/2025"',
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 0,
                    y: 3,
                    text: '"18-Sep-2025"',
                    showarrow: false,
                    xanchor: "left",
                    font: monoFont,
                  },
                  {
                    x: 4.5,
                    y: 7,
                    text: "➔ <code>ymd()</code>",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 4.5,
                    y: 5,
                    text: "➔ <code>mdy()</code>",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 4.5,
                    y: 3,
                    text: "➔ <code>dmy()</code>",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 8.25,
                    y: 5,
                    text: "<b>R Date Object</b><br>2025-09-18",
                    showarrow: false,
                    font: { color: "white", size: 14 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s7ss2-plot1": () => {
            // Feature Extraction
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const months = Array.from({ length: 12 }, (_, i) => i + 1);
            const month_sin = months.map((m) =>
              Math.sin((2 * Math.PI * m) / 12)
            );
            const month_cos = months.map((m) =>
              Math.cos((2 * Math.PI * m) / 12)
            );
            const month_labels = [
              "Jan",
              "Feb",
              "Mar",
              "Apr",
              "May",
              "Jun",
              "Jul",
              "Aug",
              "Sep",
              "Oct",
              "Nov",
              "Dec",
            ];

            Plotly.newPlot(
              "p2s7ss2-plot1",
              [
                {
                  x: month_cos,
                  y: month_sin,
                  text: month_labels,
                  type: "scatter",
                  mode: "markers+text",
                  marker: { size: 12, color: "#3b82f6" },
                  textposition: "top center",
                },
              ],
              {
                title: "Cyclical Feature Encoding for Months",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Month (Cosine Component)",
                  range: [-1.2, 1.2],
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Month (Sine Component)",
                  range: [-1.2, 1.2],
                  gridcolor: themeColors.gridColor,
                  scaleanchor: "x",
                },
                shapes: [
                  {
                    type: "circle",
                    xref: "x",
                    yref: "y",
                    x0: -1,
                    y0: -1,
                    x1: 1,
                    y1: 1,
                    line: { color: themeColors.gridColor, dash: "dot" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p2s7ss3-plot1": () => {
            // Time Zone Management
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p2s7ss3-plot1",
              [],
              {
                title: "One Instant, Many Local Times",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "circle",
                    x0: 4,
                    y0: 4,
                    x1: 6,
                    y1: 6,
                    fillcolor: "#10b981",
                  },
                  { type: "line", x0: 5, y0: 6, x1: 5, y1: 8.5 },
                  { type: "line", x0: 3.5, y0: 3.5, x1: 1, y1: 1.5 },
                  { type: "line", x0: 6.5, y0: 3.5, x1: 9, y1: 1.5 },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 5,
                    text: "<b>UTC<br>15:00</b>",
                    font: { color: "white", size: 16 },
                  },
                  {
                    x: 5,
                    y: 9,
                    text: "<b>London (GMT)</b><br>15:00",
                    bgcolor: "rgba(255,255,255,0.7)",
                  },
                  {
                    x: 0.5,
                    y: 1,
                    text: "<b>New York (EDT)</b><br>10:00",
                    bgcolor: "rgba(255,255,255,0.7)",
                  },
                  {
                    x: 9.5,
                    y: 1,
                    text: "<b>Tokyo (JST)</b><br>24:00",
                    bgcolor: "rgba(255,255,255,0.7)",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p3s1ss1-plot1": () => {
            // Data Summarization
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };

            const fig = {
              data: [],
              layout: {
                title: "Comparison of Summary Outputs",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 4.5,
                    y1: 8,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 1,
                    x1: 9.5,
                    y1: 8,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                ],
                annotations: [
                  // summary() output
                  {
                    x: 2.5,
                    y: 8.5,
                    text: "<b>Base R `summary()`</b>",
                    showarrow: false,
                  },
                  {
                    x: 0.7,
                    y: 7,
                    text: "Ozone",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.7,
                    y: 6.2,
                    text: "Min.   :  1.00",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.7,
                    y: 5.4,
                    text: "1st Qu.: 18.00",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.7,
                    y: 4.6,
                    text: "Median : 31.50",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.7,
                    y: 3.8,
                    text: "Mean   : 42.13",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.7,
                    y: 3.0,
                    text: "3rd Qu.: 63.25",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.7,
                    y: 2.2,
                    text: "Max.   :168.00",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 0.7,
                    y: 1.4,
                    text: "NA's   :37",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },

                  // skimr::skim() output
                  {
                    x: 7.5,
                    y: 8.5,
                    text: "<b>`skimr::skim()`</b>",
                    showarrow: false,
                  },
                  {
                    x: 5.7,
                    y: 7,
                    text: "Variable type: numeric",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5.7,
                    y: 6,
                    text: "skim_variable n_missing complete_rate mean   sd",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5.7,
                    y: 5,
                    text: "Ozone                37         0.758  42.1  33.0",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5.7,
                    y: 4,
                    text: " p0    p25   p50   p75  p100   hist",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                  {
                    x: 5.7,
                    y: 3,
                    text: "  1   18    31.5  63.2  168   ▇▃  ",
                    align: "left",
                    showarrow: false,
                    font: monoFont,
                  },
                ],
              },
            };
            Plotly.newPlot("p3s1ss1-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p3s1ss3-plot1": () => {
            // Data Quality Assessment
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const data = [
              {
                x: ["Ozone", "Solar.R", "Wind", "Temp", "Month", "Day"],
                y: [37, 7, 0, 0, 0, 0],
                type: "bar",
                marker: { color: "#3b82f6" },
              },
            ];

            const layout = {
              title: "Missing Values per Column in `airquality`",
              paper_bgcolor: themeColors.bgColor,
              plot_bgcolor: themeColors.bgColor,
              font: { color: themeColors.textColor },
              xaxis: { title: "Variable" },
              yaxis: {
                title: "Count of Missing Values",
                gridcolor: themeColors.gridColor,
              },
            };

            Plotly.newPlot("p3s1ss3-plot1", data, layout, { responsive: true });
          },
          "p3s1ss4-plot1": () => {
            // Relationship Analysis
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };

            const cor_matrix = [
              [1.0, -0.85, -0.85, 0.68, -0.78, 0.6],
              [-0.85, 1.0, 0.96, -0.66, 0.7, -0.52],
              [-0.85, 0.96, 1.0, -0.71, 0.75, -0.59],
              [0.68, -0.66, -0.71, 1.0, -0.75, 0.61],
              [-0.78, 0.7, 0.75, -0.75, 1.0, -0.71],
              [0.6, -0.52, -0.59, 0.61, -0.71, 1.0],
            ];
            const labels = ["mpg", "cyl", "disp", "hp", "wt", "qsec"];

            const data = [
              {
                z: cor_matrix,
                x: labels,
                y: labels,
                type: "heatmap",
                colorscale: "RdBu",
                reversescale: true,
                zmin: -1,
                zmax: 1,
              },
            ];

            const layout = {
              title: "Correlation Heatmap of mtcars Dataset",
              paper_bgcolor: themeColors.bgColor,
              plot_bgcolor: themeColors.bgColor,
              font: { color: themeColors.textColor },
            };

            Plotly.newPlot("p3s1ss4-plot1", data, layout, { responsive: true });
          },
          "p3s1ss5-plot1": () => {
            // Distribution Assessment
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Simulate right-skewed Ozone data
            const ozone_data = Array.from({ length: 116 }, () =>
              Math.exp(2.5 + Math.random() * 1.5)
            );

            const fig = {
              data: [
                {
                  x: ozone_data,
                  type: "histogram",
                  name: "Histogram",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#3b82f6" },
                },
                {
                  x: ozone_data,
                  type: "box",
                  name: "Boxplot",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#ef4444" },
                },
                // Density plot needs calculated values
                // We'll simulate a density trace for visualization
              ],
              layout: {
                title: "Three Views of a Skewed Distribution (Ozone)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 2, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  domain: [0, 0.45],
                  title: "Histogram",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: { domain: [0.6, 1], gridcolor: themeColors.gridColor },
                xaxis2: {
                  domain: [0.55, 1],
                  showticklabels: false,
                  title: "Boxplot",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: { domain: [0.6, 1], gridcolor: themeColors.gridColor },
                xaxis3: {
                  domain: [0, 1],
                  title: "Density Plot",
                  gridcolor: themeColors.gridColor,
                },
                yaxis3: { domain: [0, 0.45], gridcolor: themeColors.gridColor },
              },
            };
            // Add a simulated density trace
            const density_x = [
              ...new Set(ozone_data.map((x) => Math.round(x / 5) * 5)),
            ].sort((a, b) => a - b);
            const density_y = Array.from({ length: density_x.length }, () =>
              Math.random()
            ); // simplified for viz
            fig.data.push({
              x: density_x,
              y: density_y,
              type: "scatter",
              mode: "lines",
              fill: "tozeroy",
              name: "Density",
              xaxis: "x3",
              yaxis: "y3",
              line: { color: "#10b981" },
            });

            Plotly.newPlot("p3s1ss5-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p3s2ss1-plot1": () => {
            // Automated Reporting
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f1f5f9",
            };
            Plotly.newPlot(
              "p3s2ss1-plot1",
              [],
              {
                title: "Anatomy of a DataExplorer Report",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 0.5,
                    x1: 3.5,
                    y1: 9.5,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 5.5,
                    x1: 9.5,
                    y1: 9.5,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 0.5,
                    x1: 9.5,
                    y1: 4.5,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                ],
                annotations: [
                  {
                    x: 2,
                    y: 9,
                    text: "<b>Sidebar Menu</b>",
                    showarrow: false,
                    yanchor: "top",
                  },
                  {
                    x: 2,
                    y: 8,
                    text: "• Basic Statistics",
                    showarrow: false,
                    xanchor: "left",
                  },
                  {
                    x: 2,
                    y: 7.2,
                    text: "• Univariate Analysis",
                    showarrow: false,
                    xanchor: "left",
                  },
                  {
                    x: 2,
                    y: 6.4,
                    text: "• Correlation",
                    showarrow: false,
                    xanchor: "left",
                  },
                  {
                    x: 2,
                    y: 5.6,
                    text: "• Missing Data",
                    showarrow: false,
                    xanchor: "left",
                  },
                  {
                    x: 6.75,
                    y: 9,
                    text: "<b>Main Content Area</b>",
                    showarrow: false,
                    yanchor: "top",
                  },
                  {
                    x: 6.75,
                    y: 7.5,
                    text: "Interactive Plots",
                    showarrow: false,
                  },
                  { x: 6.75, y: 2.5, text: "Summary Tables", showarrow: false },
                ],
              },
              { responsive: true }
            );
          },
          "p3s2ss3-plot1": () => {
            // Best Practices (for AutoEDA)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p3s2ss3-plot1",
              [],
              {
                title: "The Correct EDA Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 12],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 10,
                    x1: 7,
                    y1: 11,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 7.5,
                    x1: 7,
                    y1: 8.5,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 5,
                    x1: 7,
                    y1: 6,
                    fillcolor: "#a855f7",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 2.5,
                    x1: 7,
                    y1: 3.5,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 0,
                    x1: 7,
                    y1: 1,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  { x: 5, y: 10.5, text: "Raw Data", font: { color: "white" } },
                  {
                    x: 5,
                    y: 9.25,
                    text: "⇩",
                    showarrow: false,
                    font: { size: 20 },
                  },
                  {
                    x: 5,
                    y: 8,
                    text: "Run AutoEDA Tool",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 6.75,
                    text: "⇩",
                    showarrow: false,
                    font: { size: 20 },
                  },
                  {
                    x: 5,
                    y: 5.5,
                    text: "Generate Hypotheses",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 4.25,
                    text: "⇩",
                    showarrow: false,
                    font: { size: 20 },
                  },
                  {
                    x: 5,
                    y: 3,
                    text: "Deep Dive & Rigorous Testing",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 1.75,
                    text: "⇩",
                    showarrow: false,
                    font: { size: 20 },
                  },
                  {
                    x: 5,
                    y: 0.5,
                    text: "Conclusions & Insights",
                    font: { color: "white" },
                  },
                  {
                    x: 1.5,
                    y: 5.5,
                    text: "<b>The Human Analyst</b>",
                    showarrow: true,
                    ax: -40,
                    ay: 0,
                    xanchor: "right",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p3s3ss1-plot1": () => {
            // Hypothesis Formulation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
              lineColor: isDark ? "#495057" : "#dee2e6",
            };
            Plotly.newPlot(
              "p3s3ss1-plot1",
              [],
              {
                title: {
                  text: "Hypothesis Formulation Flow",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 2.5,
                    x1: 4.5,
                    y1: 5.5,
                    fillcolor: themeColors.boxColor,
                    line: { color: themeColors.lineColor, width: 1 },
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 2.5,
                    x1: 9.5,
                    y1: 5.5,
                    fillcolor: themeColors.boxColor,
                    line: { color: themeColors.lineColor, width: 1 },
                  },
                  {
                    type: "path",
                    path: "M 5 8 L 2.5 5.5 M 5 8 L 7.5 5.5",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "<b>Start: Research Question</b>",
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 4.7,
                    text: "<b>Null Hypothesis (H₀)</b><br>The Status Quo / No Effect",
                    showarrow: false,
                  },
                  {
                    x: 2.5,
                    y: 3.3,
                    text: "<i>Example: μ ≤ 150</i>",
                    showarrow: false,
                    font: { size: 12 },
                  },
                  {
                    x: 7.5,
                    y: 4.7,
                    text: "<b>Alternative Hypothesis (Hₐ)</b><br>The New Claim / An Effect Exists",
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 3.3,
                    text: "<i>Example: μ > 150</i>",
                    showarrow: false,
                    font: { size: 12 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p3s3ss2-plot1": () => {
            // Common Statistical Tests
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
              lineColor: isDark ? "#495057" : "#dee2e6",
            };
            Plotly.newPlot(
              "p3s3ss2-plot1",
              [],
              {
                title: {
                  text: "Decision Tree for Common Tests",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 12],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 4,
                    y0: 8,
                    x1: 8,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 1,
                    y0: 5,
                    x1: 5,
                    y1: 6,
                    fillcolor: themeColors.boxColor,
                    line: { color: themeColors.lineColor },
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 5,
                    x1: 11,
                    y1: 6,
                    fillcolor: themeColors.boxColor,
                    line: { color: themeColors.lineColor },
                  },
                  {
                    type: "rect",
                    x0: 0,
                    y0: 1,
                    x1: 3,
                    y1: 2,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 3.5,
                    y0: 1,
                    x1: 6.5,
                    y1: 2,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 9,
                    y0: 1,
                    x1: 12,
                    y1: 2,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "path",
                    path: "M 6 8 L 3 6 M 6 8 L 9 6",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 1.5 5 L 1.5 2 M 4.5 5 L 4.5 2",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 10.5 5 L 10.5 2",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                ],
                annotations: [
                  {
                    x: 6,
                    y: 8.75,
                    text: "<b>Research Question</b>",
                    font: { color: "white" },
                  },
                  { x: 3, y: 5.5, text: "Comparing Means?" },
                  { x: 9, y: 5.5, text: "Testing Relationship?" },
                  { x: 1.5, y: 4, text: "2 Groups" },
                  { x: 4.5, y: 4, text: "3+ Groups" },
                  { x: 10.5, y: 4, text: "2 Categorical Vars" },
                  { x: 1.5, y: 1.5, text: "T-Test", font: { color: "white" } },
                  { x: 5, y: 1.5, text: "ANOVA", font: { color: "white" } },
                  {
                    x: 10.5,
                    y: 1.5,
                    text: "Chi-Squared",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p3s3ss3-plot1": () => {
            // Interpreting Results
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const x = [];
            const y_h0 = [];
            const y_ha = [];
            const mean_h0 = 0;
            const mean_ha = 2.5;
            const sd = 1;
            const critical_value = 1.645;
            const observed_stat = 2.1;
            for (let i = -4; i <= 7; i += 0.05) {
              x.push(i);
              y_h0.push(
                (1 / (sd * Math.sqrt(2 * Math.PI))) *
                  Math.exp(-0.5 * Math.pow((i - mean_h0) / sd, 2))
              );
              y_ha.push(
                (1 / (sd * Math.sqrt(2 * Math.PI))) *
                  Math.exp(-0.5 * Math.pow((i - mean_ha) / sd, 2))
              );
            }

            Plotly.newPlot(
              "p3s3ss3-plot1",
              [
                {
                  x,
                  y: y_h0,
                  type: "scatter",
                  mode: "lines",
                  name: "H₀ Distribution",
                  line: { color: "#3b82f6" },
                },
                {
                  x,
                  y: y_ha,
                  type: "scatter",
                  mode: "lines",
                  name: "Hₐ Distribution",
                  line: { color: "#f59e0b" },
                },
                {
                  x: x.filter((v) => v >= observed_stat),
                  y: y_h0.slice(x.findIndex((v) => v >= observed_stat)),
                  fill: "tozeroy",
                  type: "scatter",
                  mode: "none",
                  name: "p-value",
                  fillcolor: "rgba(239, 68, 68, 0.5)",
                },
                {
                  x: x.filter((v) => v >= critical_value),
                  y: y_ha.slice(x.findIndex((v) => v >= critical_value)),
                  fill: "tozeroy",
                  type: "scatter",
                  mode: "none",
                  name: "Power",
                  fillcolor: "rgba(22, 163, 74, 0.5)",
                },
              ],
              {
                title: "Interpreting Statistical Test Results",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Test Statistic / Effect Size",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: { title: "Density", gridcolor: themeColors.gridColor },
                legend: { x: 0.6, y: 0.95 },
                shapes: [
                  {
                    type: "line",
                    x0: critical_value,
                    y0: 0,
                    x1: critical_value,
                    y1: 0.4,
                    line: { color: themeColors.textColor, dash: "dot" },
                  },
                ],
                annotations: [
                  {
                    x: observed_stat,
                    y: 0.01,
                    text: "p-value",
                    ay: -30,
                    arrowhead: 2,
                    ax: 20,
                    font: { color: "#ef4444" },
                  },
                  {
                    x: 3.5,
                    y: 0.15,
                    text: "Power",
                    font: { color: "#16a34a" },
                  },
                  {
                    x: 1.25,
                    y: 0.25,
                    text: "Effect<br>Size",
                    showarrow: true,
                    xanchor: "center",
                    yanchor: "bottom",
                    ax: mean_ha * 20,
                    ay: 0,
                    arrowhead: 0,
                    arrowtail: 0,
                    arrowwidth: 2,
                    arrowcolor: themeColors.textColor,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p3s3ss4-plot1": () => {
            // Test Assumptions
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const normal_data = Array.from(
              { length: 100 },
              () => (Math.random() + Math.random() + Math.random() - 1.5) * 2
            ).sort((a, b) => a - b);
            const skewed_data = Array.from({ length: 100 }, () =>
              Math.exp(Math.random() * 2)
            ).sort((a, b) => a - b);

            const fig = {
              data: [
                {
                  x: normal_data,
                  y: normal_data,
                  mode: "markers",
                  name: "Normal",
                  marker: { color: "#10b981" },
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  x: normal_data.map((d) => d * 0.9 + 0.1),
                  y: skewed_data,
                  mode: "markers",
                  name: "Skewed",
                  marker: { color: "#ef4444" },
                  xaxis: "x2",
                  yaxis: "y2",
                },
              ],
              layout: {
                title: "Checking for Normality with Q-Q Plots",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Theoretical Quantiles",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: {
                  title: "Sample Quantiles",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "Theoretical Quantiles",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: {
                  title: "Sample Quantiles",
                  gridcolor: themeColors.gridColor,
                },
                shapes: [
                  {
                    type: "line",
                    xref: "x1",
                    yref: "y1",
                    x0: -4,
                    y0: -4,
                    x1: 4,
                    y1: 4,
                    line: { color: themeColors.textColor, dash: "dash" },
                  },
                  {
                    type: "line",
                    xref: "x2",
                    yref: "y2",
                    x0: -4,
                    y0: -4,
                    x1: 4,
                    y1: 4,
                    line: { color: themeColors.textColor, dash: "dash" },
                  },
                ],
                annotations: [
                  {
                    x: -1,
                    y: 3,
                    xref: "x1",
                    yref: "y1",
                    text: "Good Fit: Normality Met",
                    bgcolor: "rgba(22, 163, 74, 0.7)",
                    font: { color: "white" },
                  },
                  {
                    x: -1,
                    y: 7,
                    xref: "x2",
                    yref: "y2",
                    text: "Poor Fit: Normality Violated",
                    bgcolor: "rgba(239, 68, 68, 0.7)",
                    font: { color: "white" },
                  },
                ],
              },
            };
            Plotly.newPlot("p3s3ss4-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p3s3ss5-plot1": () => {
            // Multiple Testing
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const original_p = Array.from(
              { length: 100 },
              (_, i) => (i + 1) / 100
            );
            const p_bonferroni = original_p.map((p) => Math.min(p * 100, 1));
            const p_fdr = original_p.map((p, i) =>
              Math.min((p * 100) / (i + 1), 1)
            );

            Plotly.newPlot(
              "p3s3ss5-plot1",
              [
                {
                  x: [0, 1],
                  y: [0, 1],
                  type: "scatter",
                  mode: "lines",
                  name: "No Correction",
                  line: { color: themeColors.textColor, dash: "dot" },
                },
                {
                  x: original_p,
                  y: p_bonferroni,
                  type: "scatter",
                  mode: "lines",
                  name: "Bonferroni (Strict)",
                  line: { color: "#ef4444" },
                },
                {
                  x: original_p,
                  y: p_fdr,
                  type: "scatter",
                  mode: "lines",
                  name: "FDR (Powerful)",
                  line: { color: "#10b981" },
                },
              ],
              {
                title: "Comparing p-value Correction Methods",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Original p-value",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Adjusted p-value",
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.05, y: 0.95 },
              },
              { responsive: true }
            );
          },
          "p3s3ss6-plot1": () => {
            // Best Practices (Hypothesis Testing)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
              lineColor: isDark ? "#495057" : "#dee2e6",
            };
            Plotly.newPlot(
              "p3s3ss6-plot1",
              [],
              {
                title: {
                  text: "A Responsible Hypothesis Testing Workflow",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  // Nodes
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4.5,
                    x1: 3.5,
                    y1: 6.5,
                    fillcolor: themeColors.boxColor,
                    line: { color: themeColors.lineColor },
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 4.5,
                    x1: 6,
                    y1: 6.5,
                    fillcolor: themeColors.boxColor,
                    line: { color: themeColors.lineColor },
                  },
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 1,
                    x1: 9.5,
                    y1: 6.5,
                    fillcolor: themeColors.boxColor,
                    line: { color: themeColors.lineColor },
                  },
                  // Arrows
                  {
                    type: "path",
                    path: "M 5 8 L 5 6.5",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 2 4.5 L 2 2.5 L 8 2.5 L 8 1",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 5 4.5 L 5 2.5 L 8 2.5 L 8 1",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "<b>1. BEFORE THE TEST</b>",
                    font: { color: "white" },
                  },
                  { x: 2, y: 5.5, text: "State H₀ / Hₐ" },
                  { x: 5, y: 5.5, text: "Choose Alpha (α)" },
                  {
                    x: 8,
                    y: 5.5,
                    text: "<b>2. DURING THE TEST</b><br>• Check Assumptions<br>• Run ONE Planned Test",
                  },
                  {
                    x: 8,
                    y: 0.5,
                    text: "<b>3. AFTER THE TEST</b><br>• Report p-value<br>• Report Effect Size<br>• Report Confidence Interval<br>• Make Conclusion",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p3s4ss1-plot1": () => {
            // Principles of Effective Visualization
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const pie_values = [15, 30, 45, 10];
            const pie_labels = ["A", "B", "C", "D"];
            const bar_data = [
              {
                x: pie_labels,
                y: pie_values,
                type: "bar",
                marker: { color: "#10b981" },
              },
            ];

            const fig = {
              data: [
                {
                  values: pie_values,
                  labels: pie_labels,
                  type: "pie",
                  domain: { x: [0, 0.45], y: [0, 1] },
                  hole: 0.4,
                  textinfo: "label",
                },
                ...bar_data,
              ],
              layout: {
                title: "Ineffective vs. Effective Visualization",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                annotations: [
                  {
                    text: "Poor: 3D Pie Chart<br>(Hard to compare)",
                    x: 0.18,
                    y: 1.1,
                    xref: "paper",
                    yref: "paper",
                    showarrow: false,
                    font: { size: 14 },
                  },
                  {
                    text: "Good: Bar Chart<br>(Easy to compare)",
                    x: 0.82,
                    y: 1.1,
                    xref: "paper",
                    yref: "paper",
                    showarrow: false,
                    font: { size: 14 },
                  },
                ],
                xaxis2: {
                  domain: [0.55, 1],
                  anchor: "y2",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: {
                  domain: [0, 1],
                  anchor: "x2",
                  title: "Value",
                  gridcolor: themeColors.gridColor,
                },
              },
            };
            // Note: Plotly JS doesn't support 3D pie charts, so we use a 2D donut as a stand-in for a "bad" chart.
            Plotly.newPlot("p3s4ss1-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p3s4ss2-plot1": () => {
            // Foundational Plot Types
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const fig = {
              data: [
                {
                  x: ["setosa", "versicolor", "virginica"],
                  y: [3.43, 2.77, 2.97],
                  type: "bar",
                  name: "Bar",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#3b82f6" },
                },
                {
                  x: [1.4, 1.4, 4.3, 4.5, 5.1, 5.6],
                  type: "histogram",
                  name: "Histogram",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#10b981" },
                },
                {
                  x: [1.4, 4.3, 5.6],
                  y: [0.2, 1.3, 2.5],
                  mode: "markers",
                  type: "scatter",
                  name: "Scatter",
                  xaxis: "x3",
                  yaxis: "y3",
                  marker: { color: "#ef4444" },
                },
                {
                  x: [1, 2, 3, 4, 5],
                  y: [2, 3, 5, 4, 6],
                  type: "scatter",
                  mode: "lines",
                  name: "Line",
                  xaxis: "x4",
                  yaxis: "y4",
                  line: { color: "#f59e0b" },
                },
              ],
              layout: {
                title: "The Four Foundational Plot Types",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 2, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Bar Chart (Comparison)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: { gridcolor: themeColors.gridColor },
                xaxis2: {
                  title: "Histogram (Distribution)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: { gridcolor: themeColors.gridColor },
                xaxis3: {
                  title: "Scatter Plot (Relationship)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis3: { gridcolor: themeColors.gridColor },
                xaxis4: {
                  title: "Line Plot (Trend over Time)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis4: { gridcolor: themeColors.gridColor },
              },
            };
            Plotly.newPlot("p3s4ss2-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p3s4ss3-plot1": () => {
            // Distributional Plots
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Bimodal data for versicolor
            const versicolor_data = Array.from({ length: 50 }, () =>
              Math.random() < 0.5
                ? 2.5 + Math.random() * 0.5
                : 3.2 + Math.random() * 0.5
            );
            const normal_data = Array.from(
              { length: 100 },
              () => (Math.random() + Math.random() + Math.random() - 1.5) * 2
            ).sort((a, b) => a - b);

            const fig = {
              data: [
                {
                  y: versicolor_data,
                  type: "violin",
                  name: "Violin",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#8b5cf6" },
                },
                {
                  y: versicolor_data,
                  type: "box",
                  name: "Boxplot",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#8b5cf6" },
                },
                {
                  x: normal_data,
                  y: normal_data,
                  mode: "markers",
                  name: "Normal",
                  marker: { color: "#10b981" },
                  xaxis: "x3",
                  yaxis: "y3",
                },
              ],
              layout: {
                title: "Advanced Distributional Plots",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 3, pattern: "independent" },
                showlegend: false,
                xaxis1: { title: "Violin Plot", showticklabels: false },
                yaxis1: { title: "Value", gridcolor: themeColors.gridColor },
                xaxis2: { title: "Boxplot", showticklabels: false },
                yaxis2: { gridcolor: themeColors.gridColor },
                xaxis3: { title: "Q-Q Plot", gridcolor: themeColors.gridColor },
                yaxis3: { gridcolor: themeColors.gridColor },
                shapes: [
                  {
                    type: "line",
                    xref: "x3",
                    yref: "y3",
                    x0: -4,
                    y0: -4,
                    x1: 4,
                    y1: 4,
                    line: { color: themeColors.textColor, dash: "dash" },
                  },
                ],
              },
            };
            Plotly.newPlot("p3s4ss3-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p3s4ss4-plot1": () => {
            // Advanced & Custom Plots
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p3s4ss4-plot1",
              [
                {
                  type: "waterfall",
                  measure: [
                    "absolute",
                    "relative",
                    "relative",
                    "total",
                    "relative",
                    "total",
                  ],
                  x: [
                    "Revenue",
                    "Costs",
                    "Tax",
                    "Gross Profit",
                    "Other Income",
                    "Net Profit",
                  ],
                  y: [120, -50, -10, 0, 20, 0],
                  text: ["+120", "-50", "-10", "60", "+20", "80"],
                  textposition: "outside",
                  connector: { line: { color: isDark ? "grey" : "black" } },
                  increasing: { marker: { color: "#10b981" } },
                  decreasing: { marker: { color: "#ef4444" } },
                  totals: { marker: { color: "#3b82f6" } },
                },
              ],
              {
                title: "Waterfall Chart for Profit Breakdown",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Category" },
                yaxis: {
                  title: "Amount ($)",
                  gridcolor: themeColors.gridColor,
                },
              },
              { responsive: true }
            );
          },
          "p3s4ss5-plot1": () => {
            // Complex Visualizations
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const faithful = { waiting: [], eruptions: [] };
            for (let i = 0; i < 272; i++) {
              const wait =
                Math.random() < 0.5
                  ? 54 + Math.random() * 15
                  : 80 + Math.random() * 15;
              const erupt = wait * 0.05 + (Math.random() - 0.5);
              faithful.waiting.push(wait);
              faithful.eruptions.push(erupt);
            }
            Plotly.newPlot(
              "p3s4ss5-plot1",
              [
                {
                  x: faithful.waiting,
                  y: faithful.eruptions,
                  mode: "markers",
                  type: "scatter",
                  name: "Eruptions",
                  marker: { color: themeColors.textColor, opacity: 0.5 },
                },
                {
                  x: faithful.waiting,
                  y: faithful.eruptions,
                  type: "histogram2dcontour",
                  name: "Density",
                  showscale: false,
                  colorscale: "Blues",
                },
                {
                  x: [40, 100],
                  y: [1.8, 5.3],
                  type: "scatter",
                  mode: "lines",
                  name: "Trend Line",
                  line: { color: "#ef4444", width: 3 },
                },
              ],
              {
                title: "Multi-Layered Plot: Eruption Duration vs. Waiting Time",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Waiting Time (mins)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Eruption Duration (mins)",
                  gridcolor: themeColors.gridColor,
                },
                showlegend: false,
              },
              { responsive: true }
            );
          },
          "p3s4ss6-plot1": () => {
            // Best Practices (Viz)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p3s4ss6-plot1",
              [],
              {
                title: {
                  text: "The Anatomy of a Great Plot",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                annotations: [
                  {
                    x: 5,
                    y: 8,
                    text: "<b>Clear, Informative Title</b><br>States the main finding",
                    showarrow: true,
                    arrowhead: 2,
                    ax: 0,
                    ay: -40,
                  },
                  {
                    x: 1,
                    y: 5,
                    text: "<b>Clear Axis Labels</b><br>(with units)",
                    showarrow: true,
                    arrowhead: 2,
                    ax: 50,
                    ay: 0,
                  },
                  {
                    x: 5,
                    y: 1,
                    text: "<b>Source/Caption</b><br>Adds credibility",
                    showarrow: true,
                    arrowhead: 2,
                    ax: 0,
                    ay: 40,
                  },
                  {
                    x: 9,
                    y: 5,
                    text: "<b>Purposeful Annotations</b><br>Highlight key points",
                    showarrow: true,
                    arrowhead: 2,
                    ax: -50,
                    ay: -30,
                  },
                  {
                    x: 5,
                    y: 5,
                    text: "<b>The Data</b><br>(Clean Geoms,<br>Good Color Choice)",
                    showarrow: false,
                    font: { size: 16 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p3s5ss2-plot1": () => {
            // Visual Storytelling
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const mtcars_data = {
              wt: [2.62, 2.875, 2.32, 3.215, 3.44, 1.513],
              mpg: [21, 21.4, 22.8, 18.7, 18.1, 33.9],
            };
            const highlight_data = { wt: [1.513], mpg: [33.9] };
            const fig = {
              data: [
                {
                  x: mtcars_data.wt,
                  y: mtcars_data.mpg,
                  mode: "markers",
                  type: "scatter",
                  name: "Exploratory",
                  marker: { color: themeColors.textColor },
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  x: mtcars_data.wt,
                  y: mtcars_data.mpg,
                  mode: "markers",
                  type: "scatter",
                  name: "Explanatory",
                  marker: { color: "grey", opacity: 0.5 },
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  x: highlight_data.wt,
                  y: highlight_data.mpg,
                  mode: "markers",
                  type: "scatter",
                  name: "Highlight",
                  marker: { color: "#ef4444", size: 12 },
                  xaxis: "x2",
                  yaxis: "y2",
                },
              ],
              layout: {
                title: "From Exploration to Explanation",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Exploratory Plot",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: { title: "mpg", gridcolor: themeColors.gridColor },
                xaxis2: {
                  title: "Explanatory Plot",
                  gridcolor: themeColors.gridColor,
                },
                annotations: [
                  {
                    x: 2.5,
                    y: 33,
                    xref: "x2",
                    yref: "y2",
                    text: "Lightweight cars are<br>most efficient",
                    ax: -50,
                    ay: -40,
                    font: { color: "#ef4444" },
                  },
                ],
              },
            };
            Plotly.newPlot("p3s5ss2-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s1ss1-plot1": () => {
            // Simple & Multiple Linear Regression
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
              lineColor: isDark ? "#adb5bd" : "#6c757d",
            };
            const sq_m = Array.from(
              { length: 50 },
              () => 50 + Math.random() * 200
            );
            const price = sq_m.map(
              (s) => 50000 + s * 1500 + (Math.random() - 0.5) * 100000
            );
            const intercept = 50000;
            const slope = 1500;
            const line_x = [0, 250];
            const line_y = [intercept, intercept + slope * 250];

            Plotly.newPlot(
              "p4s1ss1-plot1",
              [
                {
                  x: sq_m,
                  y: price,
                  mode: "markers",
                  type: "scatter",
                  name: "Houses",
                  marker: { color: "#3b82f6", opacity: 0.7 },
                },
                {
                  x: line_x,
                  y: line_y,
                  mode: "lines",
                  type: "scatter",
                  name: "Regression Line",
                  line: { color: "#ef4444", width: 3 },
                },
              ],
              {
                title: "Interpreting a Simple Linear Regression Model",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Area (Square Meters)",
                  range: [0, 300],
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Price (PKR)",
                  gridcolor: themeColors.gridColor,
                },
                showlegend: false,
                shapes: [
                  {
                    type: "line",
                    x0: 0,
                    y0: intercept,
                    x1: 25,
                    y1: intercept,
                    line: { color: themeColors.lineColor, dash: "dot" },
                  },
                  {
                    type: "line",
                    x0: 150,
                    y0: line_y[0],
                    x1: 150,
                    y1: intercept + slope * 150,
                    line: { color: themeColors.lineColor, dash: "dot" },
                  },
                  {
                    type: "line",
                    x0: 151,
                    y0: intercept + slope * 150,
                    x1: 200,
                    y1: intercept + slope * 150,
                    line: { color: themeColors.lineColor, dash: "dot" },
                  },
                  {
                    type: "line",
                    x0: 200,
                    y0: intercept + slope * 150,
                    x1: 200,
                    y1: intercept + slope * 200,
                    line: { color: themeColors.lineColor, dash: "dot" },
                  },
                ],
                annotations: [
                  {
                    x: 25,
                    y: intercept,
                    text: `Intercept (β₀) ≈ ${intercept}`,
                    showarrow: true,
                    ax: 80,
                    ay: -20,
                  },
                  {
                    x: 175,
                    y: intercept + slope * 175,
                    text: `Slope (β₁)<br>Rise / Run`,
                    showarrow: false,
                    bgcolor: "rgba(255,255,255,0.7)",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s1ss2-plot1": () => {
            // Generalized Linear Models (GLM)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const x = [];
            const y = [];
            for (let i = -8; i <= 8; i += 0.2) {
              x.push(i);
              y.push(1 / (1 + Math.exp(-i)));
            }
            Plotly.newPlot(
              "p4s1ss2-plot1",
              [
                {
                  x: x,
                  y: y,
                  type: "scatter",
                  mode: "lines",
                  line: { color: "#8b5cf6", width: 4 },
                },
              ],
              {
                title: "The Sigmoid Curve of the Logit Link Function",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Linear Predictor (β₀ + β₁X)",
                  gridcolor: themeColors.gridColor,
                  zeroline: true,
                },
                yaxis: {
                  title: "Predicted Probability",
                  range: [-0.1, 1.1],
                  gridcolor: themeColors.gridColor,
                  zeroline: false,
                },
                shapes: [
                  {
                    type: "line",
                    x0: -8,
                    y0: 1,
                    x1: 8,
                    y1: 1,
                    line: { color: themeColors.textColor, dash: "dot" },
                  },
                  {
                    type: "line",
                    x0: -8,
                    y0: 0,
                    x1: 8,
                    y1: 0,
                    line: { color: themeColors.textColor, dash: "dot" },
                  },
                ],
                annotations: [
                  {
                    x: 4,
                    y: 0.8,
                    text: "Output is always<br>between 0 and 1",
                    showarrow: true,
                    ax: 0,
                    ay: -40,
                  },
                  {
                    x: -4,
                    y: 0.2,
                    text: "Negative inputs map<br>to probabilities < 0.5",
                    showarrow: true,
                    ax: 0,
                    ay: 40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s1ss3-plot1": () => {
            // Polynomial Regression
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const fertilizer = Array.from({ length: 50 }, (_, i) => i);
            const yield_val = fertilizer.map(
              (x) => 10 + 5 * x - 0.1 * x * x + (Math.random() - 0.5) * 20
            );

            Plotly.newPlot(
              "p4s1ss3-plot1",
              [
                {
                  x: fertilizer,
                  y: yield_val,
                  mode: "markers",
                  type: "scatter",
                  name: "Data",
                  marker: { color: themeColors.textColor, opacity: 0.6 },
                },
                {
                  x: fertilizer,
                  y: fertilizer.map((x) => 45 + 1.5 * x),
                  mode: "lines",
                  type: "scatter",
                  name: "Linear Fit (Poor)",
                  line: { color: "#3b82f6", dash: "dot", width: 3 },
                },
                {
                  x: fertilizer,
                  y: fertilizer.map((x) => 10 + 5 * x - 0.1 * x * x),
                  mode: "lines",
                  type: "scatter",
                  name: "Polynomial Fit (Good)",
                  line: { color: "#ef4444", width: 4 },
                },
              ],
              {
                title: "Fitting a Non-Linear Relationship",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Fertilizer Amount",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Crop Yield",
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.05, y: 0.1 },
              },
              { responsive: true }
            );
          },
          "p4s1ss4-plot1": () => {
            // Regularized Regression
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Simulate coefficient paths
            const lambda = Array.from({ length: 50 }, (_, i) => i / 49);
            const paths = {
              wt: lambda.map((l) => -3 * (1 - l)),
              cyl: lambda.map((l) => -1 * Math.max(0, 1 - l * 2)),
              hp: lambda.map((l) => -0.5 * Math.max(0, 1 - l * 1.5)),
              gear: lambda.map((l) => 0.2 * Math.max(0, 1 - l * 4)),
            };
            Plotly.newPlot(
              "p4s1ss4-plot1",
              [
                {
                  x: lambda,
                  y: paths.wt,
                  mode: "lines",
                  name: "wt",
                  line: { width: 3 },
                },
                {
                  x: lambda,
                  y: paths.cyl,
                  mode: "lines",
                  name: "cyl",
                  line: { width: 3 },
                },
                {
                  x: lambda,
                  y: paths.hp,
                  mode: "lines",
                  name: "hp",
                  line: { width: 3 },
                },
                {
                  x: lambda,
                  y: paths.gear,
                  mode: "lines",
                  name: "gear",
                  line: { width: 3 },
                },
              ],
              {
                title: "LASSO Coefficient Path Plot",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Penalty (Lambda)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Coefficient Value",
                  gridcolor: themeColors.gridColor,
                  zeroline: true,
                  zerolinecolor: themeColors.textColor,
                },
                annotations: [
                  {
                    x: 0.7,
                    y: 1,
                    text: "Features shrink to zero",
                    ax: -50,
                    ay: -40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s1ss5-plot1": () => {
            // Robust & Quantile Regression
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const x = [];
            const y = [];
            for (let i = 0; i < 50; i++) {
              x.push(Math.random() * 10);
              y.push(5 + 2 * x[i] + (Math.random() - 0.5) * 5);
            }
            const outlier = { x: [9], y: [45] }; // Add outlier

            Plotly.newPlot(
              "p4s1ss5-plot1",
              [
                {
                  x: x,
                  y: y,
                  mode: "markers",
                  type: "scatter",
                  name: "Data",
                  marker: { color: themeColors.textColor, opacity: 0.5 },
                },
                {
                  x: outlier.x,
                  y: outlier.y,
                  mode: "markers",
                  type: "scatter",
                  name: "Outlier",
                  marker: { color: "#ef4444", size: 12, symbol: "cross" },
                },
                {
                  x: [0, 10],
                  y: [9, 31],
                  mode: "lines",
                  type: "scatter",
                  name: "OLS (Skewed by Outlier)",
                  line: { color: "#3b82f6", dash: "dot", width: 3 },
                },
                {
                  x: [0, 10],
                  y: [5, 25],
                  mode: "lines",
                  type: "scatter",
                  name: "Robust / Median Fit",
                  line: { color: "#10b981", width: 4 },
                },
              ],
              {
                title: "Effect of an Outlier on Regression Models",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Predictor (X)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Outcome (Y)",
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.05, y: 0.95 },
              },
              { responsive: true }
            );
          },
          "p4s1ss6-plot1": () => {
            // Model Selection
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const complexity = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
            const train_error = complexity.map((c) => 1 / c + 0.1);
            const test_error = complexity.map(
              (c) => 1 / c + 0.1 + Math.pow(c - 4, 2) / 20 - 0.5
            );

            Plotly.newPlot(
              "p4s1ss6-plot1",
              [
                {
                  x: complexity,
                  y: train_error,
                  mode: "lines",
                  name: "Training Error",
                  line: { color: "#3b82f6", width: 3 },
                },
                {
                  x: complexity,
                  y: test_error,
                  mode: "lines",
                  name: "Test Error",
                  line: { color: "#f59e0b", width: 4 },
                },
              ],
              {
                title: "The Bias-Variance Tradeoff",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Model Complexity",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Error",
                  range: [0, 2],
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.7, y: 0.95 },
                shapes: [
                  {
                    type: "line",
                    x0: 4,
                    y0: 0,
                    x1: 4,
                    y1: 2,
                    line: { dash: "dot" },
                  },
                ],
                annotations: [
                  { x: 2, y: 1.5, text: "High Bias<br>(Underfitting)" },
                  { x: 8, y: 1.5, text: "High Variance<br>(Overfitting)" },
                  { x: 4, y: 0.1, text: "Optimal Model", ay: -40 },
                ],
              },
              { responsive: true }
            );
          },
          "p4s1ss7-plot1": () => {
            // Diagnostics & Validation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const normal_data = Array.from(
              { length: 100 },
              () => (Math.random() + Math.random() + Math.random() - 1.5) * 2
            ).sort((a, b) => a - b);

            const fig = {
              data: [
                {
                  x: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  y: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  mode: "markers",
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  x: normal_data,
                  y: normal_data,
                  mode: "markers",
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  x: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  y: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  mode: "markers",
                  xaxis: "x3",
                  yaxis: "y3",
                },
                {
                  x: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  y: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  mode: "markers",
                  xaxis: "x4",
                  yaxis: "y4",
                },
              ],
              layout: {
                title: "Standard Linear Model Diagnostic Plots",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor, size: 10 },
                grid: { rows: 2, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Fitted values",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: {
                  title: "Residuals",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "Theoretical Quantiles",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: {
                  title: "Standardized residuals",
                  gridcolor: themeColors.gridColor,
                },
                xaxis3: {
                  title: "Fitted values",
                  gridcolor: themeColors.gridColor,
                },
                yaxis3: {
                  title: "√|Standardized residuals|",
                  gridcolor: themeColors.gridColor,
                },
                xaxis4: { title: "Leverage", gridcolor: themeColors.gridColor },
                yaxis4: {
                  title: "Standardized residuals",
                  gridcolor: themeColors.gridColor,
                },
                shapes: [
                  {
                    type: "line",
                    xref: "x2",
                    yref: "y2",
                    x0: -4,
                    y0: -4,
                    x1: 4,
                    y1: 4,
                    line: { color: themeColors.textColor, dash: "dash" },
                  },
                ],
                annotations: [
                  {
                    xref: "paper",
                    yref: "paper",
                    x: 0.225,
                    y: 1.05,
                    text: "Residuals vs. Fitted",
                    showarrow: false,
                  },
                  {
                    xref: "paper",
                    yref: "paper",
                    x: 0.775,
                    y: 1.05,
                    text: "Normal Q-Q",
                    showarrow: false,
                  },
                  {
                    xref: "paper",
                    yref: "paper",
                    x: 0.225,
                    y: 0.45,
                    text: "Scale-Location",
                    showarrow: false,
                  },
                  {
                    xref: "paper",
                    yref: "paper",
                    x: 0.775,
                    y: 0.45,
                    text: "Residuals vs. Leverage",
                    showarrow: false,
                  },
                ],
              },
            };
            Plotly.newPlot("p4s1ss7-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s1ss3-plot1": () => {
            // Polynomial Regression
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const fertilizer = Array.from({ length: 50 }, (_, i) => i);
            const yield_val = fertilizer.map(
              (x) => 10 + 5 * x - 0.1 * x * x + (Math.random() - 0.5) * 20
            );

            Plotly.newPlot(
              "p4s1ss3-plot1",
              [
                {
                  x: fertilizer,
                  y: yield_val,
                  mode: "markers",
                  type: "scatter",
                  name: "Data",
                  marker: { color: themeColors.textColor, opacity: 0.6 },
                },
                {
                  x: fertilizer,
                  y: fertilizer.map((x) => 45 + 1.5 * x),
                  mode: "lines",
                  type: "scatter",
                  name: "Linear Fit (Poor)",
                  line: { color: "#3b82f6", dash: "dot", width: 3 },
                },
                {
                  x: fertilizer,
                  y: fertilizer.map((x) => 10 + 5 * x - 0.1 * x * x),
                  mode: "lines",
                  type: "scatter",
                  name: "Polynomial Fit (Good)",
                  line: { color: "#ef4444", width: 4 },
                },
              ],
              {
                title: "Fitting a Non-Linear Relationship",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Fertilizer Amount",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Crop Yield",
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.05, y: 0.1 },
              },
              { responsive: true }
            );
          },
          "p4s1ss4-plot1": () => {
            // Regularized Regression
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Simulate coefficient paths
            const lambda = Array.from({ length: 50 }, (_, i) => i / 49);
            const paths = {
              wt: lambda.map((l) => -3 * (1 - l)),
              cyl: lambda.map((l) => -1 * Math.max(0, 1 - l * 2)),
              hp: lambda.map((l) => -0.5 * Math.max(0, 1 - l * 1.5)),
              gear: lambda.map((l) => 0.2 * Math.max(0, 1 - l * 4)),
            };
            Plotly.newPlot(
              "p4s1ss4-plot1",
              [
                {
                  x: lambda,
                  y: paths.wt,
                  mode: "lines",
                  name: "wt",
                  line: { width: 3 },
                },
                {
                  x: lambda,
                  y: paths.cyl,
                  mode: "lines",
                  name: "cyl",
                  line: { width: 3 },
                },
                {
                  x: lambda,
                  y: paths.hp,
                  mode: "lines",
                  name: "hp",
                  line: { width: 3 },
                },
                {
                  x: lambda,
                  y: paths.gear,
                  mode: "lines",
                  name: "gear",
                  line: { width: 3 },
                },
              ],
              {
                title: "LASSO Coefficient Path Plot",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Penalty (Lambda)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Coefficient Value",
                  gridcolor: themeColors.gridColor,
                  zeroline: true,
                  zerolinecolor: themeColors.textColor,
                },
                annotations: [
                  {
                    x: 0.7,
                    y: 1,
                    text: "Features shrink to zero",
                    ax: -50,
                    ay: -40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s1ss5-plot1": () => {
            // Robust & Quantile Regression
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const x = [];
            const y = [];
            for (let i = 0; i < 50; i++) {
              x.push(Math.random() * 10);
              y.push(5 + 2 * x[i] + (Math.random() - 0.5) * 5);
            }
            const outlier = { x: [9], y: [45] }; // Add outlier

            Plotly.newPlot(
              "p4s1ss5-plot1",
              [
                {
                  x: x,
                  y: y,
                  mode: "markers",
                  type: "scatter",
                  name: "Data",
                  marker: { color: themeColors.textColor, opacity: 0.5 },
                },
                {
                  x: outlier.x,
                  y: outlier.y,
                  mode: "markers",
                  type: "scatter",
                  name: "Outlier",
                  marker: { color: "#ef4444", size: 12, symbol: "cross" },
                },
                {
                  x: [0, 10],
                  y: [9, 31],
                  mode: "lines",
                  type: "scatter",
                  name: "OLS (Skewed by Outlier)",
                  line: { color: "#3b82f6", dash: "dot", width: 3 },
                },
                {
                  x: [0, 10],
                  y: [5, 25],
                  mode: "lines",
                  type: "scatter",
                  name: "Robust / Median Fit",
                  line: { color: "#10b981", width: 4 },
                },
              ],
              {
                title: "Effect of an Outlier on Regression Models",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Predictor (X)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Outcome (Y)",
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.05, y: 0.95 },
              },
              { responsive: true }
            );
          },
          "p4s1ss6-plot1": () => {
            // Model Selection
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const complexity = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
            const train_error = complexity.map((c) => 1 / c + 0.1);
            const test_error = complexity.map(
              (c) => 1 / c + 0.1 + Math.pow(c - 4, 2) / 20 - 0.5
            );

            Plotly.newPlot(
              "p4s1ss6-plot1",
              [
                {
                  x: complexity,
                  y: train_error,
                  mode: "lines",
                  name: "Training Error",
                  line: { color: "#3b82f6", width: 3 },
                },
                {
                  x: complexity,
                  y: test_error,
                  mode: "lines",
                  name: "Test Error",
                  line: { color: "#f59e0b", width: 4 },
                },
              ],
              {
                title: "The Bias-Variance Tradeoff",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Model Complexity",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Error",
                  range: [0, 2],
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.7, y: 0.95 },
                shapes: [
                  {
                    type: "line",
                    x0: 4,
                    y0: 0,
                    x1: 4,
                    y1: 2,
                    line: { dash: "dot" },
                  },
                ],
                annotations: [
                  { x: 2, y: 1.5, text: "High Bias<br>(Underfitting)" },
                  { x: 8, y: 1.5, text: "High Variance<br>(Overfitting)" },
                  { x: 4, y: 0.1, text: "Optimal Model", ay: -40 },
                ],
              },
              { responsive: true }
            );
          },
          "p4s1ss7-plot1": () => {
            // Diagnostics & Validation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const normal_data = Array.from(
              { length: 100 },
              () => (Math.random() + Math.random() + Math.random() - 1.5) * 2
            ).sort((a, b) => a - b);

            const fig = {
              data: [
                {
                  x: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  y: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  mode: "markers",
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  x: normal_data,
                  y: normal_data,
                  mode: "markers",
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  x: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  y: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  mode: "markers",
                  xaxis: "x3",
                  yaxis: "y3",
                },
                {
                  x: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  y: Array.from({ length: 100 }, () => Math.random() * 2 - 1),
                  mode: "markers",
                  xaxis: "x4",
                  yaxis: "y4",
                },
              ],
              layout: {
                title: "Standard Linear Model Diagnostic Plots",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor, size: 10 },
                grid: { rows: 2, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Fitted values",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: {
                  title: "Residuals",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "Theoretical Quantiles",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: {
                  title: "Standardized residuals",
                  gridcolor: themeColors.gridColor,
                },
                xaxis3: {
                  title: "Fitted values",
                  gridcolor: themeColors.gridColor,
                },
                yaxis3: {
                  title: "√|Standardized residuals|",
                  gridcolor: themeColors.gridColor,
                },
                xaxis4: { title: "Leverage", gridcolor: themeColors.gridColor },
                yaxis4: {
                  title: "Standardized residuals",
                  gridcolor: themeColors.gridColor,
                },
                shapes: [
                  {
                    type: "line",
                    xref: "x2",
                    yref: "y2",
                    x0: -4,
                    y0: -4,
                    x1: 4,
                    y1: 4,
                    line: { color: themeColors.textColor, dash: "dash" },
                  },
                ],
                annotations: [
                  {
                    xref: "paper",
                    yref: "paper",
                    x: 0.225,
                    y: 1.05,
                    text: "Residuals vs. Fitted",
                    showarrow: false,
                  },
                  {
                    xref: "paper",
                    yref: "paper",
                    x: 0.775,
                    y: 1.05,
                    text: "Normal Q-Q",
                    showarrow: false,
                  },
                  {
                    xref: "paper",
                    yref: "paper",
                    x: 0.225,
                    y: 0.45,
                    text: "Scale-Location",
                    showarrow: false,
                  },
                  {
                    xref: "paper",
                    yref: "paper",
                    x: 0.775,
                    y: 0.45,
                    text: "Residuals vs. Leverage",
                    showarrow: false,
                  },
                ],
              },
            };
            Plotly.newPlot("p4s1ss7-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s2ss1-plot1": () => {
            // Logistic Regression
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const x = [];
            const y = [];
            for (let i = -8; i <= 8; i += 0.2) {
              x.push(i);
              y.push(1 / (1 + Math.exp(-i)));
            }
            Plotly.newPlot(
              "p4s2ss1-plot1",
              [
                {
                  x: x,
                  y: y,
                  type: "scatter",
                  mode: "lines",
                  line: { color: "#8b5cf6", width: 4 },
                },
              ],
              {
                title: "The Sigmoid Curve of the Logit Link Function",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Linear Predictor (Log-Odds)",
                  gridcolor: themeColors.gridColor,
                  zeroline: true,
                },
                yaxis: {
                  title: "Predicted Probability",
                  range: [-0.1, 1.1],
                  gridcolor: themeColors.gridColor,
                  zeroline: false,
                },
                shapes: [
                  {
                    type: "line",
                    x0: -8,
                    y0: 1,
                    x1: 8,
                    y1: 1,
                    line: { color: themeColors.textColor, dash: "dot" },
                  },
                  {
                    type: "line",
                    x0: -8,
                    y0: 0,
                    x1: 8,
                    y1: 0,
                    line: { color: themeColors.textColor, dash: "dot" },
                  },
                ],
                annotations: [
                  {
                    x: 4,
                    y: 0.8,
                    text: "Output is always<br>between 0 and 1",
                    showarrow: true,
                    ax: 0,
                    ay: -40,
                  },
                  {
                    x: -4,
                    y: 0.2,
                    text: "Negative inputs map<br>to probabilities < 0.5",
                    showarrow: true,
                    ax: 0,
                    ay: 40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s2ss2-plot1": () => {
            // Tree-Based Methods
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            Plotly.newPlot(
              "p4s2ss2-plot1",
              [],
              {
                title: {
                  text: "Interpreting a Decision Tree",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4.5,
                    x1: 3.5,
                    y1: 6,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 4.5,
                    x1: 6,
                    y1: 6,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 1,
                    x1: 9.5,
                    y1: 2.5,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "path",
                    path: "M 5 8 L 2 6 M 5 8 L 5 6 M 5 4.5 L 8 2.5",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                ],
                annotations: [
                  { x: 5, y: 8.75, text: "Petal.Length < 2.45?" },
                  {
                    x: 2,
                    y: 5.25,
                    text: "<b>setosa</b>",
                    font: { color: "white" },
                  },
                  { x: 5, y: 5.25, text: "Petal.Width < 1.75?" },
                  {
                    x: 8,
                    y: 1.75,
                    text: "<b>virginica</b>",
                    font: { color: "white" },
                  },
                  { x: 3, y: 7, text: "YES" },
                  { x: 6, y: 7, text: "NO" },
                  { x: 3.5, y: 5, text: "YES" },
                  { x: 6.5, y: 3.5, text: "NO" },
                ],
              },
              { responsive: true }
            );
          },
          "p4s2ss3-plot1": () => {
            // Discriminant Analysis
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p4s2ss3-plot1",
              [
                {
                  x: [1, 2, 1.5],
                  y: [7, 8, 7.5],
                  mode: "markers",
                  marker: { color: "#3b82f6" },
                },
                {
                  x: [6, 7, 6.5],
                  y: [7, 8, 7.5],
                  mode: "markers",
                  marker: { color: "#10b981" },
                },
                {
                  x: [4, 5, 4.5],
                  y: [2, 3, 2.5],
                  mode: "markers",
                  marker: { color: "#ef4444" },
                },
              ],
              {
                title: "LDA (Linear) vs. QDA (Quadratic) Decision Boundaries",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Predictor 1",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Predictor 2",
                  gridcolor: themeColors.gridColor,
                },
                showlegend: false,
                shapes: [
                  // LDA Lines
                  {
                    type: "line",
                    x0: 3.5,
                    y0: 0,
                    x1: 3.5,
                    y1: 10,
                    line: { color: themeColors.textColor, width: 3 },
                  },
                  {
                    type: "line",
                    x0: 0,
                    y0: 5,
                    x1: 10,
                    y1: 5,
                    line: { color: themeColors.textColor, width: 3 },
                  },
                  // QDA Curves
                  {
                    type: "path",
                    path: "M 0 6 C 4 5, 5 5, 6 10",
                    line: { color: "#ef4444", dash: "dot", width: 2 },
                  },
                  {
                    type: "path",
                    path: "M 2 10 C 4 4, 5 4, 10 4",
                    line: { color: "#ef4444", dash: "dot", width: 2 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s2ss4-plot1": () => {
            // Support Vector Machines (SVM)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p4s2ss4-plot1",
              [
                {
                  x: [2, 3, 4],
                  y: [7, 8, 6],
                  mode: "markers",
                  marker: { color: "#3b82f6", symbol: "circle", size: 10 },
                },
                {
                  x: [6, 7, 8],
                  y: [2, 3, 1],
                  mode: "markers",
                  marker: { color: "#ef4444", symbol: "triangle-up", size: 10 },
                },
                {
                  x: [4, 6],
                  y: [5, 4],
                  mode: "markers",
                  name: "Support Vectors",
                  marker: {
                    color: themeColors.textColor,
                    size: 14,
                    symbol: "circle-open",
                    line: { width: 3 },
                  },
                },
              ],
              {
                title: "SVM Maximum Margin Classifier",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Feature 1", gridcolor: themeColors.gridColor },
                yaxis: { title: "Feature 2", gridcolor: themeColors.gridColor },
                showlegend: false,
                shapes: [
                  {
                    type: "line",
                    x0: 0,
                    y0: 8,
                    x1: 10,
                    y1: 1,
                    line: { color: themeColors.textColor, width: 3 },
                  },
                  {
                    type: "line",
                    x0: 0,
                    y0: 9,
                    x1: 10,
                    y1: 2,
                    line: { color: themeColors.textColor, dash: "dot" },
                  },
                  {
                    type: "line",
                    x0: 0,
                    y0: 7,
                    x1: 10,
                    y1: 0,
                    line: { color: themeColors.textColor, dash: "dot" },
                  },
                ],
                annotations: [{ x: 8, y: 8, text: "Maximized Margin" }],
              },
              { responsive: true }
            );
          },
          "p4s2ss6-plot1": () => {
            // k-Nearest Neighbors
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p4s2ss6-plot1",
              [
                {
                  x: [2, 3, 4.5],
                  y: [7, 8, 7],
                  mode: "markers",
                  name: "Class A",
                  marker: { color: "#ef4444", symbol: "triangle-up", size: 10 },
                },
                {
                  x: [6, 7, 8, 3],
                  y: [6, 7, 5, 4],
                  mode: "markers",
                  name: "Class B",
                  marker: { color: "#3b82f6", symbol: "square", size: 10 },
                },
                {
                  x: [5],
                  y: [5.5],
                  mode: "markers",
                  name: "New Point",
                  marker: { color: "#10b981", symbol: "star", size: 16 },
                },
              ],
              {
                title: "k-Nearest Neighbors Classification",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Feature 1", gridcolor: themeColors.gridColor },
                yaxis: { title: "Feature 2", gridcolor: themeColors.gridColor },
                showlegend: false,
                shapes: [
                  {
                    type: "circle",
                    x0: 3,
                    y0: 3.5,
                    x1: 7,
                    y1: 7.5,
                    line: { color: themeColors.textColor, dash: "dot" },
                    name: "k=3",
                  },
                  {
                    type: "circle",
                    x0: 2,
                    y0: 2.5,
                    x1: 8,
                    y1: 8.5,
                    line: { color: themeColors.textColor, dash: "dash" },
                    name: "k=5",
                  },
                ],
                annotations: [
                  { x: 7, y: 7.5, text: "k=3<br>Vote: 2▲, 1■<br>Class: ▲" },
                  { x: 8, y: 8.5, text: "k=5<br>Vote: 2▲, 3■<br>Class: ■" },
                ],
              },
              { responsive: true }
            );
          },
          "p4s2ss7-plot1": () => {
            // Model Evaluation (Confusion Matrix)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s2ss7-plot1",
              [],
              {
                title: "Anatomy of a Confusion Matrix",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                  autorange: "reversed",
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0,
                    y0: 2,
                    x1: 10,
                    y1: 8,
                    line: { width: 2 },
                  },
                  { type: "line", x0: 5, y0: 2, x1: 5, y1: 8 },
                  { type: "line", x0: 0, y0: 5, x1: 10, y1: 5 },
                ],
                annotations: [
                  {
                    x: -0.5,
                    y: 3.5,
                    text: "<b>Actual:<br>Positive</b>",
                    showarrow: false,
                  },
                  {
                    x: -0.5,
                    y: 6.5,
                    text: "<b>Actual:<br>Negative</b>",
                    showarrow: false,
                  },
                  {
                    x: 2.5,
                    y: 1.5,
                    text: "<b>Predicted: Positive</b>",
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 1.5,
                    text: "<b>Predicted: Negative</b>",
                    showarrow: false,
                  },
                  {
                    x: 2.5,
                    y: 3.5,
                    text: "True Positive (TP)<br><b>Correct Hit</b>",
                    bgcolor: "rgba(22, 163, 74, 0.7)",
                    font: { color: "white" },
                  },
                  {
                    x: 7.5,
                    y: 3.5,
                    text: "False Negative (FN)<br><b>Miss (Type II Error)</b>",
                    bgcolor: "rgba(245, 158, 11, 0.7)",
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 6.5,
                    text: "False Positive (FP)<br><b>False Alarm (Type I Error)</b>",
                    bgcolor: "rgba(239, 68, 68, 0.7)",
                    font: { color: "white" },
                  },
                  {
                    x: 7.5,
                    y: 6.5,
                    text: "True Negative (TN)<br><b>Correct Rejection</b>",
                    bgcolor: "rgba(22, 163, 74, 0.7)",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s2ss7-plot2": () => {
            // Model Evaluation (ROC Curve)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const fpr = [0, 0.1, 0.2, 0.4, 0.6, 1];
            const tpr = [0, 0.6, 0.8, 0.9, 0.95, 1];
            Plotly.newPlot(
              "p4s2ss7-plot2",
              [
                {
                  x: [0, 1],
                  y: [0, 1],
                  type: "scatter",
                  mode: "lines",
                  name: "Random Chance (AUC=0.5)",
                  line: { color: themeColors.textColor, dash: "dot" },
                },
                {
                  x: fpr,
                  y: tpr,
                  type: "scatter",
                  mode: "lines",
                  name: "Model (AUC=0.85)",
                  fill: "tozeroy",
                  line: { color: "#3b82f6", width: 4 },
                },
              ],
              {
                title: "Receiver Operating Characteristic (ROC) Curve",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "False Positive Rate (1 - Specificity)",
                  range: [-0.05, 1.05],
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "True Positive Rate (Recall/Sensitivity)",
                  range: [-0.05, 1.05],
                  gridcolor: themeColors.gridColor,
                  scaleanchor: "x",
                },
                legend: { x: 0.4, y: 0.2 },
                annotations: [
                  {
                    x: 0.4,
                    y: 0.6,
                    text: "Better model curves<br>towards top-left corner",
                    ax: -50,
                    ay: -50,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s3ss1-plot1": () => {
            // k-Means Clustering
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Simulate 3 clusters of data
            const cluster1 = {
              x: Array.from({ length: 50 }, () => 1 + Math.random() * 2),
              y: Array.from({ length: 50 }, () => 1 + Math.random() * 2),
            };
            const cluster2 = {
              x: Array.from({ length: 50 }, () => 5 + Math.random() * 2),
              y: Array.from({ length: 50 }, () => 5 + Math.random() * 2),
            };
            const cluster3 = {
              x: Array.from({ length: 50 }, () => 2 + Math.random() * 2),
              y: Array.from({ length: 50 }, () => 6 + Math.random() * 2),
            };
            const centroids = {
              x: [
                cluster1.x.reduce((a, b) => a + b) / 50,
                cluster2.x.reduce((a, b) => a + b) / 50,
                cluster3.x.reduce((a, b) => a + b) / 50,
              ],
              y: [
                cluster1.y.reduce((a, b) => a + b) / 50,
                cluster2.y.reduce((a, b) => a + b) / 50,
                cluster3.y.reduce((a, b) => a + b) / 50,
              ],
            };

            Plotly.newPlot(
              "p4s3ss1-plot1",
              [
                {
                  x: cluster1.x,
                  y: cluster1.y,
                  mode: "markers",
                  type: "scatter",
                  name: "Cluster 1",
                  marker: { color: "#3b82f6" },
                },
                {
                  x: cluster2.x,
                  y: cluster2.y,
                  mode: "markers",
                  type: "scatter",
                  name: "Cluster 2",
                  marker: { color: "#10b981" },
                },
                {
                  x: cluster3.x,
                  y: cluster3.y,
                  mode: "markers",
                  type: "scatter",
                  name: "Cluster 3",
                  marker: { color: "#ef4444" },
                },
                {
                  x: centroids.x,
                  y: centroids.y,
                  mode: "markers",
                  type: "scatter",
                  name: "Centroids",
                  marker: {
                    color: "black",
                    symbol: "diamond",
                    size: 16,
                    line: { width: 2, color: themeColors.textColor },
                  },
                },
              ],
              {
                title: "k-Means Clustering Result (k=3)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Feature 1 (Scaled)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Feature 2 (Scaled)",
                  gridcolor: themeColors.gridColor,
                },
                showlegend: false,
              },
              { responsive: true }
            );
          },
          "p4s3ss2-plot1": () => {
            // Hierarchical Clustering
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p4s3ss2-plot1",
              [
                {
                  type: "scatter",
                  mode: "lines",
                  x: [1, 1, 2, 2, 1.5, 1.5, 3, 3, 4, 4, 3.5, 3.5, 2.5, 2.5],
                  y: [0, 1, 0, 1, 1, 2, 0, 1.5, 0, 1.5, 1.5, 2.5, 2, 4],
                  line: { color: themeColors.textColor },
                },
              ],
              {
                title: "Interpreting a Dendrogram",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Data Points / Items",
                  showticklabels: false,
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Height (Dissimilarity)",
                  gridcolor: themeColors.gridColor,
                },
                shapes: [
                  {
                    type: "line",
                    x0: 0,
                    x1: 5,
                    y0: 3,
                    y1: 3,
                    line: { color: "#ef4444", dash: "dot", width: 3 },
                  },
                ],
                annotations: [
                  {
                    x: 4.5,
                    y: 3,
                    text: "Cutting here<br>yields 3 clusters",
                    font: { color: "#ef4444" },
                  },
                  {
                    x: 2.5,
                    y: 4.2,
                    text: "Long vertical lines<br>indicate distinct groups",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s3ss3-plot1": () => {
            // Density-Based Clustering
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Simulate two crescent moons
            const moon1 = { x: [], y: [] },
              moon2 = { x: [], y: [] },
              noise = { x: [], y: [] };
            for (let i = 0; i < 100; i++) {
              const angle1 = Math.PI * Math.random();
              moon1.x.push(Math.cos(angle1) + (Math.random() - 0.5) * 0.2);
              moon1.y.push(Math.sin(angle1) + (Math.random() - 0.5) * 0.2);
              const angle2 = Math.PI + Math.PI * Math.random();
              moon2.x.push(1 + Math.cos(angle2) + (Math.random() - 0.5) * 0.2);
              moon2.y.push(
                0.5 + Math.sin(angle2) + (Math.random() - 0.5) * 0.2
              );
            }
            for (let i = 0; i < 20; i++) {
              noise.x.push(Math.random() * 3);
              noise.y.push(Math.random() * 2 - 0.5);
            }

            Plotly.newPlot(
              "p4s3ss3-plot1",
              [
                {
                  x: moon1.x,
                  y: moon1.y,
                  mode: "markers",
                  type: "scatter",
                  name: "Cluster 1",
                  marker: { color: "#3b82f6" },
                },
                {
                  x: moon2.x,
                  y: moon2.y,
                  mode: "markers",
                  type: "scatter",
                  name: "Cluster 2",
                  marker: { color: "#ef4444" },
                },
                {
                  x: noise.x,
                  y: noise.y,
                  mode: "markers",
                  type: "scatter",
                  name: "Noise",
                  marker: { color: isDark ? "grey" : "grey", symbol: "cross" },
                },
              ],
              {
                title: "DBSCAN Finding Arbitrarily Shaped Clusters",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Feature 1", gridcolor: themeColors.gridColor },
                yaxis: { title: "Feature 2", gridcolor: themeColors.gridColor },
                showlegend: false,
              },
              { responsive: true }
            );
          },
          "p4s3ss4-plot1": () => {
            // Model-Based Clustering
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p4s3ss4-plot1",
              [
                {
                  x: [1, 2, 1.5],
                  y: [7, 8, 7.5],
                  mode: "markers",
                  marker: { color: "#3b82f6" },
                },
                {
                  x: [6, 7, 6.5],
                  y: [7, 8, 7.5],
                  mode: "markers",
                  marker: { color: "#10b981" },
                },
                {
                  x: [4, 5, 4.5],
                  y: [2, 3, 2.5],
                  mode: "markers",
                  marker: { color: "#ef4444" },
                },
              ],
              {
                title: "Gaussian Mixture Model with Elliptical Clusters",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Feature 1", gridcolor: themeColors.gridColor },
                yaxis: { title: "Feature 2", gridcolor: themeColors.gridColor },
                showlegend: false,
                shapes: [
                  {
                    type: "circle",
                    x0: 0,
                    y0: 6,
                    x1: 3,
                    y1: 9,
                    line: { color: "#3b82f6", dash: "dot" },
                    opacity: 0.5,
                  },
                  {
                    type: "path",
                    path: "M 5,6 L 8,9 L 8,6 L 5,6 Z",
                    type: "path",
                    line: { color: "#10b981", dash: "dot" },
                    opacity: 0.5,
                  },
                  {
                    type: "circle",
                    x0: 3,
                    y0: 1.5,
                    x1: 6,
                    y1: 4,
                    line: { color: "#ef4444", dash: "dot" },
                    opacity: 0.5,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s3ss5-plot1": () => {
            // Cluster Validation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const k = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
            const wss = [300, 100, 40, 30, 25, 22, 20, 18, 17, 16];

            Plotly.newPlot(
              "p4s3ss5-plot1",
              [
                {
                  x: k,
                  y: wss,
                  type: "scatter",
                  mode: "lines+markers",
                  line: { color: "#3b82f6", width: 3 },
                },
              ],
              {
                title: "The Elbow Method for Optimal k",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Number of Clusters (k)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Within-Cluster Sum of Squares (WSS)",
                  gridcolor: themeColors.gridColor,
                },
                shapes: [
                  {
                    type: "circle",
                    x0: 2.5,
                    y0: 30,
                    x1: 3.5,
                    y1: 50,
                    line: { color: "#ef4444", dash: "dot", width: 3 },
                  },
                ],
                annotations: [
                  {
                    x: 3,
                    y: 60,
                    text: 'The "Elbow"<br>Optimal k=3',
                    ax: 50,
                    ay: -40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s4ss2-plot1": () => {
            // Rule Evaluation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const rules = Array.from({ length: 100 }, () => ({
              support: Math.random() * 0.1,
              confidence: 0.2 + Math.random() * 0.8,
              lift: 1 + Math.random() * 4,
            }));
            Plotly.newPlot(
              "p4s4ss2-plot1",
              [
                {
                  x: rules.map((r) => r.support),
                  y: rules.map((r) => r.confidence),
                  mode: "markers",
                  type: "scatter",
                  marker: {
                    color: rules.map((r) => r.lift),
                    colorscale: "Viridis",
                    showscale: true,
                    colorbar: { title: "Lift" },
                  },
                },
              ],
              {
                title: "Visualizing Association Rules",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Support", gridcolor: themeColors.gridColor },
                yaxis: {
                  title: "Confidence",
                  gridcolor: themeColors.gridColor,
                },
                annotations: [
                  {
                    x: 0.08,
                    y: 0.9,
                    text: "Interesting rules have<br>high support, confidence, & lift",
                    ax: 0,
                    ay: -40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s5ss1-plot1": () => {
            // Bagging
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            Plotly.newPlot(
              "p4s5ss1-plot1",
              [],
              {
                title: {
                  text: "The Bagging (Bootstrap Aggregating) Workflow",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4,
                    x1: 3,
                    y1: 5.5,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 4,
                    x1: 6.25,
                    y1: 5.5,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 4,
                    x1: 9.5,
                    y1: 5.5,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 0.5,
                    x1: 7,
                    y1: 2,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "Original Training Data",
                    font: { color: "white" },
                  },
                  { x: 1.75, y: 4.75, text: "Bootstrap Sample 1" },
                  { x: 5, y: 4.75, text: "Bootstrap Sample 2" },
                  { x: 8.25, y: 4.75, text: "Bootstrap Sample n" },
                  { x: 1.75, y: 3, text: "Train Tree 1" },
                  { x: 5, y: 3, text: "Train Tree 2" },
                  { x: 8.25, y: 3, text: "Train Tree n" },
                  {
                    x: 5,
                    y: 2.5,
                    text: "<b>AGGREGATE (VOTE)</b>",
                    showarrow: true,
                    ax: 0,
                    ay: 25,
                  },
                  {
                    x: 5,
                    y: 1.25,
                    text: "<b>Final Prediction</b>",
                    font: { color: "white", size: 16 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s5ss2-plot1": () => {
            // Boosting
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s5ss2-plot1",
              [],
              {
                title: {
                  text: "The Sequential Process of Boosting",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 3,
                    x1: 3,
                    y1: 5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 3,
                    x1: 6.25,
                    y1: 5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 3,
                    x1: 9.5,
                    y1: 5,
                    fillcolor: "#3b82f6",
                  },
                ],
                annotations: [
                  { x: 1.75, y: 5.5, text: "<b>Iteration 1</b>" },
                  {
                    x: 1.75,
                    y: 4,
                    text: "Train Model 1<br>on Data",
                    font: { color: "white" },
                  },
                  { x: 1.75, y: 2, text: "Identify Errors" },
                  { x: 3.375, y: 4, text: "➔", font: { size: 24 } },
                  { x: 5, y: 5.5, text: "<b>Iteration 2</b>" },
                  {
                    x: 5,
                    y: 4,
                    text: "Train Model 2<br>on Errors",
                    font: { color: "white" },
                  },
                  { x: 5, y: 2, text: "Identify Errors" },
                  { x: 6.625, y: 4, text: "➔", font: { size: 24 } },
                  { x: 8.25, y: 5.5, text: "<b>Iteration 3</b>" },
                  {
                    x: 8.25,
                    y: 4,
                    text: "Train Model 3<br>on Errors",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s5ss3-plot1": () => {
            // Stacking & Blending
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            Plotly.newPlot(
              "p4s5ss3-plot1",
              [],
              {
                title: {
                  text: "The Stacking Ensemble Architecture",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 5.5,
                    x1: 3,
                    y1: 7,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 5.5,
                    x1: 6.25,
                    y1: 7,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 5.5,
                    x1: 9.5,
                    y1: 7,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 2.5,
                    x1: 7,
                    y1: 4,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 0,
                    x1: 7,
                    y1: 1.5,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "Original Training Data",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 7.5,
                    text: "<b>Level 0: Base Models</b>",
                    showarrow: true,
                    ax: 0,
                    ay: 25,
                  },
                  {
                    x: 1.75,
                    y: 6.25,
                    text: "Model A<br>(e.g., Random Forest)",
                  },
                  { x: 5, y: 6.25, text: "Model B<br>(e.g., SVM)" },
                  { x: 8.25, y: 6.25, text: "Model C<br>(e.g., k-NN)" },
                  {
                    x: 5,
                    y: 4.75,
                    text: "Collect Out-of-Sample Predictions",
                    showarrow: true,
                    ax: 0,
                    ay: 25,
                  },
                  {
                    x: 5,
                    y: 3.25,
                    text: "<b>Level 1: Meta-Model</b><br>(e.g., Linear Regression)",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 2,
                    text: "<b>Final Prediction</b>",
                    showarrow: true,
                    ax: 0,
                    ay: 25,
                  },
                  {
                    x: 5,
                    y: 0.75,
                    text: "Final Prediction",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s6ss1-plot1": () => {
            // Neural Network Basics
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s6ss1-plot1",
              [],
              {
                title: {
                  text: "Anatomy of a Single Artificial Neuron",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "circle",
                    x0: 4,
                    y0: 3,
                    x1: 6,
                    y1: 5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 6,
                    y0: 3.5,
                    x1: 8,
                    y1: 4.5,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  { x: 1, y: 6, text: "Input 1 (x₁)" },
                  { x: 1, y: 2, text: "Input n (xₙ)" },
                  {
                    x: 2.5,
                    y: 5.5,
                    text: "Weight 1 (w₁)",
                    showarrow: true,
                    ax: -30,
                    ay: 0,
                  },
                  {
                    x: 2.5,
                    y: 2.5,
                    text: "Weight n (wₙ)",
                    showarrow: true,
                    ax: -30,
                    ay: 0,
                  },
                  {
                    x: 5,
                    y: 4,
                    text: "<b>Σ + b</b>",
                    font: { color: "white", size: 16 },
                  },
                  {
                    x: 7,
                    y: 4,
                    text: "Activation<br>Function",
                    font: { color: "white" },
                  },
                  { x: 9, y: 4, text: "<b>Output</b>" },
                ],
              },
              { responsive: true }
            );
          },
          "p4s6ss2-plot1": () => {
            // Feedforward Neural Networks (FNN)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s6ss2-plot1",
              [],
              {
                title: {
                  text: "Feedforward Neural Network Architecture",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  { type: "circle", x0: 1, y0: 7, x1: 2, y1: 8 },
                  { type: "circle", x0: 1, y0: 5, x1: 2, y1: 6 },
                  { type: "circle", x0: 1, y0: 3, x1: 2, y1: 4 },
                  { type: "circle", x0: 1, y0: 1, x1: 2, y1: 2 },
                  { type: "circle", x0: 3.5, y0: 6, x1: 4.5, y1: 7 },
                  { type: "circle", x0: 3.5, y0: 4, x1: 4.5, y1: 5 },
                  { type: "circle", x0: 3.5, y0: 2, x1: 4.5, y1: 3 },
                  { type: "circle", x0: 6, y0: 5, x1: 7, y1: 6 },
                  { type: "circle", x0: 6, y0: 3, x1: 7, y1: 4 },
                  { type: "circle", x0: 8.5, y0: 6, x1: 9.5, y1: 7 },
                  { type: "circle", x0: 8.5, y0: 4, x1: 9.5, y1: 5 },
                  { type: "circle", x0: 8.5, y0: 2, x1: 9.5, y1: 3 },
                ],
                annotations: [
                  { x: 1.5, y: 9, text: "Input<br>Layer" },
                  { x: 4, y: 8, text: "Hidden<br>Layer 1" },
                  { x: 6.5, y: 7, text: "Hidden<br>Layer 2" },
                  { x: 9, y: 8, text: "Output<br>Layer" },
                ],
              },
              { responsive: true }
            );
          },
          "p4s6ss3-plot1": () => {
            // Convolutional Neural Networks (CNNs)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s6ss3-plot1",
              [],
              {
                title: {
                  text: "A Typical CNN Architecture",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 6],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 2,
                    x1: 2,
                    y1: 4,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 2.5,
                    y0: 2.5,
                    x1: 4,
                    y1: 3.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 4.5,
                    y0: 2.8,
                    x1: 5.5,
                    y1: 3.2,
                    fillcolor: "#f59e0b",
                  },
                  {
                    type: "rect",
                    x0: 6,
                    y0: 1,
                    x1: 7,
                    y1: 5,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "rect",
                    x0: 7.5,
                    y0: 2,
                    x1: 8.5,
                    y1: 4,
                    fillcolor: "#ef4444",
                  },
                ],
                annotations: [
                  { x: 1.25, y: 1.5, text: "Input Image" },
                  { x: 3.25, y: 1.5, text: "Convolution" },
                  { x: 5, y: 1.5, text: "Pooling" },
                  { x: 6.5, y: 0.5, text: "Flatten" },
                  { x: 8, y: 1.5, text: "Fully Connected" },
                ],
              },
              { responsive: true }
            );
          },
          "p4s6ss4-plot1": () => {
            // Recurrent Neural Networks (RNNs)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s6ss4-plot1",
              [],
              {
                title: {
                  text: "An Unrolled Recurrent Neural Network",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 2.5,
                    y1: 3,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 1,
                    x1: 5.75,
                    y1: 3,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 1,
                    x1: 9,
                    y1: 3,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4,
                    x1: 2.5,
                    y1: 6,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 4,
                    x1: 5.75,
                    y1: 6,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 4,
                    x1: 9,
                    y1: 6,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 7,
                    x1: 2.5,
                    y1: 9,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 7,
                    x1: 5.75,
                    y1: 9,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 7,
                    x1: 9,
                    y1: 9,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  { x: 1.5, y: 2, text: "Input X₀", font: { color: "white" } },
                  { x: 4.75, y: 2, text: "Input X₁", font: { color: "white" } },
                  { x: 8, y: 2, text: "Input Xₜ", font: { color: "white" } },
                  { x: 1.5, y: 5, text: "RNN Cell", font: { color: "white" } },
                  { x: 4.75, y: 5, text: "RNN Cell", font: { color: "white" } },
                  { x: 8, y: 5, text: "RNN Cell", font: { color: "white" } },
                  { x: 1.5, y: 8, text: "Output Y₀", font: { color: "white" } },
                  {
                    x: 4.75,
                    y: 8,
                    text: "Output Y₁",
                    font: { color: "white" },
                  },
                  { x: 8, y: 8, text: "Output Yₜ", font: { color: "white" } },
                  { x: 3.125, y: 5, text: "h₀➔h₁" },
                  { x: 6.375, y: 5, text: "h₁➔h₂" },
                ],
              },
              { responsive: true }
            );
          },
          "p5s2ss1-plot1": () => {
            // Model Evaluation (Confusion Matrix)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p5s2ss1-plot1",
              [],
              {
                title: "Anatomy of a Confusion Matrix",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                  autorange: "reversed",
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0,
                    y0: 2,
                    x1: 10,
                    y1: 8,
                    line: { width: 2 },
                  },
                  { type: "line", x0: 5, y0: 2, x1: 5, y1: 8 },
                  { type: "line", x0: 0, y0: 5, x1: 10, y1: 5 },
                ],
                annotations: [
                  {
                    x: -0.5,
                    y: 3.5,
                    text: "<b>Actual:<br>Positive</b>",
                    showarrow: false,
                  },
                  {
                    x: -0.5,
                    y: 6.5,
                    text: "<b>Actual:<br>Negative</b>",
                    showarrow: false,
                  },
                  {
                    x: 2.5,
                    y: 1.5,
                    text: "<b>Predicted: Positive</b>",
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 1.5,
                    text: "<b>Predicted: Negative</b>",
                    showarrow: false,
                  },
                  {
                    x: 2.5,
                    y: 3.5,
                    text: "True Positive (TP)<br><b>Correct Hit</b>",
                    bgcolor: "rgba(22, 163, 74, 0.7)",
                    font: { color: "white" },
                  },
                  {
                    x: 7.5,
                    y: 3.5,
                    text: "False Negative (FN)<br><b>Miss (Type II Error)</b>",
                    bgcolor: "rgba(245, 158, 11, 0.7)",
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 6.5,
                    text: "False Positive (FP)<br><b>False Alarm (Type I Error)</b>",
                    bgcolor: "rgba(239, 68, 68, 0.7)",
                    font: { color: "white" },
                  },
                  {
                    x: 7.5,
                    y: 6.5,
                    text: "True Negative (TN)<br><b>Correct Rejection</b>",
                    bgcolor: "rgba(22, 163, 74, 0.7)",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p5s2ss1-plot2": () => {
            // Model Evaluation (ROC Curve)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const fpr = [0, 0.1, 0.2, 0.4, 0.6, 1];
            const tpr = [0, 0.6, 0.8, 0.9, 0.95, 1];
            Plotly.newPlot(
              "p5s2ss1-plot2",
              [
                {
                  x: [0, 1],
                  y: [0, 1],
                  type: "scatter",
                  mode: "lines",
                  name: "Random Chance (AUC=0.5)",
                  line: { color: themeColors.textColor, dash: "dot" },
                },
                {
                  x: fpr,
                  y: tpr,
                  type: "scatter",
                  mode: "lines",
                  name: "Model (AUC=0.85)",
                  fill: "tozeroy",
                  line: { color: "#3b82f6", width: 4 },
                },
              ],
              {
                title: "Receiver Operating Characteristic (ROC) Curve",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "False Positive Rate (1 - Specificity)",
                  range: [-0.05, 1.05],
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "True Positive Rate (Recall/Sensitivity)",
                  range: [-0.05, 1.05],
                  gridcolor: themeColors.gridColor,
                  scaleanchor: "x",
                },
                legend: { x: 0.4, y: 0.2 },
                annotations: [
                  {
                    x: 0.4,
                    y: 0.6,
                    text: "Better model curves<br>towards top-left corner",
                    ax: -50,
                    ay: -50,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s7ss1-plot1": () => {
            // Time Series Decomposition
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Simulate AirPassengers data
            const x = Array.from({ length: 144 }, (_, i) => i);
            const trend = x.map((i) => 100 + i * 2.5);
            const seasonal = x.map(
              (i) => Math.sin(((i % 12) / 12) * 2 * Math.PI) * 50
            );
            const residual = x.map(() => (Math.random() - 0.5) * 40);
            const observed = x.map((i) => trend[i] + seasonal[i] + residual[i]);

            const fig = {
              data: [
                {
                  x: x,
                  y: observed,
                  mode: "lines",
                  name: "Observed",
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  x: x,
                  y: trend,
                  mode: "lines",
                  name: "Trend",
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  x: x,
                  y: seasonal,
                  mode: "lines",
                  name: "Seasonal",
                  xaxis: "x3",
                  yaxis: "y3",
                },
                {
                  x: x,
                  y: residual,
                  mode: "lines",
                  name: "Residual",
                  xaxis: "x4",
                  yaxis: "y4",
                },
              ],
              layout: {
                title: "Decomposition of AirPassenger Time Series",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: {
                  rows: 4,
                  columns: 1,
                  pattern: "independent",
                  roworder: "top to bottom",
                },
                showlegend: false,
                yaxis1: { title: "Observed" },
                yaxis2: { title: "Trend" },
                yaxis3: { title: "Seasonal" },
                yaxis4: { title: "Residual" },
                xaxis4: { title: "Time" },
              },
            };
            Plotly.newPlot("p4s7ss1-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s7ss2-plot1": () => {
            // Stationarity
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const price = Array.from(
              { length: 100 },
              (_, i) => 100 + i * 0.5 + Math.random() * 10
            );
            const returns = price
              .slice(1)
              .map((p, i) => Math.log(p) - Math.log(price[i]));

            const fig = {
              data: [
                {
                  x: Array.from({ length: 100 }, (_, i) => i + 1),
                  y: price,
                  type: "scatter",
                  mode: "lines",
                  name: "Price",
                  xaxis: "x1",
                  yaxis: "y1",
                  line: { color: "#ef4444" },
                },
                {
                  x: Array.from({ length: 99 }, (_, i) => i + 1),
                  y: returns,
                  type: "scatter",
                  mode: "lines",
                  name: "Returns",
                  xaxis: "x2",
                  yaxis: "y2",
                  line: { color: "#10b981" },
                },
              ],
              layout: {
                title: "Transforming a Non-Stationary Series",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Non-Stationary (Price)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: { gridcolor: themeColors.gridColor },
                xaxis2: {
                  title: "Stationary (Returns)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: { gridcolor: themeColors.gridColor },
              },
            };
            Plotly.newPlot("p4s7ss2-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s7ss3-plot1": () => {
            // Autoregressive Models
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const lags = Array.from({ length: 20 }, (_, i) => i + 1);
            const acf = lags.map((l) => Math.pow(0.7, l));
            const pacf = lags.map((l) => (l <= 2 ? 0.7 - l * 0.2 : 0));

            const fig = {
              data: [
                {
                  x: lags,
                  y: acf,
                  type: "bar",
                  name: "ACF",
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  x: lags,
                  y: pacf,
                  type: "bar",
                  name: "PACF",
                  xaxis: "x2",
                  yaxis: "y2",
                },
              ],
              layout: {
                title: "ACF and PACF Plots for Model Identification",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: { title: "ACF", gridcolor: themeColors.gridColor },
                yaxis1: {
                  range: [-1, 1],
                  title: "Correlation",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: { title: "PACF", gridcolor: themeColors.gridColor },
                yaxis2: {
                  range: [-1, 1],
                  title: "Partial Correlation",
                  gridcolor: themeColors.gridColor,
                },
                shapes: [
                  {
                    type: "line",
                    xref: "x1 domain",
                    x0: 0,
                    x1: 1,
                    y0: 0.2,
                    y1: 0.2,
                    line: { color: "#3b82f6", dash: "dot" },
                  },
                  {
                    type: "line",
                    xref: "x1 domain",
                    x0: 0,
                    x1: 1,
                    y0: -0.2,
                    y1: -0.2,
                    line: { color: "#3b82f6", dash: "dot" },
                  },
                  {
                    type: "line",
                    xref: "x2 domain",
                    x0: 0,
                    x1: 1,
                    y0: 0.2,
                    y1: 0.2,
                    line: { color: "#3b82f6", dash: "dot" },
                  },
                  {
                    type: "line",
                    xref: "x2 domain",
                    x0: 0,
                    x1: 1,
                    y0: -0.2,
                    y1: -0.2,
                    line: { color: "#3b82f6", dash: "dot" },
                  },
                ],
              },
            };
            Plotly.newPlot("p4s7ss3-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s7ss4-plot1": () => {
            // Exponential Smoothing
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Simulate AirPassengers data
            const x_hist = Array.from({ length: 144 }, (_, i) => i);
            const x_future = Array.from({ length: 24 }, (_, i) => i + 144);
            const trend = (i) => 100 + i * 2.5;
            const seasonal = (i) =>
              Math.sin(((i % 12) / 12) * 2 * Math.PI) * 50;
            const observed = x_hist.map(
              (i) => trend(i) + seasonal(i) + (Math.random() - 0.5) * 40
            );
            const forecast = x_future.map((i) => trend(i) + seasonal(i));
            const lower_bound = forecast.map((f) => f - 30);
            const upper_bound = forecast.map((f) => f + 30);

            Plotly.newPlot(
              "p4s7ss4-plot1",
              [
                {
                  x: x_hist,
                  y: observed,
                  mode: "lines",
                  name: "Historical Data",
                  line: { color: themeColors.textColor },
                },
                {
                  x: x_future,
                  y: forecast,
                  mode: "lines",
                  name: "Forecast",
                  line: { color: "#f59e0b", width: 3 },
                },
                {
                  x: [...x_future, ...x_future.slice().reverse()],
                  y: [...upper_bound, ...lower_bound.slice().reverse()],
                  fill: "toself",
                  fillcolor: "rgba(245, 158, 11, 0.2)",
                  line: { color: "transparent" },
                  name: "Prediction Interval",
                },
              ],
              {
                title: "Holt-Winters Seasonal Forecast",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Time", gridcolor: themeColors.gridColor },
                yaxis: { title: "Value", gridcolor: themeColors.gridColor },
                showlegend: false,
              },
              { responsive: true }
            );
          },
          "p4s7ss5-plot1": () => {
            // Anomaly Detection
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const time = Array.from({ length: 100 }, (_, i) => i);
            const values = time.map(
              (t) => 50 + Math.sin(t / 5) * 10 + Math.random() * 5
            );
            const anomalies = { x: [20, 80], y: [80, 20] };
            const upper_bound = values.map((v) => v + 10);
            const lower_bound = values.map((v) => v - 10);

            Plotly.newPlot(
              "p4s7ss5-plot1",
              [
                {
                  x: time,
                  y: values,
                  mode: "lines",
                  name: "Observed Data",
                  line: { color: themeColors.textColor },
                },
                {
                  x: [...time, ...time.slice().reverse()],
                  y: [...upper_bound, ...lower_bound.slice().reverse()],
                  fill: "toself",
                  fillcolor: "rgba(59, 130, 246, 0.2)",
                  line: { color: "transparent" },
                  name: "Normal Range",
                },
                {
                  x: anomalies.x,
                  y: anomalies.y,
                  mode: "markers",
                  name: "Anomalies",
                  marker: { color: "#ef4444", size: 10 },
                },
              ],
              {
                title: "Time Series Anomaly Detection",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Time", gridcolor: themeColors.gridColor },
                yaxis: { title: "Value", gridcolor: themeColors.gridColor },
                showlegend: false,
              },
              { responsive: true }
            );
          },
          "p4s8ss2-plot1": () => {
            // Feature Extraction (NLP)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const fig = {
              data: [
                {
                  y: ["whale", "sea", "ship"],
                  x: [0.03, 0.02, 0.015],
                  type: "bar",
                  orientation: "h",
                  name: "Moby Dick",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#3b82f6" },
                },
                {
                  y: ["darcy", "elizabeth", "mrs"],
                  x: [0.04, 0.035, 0.03],
                  type: "bar",
                  orientation: "h",
                  name: "Pride & Prejudice",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#10b981" },
                },
              ],
              layout: {
                title: "Top TF-IDF Terms per Document",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: { title: "TF-IDF", gridcolor: themeColors.gridColor },
                yaxis1: { title: "Moby Dick", autorange: "reversed" },
                xaxis2: { title: "TF-IDF", gridcolor: themeColors.gridColor },
                yaxis2: { title: "Pride & Prejudice", autorange: "reversed" },
              },
            };
            Plotly.newPlot("p4s8ss2-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s8ss3-plot1": () => {
            // Text Classification
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s8ss3-plot1",
              [],
              {
                title: {
                  text: "The Text Classification Workflow",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 6],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 2.5,
                    x1: 2.5,
                    y1: 3.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 2.5,
                    x1: 5,
                    y1: 3.5,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 2.5,
                    x1: 7.5,
                    y1: 3.5,
                    fillcolor: "#a855f7",
                  },
                  {
                    type: "rect",
                    x0: 8,
                    y0: 2.5,
                    x1: 10,
                    y1: 3.5,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  { x: 1.5, y: 3, text: "Raw Text", font: { color: "white" } },
                  {
                    x: 4,
                    y: 3,
                    text: "Preprocessing",
                    font: { color: "white" },
                  },
                  {
                    x: 6.5,
                    y: 3,
                    text: "Feature Extraction",
                    font: { color: "white" },
                  },
                  { x: 9, y: 3, text: "Model", font: { color: "white" } },
                ],
              },
              { responsive: true }
            );
          },
          "p4s8ss4-plot1": () => {
            // Topic Modeling
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const fig = {
              data: [
                {
                  y: ["percent", "new", "government"],
                  x: [0.03, 0.02, 0.015],
                  type: "bar",
                  orientation: "h",
                  name: "Topic 1",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#3b82f6" },
                },
                {
                  y: ["game", "team", "season"],
                  x: [0.04, 0.035, 0.03],
                  type: "bar",
                  orientation: "h",
                  name: "Topic 2",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#10b981" },
                },
              ],
              layout: {
                title: "Top Terms per Discovered Topic (Beta Probabilities)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: { title: "Beta", gridcolor: themeColors.gridColor },
                yaxis1: { title: 'Topic 1: "Politics"', autorange: "reversed" },
                xaxis2: { title: "Beta", gridcolor: themeColors.gridColor },
                yaxis2: { title: 'Topic 2: "Sports"', autorange: "reversed" },
              },
            };
            Plotly.newPlot("p4s8ss4-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s8ss5-plot1": () => {
            // Sentiment Analysis
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const fig = {
              data: [
                {
                  x: [50, 40, 30],
                  y: ["miss", "poor", "bad"],
                  type: "bar",
                  orientation: "h",
                  name: "Negative",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#ef4444" },
                },
                {
                  x: [120, 100, 90],
                  y: ["good", "love", "happy"],
                  type: "bar",
                  orientation: "h",
                  name: "Positive",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#10b981" },
                },
              ],
              layout: {
                title: "Top Word Contributions to Sentiment",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Contribution (Negative)",
                  autorange: "reversed",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: { autorange: "reversed" },
                xaxis2: {
                  title: "Contribution (Positive)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: { autorange: "reversed" },
              },
            };
            Plotly.newPlot("p4s8ss5-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s8ss6-plot1": () => {
            // Sequence Models
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s8ss6-plot1",
              [],
              {
                title: {
                  text: "RNN (Sequential) vs. Transformer (Parallel)",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  // RNN
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 5.5,
                    x1: 2,
                    y1: 6.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 2.5,
                    y0: 5.5,
                    x1: 4,
                    y1: 6.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 4.5,
                    y0: 5.5,
                    x1: 6,
                    y1: 6.5,
                    fillcolor: "#3b82f6",
                  },
                  // Transformer
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1.5,
                    x1: 2,
                    y1: 2.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 2.5,
                    y0: 1.5,
                    x1: 4,
                    y1: 2.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 4.5,
                    y0: 1.5,
                    x1: 6,
                    y1: 2.5,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: -0.2,
                    y: 6,
                    text: "<b>RNN</b>",
                    showarrow: false,
                    xanchor: "right",
                  },
                  { x: 1.25, y: 6, text: "The", font: { color: "white" } },
                  { x: 3.25, y: 6, text: "cat", font: { color: "white" } },
                  { x: 5.25, y: 6, text: "sat", font: { color: "white" } },
                  { x: 2.25, y: 6, text: "→" },
                  { x: 4.25, y: 6, text: "→" },
                  {
                    x: -0.2,
                    y: 2,
                    text: "<b>Transformer</b>",
                    showarrow: false,
                    xanchor: "right",
                  },
                  { x: 1.25, y: 2, text: "The", font: { color: "white" } },
                  { x: 3.25, y: 2, text: "cat", font: { color: "white" } },
                  { x: 5.25, y: 2, text: "sat", font: { color: "white" } },
                  // Self-attention arrows
                  { x: 1.25, y: 2.7, ax: 20, ay: 20, arrowhead: 2 },
                  { x: 3.25, y: 2.7, ax: 0, ay: 20, arrowhead: 2 },
                  { x: 5.25, y: 2.7, ax: -20, ay: 20, arrowhead: 2 },
                ],
              },
              { responsive: true }
            );
          },
          "p4s10ss1-plot1": () => {
            // The Potential Outcomes Framework
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const monoFont = {
              family: "JetBrains Mono, monospace",
              size: 12,
              color: themeColors.textColor,
            };

            Plotly.newPlot(
              "p4s10ss1-plot1",
              [],
              {
                title: {
                  text: "The Fundamental Problem of Causal Inference",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                annotations: [
                  {
                    x: 2.5,
                    y: 7,
                    text: "<b>Customer A (Treated)</b>",
                    showarrow: false,
                  },
                  {
                    x: 2.5,
                    y: 5,
                    text: 'Outcome if Treated: Rs. 2,500 <i class="fas fa-check-circle" style="color:#10b981;"></i><br>(Observed)',
                    showarrow: false,
                  },
                  {
                    x: 2.5,
                    y: 3,
                    text: 'Outcome if Control: ??? <i class="fas fa-question-circle" style="color:grey;"></i><br>(Counterfactual)',
                    showarrow: false,
                  },

                  {
                    x: 7.5,
                    y: 7,
                    text: "<b>Customer B (Control)</b>",
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 5,
                    text: 'Outcome if Treated: ??? <i class="fas fa-question-circle" style="color:grey;"></i><br>(Counterfactual)',
                    showarrow: false,
                  },
                  {
                    x: 7.5,
                    y: 3,
                    text: 'Outcome if Control: Rs. 1,500 <i class="fas fa-check-circle" style="color:#10b981;"></i><br>(Observed)',
                    showarrow: false,
                  },

                  {
                    x: 5,
                    y: 0.5,
                    text: "<b>Causal Effect for A</b> = (Rs. 2,500 - ???) = <b>Unknowable</b>",
                    showarrow: false,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s10ss2-plot1": () => {
            // Matching Methods
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            // Simulate propensity scores
            const control_before = Array.from(
              { length: 1000 },
              () => 0.1 + Math.random() * 0.4
            );
            const treated_before = Array.from(
              { length: 1000 },
              () => 0.4 + Math.random() * 0.4
            );
            const matched = Array.from(
              { length: 1000 },
              () => 0.4 + Math.random() * 0.4
            );

            const fig = {
              data: [
                {
                  x: control_before,
                  type: "histogram",
                  name: "Control",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#ef4444", opacity: 0.7 },
                },
                {
                  x: treated_before,
                  type: "histogram",
                  name: "Treated",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#3b82f6", opacity: 0.7 },
                },
                {
                  x: matched,
                  type: "histogram",
                  name: "Control (Matched)",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#ef4444", opacity: 0.7 },
                },
                {
                  x: treated_before,
                  type: "histogram",
                  name: "Treated",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#3b82f6", opacity: 0.7 },
                },
              ],
              layout: {
                title:
                  "Distribution of Propensity Scores Before and After Matching",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                barmode: "overlay",
                legend: { x: 0.7, y: 0.9 },
                xaxis1: {
                  title: "Before Matching",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: { title: "Count", gridcolor: themeColors.gridColor },
                xaxis2: {
                  title: "After Matching",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: {
                  showticklabels: false,
                  gridcolor: themeColors.gridColor,
                },
              },
            };
            Plotly.newPlot("p4s10ss2-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s10ss4-plot1": () => {
            // Mediation Analysis
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s10ss4-plot1",
              [],
              {
                title: { text: "Mediation Path Diagram", font: { size: 18 } },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  { type: "rect", x0: 0.5, y0: 3.5, x1: 2.5, y1: 4.5 },
                  { type: "rect", x0: 7.5, y0: 3.5, x1: 9.5, y1: 4.5 },
                  { type: "rect", x0: 4, y0: 6.5, x1: 6, y1: 7.5 },
                ],
                annotations: [
                  { x: 1.5, y: 4, text: "<b>X</b><br>(Education)" },
                  { x: 8.5, y: 4, text: "<b>Y</b><br>(Income)" },
                  { x: 5, y: 7, text: "<b>M</b><br>(Job Skills)" },
                  {
                    x: 5,
                    y: 4,
                    text: "Path c / c'<br><b>Total / Direct Effect</b>",
                    showarrow: true,
                    ax: 0,
                    ay: 20,
                  },
                  {
                    x: 3.25,
                    y: 5.75,
                    text: "Path a",
                    showarrow: true,
                    ax: -30,
                    ay: -20,
                  },
                  {
                    x: 6.75,
                    y: 5.75,
                    text: "Path b",
                    showarrow: true,
                    ax: 30,
                    ay: -20,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s9ss1-plot1": () => {
            // Core Concepts (Survival)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s9ss1-plot1",
              [],
              {
                title: {
                  text: "Visualizing Censored Survival Data",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 32],
                  title: "Time (Days)",
                  zeroline: false,
                  showgrid: false,
                },
                yaxis: {
                  range: [0, 6],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "line",
                    x0: 0,
                    y0: 5,
                    x1: 10,
                    y1: 5,
                    line: { width: 3 },
                  },
                  {
                    type: "line",
                    x0: 0,
                    y0: 4,
                    x1: 20,
                    y1: 4,
                    line: { width: 3 },
                  },
                  {
                    type: "line",
                    x0: 0,
                    y0: 3,
                    x1: 15,
                    y1: 3,
                    line: { width: 3 },
                  },
                  {
                    type: "line",
                    x0: 0,
                    y0: 2,
                    x1: 25,
                    y1: 2,
                    line: { width: 3 },
                  },
                  {
                    type: "line",
                    x0: 0,
                    y0: 1,
                    x1: 30,
                    y1: 1,
                    line: { width: 3 },
                  },
                ],
                annotations: [
                  { x: -1, y: 5, text: "Patient A", xanchor: "right" },
                  { x: -1, y: 4, text: "Patient B", xanchor: "right" },
                  { x: -1, y: 3, text: "Patient C", xanchor: "right" },
                  { x: -1, y: 2, text: "Patient D", xanchor: "right" },
                  { x: -1, y: 1, text: "Patient E", xanchor: "right" },
                  {
                    x: 10,
                    y: 5,
                    text: '<i class="fas fa-skull-crossbones"></i> Event',
                    showarrow: false,
                  },
                  {
                    x: 20,
                    y: 4,
                    text: '<i class="fas fa-skull-crossbones"></i> Event',
                    showarrow: false,
                  },
                  {
                    x: 15,
                    y: 3,
                    text: '<i class="fas fa-question"></i> Censored',
                    showarrow: false,
                  },
                  {
                    x: 25,
                    y: 2,
                    text: '<i class="fas fa-skull-crossbones"></i> Event',
                    showarrow: false,
                  },
                  {
                    x: 30,
                    y: 1,
                    text: '<i class="fas fa-question"></i> Censored',
                    showarrow: false,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s9ss2-plot1": () => {
            // Kaplan-Meier
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            // Simulate K-M data
            const time = [0, 10, 20, 22, 28, 35];
            const survival = [1, 1, 0.8, 0.8, 0.6, 0.6];
            const lower_ci = survival.map((s) => Math.max(0, s - 0.15));
            const upper_ci = survival.map((s) => Math.min(1, s + 0.15));
            const censored_ticks = { x: [15, 30], y: [0.8, 0.6] };

            Plotly.newPlot(
              "p4s9ss2-plot1",
              [
                {
                  x: time,
                  y: survival,
                  type: "scatter",
                  mode: "lines",
                  line: { shape: "hv", color: "#3b82f6", width: 3 },
                  name: "K-M Estimate",
                },
                {
                  x: [...time, ...time.slice().reverse()],
                  y: [...upper_ci, ...lower_ci.slice().reverse()],
                  fill: "toself",
                  fillcolor: "rgba(59, 130, 246, 0.2)",
                  line: { color: "transparent" },
                  name: "95% CI",
                },
                {
                  x: censored_ticks.x,
                  y: censored_ticks.y,
                  mode: "markers",
                  marker: {
                    symbol: "line-ns-open",
                    size: 12,
                    color: "#3b82f6",
                  },
                  name: "Censored",
                },
              ],
              {
                title: "Reading a Kaplan-Meier Survival Curve",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Time", gridcolor: themeColors.gridColor },
                yaxis: {
                  title: "Probability of Survival, S(t)",
                  range: [-0.05, 1.05],
                  gridcolor: themeColors.gridColor,
                },
                showlegend: false,
                annotations: [
                  {
                    x: 20,
                    y: 0.8,
                    text: 'Downward "step"<br>indicates an event',
                    ax: 40,
                    ay: -40,
                  },
                  {
                    x: 15,
                    y: 0.8,
                    text: "Tick indicates<br>a censored observation",
                    ax: -40,
                    ay: -40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s9ss5-plot1": () => {
            // Competing Risks
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const time = [0, 10, 20, 30, 40, 50];
            const cif1 = [0, 0.1, 0.18, 0.25, 0.3, 0.33];
            const cif2 = [0, 0.05, 0.08, 0.1, 0.12, 0.13];

            Plotly.newPlot(
              "p4s9ss5-plot1",
              [
                {
                  x: time,
                  y: cif1,
                  type: "scatter",
                  mode: "lines",
                  line: { shape: "hv", color: "#ef4444", width: 3 },
                  name: "Event 1: Relapse",
                },
                {
                  x: time,
                  y: cif2,
                  type: "scatter",
                  mode: "lines",
                  line: { shape: "hv", color: "#3b82f6", width: 3 },
                  name: "Event 2: Other Causes",
                },
              ],
              {
                title: "Cumulative Incidence Function (CIF) Plot",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Time (Days)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Cumulative Incidence Probability",
                  range: [-0.05, 1.05],
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.05, y: 0.95 },
              },
              { responsive: true }
            );
          },
          "p4s11ss1-plot1": () => {
            // Bayesian Inference
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const trace = Array.from(
              { length: 400 },
              (_, i) =>
                2.5 + Math.sin(i / 10) * 0.5 + (Math.random() - 0.5) * 0.2
            );

            const fig = {
              data: [
                {
                  y: trace,
                  type: "scatter",
                  mode: "lines",
                  name: "Trace",
                  xaxis: "x1",
                  yaxis: "y1",
                  line: { color: "#3b82f6" },
                },
                {
                  x: trace,
                  type: "histogram",
                  name: "Posterior",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#10b981" },
                },
              ],
              layout: {
                title: "MCMC Diagnostics: Trace Plot and Posterior Density",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                yaxis1: {
                  title: "Parameter Value",
                  gridcolor: themeColors.gridColor,
                },
                xaxis1: {
                  title: "MCMC Iteration",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: { title: "Density", gridcolor: themeColors.gridColor },
                xaxis2: {
                  title: "Parameter Value",
                  gridcolor: themeColors.gridColor,
                },
                annotations: [
                  {
                    x: 0.5,
                    y: -0.2,
                    xref: "x1 domain",
                    yref: "paper",
                    text: '"Fat, hairy caterpillar"<br>indicates good convergence',
                    showarrow: false,
                  },
                  {
                    x: 0.5,
                    y: -0.2,
                    xref: "x2 domain",
                    yref: "paper",
                    text: "Histogram of trace samples<br>forms the posterior distribution",
                    showarrow: false,
                  },
                ],
              },
            };
            Plotly.newPlot("p4s11ss1-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s11ss2-plot1": () => {
            // Bayesian Regression
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const posterior = Array.from(
              { length: 1000 },
              () => 3.9 + (Math.random() - 0.5 + Math.random() - 0.5) * 1.5
            );

            Plotly.newPlot(
              "p4s11ss2-plot1",
              [
                {
                  x: posterior,
                  type: "histogram",
                  name: "Posterior",
                  marker: { color: "#3b82f6" },
                },
              ],
              {
                title:
                  "Posterior Distribution of a Regression Coefficient (β₁)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Value of Speed Coefficient",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: { title: "Density", gridcolor: themeColors.gridColor },
                shapes: [
                  {
                    type: "line",
                    x0: 2.5,
                    y0: 0,
                    x1: 2.5,
                    y1: 1,
                    yref: "paper",
                    line: { color: "#ef4444", dash: "dot" },
                  },
                  {
                    type: "line",
                    x0: 5.3,
                    y0: 0,
                    x1: 5.3,
                    y1: 1,
                    yref: "paper",
                    line: { color: "#ef4444", dash: "dot" },
                  },
                  {
                    type: "line",
                    x0: 0,
                    y0: 0,
                    x1: 0,
                    y1: 1,
                    yref: "paper",
                    line: { color: themeColors.textColor, width: 2 },
                  },
                ],
                annotations: [
                  {
                    x: 3.9,
                    y: 0.5,
                    yref: "paper",
                    text: "95% Credible Interval",
                    showarrow: false,
                    font: { size: 14 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s11ss3-plot1": () => {
            // Hierarchical Models
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const no_pool = [10, 1, 15, 25, 8];
            const partial_pool = [8, 5, 12, 15, 9];
            const complete_pool = 10;
            const y_labels = [
              "School A",
              "School B (small)",
              "School C",
              "School D",
              "School E",
            ];

            Plotly.newPlot(
              "p4s11ss3-plot1",
              [
                {
                  x: no_pool,
                  y: y_labels,
                  mode: "markers",
                  name: "No Pooling",
                  marker: { color: "#3b82f6", size: 12 },
                },
                {
                  x: partial_pool,
                  y: y_labels,
                  mode: "markers",
                  name: "Partial Pooling (Hierarchical)",
                  marker: { color: "#10b981", size: 12 },
                },
              ],
              {
                title: "Shrinkage Effect of Hierarchical Models",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Estimated Effect (e.g., avg. test score)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: { showticklabels: false },
                legend: { x: 0.6, y: 1.2 },
                shapes: [
                  {
                    type: "line",
                    x0: complete_pool,
                    y0: -1,
                    x1: complete_pool,
                    y1: 5,
                    line: { color: "#ef4444", dash: "dot" },
                  },
                ],
                annotations: [
                  {
                    x: complete_pool,
                    y: 4.5,
                    text: "Complete Pooling<br>(Overall Average)",
                    font: { color: "#ef4444" },
                  },
                  {
                    x: 1,
                    y: 1,
                    text: 'Noisy estimate for<br>small school is "shrunk"<br>towards the mean',
                    showarrow: true,
                    ax: 80,
                    ay: -40,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s11ss4-plot1": () => {
            // Model Checking
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const good_trace = Array.from(
              { length: 200 },
              (_, i) =>
                2.5 + Math.sin(i / 5) * 0.2 + (Math.random() - 0.5) * 0.1
            );
            const bad_trace = Array.from(
              { length: 200 },
              (_, i) => (i < 100 ? 1 : 4) + Math.random() * 0.5
            );

            const fig = {
              data: [
                {
                  y: good_trace,
                  type: "scatter",
                  mode: "lines",
                  name: "Good",
                  xaxis: "x1",
                  yaxis: "y1",
                  line: { color: "#10b981" },
                },
                {
                  y: good_trace.map((v) => v + Math.random() * 0.1),
                  type: "scatter",
                  mode: "lines",
                  name: "Good",
                  xaxis: "x1",
                  yaxis: "y1",
                  line: { color: "#10b981" },
                },
                {
                  y: bad_trace,
                  type: "scatter",
                  mode: "lines",
                  name: "Bad",
                  xaxis: "x2",
                  yaxis: "y2",
                  line: { color: "#ef4444" },
                },
                {
                  y: bad_trace.map((v) => v + Math.random() * 0.2 - 1),
                  type: "scatter",
                  mode: "lines",
                  name: "Bad",
                  xaxis: "x2",
                  yaxis: "y2",
                  line: { color: "#ef4444" },
                },
              ],
              layout: {
                title: "MCMC Convergence Diagnostics (Trace Plots)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Good (Converged)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: {
                  title: "Parameter Value",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "Bad (Not Converged)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: {
                  showticklabels: false,
                  gridcolor: themeColors.gridColor,
                },
              },
            };
            Plotly.newPlot("p4s11ss4-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p4s12ss1-plot1": () => {
            // Graph Theory Basics
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p4s12ss1-plot1",
              [],
              {
                title: { text: "Common Graph Types", font: { size: 18 } },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  // Undirected
                  { type: "circle", x0: 1, y0: 6, x1: 2, y1: 7 },
                  { type: "circle", x0: 3, y0: 6, x1: 4, y1: 7 },
                  { type: "line", x0: 2, y0: 6.5, x1: 3, y1: 6.5 },
                  // Directed
                  { type: "circle", x0: 1, y0: 2, x1: 2, y1: 3 },
                  { type: "circle", x0: 3, y0: 2, x1: 4, y1: 3 },
                  // Weighted
                  { type: "circle", x0: 7, y0: 6, x1: 8, y1: 7 },
                  { type: "circle", x0: 7, y0: 2, x1: 8, y1: 3 },
                  { type: "line", x0: 7.5, y0: 6, x1: 7.5, y1: 3 },
                ],
                annotations: [
                  { x: 2.5, y: 8, text: "Undirected" },
                  {
                    x: 2.5,
                    y: 1,
                    text: "Directed",
                    showarrow: true,
                    ax: 0,
                    ay: 20,
                  },
                  { x: 7.5, y: 8, text: "Weighted" },
                  { x: 7.5, y: 4.5, text: "Weight = 10", bgcolor: "white" },
                ],
              },
              { responsive: true }
            );
          },
          "p4s12ss2-plot1": () => {
            // Network Metrics
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const nodes = {
              x: [2, 2, 5, 5, 8],
              y: [7, 3, 5, 2, 3],
              size: [10, 10, 25, 10, 10], // Size by Betweenness
              text: ["Ali", "Bilal", "Dania (Bridge)", "Chirag", "Esha"],
            };
            const edges = [
              { x: [2, 2], y: [7, 3] },
              { x: [2, 5], y: [3, 2] },
              { x: [5, 5], y: [5, 2] },
              { x: [5, 8], y: [5, 3] },
            ];

            Plotly.newPlot(
              "p4s12ss2-plot1",
              [
                ...edges.map((e) => ({
                  x: e.x,
                  y: e.y,
                  mode: "lines",
                  line: { color: themeColors.textColor },
                })),
                {
                  x: nodes.x,
                  y: nodes.y,
                  mode: "markers+text",
                  text: nodes.text,
                  marker: {
                    size: nodes.size.map((s) => s * 2),
                    color: "#3b82f6",
                  },
                  textposition: "top center",
                },
              ],
              {
                title: "Visualizing Betweenness Centrality",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                showlegend: false,
              },
              { responsive: true }
            );
          },
          "p4s12ss3-plot1": () => {
            // Community Detection
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            // Simplified Zachary's Karate Club
            const nodes = {
              x: [1, 3, 3, 5, 5, 7, 7, 9],
              y: [5, 7, 3, 9, 1, 7, 3, 5],
              color: [
                "#f59e0b",
                "#f59e0b",
                "#f59e0b",
                "#f59e0b",
                "#3b82f6",
                "#3b82f6",
                "#3b82f6",
                "#3b82f6",
              ],
              text: ["1", "2", "3", "4", "5", "6", "7", "8"],
            };

            Plotly.newPlot(
              "p4s12ss3-plot1",
              [
                {
                  x: nodes.x,
                  y: nodes.y,
                  mode: "markers+text",
                  text: nodes.text,
                  marker: { size: 25, color: nodes.color },
                },
              ],
              {
                title: "Community Detection in a Social Network",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                showlegend: false,
              },
              { responsive: true }
            );
          },
          "p4s13ss1-plot1": () => {
            // Incremental Response Models
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            Plotly.newPlot(
              "p4s13ss1-plot1",
              [],
              {
                title: { text: "The Four Uplift Personas", font: { size: 18 } },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { range: [0, 10], title: "Would Convert WITHOUT Offer" },
                yaxis: {
                  range: [0, 10],
                  title: "Would Convert WITH Offer",
                  autorange: "reversed",
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 0.5,
                    x1: 9.5,
                    y1: 4.5,
                    fillcolor: "#10b981",
                  }, // Persuadables
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 5.5,
                    x1: 9.5,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  }, // Sure Things
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 0.5,
                    x1: 4.5,
                    y1: 4.5,
                    fillcolor: "#ef4444",
                  }, // Sleeping Dogs
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 5.5,
                    x1: 4.5,
                    y1: 9.5,
                    fillcolor: isDark ? "#4b5563" : "#e5e7eb",
                  }, // Lost Causes
                ],
                annotations: [
                  { x: 2.5, y: 0, text: "NO" },
                  { x: 7.5, y: 0, text: "YES" },
                  { x: 0, y: 2.5, text: "YES" },
                  { x: 0, y: 7.5, text: "NO" },
                  {
                    x: 7.5,
                    y: 2.5,
                    text: "<b>Persuadables</b><br>TARGET",
                    font: { color: "white" },
                  },
                  {
                    x: 7.5,
                    y: 7.5,
                    text: "<b>Sure Things</b><br>(Don't Waste Offer)",
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 2.5,
                    text: "<b>Sleeping Dogs</b><br>(DO NOT CONTACT)",
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 7.5,
                    text: "<b>Lost Causes</b>",
                    font: { color: isDark ? "white" : "black" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p4s13ss2-plot1": () => {
            // Uplift Model Evaluation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const x = [0, 0.2, 0.4, 0.6, 0.8, 1];
            const y_model = [0, 0.3, 0.45, 0.55, 0.6, 0.6];
            const y_random = [0, 0.12, 0.24, 0.36, 0.48, 0.6];

            Plotly.newPlot(
              "p4s13ss2-plot1",
              [
                {
                  x: [0, 1],
                  y: [0, 0.6],
                  type: "scatter",
                  mode: "lines",
                  name: "Random Targeting",
                  line: { color: themeColors.textColor, dash: "dot" },
                },
                {
                  x: x,
                  y: y_model,
                  type: "scatter",
                  mode: "lines",
                  name: "Uplift Model",
                  fill: "tonexty",
                  line: { color: "#3b82f6", width: 4 },
                },
              ],
              {
                title: "Qini Curve for Uplift Model Evaluation",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Proportion of Population Targeted",
                  range: [-0.05, 1.05],
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Cumulative Incremental Gain",
                  range: [-0.05, 0.7],
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.05, y: 0.95 },
                annotations: [
                  {
                    x: 0.5,
                    y: 0.15,
                    text: "Area between curves<br>= Qini Coefficient",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p5s1ss1-plot1": () => {
            // Data Splitting Strategies
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            Plotly.newPlot(
              "p5s1ss1-plot1",
              [],
              {
                title: {
                  text: "The Data Splitting Workflow",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4,
                    x1: 6,
                    y1: 5.5,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 3,
                    y1: 2.5,
                    fillcolor: "#f59e0b",
                  },
                  {
                    type: "rect",
                    x0: 3.5,
                    y0: 1,
                    x1: 6,
                    y1: 2.5,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 4,
                    x1: 9.5,
                    y1: 5.5,
                    fillcolor: "#ef4444",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "Full Dataset",
                    font: { color: "white" },
                  },
                  {
                    x: 3.25,
                    y: 4.75,
                    text: "Training Set (~75%)",
                    font: { color: "white" },
                  },
                  {
                    x: 1.75,
                    y: 1.75,
                    text: "Validation Set",
                    font: { color: "white" },
                  },
                  {
                    x: 4.75,
                    y: 1.75,
                    text: "Tuning Set",
                    font: { color: "white" },
                  },
                  {
                    x: 8.25,
                    y: 4.75,
                    text: "Test Set (~25%)<br><b>(Locked Away)</b>",
                    font: { color: "white" },
                  },
                  { x: 5, y: 6.75, text: "Split 1" },
                  { x: 3.25, y: 3.25, text: "Split 2 (Optional)" },
                ],
              },
              { responsive: true }
            );
          },
          "p5s1ss2-plot1": () => {
            // Cross-Validation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            const folds = ["Fold 1", "Fold 2", "Fold 3", "Fold 4", "Fold 5"];
            const colors = { train: "#3b82f6", test: "#ef4444" };

            let shapes = [];
            let annotations = [];
            folds.forEach((fold, i) => {
              annotations.push({
                x: -1,
                y: 4 - i,
                text: `<b>Run ${i + 1}</b>`,
                xanchor: "right",
                showarrow: false,
              });
              for (let j = 0; j < 5; j++) {
                shapes.push({
                  type: "rect",
                  x0: j * 2,
                  y0: 4 - i - 0.4,
                  x1: j * 2 + 1.8,
                  y1: 4 - i + 0.4,
                  fillcolor: i === j ? colors.test : colors.train,
                  line: { width: 0 },
                });
              }
            });

            Plotly.newPlot(
              "p5s1ss2-plot1",
              [],
              {
                title: {
                  text: "The 5-Fold Cross-Validation Process",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [-2, 10],
                  showgrid: false,
                  zeroline: false,
                  tickvals: [0.9, 2.9, 4.9, 6.9, 8.9],
                  ticktext: folds,
                },
                yaxis: {
                  range: [-1, 5],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: shapes,
                annotations: [
                  ...annotations,
                  {
                    x: 8.5,
                    y: 4.8,
                    text: "Train",
                    fillcolor: colors.train,
                    borderpad: 4,
                  },
                  {
                    x: 9.5,
                    y: 4.8,
                    text: "Test",
                    fillcolor: colors.test,
                    borderpad: 4,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p5s1ss3-plot1": () => {
            // Resampling Methods
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p5s1ss3-plot1",
              [],
              {
                title: {
                  text: "The Bootstrap Resampling Process",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 4.5,
                    x1: 3,
                    y1: 6,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 4.5,
                    x1: 6.25,
                    y1: 6,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 4.5,
                    x1: 9.5,
                    y1: 6,
                    fillcolor: "#eab308",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 0.5,
                    x1: 7,
                    y1: 2,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "Original Sample (Size N)",
                    font: { color: "white" },
                  },
                  { x: 5, y: 7, text: "Sample with replacement (Size N)" },
                  { x: 1.75, y: 5.25, text: "Bootstrap<br>Sample 1" },
                  { x: 5, y: 5.25, text: "Bootstrap<br>Sample 2" },
                  { x: 8.25, y: 5.25, text: "... (x 5000)" },
                  { x: 1.75, y: 3.5, text: "Calc Stat₁" },
                  { x: 5, y: 3.5, text: "Calc Stat₂" },
                  { x: 8.25, y: 3.5, text: "Calc Stat₅₀₀₀" },
                  { x: 5, y: 2.5, text: "<b>Collect All Statistics</b>" },
                  {
                    x: 5,
                    y: 1.25,
                    text: "<b>Bootstrap Distribution</b>",
                    font: { color: "white", size: 16 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p5s1ss4-plot1": () => {
            // Hyperparameter Tuning
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const fig = {
              data: [
                {
                  x: [1, 2, 3, 1, 2, 3, 1, 2, 3],
                  y: [1, 1, 1, 2, 2, 2, 3, 3, 3],
                  mode: "markers",
                  name: "Grid",
                  xaxis: "x1",
                  yaxis: "y1",
                  marker: { color: "#ef4444", size: 10 },
                },
                {
                  x: [1.2, 2.8, 0.9, 2.1, 0.5, 1.8, 2.5, 1.1, 2.9],
                  y: [2.8, 1.1, 1.9, 2.2, 0.8, 0.5, 2.9, 1.2, 0.4],
                  mode: "markers",
                  name: "Random",
                  xaxis: "x2",
                  yaxis: "y2",
                  marker: { color: "#10b981", size: 10 },
                },
              ],
              layout: {
                title: "Grid Search vs. Random Search",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Unimportant Parameter",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: {
                  title: "Important Parameter",
                  gridcolor: themeColors.gridColor,
                },
                xaxis2: {
                  title: "Unimportant Parameter",
                  gridcolor: themeColors.gridColor,
                },
                yaxis2: { showticklabels: false },
              },
            };
            Plotly.newPlot("p5s1ss4-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p5s1ss5-plot1": () => {
            // Model Complexity
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const x = Array.from({ length: 12 }, (_, i) => i);
            const y = x.map(
              (v) => Math.sin(v / 2) * 5 + v * 0.5 + (Math.random() - 0.5) * 2
            );

            const fig = {
              data: [
                {
                  x: x,
                  y: y,
                  mode: "markers",
                  type: "scatter",
                  name: "Data",
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  x: x,
                  y: y,
                  mode: "markers",
                  type: "scatter",
                  name: "Data",
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  x: x,
                  y: y,
                  mode: "markers",
                  type: "scatter",
                  name: "Data",
                  xaxis: "x3",
                  yaxis: "y3",
                },
                // Lines
                {
                  x: [-1, 12],
                  y: [2, 8],
                  mode: "lines",
                  type: "scatter",
                  name: "Underfit",
                  xaxis: "x1",
                  yaxis: "y1",
                  line: { color: "#ef4444", width: 3 },
                },
                {
                  x: x,
                  y: y.map((v) => v - (Math.random() - 0.5) * 0.5),
                  mode: "lines",
                  type: "scatter",
                  name: "Overfit",
                  xaxis: "x2",
                  yaxis: "y2",
                  line: { color: "#ef4444", width: 3, shape: "spline" },
                },
                {
                  x: x,
                  y: x.map((v) => Math.sin(v / 2) * 5 + v * 0.5),
                  mode: "lines",
                  type: "scatter",
                  name: "Good Fit",
                  xaxis: "x3",
                  yaxis: "y3",
                  line: { color: "#10b981", width: 3 },
                },
              ],
              layout: {
                title: "Underfitting vs. Overfitting",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 3, pattern: "independent" },
                showlegend: false,
                xaxis1: { title: "Underfit (High Bias)" },
                yaxis1: { showticklabels: false },
                xaxis2: { title: "Overfit (High Variance)" },
                yaxis2: { showticklabels: false },
                xaxis3: { title: "Good Fit" },
                yaxis3: { showticklabels: false },
              },
            };
            Plotly.newPlot("p5s1ss5-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p5s1ss6-plot1": () => {
            // Bias-Variance Tradeoff
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const complexity = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
            const bias_sq = complexity.map((c) => 1 / (c * 0.5));
            const variance = complexity.map((c) => Math.pow(c, 2) / 50);
            const total_error = complexity.map(
              (c, i) => bias_sq[i] + variance[i] + 0.2
            );

            Plotly.newPlot(
              "p5s1ss6-plot1",
              [
                {
                  x: complexity,
                  y: bias_sq,
                  mode: "lines",
                  name: "Bias²",
                  line: { color: "#3b82f6", width: 3, dash: "dot" },
                },
                {
                  x: complexity,
                  y: variance,
                  mode: "lines",
                  name: "Variance",
                  line: { color: "#eab308", width: 3, dash: "dot" },
                },
                {
                  x: complexity,
                  y: total_error,
                  mode: "lines",
                  name: "Total Error",
                  line: { color: "#ef4444", width: 4 },
                },
              ],
              {
                title: "The Bias-Variance Tradeoff",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Model Complexity",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Error",
                  range: [0, 3],
                  gridcolor: themeColors.gridColor,
                },
                legend: { x: 0.6, y: 0.95 },
                shapes: [
                  {
                    type: "line",
                    x0: 4.5,
                    y0: 0,
                    x1: 4.5,
                    y1: 3,
                    line: { dash: "dash" },
                  },
                ],
                annotations: [
                  { x: 4.5, y: 0.1, text: "Optimal<br>Complexity", ay: -40 },
                ],
              },
              { responsive: true }
            );
          },
          "p5s2ss2-plot1": () => {
            // Regression Metrics
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const fitted = Array.from({ length: 100 }, (_, i) => i);
            const residuals = fitted.map(() => (Math.random() - 0.5) * 20);

            Plotly.newPlot(
              "p5s2ss2-plot1",
              [
                {
                  x: fitted,
                  y: residuals,
                  mode: "markers",
                  marker: { color: themeColors.textColor, opacity: 0.6 },
                },
              ],
              {
                title: "Residuals vs. Fitted Values Plot",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Fitted (Predicted) Values",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Residuals (Actual - Predicted)",
                  gridcolor: themeColors.gridColor,
                  zeroline: true,
                },
                shapes: [
                  {
                    type: "line",
                    x0: 30,
                    x1: 30,
                    y0: 0,
                    y1: residuals[30],
                    line: { color: "#ef4444", width: 3 },
                  },
                  {
                    type: "line",
                    x0: 70,
                    x1: 70,
                    y0: 0,
                    y1: residuals[70],
                    line: { color: "#ef4444", width: 3 },
                  },
                ],
                annotations: [
                  {
                    x: 50,
                    y: 15,
                    text: "MAE is the average<br>length of these red lines",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p5s2ss3-plot1": () => {
            // Clustering Metrics
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            Plotly.newPlot(
              "p5s2ss3-plot1",
              [
                {
                  x: [1, 2, 1.5],
                  y: [7, 8, 7.5],
                  mode: "markers",
                  marker: { color: "#3b82f6" },
                }, // Cluster B
                {
                  x: [6, 7, 6.5],
                  y: [3, 4, 3.5],
                  mode: "markers",
                  marker: { color: "#10b981" },
                }, // Cluster A
                {
                  x: [5],
                  y: [4],
                  mode: "markers",
                  name: "Point P",
                  marker: {
                    color: "#10b981",
                    size: 12,
                    line: { color: "white", width: 2 },
                  },
                },
              ],
              {
                title: "Calculating the Silhouette Score",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: { title: "Feature 1", gridcolor: themeColors.gridColor },
                yaxis: { title: "Feature 2", gridcolor: themeColors.gridColor },
                showlegend: false,
                shapes: [
                  {
                    type: "line",
                    x0: 5,
                    y0: 4,
                    x1: 1.5,
                    y1: 7.5,
                    line: { color: "#ef4444", dash: "dot" },
                    name: "b",
                  },
                  {
                    type: "line",
                    x0: 5,
                    y0: 4,
                    x1: 6.5,
                    y1: 3.5,
                    line: { color: "#f59e0b", dash: "dot" },
                    name: "a",
                  },
                ],
                annotations: [
                  {
                    x: 5.75,
                    y: 3.75,
                    text: "<b>a</b> = avg distance<br>to own cluster",
                  },
                  {
                    x: 3.25,
                    y: 5.75,
                    text: "<b>b</b> = avg distance<br>to nearest cluster",
                  },
                  {
                    x: 5,
                    y: 8,
                    text: "<b>Score = (b-a)/max(a,b)</b>",
                    bgcolor: "white",
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p5s3ss1-plot1": () => {
            // Feature Importance Methods
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };

            const importance_data = {
              features: [
                "bill_length_mm",
                "flipper_length_mm",
                "bill_depth_mm",
                "body_mass_g",
                "island",
                "sex",
                "year",
              ],
              importance: [0.15, 0.12, 0.08, 0.07, 0.04, 0.02, 0.005],
            };

            Plotly.newPlot(
              "p5s3ss1-plot1",
              [
                {
                  x: importance_data.importance,
                  y: importance_data.features,
                  type: "bar",
                  orientation: "h",
                  marker: { color: "#3b82f6" },
                },
              ],
              {
                title: "Global Feature Importance (SHAP)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "mean(|SHAP value|) - Average impact on model output",
                },
                yaxis: { autorange: "reversed" },
                margin: { l: 120 },
              },
              { responsive: true }
            );
          },
          "p5s3ss2-plot1": () => {
            // Model Behavior Visualization
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const x = Array.from({ length: 30 }, (_, i) => i);
            const ice_curves_y = [];
            for (let i = 0; i < 10; i++) {
              ice_curves_y.push(
                x.map(
                  (v) =>
                    10 +
                    v * 0.5 +
                    Math.sin(v + i) * 2 +
                    (Math.random() - 0.5) * 5
                )
              );
            }
            const pdp_y = x.map(
              (_, i) =>
                ice_curves_y.reduce((acc, curve) => acc + curve[i], 0) /
                ice_curves_y.length
            );

            let traces = ice_curves_y.map((y_vals) => ({
              x: x,
              y: y_vals,
              mode: "lines",
              line: { color: themeColors.textColor, width: 0.5 },
              opacity: 0.5,
            }));
            traces.push({
              x: x,
              y: pdp_y,
              mode: "lines",
              name: "PDP",
              line: { color: "#3b82f6", width: 5 },
            });

            Plotly.newPlot(
              "p5s3ss2-plot1",
              traces,
              {
                title:
                  "Partial Dependence (PDP) and Individual Conditional Expectation (ICE) Plots",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Feature Value (e.g., Horsepower)",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: {
                  title: "Predicted Outcome (e.g., MPG)",
                  gridcolor: themeColors.gridColor,
                },
                showlegend: false,
                annotations: [
                  {
                    x: 25,
                    y: 28,
                    text: "<b>PDP (Average Effect)</b>",
                    font: { color: "#3b82f6" },
                  },
                  {
                    x: 5,
                    y: 5,
                    text: "<i>ICE Curves<br>(Individual Effects)</i>",
                    font: { color: themeColors.textColor, size: 10 },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p5s4ss7-plot1": () => {
            // Best Practices (Ethics)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p5s4ss7-plot1",
              [],
              {
                title: {
                  text: "The Responsible AI Development Lifecycle",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  { type: "circle", x0: 4, y0: 8, x1: 6, y1: 10 },
                  { type: "circle", x0: 7, y0: 5, x1: 9, y1: 7 },
                  { type: "circle", x0: 4, y0: 0, x1: 6, y1: 2 },
                  { type: "circle", x0: 1, y0: 5, x1: 3, y1: 7 },
                ],
                annotations: [
                  { x: 5, y: 9, text: "1. Plan &<br>Design" },
                  { x: 8, y: 6, text: "2. Build &<br>Validate" },
                  { x: 5, y: 1, text: "3. Deploy &<br>Monitor" },
                  { x: 2, y: 6, text: "4. Refine &<br>Iterate" },
                  { x: 6.5, y: 7.5, text: "➔", font: { size: 24 } },
                  { x: 6.5, y: 3.5, text: "➔", font: { size: 24 } },
                  { x: 3.5, y: 3.5, text: "➔", font: { size: 24 } },
                  { x: 3.5, y: 7.5, text: "➔", font: { size: 24 } },
                  {
                    x: 5,
                    y: 5,
                    text: "<b>Ethical Review<br>at Every Stage</b>",
                    bgcolor: "rgba(239, 68, 68, 0.7)",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p6s2ss2-plot1": () => {
            // Dashboard Creation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f1f5f9",
            };
            Plotly.newPlot(
              "p6s2ss2-plot1",
              [],
              {
                title: "Anatomy of a Dashboard",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 0.5,
                    x1: 3.5,
                    y1: 9.5,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 5.5,
                    x1: 9.5,
                    y1: 9.5,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 4,
                    y0: 0.5,
                    x1: 6.5,
                    y1: 4.5,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 0.5,
                    x1: 9.5,
                    y1: 4.5,
                    fillcolor: themeColors.boxColor,
                    line: { width: 0 },
                  },
                ],
                annotations: [
                  {
                    x: 2,
                    y: 9,
                    text: "<b>Sidebar</b>",
                    showarrow: false,
                    yanchor: "top",
                  },
                  { x: 2, y: 8, text: "Input Controls", showarrow: false },
                  {
                    x: 2,
                    y: 7,
                    text: "(Sliders, Dropdowns)",
                    showarrow: false,
                    font: { size: 10 },
                  },
                  {
                    x: 6.75,
                    y: 9,
                    text: "<b>Main Panel</b>",
                    showarrow: false,
                    yanchor: "top",
                  },
                  { x: 6.75, y: 7.5, text: "Plot Output", showarrow: false },
                  { x: 5.25, y: 2.5, text: "Value Box 1", showarrow: false },
                  { x: 8.25, y: 2.5, text: "Value Box 2", showarrow: false },
                ],
              },
              { responsive: true }
            );
          },
          "p6s1ss2-plot1": () => {
            // Visualization for Communication
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const mtcars_data = {
              wt: [2.62, 2.875, 2.32, 3.215, 3.44, 1.513, 3.17, 3.15],
              mpg: [21, 21.4, 22.8, 18.7, 18.1, 33.9, 24.4, 22.8],
            };
            const highlight_data = { wt: [1.513], mpg: [33.9] };
            const fig = {
              data: [
                {
                  x: mtcars_data.wt,
                  y: mtcars_data.mpg,
                  mode: "markers",
                  type: "scatter",
                  name: "Exploratory",
                  marker: { color: themeColors.textColor },
                  xaxis: "x1",
                  yaxis: "y1",
                },
                {
                  x: mtcars_data.wt,
                  y: mtcars_data.mpg,
                  mode: "markers",
                  type: "scatter",
                  name: "Explanatory",
                  marker: { color: "grey", opacity: 0.5 },
                  xaxis: "x2",
                  yaxis: "y2",
                },
                {
                  x: highlight_data.wt,
                  y: highlight_data.mpg,
                  mode: "markers",
                  type: "scatter",
                  name: "Highlight",
                  marker: { color: "#ef4444", size: 12 },
                  xaxis: "x2",
                  yaxis: "y2",
                },
              ],
              layout: {
                title: "From Exploration to Explanation",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                grid: { rows: 1, columns: 2, pattern: "independent" },
                showlegend: false,
                xaxis1: {
                  title: "Exploratory Plot",
                  gridcolor: themeColors.gridColor,
                },
                yaxis1: { title: "mpg", gridcolor: themeColors.gridColor },
                xaxis2: {
                  title: "Explanatory Plot",
                  gridcolor: themeColors.gridColor,
                },
                annotations: [
                  {
                    x: 2.5,
                    y: 33,
                    xref: "x2",
                    yref: "y2",
                    text: "Lightweight cars are<br>most efficient",
                    ax: -50,
                    ay: -40,
                    font: { color: "#ef4444" },
                  },
                ],
              },
            };
            Plotly.newPlot("p6s1ss2-plot1", fig.data, fig.layout, {
              responsive: true,
            });
          },
          "p6s3ss2-plot1": () => {
            // Versioned & Parameterized Reports
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            Plotly.newPlot(
              "p6s3ss2-plot1",
              [],
              {
                title: {
                  text: "Parameterized Reporting Workflow",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 3,
                    x1: 3.5,
                    y1: 7,
                    fillcolor: "#a855f7",
                  }, // Rmd Template
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 7,
                    x1: 9.5,
                    y1: 9,
                    fillcolor: "#10b981",
                  }, // Report A
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 4,
                    x1: 9.5,
                    y1: 6,
                    fillcolor: "#10b981",
                  }, // Report B
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 1,
                    x1: 9.5,
                    y1: 3,
                    fillcolor: "#10b981",
                  }, // Report C
                ],
                annotations: [
                  {
                    x: 2,
                    y: 7.5,
                    text: "<b>.Rmd Template</b><br>with <code>params</code>",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 8,
                    text: '<i>params = "Karachi"</i>',
                    showarrow: true,
                    ax: -50,
                    ay: 0,
                  },
                  {
                    x: 5,
                    y: 5,
                    text: '<i>params = "Lahore"</i>',
                    showarrow: true,
                    ax: -50,
                    ay: 0,
                  },
                  {
                    x: 5,
                    y: 2,
                    text: '<i>params = "Islamabad"</i>',
                    showarrow: true,
                    ax: -50,
                    ay: 0,
                  },
                  {
                    x: 8,
                    y: 8,
                    text: "Karachi<br>Report.html",
                    font: { color: "white" },
                  },
                  {
                    x: 8,
                    y: 5,
                    text: "Lahore<br>Report.html",
                    font: { color: "white" },
                  },
                  {
                    x: 8,
                    y: 2,
                    text: "Islamabad<br>Report.html",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p6s3ss4-plot1": () => {
            // Workflow Automation
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            const nodes = {
              x: [1, 3, 3, 5, 7],
              y: [4, 6, 2, 4, 4],
              text: ["Raw Data", "Clean Data", "Plot", "Model", "Report"],
              color: ["#eab308", "#3b82f6", "#10b981", "#a855f7", "#ef4444"],
            };
            const edges = [
              { x0: 1.5, y0: 4, x1: 2.5, y1: 5.5, label: "" },
              { x0: 1.5, y0: 4, x1: 2.5, y1: 2.5, label: "" },
              { x0: 3.5, y0: 6, x1: 4.5, y1: 4.5, label: "" },
              { x0: 3.5, y0: 2, x1: 6.5, y1: 3.5, label: "" },
              { x0: 5.5, y0: 4, x1: 6.5, y1: 4, label: "" },
            ];
            Plotly.newPlot(
              "p6s3ss4-plot1",
              [
                {
                  type: "scatter",
                  x: nodes.x,
                  y: nodes.y,
                  mode: "markers+text",
                  marker: { size: 60, color: nodes.color },
                  text: nodes.text,
                  textfont: { color: "white", size: 12 },
                },
              ],
              {
                title: "Analysis Dependency Graph (DAG)",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                  scaleanchor: "x",
                },
                showlegend: false,
                shapes: edges.map((e) => ({
                  type: "line",
                  x0: e.x0,
                  y0: e.y0,
                  x1: e.x1,
                  y1: e.y1,
                  line: { color: themeColors.textColor, width: 2 },
                })),
              },
              { responsive: true }
            );
          },
          "p6s4ss2-plot1": () => {
            // REST APIs
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p6s4ss2-plot1",
              [],
              {
                title: {
                  text: "The API Request-Response Cycle",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 3,
                    x1: 3.5,
                    y1: 5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 6.5,
                    y0: 3,
                    x1: 9.5,
                    y1: 5,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  {
                    x: 2,
                    y: 5.5,
                    text: "<b>Client Application</b><br>(e.g., Web Browser, Python Script)",
                  },
                  {
                    x: 8,
                    y: 5.5,
                    text: "<b>Plumber API Server</b><br>(Running your R code)",
                  },
                  {
                    x: 5,
                    y: 4.5,
                    text: "<b>HTTP Request ➔</b><br>GET /predict?wt=3.2",
                    showarrow: true,
                    ax: -80,
                    ay: 0,
                  },
                  {
                    x: 5,
                    y: 3.5,
                    text: '<b>JSON Response ￩</b><br>{ "predicted_mpg": 21.5 }',
                    showarrow: true,
                    ax: 80,
                    ay: 0,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p6s4ss3-plot1": () => {
            // Model Monitoring
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            const train_data = Array.from(
              { length: 1000 },
              () => 10 + (Math.random() - 0.5 + Math.random() - 0.5) * 4
            );
            const live_data = Array.from(
              { length: 500 },
              () => 15 + (Math.random() - 0.5 + Math.random() - 0.5) * 4
            );

            Plotly.newPlot(
              "p6s4ss3-plot1",
              [
                {
                  x: train_data,
                  type: "histogram",
                  name: "Training Distribution",
                  marker: { color: "#3b82f6" },
                  histnorm: "probability density",
                  opacity: 0.7,
                },
                {
                  x: live_data,
                  type: "histogram",
                  name: "Live Data Distribution",
                  marker: { color: "#f59e0b" },
                  histnorm: "probability density",
                  opacity: 0.7,
                },
              ],
              {
                title: "Detecting Data Drift",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Feature Value",
                  gridcolor: themeColors.gridColor,
                },
                yaxis: { title: "Density", gridcolor: themeColors.gridColor },
                legend: { x: 0.6, y: 0.95 },
                barmode: "overlay",
                annotations: [
                  {
                    x: 16,
                    y: 0.1,
                    text: "<b>Significant Drift Detected!</b><br>Live data has a different<br>distribution than training data.",
                    bgcolor: "rgba(239, 68, 68, 0.7)",
                    font: { color: "white" },
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p6s5ss1-plot1": () => {
            // Docker for R
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p6s5ss1-plot1",
              [],
              {
                title: { text: "The Docker Workflow", font: { size: 18 } },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 6],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 2.5,
                    x1: 2.5,
                    y1: 3.5,
                    fillcolor: "#a855f7",
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 2.5,
                    x1: 5.75,
                    y1: 3.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 2.5,
                    x1: 9,
                    y1: 3.5,
                    fillcolor: "#10b981",
                  },
                ],
                annotations: [
                  { x: 1.5, y: 4, text: "<b>Dockerfile</b><br>(Text Recipe)" },
                  {
                    x: 4.75,
                    y: 4,
                    text: "<b>Image</b><br>(Read-only Template)",
                  },
                  {
                    x: 8,
                    y: 4,
                    text: "<b>Container</b><br>(Running Instance)",
                  },
                  {
                    x: 3.125,
                    y: 3,
                    text: "docker build ➔",
                    font: { size: 14 },
                  },
                  { x: 6.375, y: 3, text: "docker run ➔", font: { size: 14 } },
                ],
              },
              { responsive: true }
            );
          },
          "p6s5ss3-plot1": () => {
            // Orchestration
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            Plotly.newPlot(
              "p6s5ss3-plot1",
              [],
              {
                title: {
                  text: "Simplified Kubernetes Architecture",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 7,
                    x1: 7,
                    y1: 9,
                    fillcolor: "#ef4444",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 3,
                    y1: 5,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 3.75,
                    y0: 1,
                    x1: 6.25,
                    y1: 5,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 7,
                    y0: 1,
                    x1: 9.5,
                    y1: 5,
                    fillcolor: themeColors.boxColor,
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8,
                    text: "<b>Control Plane (The Brain)</b>",
                    font: { color: "white" },
                  },
                  { x: 1.75, y: 5.5, text: "<b>Worker Node 1</b>" },
                  { x: 5, y: 5.5, text: "<b>Worker Node 2</b>" },
                  { x: 8.25, y: 5.5, text: "<b>Worker Node 3</b>" },
                  { x: 1.75, y: 3, text: "Pod A\nPod B" },
                  { x: 5, y: 3, text: "Pod C" },
                  { x: 8.25, y: 3, text: "Pod D\nPod E\nPod F" },
                  {
                    x: 5,
                    y: 6,
                    text: "Manages",
                    showarrow: true,
                    ax: 0,
                    ay: -30,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p6s5ss4-plot1": () => {
            // Cloud Data Management
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p6s5ss4-plot1",
              [],
              {
                title: { text: "Cloud Data Architecture", font: { size: 18 } },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 7,
                    x1: 7,
                    y1: 9,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 0.5,
                    y0: 1,
                    x1: 4.5,
                    y1: 3,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 1,
                    x1: 9.5,
                    y1: 3,
                    fillcolor: "#eab308",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8,
                    text: "<b>Application Servers (Compute)</b>",
                    font: { color: "white" },
                  },
                  {
                    x: 2.5,
                    y: 2,
                    text: "<b>Object Storage (S3)</b><br>Raw Files, Models",
                    font: { color: "white" },
                  },
                  {
                    x: 7.5,
                    y: 2,
                    text: "<b>Managed Database (RDS)</b><br>Structured User Data",
                    font: { color: "white" },
                  },
                  {
                    x: 3.5,
                    y: 5,
                    text: "Reads large files from",
                    showarrow: true,
                    ax: 0,
                    ay: 30,
                  },
                  {
                    x: 6.5,
                    y: 5,
                    text: "Reads/writes structured data to",
                    showarrow: true,
                    ax: 0,
                    ay: 30,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p6s6ss1-plot1": () => {
            // Continuous Integration (CI)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p6s6ss1-plot1",
              [],
              {
                title: {
                  text: "The Continuous Integration (CI) Feedback Loop",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  { type: "rect", x0: 0.5, y0: 3, x1: 2.5, y1: 5 },
                  { type: "rect", x0: 3.5, y0: 3, x1: 5.5, y1: 5 },
                  { type: "rect", x0: 6.5, y0: 3, x1: 8.5, y1: 5 },
                ],
                annotations: [
                  { x: 1.5, y: 5.5, text: "1. Developer<br>Pushes Code" },
                  { x: 4.5, y: 5.5, text: "2. GitHub<br>Triggers Webhook" },
                  { x: 7.5, y: 5.5, text: "3. CI Server<br>Builds & Tests" },
                  {
                    x: 1.5,
                    y: 4,
                    text: '<i class="fas fa-user-edit fa-2x"></i>',
                  },
                  { x: 4.5, y: 4, text: '<i class="fab fa-github fa-2x"></i>' },
                  { x: 7.5, y: 4, text: '<i class="fas fa-cogs fa-2x"></i>' },
                  { x: 3, y: 4, text: "➔" },
                  { x: 6, y: 4, text: "➔" },
                  {
                    x: 7.5,
                    y: 2,
                    text: "4. Report Status<br>Pass / Fail",
                    showarrow: true,
                    ax: -30,
                    ay: 30,
                    arrowhead: 2,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p6s6ss2-plot1": () => {
            // Continuous Deployment (CD)
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p6s6ss2-plot1",
              [],
              {
                title: { text: "The Full CI/CD Pipeline", font: { size: 18 } },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  { type: "rect", x0: 0, y0: 3, x1: 2, y1: 5 },
                  { type: "rect", x0: 2.5, y0: 3, x1: 4.5, y1: 5 },
                  { type: "rect", x0: 5, y0: 3, x1: 7, y1: 5 },
                  { type: "rect", x0: 7.5, y0: 3, x1: 9.5, y1: 5 },
                ],
                annotations: [
                  { x: 1, y: 5.5, text: "Build" },
                  { x: 3.5, y: 5.5, text: "Test" },
                  { x: 6, y: 5.5, text: "Release" },
                  { x: 8.5, y: 5.5, text: "Deploy" },
                  { x: 1, y: 4, text: '<i class="fas fa-box fa-2x"></i>' },
                  { x: 3.5, y: 4, text: '<i class="fas fa-vial fa-2x"></i>' },
                  { x: 6, y: 4, text: '<i class="fas fa-tag fa-2x"></i>' },
                  { x: 8.5, y: 4, text: '<i class="fas fa-rocket fa-2x"></i>' },
                  { x: 2.25, y: 4, text: "➔" },
                  { x: 4.75, y: 4, text: "➔" },
                  { x: 7.25, y: 4, text: "➔" },
                  { x: 2.25, y: 2, text: "<b>Continuous Integration (CI)</b>" },
                  { x: 7.25, y: 2, text: "<b>Continuous Deployment (CD)</b>" },
                ],
              },
              { responsive: true }
            );
          },
          "p6s6ss3-plot1": () => {
            // Automated Reporting & Monitoring
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
            };
            Plotly.newPlot(
              "p6s6ss3-plot1",
              [],
              {
                title: {
                  text: "The MLOps Monitoring & Retraining Loop",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  {
                    type: "rect",
                    x0: 3,
                    y0: 8,
                    x1: 7,
                    y1: 9.5,
                    fillcolor: "#3b82f6",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 4.5,
                    x1: 7,
                    y1: 6,
                    fillcolor: "#10b981",
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 1,
                    x1: 7,
                    y1: 2.5,
                    fillcolor: "#ef4444",
                  },
                ],
                annotations: [
                  {
                    x: 5,
                    y: 8.75,
                    text: "CI/CD Pipeline Trains & Deploys",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 5.25,
                    text: "Model is Live in Production",
                    font: { color: "white" },
                  },
                  {
                    x: 5,
                    y: 1.75,
                    text: "Monitoring System Detects Drift",
                    font: { color: "white" },
                  },
                  { x: 5, y: 7, text: "↓" },
                  { x: 5, y: 3.5, text: "↓" },
                  {
                    x: 1.5,
                    y: 5.25,
                    text: "Alert! Retrain needed.",
                    showarrow: true,
                    ax: 45,
                    ay: 0,
                    arrowhead: 2,
                  },
                ],
              },
              { responsive: true }
            );
          },
          "p6s7ss2-plot1": () => {
            // Project Management Methodologies
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              boxColor: isDark ? "#282c34" : "#f8f9fa",
            };
            Plotly.newPlot(
              "p6s7ss2-plot1",
              [],
              {
                title: {
                  text: "A Kanban Board for Visualizing Workflow",
                  font: { size: 18 },
                },
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  range: [0, 10],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  range: [0, 8],
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                shapes: [
                  { type: "rect", x0: 0.5, y0: 1, x1: 2.3, y1: 7 },
                  { type: "rect", x0: 2.7, y0: 1, x1: 4.8, y1: 7 },
                  { type: "rect", x0: 5.2, y0: 1, x1: 7.3, y1: 7 },
                  { type: "rect", x0: 7.7, y0: 1, x1: 9.5, y1: 7 },
                  {
                    type: "rect",
                    x0: 1,
                    y0: 5,
                    x1: 1.8,
                    y1: 6,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 3,
                    y0: 4,
                    x1: 4.5,
                    y1: 6,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 5.5,
                    y0: 5.5,
                    x1: 7,
                    y1: 6.5,
                    fillcolor: themeColors.boxColor,
                  },
                  {
                    type: "rect",
                    x0: 8,
                    y0: 2,
                    x1: 9,
                    y1: 3,
                    fillcolor: themeColors.boxColor,
                  },
                ],
                annotations: [
                  { x: 1.4, y: 7.5, text: "To Do" },
                  { x: 3.75, y: 7.5, text: "In Progress" },
                  { x: 6.25, y: 7.5, text: "In Review" },
                  { x: 8.6, y: 7.5, text: "Done" },
                ],
              },
              { responsive: true }
            );
          },
          "p7s7ss8-plot1": () => {
            // Team Workflows
            const isDark = document.body.classList.contains("dark-theme");
            const themeColors = {
              textColor: isDark ? "#e9ecef" : "#1f2937",
              bgColor: "transparent",
              gridColor: isDark ? "#373a40" : "#e2e8f0",
            };
            Plotly.newPlot(
              "p7s7ss8-plot1",
              [
                {
                  x: [1, 2, 5, 6, 7],
                  y: [1, 1, 1, 1, 1],
                  type: "scatter",
                  mode: "lines+markers",
                  name: "main",
                  line: { color: "#3b82f6", width: 3 },
                },
                {
                  x: [2, 3, 4, 5],
                  y: [1, 2, 2, 1],
                  type: "scatter",
                  mode: "lines+markers",
                  name: "feature",
                  line: { color: "#f59e0b", width: 3, dash: "dash" },
                },
              ],
              {
                title: "Git Branching Workflow",
                paper_bgcolor: themeColors.bgColor,
                plot_bgcolor: themeColors.bgColor,
                font: { color: themeColors.textColor },
                xaxis: {
                  title: "Commit History",
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                yaxis: {
                  showgrid: false,
                  zeroline: false,
                  showticklabels: false,
                },
                showlegend: true,
                legend: { x: 0.1, y: 0.9 },
                annotations: [
                  {
                    x: 2,
                    y: 1.2,
                    text: "Create Branch",
                    showarrow: true,
                    arrowhead: 4,
                    ax: 0,
                    ay: -30,
                  },
                  {
                    x: 3.5,
                    y: 2.2,
                    text: "Work in Isolation",
                    showarrow: false,
                  },
                  {
                    x: 5,
                    y: 1.2,
                    text: "Merge PR",
                    showarrow: true,
                    arrowhead: 4,
                    ax: 0,
                    ay: -30,
                  },
                ],
              },
              { responsive: true }
            );
          },
        };

        function updatePlotlyThemes() {
          const isDark = document.body.classList.contains("dark-theme");
          const themeColors = {
            textColor: isDark ? "#e9ecef" : "#1f2937",
            bgColor: "transparent",
            gridColor: isDark ? "#373a40" : "#e2e8f0",
            lineColor: isDark ? "#6b7280" : "#cbd5e1",
          };
          const layoutUpdate = {
            "font.color": themeColors.textColor,
            paper_bgcolor: themeColors.bgColor,
            plot_bgcolor: themeColors.bgColor,
            "xaxis.title.font.color": themeColors.textColor,
            "yaxis.title.font.color": themeColors.textColor,
            "xaxis.tickfont.color": themeColors.textColor,
            "yaxis.tickfont.color": themeColors.textColor,
            "legend.font.color": themeColors.textColor,
            "xaxis.gridcolor": themeColors.gridColor,
            "yaxis.gridcolor": themeColors.gridColor,
            "xaxis.linecolor": themeColors.lineColor,
            "yaxis.linecolor": themeColors.lineColor,
          };
          document.querySelectorAll(".plotly-chart").forEach((div) => {
            if (div.data) {
              Plotly.relayout(div, layoutUpdate);
            }
          });
        }

        const renderedPlots = new Set();

        function findItemById(id) {
          for (const phase of learningPathData.phases) {
            for (const section of phase.sections) {
              if (section.id === id) return { item: section, parent: phase };
              if (section.subsections) {
                for (const subsection of section.subsections) {
                  if (subsection.id === id)
                    return { item: subsection, parent: section };
                  if (subsection.topics) {
                    for (const topic of subsection.topics) {
                      if (topic.id === id)
                        return { item: topic, parent: subsection };
                    }
                  }
                }
              }
            }
          }
          return null;
        }

        function generateAndAppendPage(sectionId) {
          const data = findItemById(sectionId);
          if (!data || !data.item) return null;
          const { item, parent } = data;
          const pageElement = document.createElement("div");
          const difficulty = item.difficulty || parent.difficulty || "basic";
          pageElement.className = `content-section level-${difficulty}`;
          pageElement.id = item.id;
          const tagsHtml = (item.tags || [])
            .map((tag) => `<span class="tag">${tag}</span>`)
            .join("");
          const mainContentHtml = pageContent[item.id] || "";
          pageElement.innerHTML = `
            <div class="card-header">
                <h1><i class="fas ${item.icon || parent.icon}"></i><span>${
            item.title
          }</span></h1>
                <div class="card-tags">${tagsHtml}</div>
            </div>
            <div class="tab-content">
                <div class="subsection">
                    <h3 class="subsection-title">Overview</h3>
                    <div class="scenario-content"><p>${
                      item.description
                    }</p></div>
                </div>
                <div class="subsection">
                    <h3 class="subsection-title">Why learn this?</h3>
                    <div class="scenario-content"><p>${
                      item.why || "No specific reason provided yet."
                    }</p></div>
                </div>
                ${mainContentHtml}
            </div>`;
          contentWrapper.appendChild(pageElement);
          pageElement
            .querySelectorAll("pre code")
            .forEach((block) => hljs.highlightElement(block));
          pageElement.querySelectorAll(".copy-btn").forEach((button) => {
            button.addEventListener("click", () => {
              const code =
                button.nextElementSibling.querySelector("code").innerText;
              navigator.clipboard.writeText(code).then(() => {
                const originalIcon = button.innerHTML;
                button.innerHTML = '<i class="fas fa-check"></i>';
                setTimeout(() => (button.innerHTML = originalIcon), 1500);
              });
            });
          });
          pageElement.querySelectorAll(".tab-button").forEach((button) => {
            button.addEventListener("click", () => {
              const tabContainer = button.closest(".subsection");
              const tabId = button.dataset.tab;
              tabContainer
                .querySelectorAll(".tab-button")
                .forEach((btn) => btn.classList.remove("active"));
              button.classList.add("active");
              tabContainer
                .querySelectorAll(".tab-pane")
                .forEach((pane) =>
                  pane.classList.toggle("active", pane.id === tabId)
                );
            });
          });
          if (window.MathJax) {
            window.MathJax.typesetPromise([pageElement]);
          }
          return pageElement;
        }

        function generateNavigation() {
          let navHtml =
            '<div class="nav-category"><div class="nav-category-title"><i class="fas fa-map"></i> Overview</div><ul class="nav-items"><li><a href="#" class="nav-item" data-id="overview"><span>Learning Path Overview</span></a></li></ul></div>';
          learningPathData.phases.forEach((phase) => {
            navHtml += `<div class="nav-category"><div class="nav-category-title"><i class="fas ${phase.icon}"></i> ${phase.title}</div><ul class="nav-items">`;
            phase.sections.forEach((section) => {
              let difficulty = section.difficulty || "";
              let difficultyClass =
                difficulty === "basic"
                  ? "level-basic"
                  : difficulty === "intermediate"
                  ? "level-intermediate"
                  : difficulty === "advanced"
                  ? "level-advanced"
                  : "";
              const hasSubsections =
                section.subsections && section.subsections.length > 0;
              const hasChildren = hasSubsections ? "has-children" : "";
              const toggleIcon = hasSubsections
                ? '<svg class="toggle-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5"><polyline points="6 9 12 15 18 9"></polyline></svg>'
                : "";
              navHtml += `<li class="${difficultyClass}"><a href="#" class="nav-item ${hasChildren}" data-id="${section.id}"><span>${section.title}</span>${toggleIcon}</a>`;
              if (hasSubsections) {
                navHtml += '<ul class="nested">';
                section.subsections.forEach((subsection) => {
                  let subDifficulty = subsection.difficulty || difficulty;
                  let subDifficultyClass =
                    subDifficulty === "basic"
                      ? "level-basic"
                      : subDifficulty === "intermediate"
                      ? "level-intermediate"
                      : subDifficulty === "advanced"
                      ? "level-advanced"
                      : "";
                  const hasTopics =
                    subsection.topics && subsection.topics.length > 0;
                  const subHasChildren = hasTopics ? "has-children" : "";
                  const subToggleIcon = hasTopics
                    ? '<svg class="toggle-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5"><polyline points="6 9 12 15 18 9"></polyline></svg>'
                    : "";
                  navHtml += `<li class="${subDifficultyClass}"><a href="#" class="nav-item ${subHasChildren}" data-id="${subsection.id}"><span>${subsection.title}</span>${subToggleIcon}</a>`;
                  if (hasTopics) {
                    navHtml += '<ul class="nested">';
                    subsection.topics.forEach((topic) => {
                      let topicDifficulty = topic.difficulty || subDifficulty;
                      let topicDifficultyClass =
                        topicDifficulty === "basic"
                          ? "level-basic"
                          : topicDifficulty === "intermediate"
                          ? "level-intermediate"
                          : topicDifficulty === "advanced"
                          ? "level-advanced"
                          : "";
                      navHtml += `<li class="${topicDifficultyClass}"><a href="#" class="nav-item" data-id="${topic.id}"><span>${topic.title}</span></a></li>`;
                    });
                    navHtml += "</ul>";
                  }
                  navHtml += "</li>";
                });
                navHtml += "</ul>";
              }
              navHtml += "</li>";
            });
            navHtml += "</ul></div>";
          });
          navContainer.innerHTML = navHtml;
        }

        function generateOverviewContent() {
          const overviewSection = document.createElement("div");
          overviewSection.className = "content-section active";
          overviewSection.id = "overview";

          let combinedHtml = `
            <div class="tab-content">
              <div class="overview-card animate-on-scroll">
                  <div class="intro-header">
                      <div class="site-logo">R</div>
                      <div class="intro-header-text">
                          <h1>Master Data Science with R</h1>
                          <p class="subtitle">A Guided, Project-Based Learning Path</p>
                          <p class="author">Curated by Anas Riaz</p>
                      </div>
                  </div>
                  <div class="intro-content">
                      <p>
                          I developed this path to address a common challenge: valuable data science knowledge for R is often fragmented across countless books, articles, and courses. Drawing from extensive experience in the field, my goal was to create a single, consolidated curriculum that provides a structured and practical roadmap for mastering the R programming language.
                      </p>
                      <p>
                          This path is designed for both aspiring students and seasoned professionals. It provides a clear roadmap to mastering R for data analysis, machine learning, and beyond. R is a powerhouse in the world of statistical computing and data visualization, and mastering it opens doors to solving complex, real-world problems.
                      </p>
                      <div class="cta-buttons">
                          <a href="mailto:raoanasriaz@gmail.com" class="cta-btn primary"><i class="fas fa-envelope"></i> Contact Me</a>
                          <a href="https://linkedin.com/in/raoanasriaz" target="_blank" class="cta-btn secondary"><i class="fab fa-linkedin"></i> LinkedIn</a>
                          <a href="https://github.com/rao-anas-riaz" target="_blank" class="cta-btn secondary"><i class="fab fa-github"></i> GitHub</a>
                      </div>
                  </div>
              </div>

              <div class="overview-card animate-on-scroll">
                  <div class="card-header overview-card-header">
                      <h1><i class="fas fa-bullseye"></i> Learning Objectives</h1>
                      <p>By the end of this journey, you will be able to:</p>
                  </div>
                  <div class="objectives-content">
                      <ul class="objectives-list">
                          <li><i class="fas fa-check-circle"></i> <strong>Master R Programming:</strong> Go from foundational syntax and data structures to advanced paradigms like Object-Oriented Programming (OOP) and performance optimization techniques.</li>
						  <li><i class="fas fa-check-circle"></i> <strong>Grasp Mathematical Foundations:</strong> Understand the core principles of statistics, probability, linear algebra, and calculus that power modern data science and machine learning algorithms.</li>
						  <li><i class="fas fa-check-circle"></i> <strong>Wrangle and Engineer Data:</strong> Master the complete data preparation workflow, from acquisition and cleaning to advanced feature engineering for optimal model performance.</li>
                          <li><i class="fas fa-check-circle"></i> <strong>Execute the Full Data Science Lifecycle:</strong> Confidently manage every step of a project, from acquiring and cleaning raw data to advanced feature engineering and in-depth exploratory data analysis (EDA).</li>
                          <li><i class="fas fa-check-circle"></i> <strong>Build & Validate Powerful Models:</strong> Implement a wide range of algorithms, including regression, classification, deep learning, time series forecasting, Natural Language Processing (NLP), and causal inference models.</li>
                          <li><i class="fas fa-check-circle"></i> <strong>Analyze and Visualize Insights:</strong> Perform in-depth exploratory data analysis (EDA), apply statistical hypothesis testing, and create compelling, publication-quality visualizations to uncover and communicate hidden patterns.</li>
                          <li><i class="fas fa-check-circle"></i> <strong>Deploy Models to Production:</strong> Understand and apply MLOps principles to deploy, monitor, and maintain models in real-world environments.</li>
						  <li><i class="fas fa-check-circle"></i> <strong>Effectively Communicate Results:</strong> Translate models into real-world value by building interactive Shiny dashboards, creating reproducible reports, and deploying scalable applications using MLOps, Docker, and CI/CD principles.</li>
                          <li><i class="fas fa-check-circle"></i> <strong>Implement Responsible AI:</strong> Learn to validate models rigorously, interpret their decisions using Explainable AI (XAI) techniques, and audit systems for fairness, bias, and ethical implications.</li>
                      </ul>
                  </div>
              </div>
              
                <div class="overview-card">
                    <div class="card-header overview-card-header">
                        <h1><i class="fas fa-project-diagram"></i> Data Science Learning Path with R</h1>
                        <p>A comprehensive, structured approach to mastering data science with R, from foundational concepts to production deployment.</p>
                    </div>
                    <div class="flow-diagram" style="padding: 1.5rem;">
          `;
          learningPathData.phases.forEach((phase, index) => {
            combinedHtml += `
              <div class="flow-node">
                  <div class="flow-node-header" data-target="content-${phase.id}">
                      <h3><i class="fas ${phase.icon}"></i>${phase.title}</h3>
                      <p>${phase.description}</p>
                  </div>
                  <div class="flow-node-content" id="content-${phase.id}">
                      <ul class="phase-topics">
            `;
            phase.sections.forEach((section) => {
              combinedHtml += `
                  <li>
                      <a href="#" class="topic-link" data-id="${section.id}">
                          <i class="fas ${section.icon}"></i>${section.title}
                      </a>
                      <div class="topic-description">${section.description}</div>
                  </li>
              `;
            });
            combinedHtml += `</ul></div></div>`;
            if (index < learningPathData.phases.length - 1) {
              combinedHtml +=
                '<div class="flow-arrow"><i class="fas fa-arrow-down"></i></div>';
            }
          });
          combinedHtml += `
                    </div>
                </div>

                <div class="overview-card" style="margin-top: 2rem;">
                    <div class="card-header overview-card-header">
                        <h1><i class="fas fa-sitemap"></i> Scenario-Based Learning Path</h1>
                        <p>An interactive, scenario-driven view of the learning curriculum. Click nodes to expand and explore.</p>
                    </div>
                    <div id="tree-container-wrapper" style="position: relative; width: 100%; height: 80vh; min-height: 600px; background: transparent; border: none; box-shadow: none;">
                        <svg id="tree-container" style="width: 100%; height: 100%;"></svg>
                        <div class="d3-controls" style="position: absolute; top: 1rem; right: 1rem; z-index: 10; display: flex; gap: 0.5rem;">
    <button id="d3-toggle-all" title="Expand All" style="background-color: var(--flow-bg); border: 1px solid var(--border-color); color: var(--text-muted); font-weight: 600; font-size: 1rem; border-radius: 6px; width: 36px; height: 36px; cursor: pointer; display: flex; align-items: center; justify-content: center; transition: all 0.2s ease; box-shadow: var(--shadow);">
        <i class="fas fa-expand-arrows-alt"></i>
    </button>
    <button id="d3-zoom-in" title="Zoom In" style="background-color: var(--flow-bg); border: 1px solid var(--border-color); color: var(--text-muted); font-weight: 600; font-size: 1.2rem; border-radius: 6px; width: 36px; height: 36px; cursor: pointer; display: flex; align-items: center; justify-content: center; transition: all 0.2s ease; box-shadow: var(--shadow);">+</button>
    <button id="d3-zoom-out" title="Zoom Out" style="background-color: var(--flow-bg); border: 1px solid var(--border-color); color: var(--text-muted); font-weight: 600; font-size: 1.2rem; border-radius: 6px; width: 36px; height: 36px; cursor: pointer; display: flex; align-items: center; justify-content: center; transition: all 0.2s ease; box-shadow: var(--shadow);">-</button>
</div>
                    </div>
                    <div class="tree-navigation" style="display: flex; justify-content: center; align-items: center; padding: 1.5rem; border-top: 1px solid var(--border-color); gap: 1rem;">
                        <button class="tree-nav-arrow" id="tree-prev" title="Previous Phase" style="background: var(--flow-bg); border: 1px solid var(--border-color); color: var(--text-muted); border-radius: 50%; width: 40px; height: 40px; font-size: 1.5rem; cursor: pointer; transition: all 0.2s ease; box-shadow: var(--shadow);">&lt;</button>
                        <div class="tree-nav-dots" id="tree-dots" style="display: flex; gap: 0.75rem;"></div>
                        <button class="tree-nav-arrow" id="tree-next" title="Next Phase" style="background: var(--flow-bg); border: 1px solid var(--border-color); color: var(--text-muted); border-radius: 50%; width: 40px; height: 40px; font-size: 1.5rem; cursor: pointer; transition: all 0.2s ease; box-shadow: var(--shadow);">&gt;</button>
                    </div>
                </div>
            </div>`;
          overviewSection.innerHTML = combinedHtml;
          contentWrapper.appendChild(overviewSection);
        }

        generateNavigation();
        generateOverviewContent();

        (function () {
          let currentPhaseIndex = 0;
          let currentD3Zoom;
          let isTreeExpanded = false;

          function transformDataForD3(node) {
            const newNode = { ...node, children: undefined };
            if (node.sections)
              newNode.children = node.sections.map(transformDataForD3);
            else if (node.subsections)
              newNode.children = node.subsections.map(transformDataForD3);
            else if (node.topics)
              newNode.children = node.topics.map(transformDataForD3);
            return newNode;
          }

          function renderD3Tree(phaseIndex) {
            const phaseData = learningPathData.phases[phaseIndex];
            if (!phaseData) return;
            const rootData = transformDataForD3(phaseData);
            const svg = d3.select("svg#tree-container");
            svg.selectAll("*").remove();
            const container = document.getElementById("tree-container-wrapper");
            if (!container) return;
            const width = container.clientWidth,
              height = container.clientHeight,
              margin = { top: 40, right: 40, bottom: 40, left: 40 };
            const g = svg
              .append("g")
              .attr("transform", `translate(${margin.left},${margin.top})`);
            const root = d3.hierarchy(rootData);
            root.x0 = width / 2;
            root.y0 = 0;
            function collapse(d) {
              if (d.children) {
                d._children = d.children;
                d._children.forEach(collapse);
                d.children = null;
              }
            }
            root.children.forEach(collapse);
            update(null, root);
            function update(event, source) {
              const duration = 100,
                nodeWidth = 280,
                horizontalSeparation = 320,
                verticalSeparation = 200;
              const treeLayout = d3
                .tree()
                .nodeSize([horizontalSeparation, verticalSeparation])
                .separation((a, b) =>
                  a.parent === b.parent &&
                  a._verticalChildren &&
                  b._verticalChildren
                    ? 0.6
                    : 1
                );
              root.eachBefore((d) => {
                if (
                  d.children &&
                  d.children.every(
                    (child) => !child.children && !child._children
                  )
                ) {
                  d._verticalChildren = d.children;
                  d.children = [
                    {
                      data: { id: d.data.id + "-phantom" },
                      depth: d.depth + 1,
                      height: 0,
                      parent: d,
                    },
                  ];
                }
              });
              const treeData = treeLayout(root);
              treeData
                .descendants()
                .forEach(
                  (d) =>
                    d._verticalChildren &&
                    ((d.children = d._verticalChildren),
                    delete d._verticalChildren)
                );
              const nodes = root.descendants().reverse();
              const tempDiv = d3
                .select("body")
                .append("div")
                .style("position", "absolute")
                .style("visibility", "hidden")
                .style("width", nodeWidth + "px")
                .style("font-family", `"Inter", sans-serif`)
                .style("padding", "10px")
                .style("line-height", "1.4")
                .style("box-sizing", "border-box");
              nodes.forEach((d) => {
                if (d.data.id.endsWith("-phantom")) return;
                tempDiv.html(
                  `<strong style="font-weight: 600; font-size: 14px; margin-bottom: 5px;">${
                    d.data.title
                  }</strong><p style="font-size: 12px; opacity: 0.9; margin: 0;">${
                    d.data.scenario || ""
                  }</p>`
                );
                d.nodeSize = {
                  width: nodeWidth,
                  height: tempDiv.node().getBoundingClientRect().height,
                };
              });
              tempDiv.remove();
              nodes.forEach((d) => {
                if (
                  d.children &&
                  d.children.every(
                    (child) => !child.children && !child._children
                  )
                ) {
                  let accumulatedHeight = d.y + d.nodeSize.height / 2 + 40;
                  d.children.forEach((child) => {
                    child.x = d.x;
                    child.y = accumulatedHeight + child.nodeSize.height / 2;
                    accumulatedHeight += child.nodeSize.height + 20;
                  });
                }
              });
              const links = root.links(),
                visibleLinks = links.filter(
                  (link) => !link.target.data.id.endsWith("-phantom")
                ),
                node = g.selectAll("g.node").data(
                  nodes.filter((d) => !d.data.id.endsWith("-phantom")),
                  (d) => d.data.id
                );
              const nodeEnter = node
                .enter()
                .append("g")
                .attr(
                  "class",
                  (d) =>
                    "node" +
                    (d._children || d.children
                      ? " node--internal"
                      : " node--leaf")
                )
                .attr("transform", `translate(${source.x0}, ${source.y0})`)
                .on("click", (event, d) => {
                  if (d.children || d._children) {
                    if (d.children) {
                      d._children = d.children;
                      d.children = null;
                    } else {
                      d.children = d._children;
                      d._children = null;
                    }
                    update(event, d);
                  }
                });
              nodeEnter
                .append("rect")
                .attr("width", (d) => d.nodeSize.width)
                .attr("height", (d) => d.nodeSize.height)
                .attr("x", (d) => -d.nodeSize.width / 2)
                .attr("y", (d) => -d.nodeSize.height / 2);
              const fo = nodeEnter
                .append("foreignObject")
                .attr("width", (d) => d.nodeSize.width)
                .attr("height", (d) => d.nodeSize.height)
                .attr("x", (d) => -d.nodeSize.width / 2)
                .attr("y", (d) => -d.nodeSize.height / 2);
              const div = fo
                .append("xhtml:div")
                .style("width", "100%")
                .style("height", "100%")
                .style("display", "flex")
                .style("flex-direction", "column")
                .style("justify-content", "center")
                .style("align-items", "center")
                .style("text-align", "center")
                .style("padding", "10px")
                .style("line-height", "1.4")
                .style("box-sizing", "border-box");
              div
                .append("xhtml:strong")
                .style("font-weight", "600")
                .style("font-size", "14px")
                .style("margin-bottom", "5px")
                .text((d) => d.data.title);
              div
                .append("xhtml:p")
                .style("font-size", "12px")
                .style("opacity", "0.9")
                .style("margin", "0")
                .text((d) => d.data.scenario || "");

              const navIcon = nodeEnter.append("g").on("click", (event, d) => {
                event.stopPropagation();
                const navLink = document.querySelector(
                  `.nav-item[data-id="${d.data.id}"]`
                );
                if (navLink) {
                  navLink.click();
                  setTimeout(() => {
                    navLink.scrollIntoView({
                      behavior: "smooth",
                      block: "center",
                    });
                  }, 150);
                }
              });

              navIcon.append("circle").attr("r", 12);
              navIcon
                .append("path")
                .attr("d", "M -3 -5 L 3 0 L -3 5")
                .attr("fill", "none")
                .attr("stroke", "#ffffff")
                .attr("stroke-width", 2);
              const nodeUpdate = node.merge(nodeEnter);
              nodeUpdate
                .select("g")
                .attr(
                  "transform",
                  (d) => `translate(${d.nodeSize.width / 2 + 15}, 0)`
                );
              nodeUpdate
                .transition()
                .duration(duration)
                .attr("transform", (d) => `translate(${d.x}, ${d.y})`);
              nodeUpdate.attr(
                "class",
                (d) =>
                  "node" +
                  (d._children || d.children
                    ? " node--internal"
                    : " node--leaf") +
                  (d.children ? " node--open" : "")
              );
              node
                .exit()
                .transition()
                .duration(duration)
                .attr("transform", `translate(${source.x}, ${source.y})`)
                .remove();
              const link = g
                .selectAll("path.link")
                .data(visibleLinks, (d) => d.target.data.id);
              const linkEnter = link
                .enter()
                .insert("path", "g")
                .attr("class", "link")
                .attr("d", (d) => {
                  const o = {
                    x: source.x0,
                    y: source.y0,
                    nodeSize: { height: 0 },
                  };
                  return diagonal({ source: o, target: o });
                });
              link
                .merge(linkEnter)
                .transition()
                .duration(duration)
                .attr("d", diagonal);
              link
                .exit()
                .transition()
                .duration(duration)
                .attr("d", (d) => {
                  const o = {
                    x: source.x,
                    y: source.y,
                    nodeSize: { height: 0 },
                  };
                  return diagonal({ source: o, target: o });
                })
                .remove();
              root.each((d) => {
                d.x0 = d.x;
                d.y0 = d.y;
              });
              function diagonal(d) {
                const s = d.source,
                  t = d.target,
                  sHeight = s.nodeSize.height / 2,
                  tHeight = t.nodeSize.height / 2;
                return t.parent &&
                  t.parent.children &&
                  t.parent.children.every((c) => !c.children && !c._children) &&
                  t.x === s.x
                  ? `M ${s.x} ${s.y + sHeight} L ${t.x} ${t.y - tHeight}`
                  : `M ${s.x} ${s.y + sHeight} C ${s.x} ${(s.y + t.y) / 2}, ${
                      t.x
                    } ${(s.y + t.y) / 2}, ${t.x} ${t.y - tHeight}`;
              }
            }

            const toggleButton = document.getElementById("d3-toggle-all");

            isTreeExpanded = false;
            const toggleIcon = toggleButton.querySelector("i");
            toggleIcon.classList.remove("fa-compress-arrows-alt");
            toggleIcon.classList.add("fa-expand-arrows-alt");
            toggleButton.title = "Expand All";

            toggleButton.onclick = () => {
              function expandAll(d) {
                if (d._children) {
                  d.children = d._children;
                  d._children = null;
                }
                if (d.children) {
                  d.children.forEach(expandAll);
                }
              }

              function collapseAll(d) {
                if (d.children) {
                  d._children = d.children;
                  d.children = null;
                }
                if (d._children) {
                  d._children.forEach(collapseAll);
                }
              }

              if (isTreeExpanded) {
                collapseAll(root);
              } else {
                expandAll(root);
              }

              isTreeExpanded = !isTreeExpanded;
              update(null, root);

              const toggleIcon = toggleButton.querySelector("i");
              if (isTreeExpanded) {
                toggleIcon.classList.remove("fa-expand-arrows-alt");
                toggleIcon.classList.add("fa-compress-arrows-alt");
                toggleButton.title = "Collapse All";
              } else {
                toggleIcon.classList.remove("fa-compress-arrows-alt");
                toggleIcon.classList.add("fa-expand-arrows-alt");
                toggleButton.title = "Expand All";
              }
            };

            const zoom = d3
              .zoom()
              .scaleExtent([0.2, 1.5])
              .on("zoom", (event) => g.attr("transform", event.transform));
            svg.call(zoom);
            currentD3Zoom = zoom;
            const initialScale = 0.7;
            const initialTransform = d3.zoomIdentity
              .translate(width / 2, margin.top + 50)
              .scale(initialScale);
            svg
              .transition()
              .duration(750)
              .call(zoom.transform, initialTransform);
          }
          function initializeTreeNavigation() {
            const dotsContainer = document.getElementById("tree-dots"),
              prevBtn = document.getElementById("tree-prev"),
              nextBtn = document.getElementById("tree-next");
            if (!dotsContainer || !prevBtn || !nextBtn) return;
            dotsContainer.innerHTML = "";
            learningPathData.phases.forEach((_, index) => {
              const dot = document.createElement("div");
              dot.className = "tree-nav-dot";
              dot.dataset.index = index;
              dot.addEventListener("click", () => {
                currentPhaseIndex = index;
                renderD3Tree(currentPhaseIndex);
                updateTreeNavigation();
              });
              dotsContainer.appendChild(dot);
            });
            prevBtn.addEventListener("click", () => {
              if (currentPhaseIndex > 0) {
                currentPhaseIndex--;
                renderD3Tree(currentPhaseIndex);
                updateTreeNavigation();
              }
            });
            nextBtn.addEventListener("click", () => {
              if (currentPhaseIndex < learningPathData.phases.length - 1) {
                currentPhaseIndex++;
                renderD3Tree(currentPhaseIndex);
                updateTreeNavigation();
              }
            });
            updateTreeNavigation();
          }
          function updateTreeNavigation() {
            document.querySelectorAll(".tree-nav-dot").forEach((dot, index) => {
              const isActive = index === currentPhaseIndex;
              dot.style.backgroundColor = isActive
                ? "var(--primary)"
                : "var(--border-color)";
              dot.style.transform = isActive ? "scale(1.2)" : "scale(1)";
            });
            document.getElementById("tree-prev").disabled =
              currentPhaseIndex === 0;
            document.getElementById("tree-next").disabled =
              currentPhaseIndex === learningPathData.phases.length - 1;
          }
          renderD3Tree(currentPhaseIndex);
          initializeTreeNavigation();
          document
            .getElementById("d3-zoom-in")
            ?.addEventListener(
              "click",
              () =>
                currentD3Zoom &&
                d3
                  .select("svg#tree-container")
                  .transition()
                  .duration(250)
                  .call(currentD3Zoom.scaleBy, 1.2)
            );
          document
            .getElementById("d3-zoom-out")
            ?.addEventListener(
              "click",
              () =>
                currentD3Zoom &&
                d3
                  .select("svg#tree-container")
                  .transition()
                  .duration(250)
                  .call(currentD3Zoom.scaleBy, 0.8)
            );
          window.addEventListener("resize", () =>
            renderD3Tree(currentPhaseIndex)
          );
        })();

        hljs.highlightAll();

        const themeToggle = document.getElementById("theme-toggle");
        const sunIcon = themeToggle.querySelector(".sun-icon");
        const moonIcon = themeToggle.querySelector(".moon-icon");
        const currentTheme = localStorage.getItem("theme");

        function setTheme(theme) {
          if (theme === "dark") {
            document.body.classList.add("dark-theme");
            sunIcon.style.display = "none";
            moonIcon.style.display = "block";
          } else {
            document.body.classList.remove("dark-theme");
            sunIcon.style.display = "block";
            moonIcon.style.display = "none";
          }
          localStorage.setItem("theme", theme);
          updatePlotlyThemes();
        }

        setTheme(currentTheme || "dark");
        themeToggle.addEventListener("click", () => {
          const newTheme = document.body.classList.contains("dark-theme")
            ? "light"
            : "dark";
          setTheme(newTheme);
        });

        const sidebarToggle = document.getElementById("sidebar-toggle");
        const openIcon = sidebarToggle.querySelector(".open-icon");
        const closeIcon = sidebarToggle.querySelector(".close-icon");
        const headerTitle = document.getElementById("header-title");
        const searchInput = document.getElementById("nav-search");

        if (window.innerWidth <= 768) {
          document.body.classList.add("sidebar-collapsed");
        } else {
          document.body.classList.add("sidebar-manual-toggle");
        }

        function updateToggleIcon() {
          const isCollapsed =
            document.body.classList.contains("sidebar-collapsed");
          openIcon.style.display = isCollapsed ? "none" : "block";
          closeIcon.style.display = isCollapsed ? "block" : "none";
        }

        function showContent(sectionId) {
          document
            .querySelectorAll(".content-section")
            .forEach((section) => section.classList.remove("active"));
          const targetSection = document.getElementById(sectionId);
          if (targetSection) {
            targetSection.classList.add("active");
            if (sectionId === "privacy-policy") {
              headerTitle.textContent = "Privacy Policy";
            } else if (sectionId === "terms-of-service") {
              headerTitle.textContent = "Terms of Service";
            } else if (sectionId !== "overview") {
              const headerText = targetSection.querySelector(
                ".card-header h1 > span"
              ).textContent;
              headerTitle.textContent = headerText;
            } else {
              headerTitle.textContent = "Learning Path Overview";
            }
            contentWrapper.scrollTop = 0;
            const plots = targetSection.querySelectorAll(".plotly-chart");
            plots.forEach((plotDiv) => {
              if (plotFunctions[plotDiv.id] && !renderedPlots.has(plotDiv.id)) {
                plotFunctions[plotDiv.id]();
                renderedPlots.add(plotDiv.id);
              }
            });
            updatePlotlyThemes();
          }
        }

        function setActiveNavItem(sectionId) {
          document
            .querySelectorAll(".sidebar .nav-item, .footer-link")
            .forEach((item) => item.classList.remove("active"));
          const targetItem = document.querySelector(
            `.nav-item[data-id="${sectionId}"], .footer-link[data-id="${sectionId}"]`
          );
          if (targetItem) {
            targetItem.classList.add("active");
            if (targetItem.closest(".nav-tree-container")) {
              let parent = targetItem.closest("ul.nested");
              while (parent) {
                const parentNavItem = parent.previousElementSibling;
                if (
                  parentNavItem &&
                  !parentNavItem.classList.contains("expanded")
                ) {
                  parentNavItem.classList.add("expanded");
                }
                if (
                  !parent.style.maxHeight ||
                  parent.style.maxHeight === "0px"
                ) {
                  parent.style.maxHeight = parent.scrollHeight + "px";
                  updateParentHeights(parent);
                }
                parent = parent.parentElement.closest("ul.nested");
              }
            }
          }
        }

        function filterNavTree(searchTerm) {
          const navContainer = document.getElementById("nav-tree-container");
          navContainer.scrollTop = 0;
          const term = searchTerm.toLowerCase().trim();
          if (!term) {
            document
              .querySelectorAll(".nav-tree-container li")
              .forEach((li) => (li.style.display = ""));
            document
              .querySelectorAll(".nav-category")
              .forEach((cat) => (cat.style.display = ""));
            document.querySelectorAll(".nav-tree ul.nested").forEach((ul) => {
              ul.style.maxHeight = null;
            });
            document
              .querySelectorAll(".nav-item.has-children.expanded")
              .forEach((item) => item.classList.remove("expanded"));
            const activeItem = document.querySelector(".nav-item.active");
            activeItem && setActiveNavItem(activeItem.dataset.id);
            return;
          }
          document.querySelectorAll(".nav-tree-container li").forEach((li) => {
            const link = li.querySelector("a.nav-item");
            if (!link) return;
            const itemId = link.dataset.id,
              itemText =
                link.querySelector("span")?.textContent.toLowerCase() || "";
            const data = findItemById(itemId);
            let tagsMatch = false;
            data &&
              data.item &&
              data.item.tags &&
              (tagsMatch = data.item.tags.some((tag) =>
                tag.toLowerCase().includes(term)
              ));
            const textMatch = itemText.includes(term);
            li.style.display = textMatch || tagsMatch ? "" : "none";
          });
          document.querySelectorAll(".nav-category").forEach((category) => {
            let hasVisibleChildren = false;
            category
              .querySelectorAll("li")
              .forEach(
                (li) =>
                  li.style.display !== "none" && (hasVisibleChildren = true)
              );
            category.style.display = hasVisibleChildren ? "" : "none";
          });
          document.querySelectorAll(".nav-tree-container li").forEach((li) => {
            if (li.style.display !== "none") {
              let parent = li.closest("ul.nested");
              while (parent) {
                const parentLi = parent.closest("li");
                parentLi && (parentLi.style.display = "");
                const parentItem = parent.previousElementSibling;
                if (parentItem) {
                  parentItem.classList.add("expanded");
                  parent.style.maxHeight = parent.scrollHeight + "px";
                  updateParentHeights(parent);
                }
                parent = parent.parentElement.closest("ul.nested");
              }
            }
          });
        }

        showContent("overview");
        setActiveNavItem("overview");
        updateToggleIcon();

        sidebarToggle.addEventListener("click", () => {
          const body = document.body;

          // This logic handles both mobile and desktop toggling
          if (window.innerWidth <= 768) {
            body.classList.toggle("sidebar-expanded-mobile");
          } else {
            body.classList.toggle("sidebar-collapsed");
            body.classList.add("sidebar-manual-toggle");
          }
          updateToggleIcon();
        });

        const overlay = document.getElementById("overlay");
        if (overlay) {
          overlay.addEventListener("click", () => {
            document.body.classList.remove("sidebar-expanded-mobile");
            updateToggleIcon(); // Ensure icon updates
          });
        }

        // ===== REPLACE YOUR ENTIRE EXISTING SIDEBAR CLICK HANDLER WITH THIS BLOCK =====

        // Helper function to update the max-height of all open parent menus
        function updateParentHeights(element) {
          let parent = element.closest("ul.nested");
          while (parent) {
            const parentLink = parent.previousElementSibling;
            // Only update if the parent is meant to be expanded
            if (parentLink && parentLink.classList.contains("expanded")) {
              // Set max-height to its new scrollHeight to accommodate the change
              parent.style.maxHeight = parent.scrollHeight + "px";
            }
            parent = parent.parentElement.closest("ul.nested");
          }
        }

        // Helper function to handle the collapse animation and update parents
        function collapseSection(element) {
          if (!element) return;

          // Set max-height to its current height to animate smoothly FROM this value
          element.style.maxHeight = element.scrollHeight + "px";

          // On the next browser frame, set max-height to 0 to trigger the animation
          requestAnimationFrame(() => {
            element.style.maxHeight = "0px";
            element.style.opacity = "0";
          });

          // Use a timeout to update parents AFTER the transition. This is crucial.
          // The duration should match your CSS transition duration (300ms).
          setTimeout(() => {
            updateParentHeights(element);
          }, 300);
        }

        // Helper function to handle the expand animation and update parents
        function expandSection(element) {
          if (!element) return;

          // Set the max-height to its content's scrollHeight to trigger the animation
          element.style.maxHeight = element.scrollHeight + "px";
          element.style.opacity = "1";

          // CRITICAL FIX: Use a minimal timeout to push the parent update to the next event loop.
          // This gives the browser a chance to reflow and calculate the new, larger scrollHeight.
          setTimeout(() => {
            updateParentHeights(element);
          }, 300);
        }

        document.querySelector(".sidebar").addEventListener("click", (e) => {
          const targetItem = e.target.closest(".nav-item");
          if (!targetItem) return;

          // Standard page loading & active state logic
          const sectionId = targetItem.dataset.id;
          if (
            targetItem.getAttribute("href") === "#" ||
            targetItem.classList.contains("has-children")
          ) {
            e.preventDefault();
          }
          let targetSection = document.getElementById(sectionId);
          if (!targetSection && sectionId !== "overview") {
            targetSection = generateAndAppendPage(sectionId);
          }
          if (targetSection) {
            showContent(sectionId);
          }
          setActiveNavItem(sectionId);

          // Animation Logic
          if (targetItem.classList.contains("has-children")) {
            const nestedList = targetItem.nextElementSibling;
            if (!nestedList) return;

            const isCurrentlyExpanded =
              targetItem.classList.contains("expanded");

            // Accordion logic: close other open items that aren't parents of this one
            if (!isCurrentlyExpanded) {
              document
                .querySelectorAll(".nav-tree .nav-item.expanded")
                .forEach((openItem) => {
                  if (
                    openItem !== targetItem &&
                    !openItem.parentElement.contains(targetItem)
                  ) {
                    const siblingList = openItem.nextElementSibling;
                    if (siblingList) {
                      openItem.classList.remove("expanded");
                      collapseSection(siblingList);
                    }
                  }
                });
            }

            // Toggle the clicked item
            targetItem.classList.toggle("expanded");
            if (targetItem.classList.contains("expanded")) {
              expandSection(nestedList);
            } else {
              collapseSection(nestedList);
            }
          }

          // Mobile sidebar collapse
          if (window.innerWidth <= 768) {
            document.body.classList.remove("sidebar-expanded-mobile");
            updateToggleIcon(); // Ensure icon updates
          }
        });

        document.getElementById("overview").addEventListener("click", (e) => {
          const topicLink = e.target.closest(".topic-link"),
            header = e.target.closest(".flow-node-header");
          if (topicLink) {
            e.preventDefault();
            const sectionId = topicLink.dataset.id,
              navLink = document.querySelector(
                `.nav-item[data-id="${sectionId}"]`
              );
            if (navLink) {
              navLink.click();
              setTimeout(
                () =>
                  navLink.scrollIntoView({
                    behavior: "smooth",
                    block: "center",
                  }),
                150
              );
            }
          } else if (header) {
            const targetId = header.dataset.target,
              contentNode = document.getElementById(targetId);
            if (contentNode) {
              const isExpanded =
                contentNode.style.maxHeight &&
                contentNode.style.maxHeight !== "0px";
              contentNode.style.maxHeight = isExpanded
                ? "0px"
                : contentNode.scrollHeight + "px";
            }
          }
        });

        document
          .querySelector(".main-footer-content")
          .addEventListener("click", (e) => {
            const targetLink = e.target.closest(".footer-link");
            if (!targetLink) return;
            e.preventDefault();
            const sectionId = targetLink.dataset.id;
            showContent(sectionId);
            setActiveNavItem(sectionId);
          });

        searchInput.addEventListener("input", (e) =>
          filterNavTree(e.target.value)
        );

        const animatedBg = document.getElementById("animated-bg"),
          mainContentWrapper = document.getElementById("content-wrapper");
        mainContentWrapper.addEventListener("scroll", () => {
          const scrollTop = mainContentWrapper.scrollTop;
          animatedBg &&
            animatedBg.style.setProperty("--scroll-y", `${scrollTop * 0.4}px`);
        });
        function createDots() {
          if (!animatedBg) return;
          const numDots = 30;
          for (let i = 0; i < numDots; i++) {
            const dot = document.createElement("div");
            dot.classList.add("dot");
            const size = Math.random() * 5 + 2;
            dot.style.width = `${size}px`;
            dot.style.height = `${size}px`;
            dot.style.left = `${Math.random() * 100}%`;
            const duration = Math.random() * 20 + 15;
            dot.style.animationDuration = `${duration}s`;
            const delay = Math.random() * 15;
            dot.style.animationDelay = `${delay}s`;
            dot.style.setProperty("--x-end", `${Math.random() * 200 - 100}px`);
            dot.style.opacity = "0";
            animatedBg.appendChild(dot);
          }
        }
        createDots();
        const observer = new IntersectionObserver(
          (entries) => {
            entries.forEach((entry) => {
              if (entry.isIntersecting) {
                entry.target.classList.add("is-visible");
                observer.unobserve(entry.target);
              }
            });
          },
          { threshold: 0.1 }
        );
        document
          .querySelectorAll(".animate-on-scroll")
          .forEach((el) => observer.observe(el));
      });
    </script>
  </body>
</html>
